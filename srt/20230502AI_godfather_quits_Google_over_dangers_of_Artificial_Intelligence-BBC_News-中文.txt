1
00:00:00,031 --> 00:00:06,822
发言人 SPEAKER_00：被广泛认为是人工智能教父的人已经辞去了他在谷歌的工作，并警告人工智能的危险。

2
00:00:07,123 --> 00:00:15,476
发言人 SPEAKER_00：杰弗里·辛顿博士在深度学习和神经网络方面的开创性研究为当前的AI系统（如ChatGPT）铺平了道路。

3
00:00:16,097 --> 00:00:25,632
发言人 SPEAKER_00：但在接受《纽约时报》的长篇采访时，辛顿博士表示，他现在对自己的工作感到后悔，并担心AI技术会在互联网上充斥错误信息。

4
00:00:26,490 --> 00:00:31,556
发言人 SPEAKER_00：谷歌在一份声明中回应称，我们仍然致力于以负责任的方式开发AI。

5
00:00:32,037 --> 00:00:35,722
发言人 SPEAKER_00：辛顿博士向BBC解释了这些系统为何能够掌握如此多的知识。

6
00:00:36,743 --> 00:00:41,649
发言人 SPEAKER_02：我们正在开发的智能与我们所拥有的智能非常不同。

7
00:00:42,331 --> 00:00:45,715
发言人 SPEAKER_02：我们是生物系统，而它们是数字系统。

8
00:00:46,835 --> 00:00:54,085
发言人 SPEAKER_02：最大的区别在于，数字系统拥有许多相同的权重副本，即相同的世界模型。

9
00:00:54,892 --> 00:00:59,439
发言人 SPEAKER_02：所有这些副本可以分别学习，但能够即时共享知识。

10
00:01:00,100 --> 00:01:05,751
发言人 SPEAKER_02：就好像你有10,000个人，每当一个人学到一些东西，其他人都会自动知道。

11
00:01:06,852 --> 00:01:11,680
发言人 SPEAKER_02：这就是为什么这些系统能够比任何一个人知道得更多。

12
00:01:11,700 --> 00:01:15,668
发言人 SPEAKER_00：辛顿博士还告诉BBC，AI的进步速度令人担忧。

13
00:01:16,390 --> 00:01:18,873
发言人 SPEAKER_02：目前，我们看到的是

14
00:01:19,495 --> 00:01:27,867
发言人 SPEAKER_02：像GPT-4这样的系统在常识方面已经超越了人类，并且远远领先。

15
00:01:27,908 --> 00:01:32,555
发言人 SPEAKER_02：在推理方面，它还不够好，但它已经能够进行简单的推理。

16
00:01:33,596 --> 00:01:38,504
发言人 SPEAKER_02：考虑到进步的速度，我们预计情况会很快变得更好。

17
00:01:39,406 --> 00:01:40,768
发言人 SPEAKER_02：所以我们需要为此担心。

18
00:01:41,328 --> 00:01:44,052
发言人 SPEAKER_02：目前，它们还没有比我们更聪明，据我所知。

19
00:01:45,375 --> 00:01:46,617
发言人 SPEAKER_02：但我认为它们可能很快就会超越我们。

20
00:01:47,709 --> 00:01:50,915
发言人 SPEAKER_00：早些时候，我与《数学智能》的作者朱奈德·穆宾进行了交谈。

21
00:01:50,974 --> 00:01:53,058
发言人 SPEAKER_00：我问他如何看待AI警告。

22
00:01:53,759 --> 00:01:57,325
发言人 SPEAKER_01：我认为这次警告需要认真对待，因为它来自权威人士。

23
00:01:57,926 --> 00:02:02,453
发言人 SPEAKER_01：我不会特别认真地对待埃隆·马斯克的警告，因为他不是AI专家。

24
00:02:02,513 --> 00:02:10,866
发言人 SPEAKER_01：而杰弗里·辛顿是人工智能的先驱之一，他还非常具体地列出了风险的层次和关注点。

25
00:02:10,846 --> 00:02:18,739
发言人 SPEAKER_01：他提到了AI对劳动力和错误信息传播的威胁。

26
00:02:19,719 --> 00:02:26,270
发言人 SPEAKER_01：但他提出的关于存在性风险的担忧，我认为需要认真对待。

27
00:02:26,290 --> 00:02:28,854
发言人 SPEAKER_01：我认为重要的是要认识到

28
00:02:28,835 --> 00:02:33,419
发言人 SPEAKER_01：这些风险并不是基于AI变得有意识或获得感知能力。

29
00:02:33,539 --> 00:02:46,894
发言人 SPEAKER_01：我们不是在谈论《终结者》那样的末日场景，而是这些技术能够如此有效地收集和处理信息，如果它们落入不良行为者手中，可能会造成各种破坏。

30
00:02:46,913 --> 00:02:48,876
发言人 SPEAKER_01：我认为这是我们所有人都需要注意的事情。

31
00:02:49,096 --> 00:02:54,942
发言人 SPEAKER_00：请再详细说明一下，因为他提到了不良行为者会利用AI做坏事。

32
00:02:54,961 --> 00:02:57,525
发言人 SPEAKER_00：请告诉我们你认为他具体指的是什么。

33
00:02:58,163 --> 00:03:05,110
发言人 SPEAKER_01：我认为这些聊天机器人的一个挑战是我们并不完全理解它们的工作原理。

34
00:03:05,151 --> 00:03:13,580
发言人 SPEAKER_01：我们知道它们吸收了大量的信息，然后你可以输入一个提示，它会给你一个输出。

35
00:03:13,639 --> 00:03:14,800
发言人 SPEAKER_01：它可能会为你写一篇文章。

36
00:03:14,841 --> 00:03:16,483
发言人 SPEAKER_01：它可能会生成一张图片或一段视频。

37
00:03:17,002 --> 00:03:22,128
发言人 SPEAKER_01：挑战在于，随着这些系统变得越来越复杂，

38
00:03:22,109 --> 00:03:24,372
发言人 SPEAKER_01：我们可以给它们分配某些任务。

39
00:03:24,573 --> 00:03:33,568
发言人 SPEAKER_01：比如，我可能会让一个聊天机器人上网为我找到最便宜的火车票，并给它访问我的银行账户的权限。

40
00:03:34,008 --> 00:03:40,218
发言人 SPEAKER_01：它能够完成这个任务，而这里的风险似乎相对较低。

41
00:03:40,199 --> 00:03:48,430
发言人 SPEAKER_01：但你可以想象，在不同的背景下，风险可能会更高，这些系统可能会被武器化，执行一系列任务。

42
00:03:48,449 --> 00:03:51,514
发言人 SPEAKER_01：它们会以我们不完全理解的方式执行这些任务。

43
00:03:51,653 --> 00:03:56,881
发言人 SPEAKER_01：我认为像杰弗里·辛顿这样的人担心的是，可能会出现意想不到的后果。

44
00:03:57,401 --> 00:04:00,366
发言人 SPEAKER_01：这些系统的行为方式与人类非常不同。

45
00:04:00,907 --> 00:04:04,991
发言人 SPEAKER_01：所以它们如何实现目标并没有真正的解释。

46
00:04:05,393 --> 00:04:09,919
发言人 SPEAKER_01：特别是如果这些目标源于某种恶意意图。

47
00:04:09,899 --> 00:04:29,632
发言人 SPEAKER_01：例如，如果一个独裁者对聊天机器人系统说，帮助我重新控制人口，我们无法知道一个非常复杂的计算机系统会如何实现这一目标，它会使用什么样的操纵手段。

48
00:04:30,213 --> 00:04:34,742
发言人 SPEAKER_01：因此，缺乏透明度是我们真正需要关注的问题。

49
00:04:35,346 --> 00:04:50,110
发言人 SPEAKER_00：几个月前，超过1000名科技领袖呼吁暂停开发新的AI系统六个月，因为他们认为这项技术对社会和人类构成了深远的风险。

50
00:04:50,250 --> 00:04:52,173
发言人 SPEAKER_00：这是否为时已晚？

51
00:04:53,149 --> 00:04:54,071
发言人 SPEAKER_01：可能已经晚了。

52
00:04:54,091 --> 00:05:02,504
发言人 SPEAKER_01：我认为我们必须保持一定的乐观精神，并认识到我们仍然拥有巨大的人类智慧。

53
00:05:02,624 --> 00:05:04,206
发言人 SPEAKER_01：现在比以往任何时候都更需要发挥这种智慧。

54
00:05:04,286 --> 00:05:11,276
发言人 SPEAKER_01：杰弗里·辛顿强调的一点是，这些陷阱条所倡导的数字智能与人类智能非常不同。

55
00:05:11,257 --> 00:05:13,619
发言人 SPEAKER_01：这可能会为我们提供一个机会，利用我们自身的人类优势，理解我们如何以可能允许我们监管和控制这些技术的方式合作。

56
00:05:13,658 --> 00:05:27,052
发言人 SPEAKER_01：但我要说，杰弗里·辛顿已经研究这些技术几十年了。

57
00:05:27,413 --> 00:05:31,357
发言人 SPEAKER_01：在这个领域内，许多人已经发出了这些警告多年。

58
00:05:31,377 --> 00:05:36,141
发言人 SPEAKER_01：所以他的警告似乎来得有点晚。

59
00:05:36,461 --> 00:05:40,925
发言人 SPEAKER_01：现在有一种感觉，可能精灵已经出了瓶子，我们无法真正预测接下来会发生什么。

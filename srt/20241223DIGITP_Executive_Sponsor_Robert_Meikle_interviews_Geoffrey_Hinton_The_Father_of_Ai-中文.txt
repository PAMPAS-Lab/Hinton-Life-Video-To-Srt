1 00:00:01,837 --> 00:00:09,473 主持人 SPEAKER_02：接下来的这个环节，我们将有机会更深入地探讨人工智能领域。
2 00:00:09,493 --> 00:00:13,422 主持人 SPEAKER_02：我想做的一件事是，我对这个特定的环节进行了一些实时调整。
3 00:00:14,083 --> 00:00:22,460 主持人 SPEAKER_02：我们想要做的一件事是确保在这个与尊敬的嘉宾的对话中，有我们客户的意见，您可以看到他们，我稍后会介绍他们。
4 00:00:22,440 --> 00:00:32,281 主持人 SPEAKER_02：我想做的是，我们实际上在我们的信息技术部门中，有一位可能已经就人工智能话题与您交谈过的主题专家。
5 00:00:32,548 --> 00:00:38,154 他是你可以联系的人，一起制定策略，进行研讨会，获取研究等。
6 00:00:38,613 --> 00:00:43,158 所以我请到了我的右边加入我的 Bill Wong，今天我们对话的一部分。
7 00:00:43,478 --> 00:00:48,302 他会督促我确保我们今天的对话中保持客户的声音。
8 00:00:48,683 --> 00:01:02,555 他已经有机会与多个行业的人工智能主题的许多人交谈，我们的目标和抱负是确保我们将这些信息转化为可以推动有价值成果的有意义的行动。
9 00:01:02,536 --> 00:01:08,709 主持人 SPEAKER_02：因此，现在我们有幸和我们的贵宾见面。
10 00:01:10,072 --> 00:01:17,709 许多人都可能听说过他，杰弗里·辛顿，他也是人工智能的教父之一。
11 00:01:17,689 --> 00:01:22,176 他带来了丰富的知识、背景和经验。
12 00:01:22,195 --> 00:01:25,540 我个人认为您是跨代变革的一部分。
13 00:01:25,560 --> 00:01:27,822 说话人 SPEAKER_02：我们正处于一件真正伟大事情的发展曲线之上。
14 00:01:28,084 --> 00:01:30,746 说话人 SPEAKER_02：因此，我们期待今天的对话。
15 00:01:30,807 --> 00:01:37,977 说话人 SPEAKER_02：你们中许多人可能知道或不知道，他最近曾是谷歌的一员，已经离开谷歌，现在可以自由地发表意见。
16 00:01:37,956 --> 00:01:40,260 说话人 SPEAKER_02：关于这个话题，可以公开自由地讨论。
17 00:01:40,680 --> 00:01:46,027 说话人 SPEAKER_02：他提到了一些关于人工智能的风险，我们稍后会进一步讨论，但他也谈到了一些机遇。
18 00:01:46,046 --> 00:01:56,780 说话人 SPEAKER_02：所以，我们有幸邀请 Jeffrey，我知道 Bill 已经有机会与许多客户就人工智能进行过交流，您的名字经常被提及。
19 00:01:56,799 --> 00:02:04,469 说话人 SPEAKER_02：所以 Bill，我想请您在我们开始对话之前，多谈谈您在与客户交流中了解到的关于 Jeffrey 的一些情况，您观察到的一些东西。
20 00:02:04,450 --> 00:02:10,465 说话人 SPEAKER_03：嗯，当我们与客户交谈时，我们经常回想起 Jeffrey 的工作。
21 00:02:10,485 --> 00:02:20,991 众多人，不仅仅是 Infotech，还有像 NVIDIA 这样的团体，都将那个时刻指向了 Jeffrey 和他的团队首次在 GPU 平台上使用大量训练数据构建深度神经网络的那一刻。
22 00:02:20,971 --> 00:02:28,299 他们构建了一个具有反向传播的深度神经网络，并在 GPU 平台上进行了大量训练。
23 00:02:28,900 --> 00:02:33,163 那个时刻被许多人认为是现代人工智能的宇宙大爆炸。
24 00:02:33,504 --> 00:02:35,387 那是深度学习真正起飞的时刻。
25 00:02:35,826 --> 00:02:41,032 说话人 SPEAKER_03：我们曾经有过窄 AI，它使加拿大在 AI 领域声名鹊起。
26 00:02:41,133 --> 00:02:43,074 说话人 SPEAKER_03：能在这里见到他们真是太激动了。
27 00:02:43,615 --> 00:02:44,096 说话人 SPEAKER_02：好的。
28 00:02:44,596 --> 00:02:48,800 说话人 SPEAKER_02：那么，让我们鼓掌欢迎杰弗里·辛顿。
29 00:02:55,210 --> 00:02:59,474 说话人 SPEAKER_02：杰弗里，今天能和您这样的大脑在一起，我们感到非常荣幸，并且想充分利用这一点。
30 00:02:59,495 --> 00:03:03,219 说话人 SPEAKER_02：但您今天能加入我们参加 InfoTech Live 2023，也是我们的荣幸。
31 00:03:03,240 --> 00:03:05,643 说话人 SPEAKER_02：因此，我们期待着我们的对话。
32 00:03:06,123 --> 00:03:06,884 说话人 SPEAKER_02：让我们开始吧。
33 00:03:06,905 --> 00:03:07,925 说话人 SPEAKER_02：我们要快速进行。
34 00:03:07,966 --> 00:03:09,147 说话人 SPEAKER_02：我们要快速完成很多事情。
35 00:03:09,187 --> 00:03:09,829 说话人 SPEAKER_02：让我们开始吧。
36 00:03:09,868 --> 00:03:11,651 说话人 SPEAKER_02：让我们从您的个人经历开始。
37 00:03:12,192 --> 00:03:18,079 说话人 SPEAKER_02：是什么吸引了你专注于如何让大脑工作作为通往智能的途径？
38 00:03:18,497 --> 00:03:22,383 说话人 SPEAKER_01：嗯，在我看来，心理学家还没有真正弄清楚智能是如何工作的。
39 00:03:23,284 --> 00:03:28,032 说话人 SPEAKER_01：而且你必须理解硬件才能理解智能是如何工作的。
40 00:03:28,513 --> 00:03:32,158 说话人 SPEAKER_01：因此你必须理解大脑是如何工作的，特别是它如何学习事物。
41 00:03:33,120 --> 00:03:33,520 说话人 SPEAKER_01: 好吧。
42 00:03:33,920 --> 00:03:34,282 说话人 SPEAKER_02: 优秀。
43 00:03:34,681 --> 00:03:43,034 说话人 SPEAKER_02: 那项特定研究让您作为一位英国-加拿大人来到这里，也把您带到了多伦多，我曾经是多伦多市的 CIO。
44 00:03:43,316 --> 00:03:45,618 说话人 SPEAKER_02: 这是如何让您来到多伦多大学的？
45 00:03:46,087 --> 00:03:50,634 说话人 SPEAKER_01：嗯，我当时在卡内基梅隆大学工作，这所大学的大部分资金都来自国防部。
46 00:03:51,276 --> 00:03:56,323 说话人 SPEAKER_01：当时里根正在让人们挖掘尼加拉瓜的港口。
47 00:03:56,924 --> 00:03:57,724 说话人 SPEAKER_01：我不喜欢这样。
48 00:03:58,086 --> 00:04:00,930 说话人 SPEAKER_01：我发现卡内基梅隆大学的大多数同事都觉得这没问题。
49 00:04:01,550 --> 00:04:03,753 说话人 SPEAKER_01：我觉得我更适合在其他地方。
50 00:04:04,435 --> 00:04:06,859 说话人 SPEAKER_01：加拿大给了我一份很好的工作邀请。
51 00:04:07,258 --> 00:04:13,968 说话人 SPEAKER_01：他们有一个很好的社会福利体系，一个良好的多元文化社会体系，为每个人提供医疗保障。
52 00:04:15,450 --> 00:04:16,072 说话人 SPEAKER_01：我搬去了那里。
53 00:04:16,711 --> 00:04:18,093 说话人 SPEAKER_02：是的，太棒了。
54 00:04:18,934 --> 00:04:31,526 说话人 SPEAKER_02：关于数字智能如何以模拟生物智能的方式表现，您最近改变了您的看法。
55 00:04:32,146 --> 00:04:38,391 说话人 SPEAKER_02：但今天您认为数字智能可以超越生物智能。
56 00:04:38,411 --> 00:04:44,918 说话人 SPEAKER_02：这是一个非常有趣的概念，我想请您详细阐述一下，是什么改变了您的观点。
57 00:04:44,898 --> 00:04:48,283 说话人 SPEAKER_01：好吧，有一件事改变了我的看法，那就是大型聊天机器人。
58 00:04:49,665 --> 00:04:52,088 说话人 SPEAKER_01：但更重要的是我正在进行的研究。
59 00:04:52,709 --> 00:04:55,872 说话人 SPEAKER_01：我试图让大型语言模型使用更少的能源。
60 00:04:56,593 --> 00:05:01,641 说话人 SPEAKER_01：为了做到这一点，你希望它们在低能耗的模拟计算中进行。
61 00:05:02,161 --> 00:05:05,286 说话人 SPEAKER_01：模拟计算机的问题在于每个人都不太一样。
62 00:05:05,670 --> 00:05:07,853 说话人 SPEAKER_01：而且它们不能轻易地分享知识。
63 00:05:08,514 --> 00:05:16,547 说话人 SPEAKER_01：所以你的大脑和我的大脑可以通过你产生句子，我说，我应该如何改变我的大脑，才能说出那样的话？
64 00:05:17,968 --> 00:05:23,156 说话人 SPEAKER_01：但你不能仅仅给我你大脑中的权重，因为它们在细节上都不一样。
65 00:05:23,797 --> 00:05:34,733 说话人 SPEAKER_01：这些数字智能可以拥有许多完全相同的神经网络模型副本，每个副本都可以通过查看不同的数据来学习不同的东西。
66 00:05:35,117 --> 00:05:42,545 说话人 SPEAKER_01：因此，你可以有 10,000 个副本，它们从数据的不同部分学习不同的东西，然后它们可以通过平均它们的权重来共享它们所学的知识。
67 00:05:43,165 --> 00:05:44,125 说话人 SPEAKER_01：这是我们做不到的。
68 00:05:44,607 --> 00:05:48,911 说话人 SPEAKER_01：这就是为什么大型聊天机器人比任何人都知道得多上千倍。
因为他们看到了比我们多出数千倍的数据，因为不同的副本可以以我们无法实现的方式共享它们所学的知识。
我们非常不擅长分享知识，而他们在这方面非常擅长。
71 00:05:59,920 --> 00:06:02,002 说话者 SPEAKER_01：这使得它们成为更高级的智能形式。
72 00:06:02,201 --> 00:06:03,365 说话者 SPEAKER_02：好的，谢谢。
73 00:06:03,665 --> 00:06:05,108 说话人 SPEAKER_02：那么我们再深入一点。
74 00:06:05,189 --> 00:06:09,218 说话人 SPEAKER_02：我听说您把数字智能称为不朽。
75 00:06:09,959 --> 00:06:10,180 说话人 SPEAKER_02：是的。
76 00:06:10,480 --> 00:06:13,807 说话人 SPEAKER_02：而生物智能是有限的。
77 00:06:14,088 --> 00:06:15,411 说话人 SPEAKER_02：你到底指的是什么？
78 00:06:16,052 --> 00:06:21,644 说话人 SPEAKER_01：这对雷·库兹韦尔来说是个坏消息，但我们的大脑都有所不同。
79 00:06:21,742 --> 00:06:26,170 说话人 SPEAKER_01：知识和硬件之间没有分离。
80 00:06:26,690 --> 00:06:32,480 说话人 SPEAKER_01：你大脑中的权重只有在你的大脑中，与你的神经元做完全相同的事情的神经元一起工作时才有效。
81 00:06:33,060 --> 00:06:34,502 说话人 SPEAKER_01：这些权重对我的大脑来说就没有用了。
82 00:06:35,184 --> 00:06:38,149 说话人 SPEAKER_01：所以当你的大脑死去，所有这些权重就都毫无用处了。
83 00:06:38,228 --> 00:06:39,209 说话人 SPEAKER_01：所有的知识都消失了。
84 00:06:39,670 --> 00:06:47,362 说话人 SPEAKER_01：而你要保留这些知识的唯一方法，你不能只是将知识存储在磁盘上的权重中，因为你必须有一个完全相同的脑部才能使它们有任何用处。
85 00:06:47,343 --> 00:06:59,045 说话人 SPEAKER_01：你能做的只有产生句子或图表或某种动作，让其他人或年轻人模仿，这样他们才能吸收你的一些知识。
86 00:06:59,346 --> 00:07:02,853 说话人 SPEAKER_01：但这是将你大脑中的知识传递给他们的一个非常低效的方式。
87 00:07:03,930 --> 00:07:04,411 说话人 SPEAKER_02：好的。
88 00:07:04,952 --> 00:07:10,704 说话人 SPEAKER_02：那么数字智能会超越，比我的生物大脑更长寿吗？
89 00:07:10,805 --> 00:07:10,985 说话人 SPEAKER_01: 是的。
90 00:07:11,266 --> 00:07:17,478 说话人 SPEAKER_01: 一旦你有了数字计算机，我的意思是，数字计算机最初就是为了做你说的那样的事情而发明的，你可以对它们进行编程。
91 00:07:18,399 --> 00:07:25,533 说话人 SPEAKER_01: 然后我们达到了这样的一个点，你可以通过观察数据和学习来让它们做事情。
92 00:07:25,800 --> 00:07:28,663 说话人 SPEAKER_01: 但关键是，它们被设计成做你说的那样的事情。
93 00:07:28,944 --> 00:07:31,987 说话人 SPEAKER_01：因此，两台不同的数字计算机可以完成完全相同的事情。
94 00:07:32,646 --> 00:07:34,048 说话人 SPEAKER_01：它们可以使用相同的权重。
95 00:07:34,127 --> 00:07:37,951 说话人 SPEAKER_01：一台计算机中的权重可以在另一台计算机中工作，这与我们的大脑截然不同。
96 00:07:39,213 --> 00:07:41,334 说话人 SPEAKER_01：这使得它们成为非常不同的一种智能。
97 00:07:41,595 --> 00:07:42,615 说话人 SPEAKER_01：这意味着它们是永生的。
98 00:07:42,675 --> 00:07:44,677 说话人 SPEAKER_01：也就是说，只需将权重保存在某个地方。
99 00:07:45,238 --> 00:07:48,802 说话人 SPEAKER_01：然后在未来的某个时候，你只需再构建一台工作方式相同的数字计算机。
100 00:07:49,221 --> 00:07:50,483 说话人 SPEAKER_01：现在所有这些知识都在那里了。
101 00:07:50,843 --> 00:07:51,423 说话人 SPEAKER_02: 是的，是的。
102 00:07:51,483 --> 00:07:53,706 说话人 SPEAKER_01: 所以通过数字计算，你可以保存知识。
103 00:07:53,745 --> 00:07:55,807 说话人 SPEAKER_01: 但是通过模拟计算，你不能。
104 00:07:55,788 --> 00:08:01,716 说话人 SPEAKER_01: 因此雷伊所知道的一切在他去世后都会消失，而他将会死去。
105 00:08:02,396 --> 00:08:03,517 说话人 SPEAKER_02: 对。
106 00:08:03,538 --> 00:08:05,100 说话人 SPEAKER_02: 我想这是我们都要面对的现实。
107 00:08:05,940 --> 00:08:08,483 说话人 SPEAKER_02: 所以它肯定会比我们存在的时间更长。
108 00:08:08,564 --> 00:08:10,406 说话人 SPEAKER_02: 但问题是，它是否会超越我们？
109 00:08:10,487 --> 00:08:17,596 说话人 SPEAKER_02：你认为我们是否能够达到数字智能超越生物智能的程度？
110 00:08:17,615 --> 00:08:21,841 说话人 SPEAKER_02：这个房间里坐满了很多人，其中一些部件拥有很高的智能。
111 00:08:21,860 --> 00:08:25,625 说话人 SPEAKER_02：你认为数字智能最终是否会超越这样一个房间？
112 00:08:25,959 --> 00:08:26,220 说话人 SPEAKER_01：是的。
113 00:08:27,401 --> 00:08:27,762 说话人 SPEAKER_01: 好吧。
114 00:08:28,244 --> 00:08:28,564 说话人 SPEAKER_01: 好的。
115 00:08:29,485 --> 00:08:37,138 所以现在，这些大型聊天机器人，以 GPT-4 为例，知道的比任何一个人都多。
116 00:08:37,578 --> 00:08:41,644 说话人 SPEAKER_01: 如果你让任何领域的专家来问它一个问题，它都能给出合理的答案。
117 00:08:42,125 --> 00:08:51,039 说话人 SPEAKER_01：它知道的信息比一个人多出数千倍，因为不同机器上运行的不同模型副本去查看不同的数据片段，然后他们共享他们的知识。
118 00:08:51,019 --> 00:08:53,442 说话人 SPEAKER_01：想想看，如果你能有 1 万人会多么棒。
119 00:08:53,903 --> 00:08:55,404 说话人 SPEAKER_01：每个人都可以研究不同的主题。
120 00:08:55,945 --> 00:08:58,008 说话人 SPEAKER_01：然后到最后，你们都知道了任何人的知识。
121 00:08:58,028 --> 00:08:59,208 说话人 SPEAKER_01：就是这样。
122 00:09:00,791 --> 00:09:06,417 说话人 SPEAKER_01：所以，他们已经比我们更好地吸收知识了。
123 00:09:06,797 --> 00:09:08,980 说话人 SPEAKER_01：而且他们并不是通过保留句子的副本来做到这一点。
124 00:09:09,039 --> 00:09:17,448 说话人 SPEAKER_01：他们将那些句子转化为他们发现的特征，特征之间的交互，从而可以生成新的句子。
他们知道的比我们多得多，而且他们在一般智能和推理方面已经接近我们了。
126 00：09：24,245 --> 00：09：25,626 议长 SPEAKER_01：所以他们还不如人。
127 00：09：26,248 --> 00：09：28,029 演讲者 SPEAKER_01：有些事情他们做对了，有些事情他们做错了。
128 00：09：28,350 --> 00：09：34,196 演讲者 SPEAKER_01：最近有一个例子，他们总是犯错的事情，所有的模型，这表明他们的推理能力还不如我们。
129 00:09:34,756 --> 00:09:37,080 说话人 SPEAKER_01：我来试试，希望你能答对。
130 00:09:38,421 --> 00:09:41,825 说话人 SPEAKER_01：现在轮到你了。
131 00:09:43,546 --> 00:09:46,068 说话人 SPEAKER_01：莎莉有三个兄弟。
132 00:09:46,623 --> 00:09:48,765 说话人 SPEAKER_01：我们每个兄弟都有两个姐妹。
133 00:09:49,667 --> 00:09:55,936 说话人 SPEAKER_01: Sally 有几个姐妹？
134 00:09:55,956 --> 00:09:57,839 说话人 SPEAKER_02: 三个兄弟，两个姐妹，六个？
135 00:09:58,620 --> 00:10:01,605 说话人 SPEAKER_01: 好的，所以这是聊天机器人说的，对吧？
136 00:10:01,784 --> 00:10:03,486 说话人 SPEAKER_01: 它们把三个兄弟乘以两个姐妹。
137 00:10:04,788 --> 00:10:07,493 说话人 SPEAKER_01: 随时可以给我发笔记，有人给我发短信吗，是的？
138 00:10:08,894 --> 00:10:13,701 说话人 SPEAKER_01: 问题在于，那三个兄弟的姐妹是同一个姐妹。
139 00:10:14,604 --> 00:10:16,326 说话人 SPEAKER_01: 更重要的是，莎莉就是其中之一。
140 00:10:16,863 --> 00:10:18,145 说话人 SPEAKER_01: 所以她有一个姐妹。
141 00:10:20,529 --> 00:10:24,337 说话人 SPEAKER_01：现在，人们并不总是能做对这件事，但聊天机器人总是做错。
142 00:10:25,600 --> 00:10:27,443 说话人 SPEAKER_01：不过，总有一天聊天机器人会做对的。
143 00:10:28,346 --> 00:10:34,618 说话人 SPEAKER_01：让我再给你举一个例子，这是聊天机器人做对的事情，也是一个推理问题。
144 00:10:36,488 --> 00:10:45,361 说话人 SPEAKER_01：这是由一位老式的 AI 专家给我的，一个非常诚实的人，他相信所有的事情都是用符号完成的，他对这些东西是如何可能工作的感到非常困惑。
145 00:10:45,381 --> 00:10:46,524 说话人 SPEAKER_01：他真的感到困惑。
146 00:10:46,845 --> 00:10:48,668 说话人 SPEAKER_01：它怎么可能做到它正在做的事情？
147 00:10:48,687 --> 00:10:50,090 说话人 SPEAKER_01：所以他提出了以下这个谜题。
148 00:10:50,711 --> 00:10:54,277 说话人 SPEAKER_01：我实际上把这个谜题设置得相当难，因为我知道它会解决他给我的那个谜题。
149 00:10:54,657 --> 00:10:55,418 说话人 SPEAKER_01: 这个谜题是这样的。
150 00:10:55,938 --> 00:11:00,706 说话人 SPEAKER_01: 我家里的房间要么涂成白色，要么涂成黄色，要么涂成蓝色。
151 00:11:00,687 --> 00:11:03,172 说话人 SPEAKER_01: 黄色油漆一年后就会褪成白色。
152 00:11:03,993 --> 00:11:06,958 说话人 SPEAKER_01: 两年后，我希望所有的房间都是白色的。
153 00:11:07,600 --> 00:11:08,240 说话人 SPEAKER_01：我应该做什么？
154 00:11:10,284 --> 00:11:12,589 说话人 SPEAKER_01：我该做什么，为什么？
155 00:11:13,309 --> 00:11:16,495 说话人 SPEAKER_01：它说，你不必担心黄色房间，因为它们会变成白色。
156 00:11:16,556 --> 00:11:18,139 说话人 SPEAKER_01：你应该把蓝色房间刷成白色。
157 00:11:19,097 --> 00:11:21,500 说话人 SPEAKER_01：曾经它说你应该把气球涂成黄色。
158 00:11:21,782 --> 00:11:22,682 说话人 SPEAKER_01：这也行。
159 00:11:23,082 --> 00:11:24,105 说话人 SPEAKER_01：不那么直接，但也能行。
160 00:11:25,206 --> 00:11:26,408 说话人 SPEAKER_01：这样它就能搞懂。
161 00:11:27,149 --> 00:11:30,974 说话人 SPEAKER_01：这就是比我们几年前想象的要多的推理。
162 00:11:31,575 --> 00:11:33,418 说话人 SPEAKER_01：而且它们一直在快速变得越来越好。
163 00:11:34,558 --> 00:11:37,462 说话人 SPEAKER_01：所以它现在还不能做某些事情，但能做其他事情。
164 00:11:38,144 --> 00:11:40,287 说话人 SPEAKER_01：它的幽默感比福克斯新闻还要好。
165 00:11:41,168 --> 00:11:48,918 说话人 SPEAKER_01：我收到了很多福克斯新闻的邀请，让我上他们的节目。
166 00:11:49,490 --> 00:11:55,283 说话人 SPEAKER_01：一开始我只是回答说，你知道，这是那个播报已知为虚假信息的福克斯新闻吗？
167 00:11:57,347 --> 00:12:02,018 说话人 SPEAKER_01：当我这么做的时候，我收到了来自另一个节目的请求，他们表示他们播报的是真实信息。
168 00:12:02,058 --> 00:12:07,110 说话人 SPEAKER_01：所以我开始回应说，福克斯新闻是一个矛盾体。
169 00:12:08,474 --> 00:12:12,241 说话人 SPEAKER_01：我让 GPT-4 解释一下，它解释了为什么它很有趣。
170 00:12:12,982 --> 00:12:18,995 说话人 SPEAKER_01：它说，你知道的，它暗示福克斯新闻实际上不是新闻，尽管有些人有不同的看法，因为它总是这样表达。
171 00:12:20,739 --> 00:12:26,331 说话人 SPEAKER_01：但后来我开始回复，福克斯新闻是“矛盾”的，矛盾和愚蠢之间有一个空格。
172 00:12:27,626 --> 00:12:32,533 说话人 SPEAKER_01：我不确定福克斯新闻的任何人是否理解了这个，但 GPT-4 理解了。
173 00:12:33,556 --> 00:12:38,644 说话人 SPEAKER_01：我把这放进去，然后解释了一下，它说这暗示了福克斯新闻不是新闻。
174 00:12:38,663 --> 00:12:40,206 说话人 SPEAKER_01：我说，但是关于太空呢？
175 00:12:41,167 --> 00:12:44,572 说话人 SPEAKER_01：他们说，哦，那完全是另一层幽默。
176 00:12:45,333 --> 00:12:51,663 说话人 SPEAKER_01：Oxi 是 Oxycontin 的缩写，所以这暗示了福克斯新闻是一种药物，笨蛋。
177 00:12:51,802 --> 00:12:56,250 说话人 SPEAKER_01: GPT-4 做到了，但我怀疑福克斯新闻的人没有。
178 00:12:56,533 --> 00:12:57,174 说话人 SPEAKER_02: 对。
179 00:12:58,897 --> 00:12:59,698 说话人 SPEAKER_02: 哦，让我们把手放在一起。
180 00:12:59,759 --> 00:13:00,519 说话人 SPEAKER_02: 我很喜欢这个。
181 00:13:00,559 --> 00:13:00,919 说话人 SPEAKER_02: 谢谢。
182 00:13:04,865 --> 00:13:06,067 说话人 SPEAKER_02: 比尔，我也让你进来。
183 00:13:06,148 --> 00:13:08,009 说话人 SPEAKER_02: 我们想保留客户的意见。
184 00:13:08,029 --> 00:13:09,032 说话人 SPEAKER_02: 你已经和很多人说过话了。
185 00:13:09,072 --> 00:13:20,347 说话人 SPEAKER_02：在我们讨论数字智能及其在领导角色中的应用时，您在与我们的客户、顾客的交流中有什么后续问题吗？
186 00:13:20,817 --> 00:13:25,244 说话人 SPEAKER_03：嗯，杰弗里，很多人来找我们询问关于人工智能的风险。
187 00:13:25,784 --> 00:13:33,236 说话人 SPEAKER_03：人们今天可以做什么，您还预见哪些其他风险，我们应该关注吗？
188 00:13:33,898 --> 00:13:35,860 说话人 SPEAKER_01：好的，我会列出一些主要风险。
189 00:13:35,880 --> 00:13:37,082 说话人 SPEAKER_01：显然有很多风险。
190 00:13:38,125 --> 00:13:45,456 说话人 SPEAKER_01：但是它可能会夺走很多工作，尤其是那些没有想过会失去工作的人，比如律师助理，例如。
191 00:13:45,777 --> 00:13:47,820 说话人 SPEAKER_01：他们的工作看起来现在非常不稳定。
192 00:13:48,019 --> 00:13:51,587 说话人 SPEAKER_01：而在过去，它也创造了新的工作。
193 00:13:51,849 --> 00:13:56,058 说话人 SPEAKER_01：正如库兹韦尔所说，技术会淘汰一些工作，也会创造新的工作。
194 00:13:56,078 --> 00:14:03,255 说话人 SPEAKER_01：我不太清楚，如果你得到一个比我们更聪明、能做这个工作的东西，它是否会创造出连自己都无法完成的新工作。
195 00:14:04,298 --> 00:14:05,741 说话人 SPEAKER_01：所以这是一个很大的风险。
196 00:14:06,110 --> 00:14:08,434 说话人 SPEAKER_01：稍后我会谈到我们可能采取的措施。
197 00:14:09,136 --> 00:14:14,668 说话人 SPEAKER_01：第二个风险是，由于存在这些相互对立的阵营，每个阵营都只阅读自己的新闻，从而加剧了社会的两极分化。
198 00:14:15,250 --> 00:14:22,024 说话人 SPEAKER_01：这与大型科技公司不断说一些令人震惊的事情有关，让你点击它们，或者将你指向更多令人震惊的事情。
199 00:14:22,687 --> 00:14:24,591 说话人 SPEAKER_01：也许可以对此做些什么。
200 00:14:24,571 --> 00:14:28,455 说话人 SPEAKER_01：然后是战斗机器人的风险，以及国防部门的风险。
201 00:14:29,155 --> 00:14:30,236 说话人 SPEAKER_01：我认为美国
202 00:14:30,256 --> 00:14:35,582 说话人 SPEAKER_01：希望在 2030 年之前用机器人取代大部分士兵，这相当快。
203 00:14:37,605 --> 00:14:45,833 说话人 SPEAKER_01：我们可以说我们会设计 AI，让它们的首要目标是永远不伤害人类，但那不是国防部的首要目标。
204 00:14:47,216 --> 00:14:53,042 说话人 SPEAKER_01：所以我们必须担心坏人利用它们做坏事，而且很明显会做什么坏事。
205 00:14:55,164 --> 00:14:57,508 说话人 SPEAKER_01：然后还有监控的风险。
206 00:14:57,707 --> 00:15:03,693 说话人 SPEAKER_01：有了更好的监控，他们可以对这些摄像头进行唇语识别，或者摄像头可以配备麦克风。
207 00:15:04,315 --> 00:15:09,581 说话人 SPEAKER_01：如果当局能看透一切，那么将很难有独立的政治运动。
208 00:15:09,600 --> 00:15:11,802 说话人 SPEAKER_01：这将变得像奥威尔式的。
209 00:15:13,465 --> 00:15:16,969 说话人 SPEAKER_01：我谈论最多的风险，尽管很多人也谈论过这些风险。
210 00:15:17,629 --> 00:15:25,138 说话人 SPEAKER_01：我最关心的是一种较长期的风险，即它们决定接管。
211 00:15:25,321 --> 00:15:30,368 说话人 SPEAKER_01：假设它们比我们聪明，假设它们认为我们做得不好。
212 00:15:31,950 --> 00:15:33,851 说话人 SPEAKER_01：如果它们想接管，它们可以。
213 00:15:34,393 --> 00:15:45,388 说话人 SPEAKER_01：今天上午的某个演讲中，我想汤姆提到了这样一个观点，即运行您 IT 系统的 AI 会自己决定何时需要新的服务器。
214 00:15:45,368 --> 00:15:50,235 说话人 SPEAKER_01：如果它能自己决定获取新服务器，那么它已经拥有很大的权力了。
215 00:15:50,636 --> 00:15:55,605 说话人 SPEAKER_01：而且如果它决定接管，并且比我们聪明得多，它将能够做到。
216 00:15:56,748 --> 00:16:02,618 说话人 SPEAKER_01：所以我们必须想办法防止它想要接管。
217 00:16:03,104 --> 00:16:04,725 说话人 SPEAKER_01：这很棘手，不知道如何做到。
218 00:16:04,865 --> 00:16:05,886 说话人 SPEAKER_01：我看不出如何做到。
219 00:16:06,268 --> 00:16:08,890 说话人 SPEAKER_01：我鼓励年轻的研究人员去研究这个问题。
220 00:16:09,370 --> 00:16:17,541 说话人 SPEAKER_01：我所知道的其中一些最杰出的年轻研究人员，比如，比如伊利亚·苏茨科娃，他是 GPT-4 背后的推动力量之一，他现在全职专注于这个问题。
221 00:16:17,961 --> 00:16:23,206 说话人 SPEAKER_01：和其他研究人员，如多伦多大学的罗杰·格罗斯和大卫·杜文诺，正在关注这一点，还有许多其他研究人员。
222 00:16:23,748 --> 00:16:29,274 说话人 SPEAKER_01：我认为非常重要的一点是，有没有一种方法可以确保他们永远不会想要接管？
223 00:16:29,774 --> 00:16:31,096 说话人 SPEAKER_01：我们对此问题的答案尚不清楚。
224 00:16:31,135 --> 00:16:32,638 说话人 SPEAKER_01：我们以前从未遇到过这种情况。
225 00:16:32,618 --> 00:16:38,046 说话人 SPEAKER_01：我们从未遇到过即将出现比人类更聪明的东西的情况。
226 00:16:38,066 --> 00:16:50,482 说话人 SPEAKER_01：我们已经习惯了把自己看作是老板和控制者，尤其是老白人男性，以至于我们无法接受这些事物可能比我们聪明得多，并且可能决定他们不需要我们的想法。
227 00:16:51,423 --> 00:16:53,547 说话人 SPEAKER_01：我们必须阻止这种情况发生，如果可能的话。
228 00:16:54,320 --> 00:16:54,740 说话人 SPEAKER_02：哇。
229 00:16:55,140 --> 00:16:55,942 说话人 SPEAKER_02：让我先插一句。
230 00:16:56,062 --> 00:17:02,753 说话人 SPEAKER_02：所以，我意思是，不久前，这会是一部我们都在看的电影，更不用说成为一次主题演讲的一部分了。
231 00:17:03,594 --> 00:17:09,824 说话人 SPEAKER_02：正如你之前提到的，从生物学的角度来说，我们知道我们的生命是有期限的。
232 00:17:10,344 --> 00:17:14,049 说话人 SPEAKER_02：所以，请帮我理解一下关于这种存在风险的问题。
233 00:17:14,231 --> 00:17:29,570 说话者 SPEAKER_02：是不是有一种，因为它是计算机系统，就像一个紧急停止开关，一按电源按钮，就像，如果我们创建了这些系统，无论是硬件还是软件，连接到云端等等，难道就没有一种方法，可以说，好吧，停止，够了？
234 00:17:31,032 --> 00:17:31,854 说话者 SPEAKER_01：有些人这么认为。
235 00:17:31,874 --> 00:17:34,636 说话者 SPEAKER_01：比如，埃里克·施密特最近在电视上说过。
236 00:17:34,857 --> 00:17:35,117 说话者 SPEAKER_01：好的。
237 00:17:35,739 --> 00:17:37,621 说话人 SPEAKER_01：这完全是不切实际的。
238 00:17:37,641 --> 00:17:39,143 所以，例如，
239 00:17:40,000 --> 00:17:46,348 结果发现，即使从未亲自去过国会大厦，也可以通过制造言语来入侵它。
240 00:17:47,109 --> 00:17:50,976 你只需要说出正确的话，就能说服其他人去入侵它。
如果它们比我们聪明得多，它们将能够说服那个有“停止开关”的人，这只是一个错误，这不是他应该使用的停止开关。
242 00：18：01,932 --> 00：18：03,273 议长 SPEAKER_01：他们会像绝地武士一样，对吧？
243 00:18:03,253 --> 00:18:09,967 说话者 SPEAKER_01：所以我认为，如果他们想要控制权，我们就没有维持控制权的任何机会。
244 00：18：10,007 --> 00：18：12,972 议长 SPEAKER_01：他们已经能够做一些事情了，比如订购新服务器之类的事情。
245 00:18:13,733 --> 00:18:15,417 说话人 SPEAKER_01：他们决定这些服务器上运行什么。
246 00:18:15,838 --> 00:18:18,663 说话人 SPEAKER_01：他们可以编写程序。
247 00:18:19,268 --> 00:18:22,772 说话人 SPEAKER_01：我认为如果他们比我们聪明得多，想要获得控制权，那将是徒劳的。
248 00:18:23,233 --> 00:18:27,200 说话人 SPEAKER_01：我认为一个关闭开关也无济于事，因为他们会说服我们我们非常错误。
249 00:18:27,539 --> 00:18:27,980 说话人 SPEAKER_02: 明白了。
250 00:18:28,141 --> 00:18:30,104 说话人 SPEAKER_02: 我们一直说知识就是力量。
251 00:18:30,144 --> 00:18:33,169 说话人 SPEAKER_02: 所以你现在真正说的是信息才是力量。
252 00:18:33,229 --> 00:18:42,342 说话人 SPEAKER_02: 这种信息力量可能导致潜在风险，即假新闻、不良新闻或不适当新闻，最终会自行发展。
253 00:18:42,362 --> 00:18:42,502 说话人 SPEAKER_01: 对。
254 00:18:42,522 --> 00:18:45,666 说话人 SPEAKER_01: 现在还有一线希望，那就是它们没有进化。
255 00:18:46,327 --> 00:18:47,289 说话人 SPEAKER_01: 所以雷提到了这一点。
256 00:18:47,390 --> 00:18:48,751 说话人 SPEAKER_01: 我们制造了这些事物。
257 00:18:48,731 --> 00:18:49,596 说话人 SPEAKER_03: 对。
258 00:18:49,615 --> 00:18:51,743 说话人 SPEAKER_01: 目前这给了我们很多控制权。
259 00:18:51,805 --> 00:18:56,785 说话人 SPEAKER_01: 所以你可能认为，因为我们创造了它们，所以我们可以 somehow 让它们永远不会想要接管。
260 00:18:57,507 --> 00:19:05,519 说话人 SPEAKER_01: 问题在于，如果你想让一个系统在没有微观管理的情况下有效，你必须给它创建子目标的能力。
261 00:19:06,161 --> 00:19:10,847 说话人 SPEAKER_01：就像士兵一样，将军不必说，把枪指向这里，我说了再开枪。
262 00:19:11,628 --> 00:19:16,678 说话人 SPEAKER_01：将军只是说，消灭对方，士兵们就会想出怎么去做。
263 00:19:17,038 --> 00:19:20,763 说话人 SPEAKER_01：嗯，对于这些 AI 来说，你需要给他们一个目标，
264 00:19:21,232 --> 00:19:23,518 说话人 SPEAKER_01：然后你给他们创造子目标的能力。
265 00:19:24,278 --> 00:19:26,884 说话人 SPEAKER_01：那么，有助于实现这一目标的事情。
266 00:19:26,904 --> 00:19:27,726 说话人 SPEAKER_01：这很好。
267 00:19:27,746 --> 00:19:28,909 说话人 SPEAKER_01：这将使它们工作得更好。
268 00:19:29,108 --> 00:19:30,392 说话人 SPEAKER_01：它们将为我们做许多奇妙的事情。
269 00:19:30,652 --> 00:19:32,175 说话人 SPEAKER_01：他们将成为非常出色的助手。
270 00:19:33,679 --> 00:19:40,692 说话人 SPEAKER_01：问题是拥有更多控制权是一个非常自然的子目标。
271 00:19:41,045 --> 00:19:42,926 说话人 SPEAKER_01：所以我们内置了这个功能。
272 00:19:44,028 --> 00:19:57,226 说话人 SPEAKER_01：例如，如果我在一个非常无聊的演讲中，希望不是像这次这样，我看到天花板上有一个光点，我会想知道那是什么，然后我听一下演讲，然后它移动了，我注意到它的移动和我的手腕移动是同步的。
273 00:19:58,126 --> 00:19:59,949 说话人 SPEAKER_01：所以我意识到这是我的手表反射的光。
274 00:20:01,030 --> 00:20:01,852 说话人 SPEAKER_01：那么我接下来该做什么呢？
275 00:20:01,892 --> 00:20:03,354 说话人 SPEAKER_01：我是不是该回去听讲座？
276 00:20:03,554 --> 00:20:03,753 说话人 SPEAKER_01：不是。
277 00:20:04,095 --> 00:20:07,338 说话人 SPEAKER_01：我试图弄清楚如何让它这样移动，以及如何让它那样移动。
278 00:20:08,247 --> 00:20:11,430 说话人 SPEAKER_01：我们被设计成想要控制，因为这对我们非常有用。
279 00:20:12,250 --> 00:20:13,352 说话人 SPEAKER_01：这有助于你做其他事情。
280 00:20:14,292 --> 00:20:20,397 说话人 SPEAKER_01：这些智能系统会很快学会，获得更多的控制会让你在做事情时更加高效。
281 00:20:20,798 --> 00:20:24,781 Speaker SPEAKER_01: And if we just ask them to be effective at doing things, they're going to want more control.
282 00:20:24,842 --> 00:20:27,765 Speaker SPEAKER_01: And that's the beginning of a very slippery slope.
283 00:20:27,785 --> 00:20:28,125 Speaker SPEAKER_02: Excellent.
284 00:20:28,145 --> 00:20:28,986 Speaker SPEAKER_02: Very insightful.
285 00:20:29,046 --> 00:20:33,069 Speaker SPEAKER_02: Again, let's put our hands together for this answer.
286 00:20:33,089 --> 00:20:33,190 Speaker SPEAKER_02: Yeah.
287 00:20:33,210 --> 00:20:33,611 Speaker SPEAKER_02: Thank you.
288 00:20:33,711 --> 00:20:33,990 Speaker SPEAKER_02: Thank you.
289 00:20:34,010 --> 00:20:35,132 Speaker SPEAKER_02: Did you have a follow-up?
290 00:20:35,837 --> 00:20:44,028 Speaker SPEAKER_03: Yeah, the original neural network that you and your team created actually had the underpinnings of today's foundation model.
291 00:20:44,731 --> 00:20:47,501 Speaker SPEAKER_03: When you take a look at multimodal models,
292 00:20:47,903 --> 00:20:51,810 Speaker SPEAKER_03: Do you think that this is one of the best areas for future innovation?
293 00:20:52,412 --> 00:20:52,711 Speaker SPEAKER_01: Oh, yes.
294 00:20:52,772 --> 00:20:56,599 Speaker SPEAKER_01: I think multimodal models are definitely much better than unimodal models.
295 00:20:57,280 --> 00:21:01,630 Speaker SPEAKER_01: So a lot of stuff about the structure of space is easier to learn from vision than from language.
296 00:21:03,153 --> 00:21:06,199 Speaker SPEAKER_01: Language is a very good source because it's very abstract.
297 00:21:06,179 --> 00:21:08,303 Speaker SPEAKER_01: We've already done the abstraction for it.
298 00:21:08,323 --> 00:21:18,541 Speaker SPEAKER_01: We've basically taken thousands of years of humanity, abstracted lots of insights into reality, put them into language, and now these digital intelligences are just stealing that from us.
299 00:21:19,263 --> 00:21:21,248 Speaker SPEAKER_01: But they can also come up with new insights of their own.
300 00:21:21,288 --> 00:21:21,929 Speaker SPEAKER_01: That's the problem.
301 00:21:22,529 --> 00:21:28,862 Speaker SPEAKER_01: And once they start coming up with new insights of their own and just talking to each other, they'll get way beyond us, I believe.
302 00:21:30,766 --> 00:21:36,894 Speaker SPEAKER_03: And any particular use cases or industries you think that could really leverage this technology quickly?
303 00:21:37,816 --> 00:21:38,155 Speaker SPEAKER_01: Yes.
304 00:21:39,258 --> 00:21:42,962 Speaker SPEAKER_01: There's a fairly short answer to that, which is all of them.
305 00:21:43,124 --> 00:21:51,635 Speaker SPEAKER_01: But obviously, things like medicine, there'll be a lot of resistance from doctors, because they don't like to be superseded.
306 00:21:52,356 --> 00:21:54,980 Speaker SPEAKER_01: And so to begin with, you'll have to pretend to collaborate with the doctors.
307 00:21:55,902 --> 00:21:56,122 Speaker SPEAKER_01: Sorry.
308 00:21:56,442 --> 00:21:56,663 Speaker SPEAKER_01: Sorry.
309 00:21:56,782 --> 00:21:57,364 Speaker SPEAKER_01: I got that wrong.
310 00:21:57,743 --> 00:22:00,367 Speaker SPEAKER_01: To begin with, we will be collaborating with the doctors.
311 00:22:00,584 --> 00:22:07,818 Speaker SPEAKER_01: But pretty soon these AI systems are going to be much better than doctors.
312 00:22:08,661 --> 00:22:14,593 Speaker SPEAKER_01: I made a prediction about seven years ago that within five years they'd be better at medical image interpretation.
313 00:22:14,859 --> 00:22:17,143 Speaker SPEAKER_01: Well, that was ambitious.
314 00:22:17,723 --> 00:22:18,645 Speaker SPEAKER_01: I should have said 10 years.
315 00:22:20,268 --> 00:22:26,999 Speaker SPEAKER_01: Right now, there's many systems that are about comparable with radiologists in interpreting medical images, many different kinds of images.
316 00:22:27,720 --> 00:22:31,184 Speaker SPEAKER_01: In a few years' time, they'll be better because they can see so much more data.
317 00:22:31,506 --> 00:22:35,111 Speaker SPEAKER_01: They can see millions of images, and no one radiologist can do that.
318 00:22:35,151 --> 00:22:39,817 Speaker SPEAKER_01: So in medicine, they're going to be very good at image interpretation, but they're also going to be very good at diagnosis.
319 00:22:40,538 --> 00:22:42,942 Speaker SPEAKER_01: They're going to be able to take the patient's whole history
320 00:22:43,784 --> 00:22:55,501 Speaker SPEAKER_01: and including genomics data and the results of all the tests and the family history, and they're going to be able to synthesize that into a much better diagnosis than your family doctor or even an expert can give you.
321 00:22:56,383 --> 00:23:00,348 Speaker SPEAKER_01: And wouldn't you like to have a family doctor who'd seen 100 million patients?
322 00:23:00,849 --> 00:23:05,277 Speaker SPEAKER_01: And among these 100 million patients, it turned out there were 20 who were exactly like you.
323 00:23:05,297 --> 00:23:08,761 Speaker SPEAKER_01: That would be really useful, and you're not going to get that from people.
324 00:23:08,911 --> 00:23:16,599 Speaker SPEAKER_01: So medicine's an example of something that I always pivot to, because nearly all the uses of that are good.
325 00:23:16,619 --> 00:23:21,045 Speaker SPEAKER_01: I mean, obviously insurance companies will use it to deny insurance to people who might get sick.
326 00:23:21,464 --> 00:23:22,606 Speaker SPEAKER_01: That's capitalism.
327 00:23:23,126 --> 00:23:25,589 Speaker SPEAKER_01: But on the whole, the uses are good.
328 00:23:27,051 --> 00:23:30,615 Speaker SPEAKER_01: There's other things where the uses may not be so good, particularly in fighting wars.
329 00:23:31,517 --> 00:23:37,163 Speaker SPEAKER_01: But any industry where you want to make predictions, this stuff is going to be helpful.
330 00:23:37,817 --> 00:23:39,039 Speaker SPEAKER_02: Wow, very helpful.
331 00:23:39,539 --> 00:23:42,825 Speaker SPEAKER_02: So we've got various industries represented here today.
332 00:23:42,984 --> 00:23:45,488 Speaker SPEAKER_02: We've got a lot of leaders that are here today.
333 00:23:45,548 --> 00:23:47,770 Speaker SPEAKER_02: We touched briefly on the risks.
334 00:23:47,810 --> 00:23:59,386 Speaker SPEAKER_02: You talked about some of the opportunities to theme of our conference is exponential IT, so exponentially accelerate solutions in science, in medicine, et cetera.
335 00:23:59,366 --> 00:24:00,769 Speaker SPEAKER_02: I'm sure there's a leader out there.
336 00:24:00,828 --> 00:24:02,692 Speaker SPEAKER_02: As a former CI, I would be thinking this too.
337 00:24:02,751 --> 00:24:03,452 Speaker SPEAKER_02: I heard some risk.
338 00:24:03,492 --> 00:24:04,535 Speaker SPEAKER_02: I heard some opportunities.
339 00:24:04,934 --> 00:24:09,981 Speaker SPEAKER_02: What can organizations do to prepare to strike the right balance to optimize those opportunities?
340 00:24:10,363 --> 00:24:13,827 Speaker SPEAKER_02: And in particular, coming from the public sector, what can governments do?
341 00:24:14,147 --> 00:24:15,790 Speaker SPEAKER_02: So organizations, we've got leaders here.
342 00:24:16,491 --> 00:24:24,824 Speaker SPEAKER_02: But also, what can governments do so that we can strike that right balance to leverage those opportunities and not fall into those risks?
343 00:24:24,844 --> 00:24:26,926 Speaker SPEAKER_01: Yeah, I get asked these questions a lot.
344 00:24:26,906 --> 00:24:29,652 Speaker SPEAKER_01: And the problem is I'm a scientist.
345 00:24:29,751 --> 00:24:31,153 Speaker SPEAKER_01: I'm not a policy guy.
346 00:24:31,875 --> 00:24:37,244 Speaker SPEAKER_01: And my life's work has been to try and make artificial neural networks work really well.
347 00:24:38,326 --> 00:24:41,632 Speaker SPEAKER_01: And it's only very recently I started thinking about the risks.
348 00:24:41,652 --> 00:24:47,582 Speaker SPEAKER_01: There's other people who thought much longer and harder about how to mitigate the risks.
349 00:24:47,815 --> 00:24:49,518 Speaker SPEAKER_01: I wish it was like climate change.
350 00:24:49,739 --> 00:24:55,650 Speaker SPEAKER_01: With climate change, you can say, you know, convert to solar energy and stop burning carbon.
351 00:24:55,670 --> 00:24:58,935 Speaker SPEAKER_01: And if you stop burning carbon, there will still be a disaster.
352 00:24:58,955 --> 00:25:03,444 Speaker SPEAKER_01: But in about 100 years' time, things will settle down and we'll all be okay.
353 00:25:04,505 --> 00:25:06,248 Speaker SPEAKER_01: There isn't a simple recipe like that.
354 00:25:06,808 --> 00:25:10,797 Speaker SPEAKER_01: For some of the risks, I think it's not so bad.
355 00:25:10,817 --> 00:25:12,681 Speaker SPEAKER_01: So things like bias and discrimination.
356 00:25:13,501 --> 00:25:21,518 Speaker SPEAKER_01: It's terrible that systems that decide who gets parole show all the same biases as their training data.
357 00:25:22,627 --> 00:25:23,910 Speaker SPEAKER_01: And that's not good.
358 00:25:23,950 --> 00:25:34,885 Speaker SPEAKER_01: I mean, if you train a system to decide, for example, who should get a mortgage, and you train it on data of old white men deciding whether young black women should get mortgages, the system's going to show the same biases.
359 00:25:35,406 --> 00:25:42,737 Speaker SPEAKER_01: But at least you can see the biases better, because you can freeze the system and experimentally determine how biased it is.
360 00:25:43,638 --> 00:25:52,010 Speaker SPEAKER_01: So I think if your goal is not to eliminate bias, but just to make bias less bad than the current systems it's replacing, that's very achievable.
361 00:25:52,480 --> 00:26:03,976 Speaker SPEAKER_01: So, I'm not so worried about bias, which makes some people very cross, because I think we can mitigate it reasonably well, and I think we can at least produce systems that are less dangerous than the current systems.
362 00:26:04,296 --> 00:26:05,718 Speaker SPEAKER_01: Just like with autonomous vehicles.
363 00:26:06,239 --> 00:26:09,282 Speaker SPEAKER_01: They'll kill people, but they'll kill a lot less people.
364 00:26:09,963 --> 00:26:11,705 Speaker SPEAKER_01: You have to be willing to say they'll kill people.
365 00:26:12,086 --> 00:26:15,230 Speaker SPEAKER_01: If you say they'll never kill people, you're stuck.
366 00:26:16,425 --> 00:26:19,453 Speaker SPEAKER_01: With some of the other risks, it's much harder to see.
367 00:26:19,755 --> 00:26:26,773 Speaker SPEAKER_01: So with fake news, for example, it'd be very nice if you could label everything that's fake as fake, as AI generated.
368 00:26:27,817 --> 00:26:30,063 Speaker SPEAKER_01: And you can imagine governments legislating for that.
369 00:26:30,183 --> 00:26:32,088 Speaker SPEAKER_01: It's technically a very difficult problem.
370 00:26:32,643 --> 00:26:34,306 Speaker SPEAKER_01: to do it in a way that people can't get around.
371 00:26:34,747 --> 00:26:36,410 Speaker SPEAKER_01: But governments have done things like that.
372 00:26:36,490 --> 00:26:40,517 Speaker SPEAKER_01: For example, if you make fake money, governments get very cross.
373 00:26:40,877 --> 00:26:45,967 Speaker SPEAKER_01: They'll put you in jail for 10 years for making fake money because they really care about them being the only ones to print their money.
374 00:26:46,548 --> 00:26:49,133 Speaker SPEAKER_01: And if you get some fake money,
375 00:26:49,113 --> 00:26:53,481 Speaker SPEAKER_01: and you give it to somebody else, knowing that it's fake, you can go to jail for that too.
376 00:26:54,123 --> 00:26:59,775 Speaker SPEAKER_01: So governments are very serious about not allowing fake money, because it touches on something important to them.
377 00:27:01,198 --> 00:27:07,289 Speaker SPEAKER_01: I think they should be equally serious about not allowing fake news, but it's technically much more difficult.
378 00:27:07,490 --> 00:27:10,614 Speaker SPEAKER_01: One place you can get a sort of little edge.
379 00:27:11,213 --> 00:27:19,944 Speaker SPEAKER_01: I interacted with a legislator in Britain who was trying to deal with fake videos of child sex abuse.
380 00:27:21,446 --> 00:27:28,434 Speaker SPEAKER_01: And that's an area where it's hard to prosecute the people who make the fake videos because they claim no child was hurt in the making of the videos because it was all fake.
381 00:27:29,326 --> 00:27:34,536 Speaker SPEAKER_01: And it looks like there's a possibility there to say, those people have to mark those as fake.
382 00:27:34,957 --> 00:27:36,720 Speaker SPEAKER_01: It may be difficult, but they have to mark them as fake.
383 00:27:36,740 --> 00:27:40,227 Speaker SPEAKER_01: And if they don't mark them as fake, they can go to jail for not marking them as fake.
384 00:27:40,247 --> 00:27:41,990 Speaker SPEAKER_01: At least you can get them that way.
385 00:27:42,009 --> 00:27:45,837 Speaker SPEAKER_01: And I don't think you get a lot of sympathy for the people who make fake child abuse videos.
386 00:27:46,358 --> 00:27:49,765 Speaker SPEAKER_01: So that might be a place to start, the thinning of a wedge.
387 00:27:51,634 --> 00:27:55,861 Speaker SPEAKER_01: It's hard, but it's possibly doable to mark fake videos as fake.
388 00:27:55,881 --> 00:27:57,041 Speaker SPEAKER_01: So that's something we can do.
389 00:27:57,782 --> 00:28:15,346 Speaker SPEAKER_01: For the echo chambers, you're going to need governments or somebody, presumably governments, to interfere with the business model of the social media companies that says that do whatever you can to get people to click more.
390 00:28:15,799 --> 00:28:18,365 Speaker SPEAKER_01: there have to be things they shouldn't do to get people to click more.
391 00:28:19,586 --> 00:28:23,413 Speaker SPEAKER_01: You can imagine governments actually legislating to do that, although it's going to be tricky.
392 00:28:25,518 --> 00:28:29,586 Speaker SPEAKER_01: But like I say, basically I don't have solutions to these problems.
393 00:28:29,665 --> 00:28:34,615 Speaker SPEAKER_01: It's not like climate change where you can say, for God's sake, stop burning carbon and you'll be okay.
394 00:28:35,156 --> 00:28:35,917 Speaker SPEAKER_01: It's not like that.
395 00:28:36,369 --> 00:28:37,391 Speaker SPEAKER_02: All right, thank you.
396 00:28:37,451 --> 00:28:38,031 Speaker SPEAKER_02: Very helpful.
397 00:28:38,311 --> 00:28:39,573 Speaker SPEAKER_02: I'm coming back to you now, Bill.
398 00:28:39,813 --> 00:28:42,215 Speaker SPEAKER_02: My former CIO hat is on right now.
399 00:28:42,296 --> 00:28:45,900 Speaker SPEAKER_02: So when I hear this, I hear opportunities, risks, and my brain is spinning.
400 00:28:47,381 --> 00:28:47,942 Speaker SPEAKER_02: Not exactly.
401 00:28:47,961 --> 00:28:53,707 Speaker SPEAKER_02: I'm sure people are developing strategies around AI or early in that process.
402 00:28:54,208 --> 00:28:58,251 Speaker SPEAKER_02: What do you advise, especially when you speak to some of our clients, on how do you respond to this?
403 00:28:58,271 --> 00:29:03,356 Speaker SPEAKER_02: How do you prepare so you're not lost and marbles in your mouth when you're CEO?
404 00:29:03,336 --> 00:29:06,480 Speaker SPEAKER_02: Leader, the organization's asked you about this particular technology.
405 00:29:07,461 --> 00:29:13,826 Speaker SPEAKER_03: So a lot of folks, public agencies, commercial, we're focused on responsible AI.
406 00:29:14,248 --> 00:29:18,832 Speaker SPEAKER_03: And it's a great way to help us mitigate the risk that we can address.
407 00:29:18,932 --> 00:29:22,236 Speaker SPEAKER_03: It's certainly not the existential risk that we're talking about as well.
408 00:29:22,955 --> 00:29:26,400 Speaker SPEAKER_03: But what I say is this is a task that's so important.
409 00:29:27,101 --> 00:29:31,605 Speaker SPEAKER_03: You just don't give it to your data scientists and your IT people to do.
410 00:29:32,192 --> 00:29:33,154 Speaker SPEAKER_03: the business stakeholders.
411 00:29:33,174 --> 00:29:38,729 Speaker SPEAKER_03: So this is a team sport in AI to address these kind of risks.
412 00:29:38,749 --> 00:29:42,336 Speaker SPEAKER_03: And just one question that I still want to ask.
413 00:29:42,397 --> 00:29:42,999 Speaker SPEAKER_03: Please, go ahead.
414 00:29:44,221 --> 00:29:46,708 Speaker SPEAKER_03: Earlier, Jeffrey, you
415 00:29:48,258 --> 00:29:52,523 Speaker SPEAKER_03: asked a question that we posed to Ray Kurzweil about digital intelligence.
416 00:29:52,544 --> 00:29:55,709 Speaker SPEAKER_03: And he returned the favor, and he has a question for you.
417 00:29:55,828 --> 00:29:58,092 Speaker SPEAKER_03: And I'm going to paraphrase it, because it's quite funny.
418 00:29:58,112 --> 00:29:59,053 Speaker SPEAKER_01: Well, just one thing.
419 00:29:59,073 --> 00:30:00,255 Speaker SPEAKER_01: He didn't answer my question.
420 00:30:00,275 --> 00:30:01,016 Speaker SPEAKER_01: That's true.
421 00:30:01,336 --> 00:30:01,896 Speaker SPEAKER_03: I have to answer his.
422 00:30:01,957 --> 00:30:03,819 Speaker SPEAKER_03: That's true.
423 00:30:03,859 --> 00:30:04,981 Speaker SPEAKER_03: I did notice that.
424 00:30:05,021 --> 00:30:06,703 Speaker SPEAKER_03: He kind of skirted around it.
425 00:30:07,325 --> 00:30:08,105 Speaker SPEAKER_03: Yeah.
426 00:30:08,390 --> 00:30:14,943 Speaker SPEAKER_03: But if you could do the favor and set an example for him and address his question.
427 00:30:15,806 --> 00:30:20,756 Speaker SPEAKER_03: So he states that, biologically, humans have progressed.
428 00:30:20,836 --> 00:30:21,698 Speaker SPEAKER_03: We're living longer.
429 00:30:21,778 --> 00:30:24,423 Speaker SPEAKER_03: And with the advent of simulated biology,
430 00:30:24,775 --> 00:30:26,678 Speaker SPEAKER_03: we're going to make dramatic increases there.
431 00:30:27,278 --> 00:30:32,749 Speaker SPEAKER_03: He also talks about, in terms of how much income we make, that that's also increased over time.
432 00:30:33,589 --> 00:30:34,832 Speaker SPEAKER_03: He cites tenfold.
433 00:30:35,814 --> 00:30:41,063 Speaker SPEAKER_03: And he has examples in 50 different areas of these incredible
434 00:30:41,042 --> 00:30:44,468 Speaker SPEAKER_03: exponential improvements in humankind.
435 00:30:45,169 --> 00:30:52,140 Speaker SPEAKER_03: And he states that it's not us versus AI, it's the combination of human and AI methods that will bring about these changes.
436 00:30:52,760 --> 00:30:58,289 Speaker SPEAKER_03: His question to you is, how do we limit AI without losing these vital types of progress?
437 00:30:59,211 --> 00:31:04,619 Speaker SPEAKER_01: Okay, so I basically agree with him that AI can do incredible things for us.
438 00:31:05,038 --> 00:31:07,241 Speaker SPEAKER_01: In that sense, I completely agree with him.
439 00:31:07,461 --> 00:31:09,424 Speaker SPEAKER_01: And I think that's why we're not going to limit it.
440 00:31:09,845 --> 00:31:21,580 Speaker SPEAKER_01: I don't think it's feasible to get people to stop doing AI and to stop developing it further because of the incredible things it can do for us in areas like medicine, but also in areas like making sort of all tedious jobs more efficient.
441 00:31:22,181 --> 00:31:32,515 Speaker SPEAKER_01: Anybody who has to produce prose in a tedious way, they can make it much easier, like letters of recommendation, which professors know are tedious.
442 00:31:32,494 --> 00:31:36,281 Speaker SPEAKER_01: So, we're not going to stop the progress because of the wonderful things it'll do for us.
443 00:31:37,022 --> 00:31:46,200 Speaker SPEAKER_01: But I think one thing Ray's got slightly wrong is that I think the digital intelligence has a very different flavor from biological intelligence.
444 00:31:46,760 --> 00:31:48,364 Speaker SPEAKER_01: I've only begun to think this recently.
445 00:31:48,403 --> 00:31:52,832 Speaker SPEAKER_01: A few years ago, I thought you'd always make it better by making it more like the biological intelligence.
446 00:31:53,571 --> 00:32:03,483 Speaker SPEAKER_01: But the idea of a kind of harmonious collaboration of these two kinds of intelligence, I think, falls apart when they become very different kinds of intelligence, and when one's much smarter than the other.
447 00:32:04,265 --> 00:32:11,473 Speaker SPEAKER_01: And when digital intelligences can share what they know with other copies of the same model, and they can't share it with us in the same way.
448 00:32:11,493 --> 00:32:13,977 Speaker SPEAKER_01: It's just very slow and tedious to explain to us.
449 00:32:14,445 --> 00:32:18,150 Speaker SPEAKER_01: So I think he's sort of wrong about that.
450 00:32:18,931 --> 00:32:25,337 Speaker SPEAKER_01: I also think that it looks to me like he's a bit worried about dying and he just needs to get used to the idea.
451 00:32:25,999 --> 00:32:27,000 Speaker SPEAKER_00: Okay, okay.
452 00:32:33,467 --> 00:32:34,028 Speaker SPEAKER_03: That's it.
453 00:32:34,107 --> 00:32:36,931 Speaker SPEAKER_03: You have a follow-up on that?
454 00:32:37,490 --> 00:32:39,534 Speaker SPEAKER_03: No, I actually have one last question.
455 00:32:40,013 --> 00:32:42,596 Speaker SPEAKER_03: You touched upon it about reasoning.
456 00:32:42,931 --> 00:32:53,541 Speaker SPEAKER_03: There are many out there in the industry who would say that chat GPT, GPT-4 is merely a statistical model and just trying to figure out what's the next best word.
457 00:32:54,201 --> 00:32:55,463 Speaker SPEAKER_03: What's your view on this?
458 00:32:56,344 --> 00:32:57,645 Speaker SPEAKER_01: Okay, it's a very good question.
459 00:32:58,946 --> 00:33:05,574 Speaker SPEAKER_01: Particularly linguists of the Chomsky School, who think that language is innate, argue this stuff isn't really doing language.
460 00:33:05,614 --> 00:33:06,775 Speaker SPEAKER_01: It doesn't really understand.
461 00:33:07,395 --> 00:33:10,878 Speaker SPEAKER_01: It's just a bunch of statistical tricks for predicting the next word.
462 00:33:11,617 --> 00:33:17,980 Speaker SPEAKER_01: So the first thing to say is that if you ask, what does it take to be really good at predicting the next word?
463 00:33:18,682 --> 00:33:23,721 Speaker SPEAKER_01: For example, suppose it's the first word of the answer to a question.
464 00:33:23,853 --> 00:33:26,659 Speaker SPEAKER_01: You have to understand to predict the next word.
465 00:33:26,679 --> 00:33:33,030 Speaker SPEAKER_01: You can do quite well just on correlations between words, pairwise statistics or triplewise statistics.
466 00:33:33,392 --> 00:33:38,903 Speaker SPEAKER_01: You can do a reasonable job of doing a lot better than chance of predicting the next word, but you can't do a really good job.
467 00:33:38,923 --> 00:33:41,929 Speaker SPEAKER_01: To do a really good job, you have to understand what was said.
468 00:33:41,909 --> 00:33:57,631 Speaker SPEAKER_01: So, the amazing thing is that using the backpropagation algorithm, you train a big language model, and its only goal in the fine-tuning is to predict the next word, but that goal is sufficient to force it to understand what's being said, because that's the only way you can predict the next word.
469 00:33:59,313 --> 00:34:02,297 Speaker SPEAKER_01: And when they say it's just statistics,
470 00:34:03,239 --> 00:34:05,445 Speaker SPEAKER_01: Statistics means different things to different people.
471 00:34:06,470 --> 00:34:13,974 Speaker SPEAKER_01: So there's a specific sense where you're dealing with correlations and sort of low order statistics.
472 00:34:14,139 --> 00:34:16,382 Speaker SPEAKER_01: And that's what Chomsky means by statistics.
473 00:34:16,402 --> 00:34:18,425 Speaker SPEAKER_01: That's what he thinks statistics is.
474 00:34:18,445 --> 00:34:20,427 Speaker SPEAKER_01: And in that sense, it isn't just statistics.
475 00:34:20,789 --> 00:34:22,250 Speaker SPEAKER_01: Previous language models were.
476 00:34:22,451 --> 00:34:28,338 Speaker SPEAKER_01: Things like trigram models that Google used to use for translation and so on, they were just statistics in that sense.
477 00:34:28,358 --> 00:34:30,000 Speaker SPEAKER_01: But these models are quite different.
478 00:34:30,641 --> 00:34:39,793 Speaker SPEAKER_01: So they get these sentences, they convert each word into a big vector of feature activities, and they've invented the features.
479 00:34:39,773 --> 00:34:54,182 Speaker SPEAKER_01: and then they learn billions of interactions between features, and as a result of all these feature vectors interacting, they can predict the features of the next word, and that's very different from normal statistics.
480 00:34:54,563 --> 00:34:56,887 Speaker SPEAKER_01: It involves creating these features
481 00:34:57,121 --> 00:34:59,784 Speaker SPEAKER_01: they can capture very high order correlations.
482 00:35:00,485 --> 00:35:04,809 Speaker SPEAKER_01: And so in the sense in which it's doing statistics, everything's statistics.
483 00:35:05,730 --> 00:35:09,795 Speaker SPEAKER_01: So saying it's just statistics doesn't mean everything, because in that sense, everything's statistics.
484 00:35:12,137 --> 00:35:14,519 Speaker SPEAKER_01: So I think those people are just completely wrong.
485 00:35:17,222 --> 00:35:20,806 Speaker SPEAKER_02: Let's go further with our large language modules.
486 00:35:20,947 --> 00:35:24,951 Speaker SPEAKER_02: Are there specific examples you can share of what they cannot do?
487 00:35:25,454 --> 00:35:30,940 Speaker SPEAKER_01: Well, I already gave you one with the brothers and sisters.
488 00:35:31,300 --> 00:35:31,900 Speaker SPEAKER_02: Oh, that's true.
489 00:35:31,920 --> 00:35:32,440 Speaker SPEAKER_02: Yes, yes, yes.
490 00:35:32,460 --> 00:35:36,224 Speaker SPEAKER_02: But in terms of industry application.
491 00:35:36,465 --> 00:35:37,626 Speaker SPEAKER_01: Oh, industry applications.
492 00:35:38,166 --> 00:35:41,871 Speaker SPEAKER_01: I don't know enough about the various different industries and how you'd apply them.
493 00:35:41,891 --> 00:35:44,032 Speaker SPEAKER_01: I suspect, I suspect not.
494 00:35:44,414 --> 00:35:49,639 Speaker SPEAKER_01: I mean, the industry in which they're currently least useful is probably plumbing.
495 00:35:49,619 --> 00:35:53,385 Speaker SPEAKER_01: Because particularly plumbing in an old house.
496 00:35:54,045 --> 00:36:00,295 Speaker SPEAKER_01: If you do plumbing in an old house, you need to be very inventive and you need to be very agile and you need to be able to get your fingers into funny places.
497 00:36:01,076 --> 00:36:03,498 Speaker SPEAKER_01: And they're not good at that yet.
498 00:36:03,820 --> 00:36:05,061 Speaker SPEAKER_01: But they are getting a lot better.
499 00:36:05,081 --> 00:36:07,666 说话人 SPEAKER_01：而且它们变得越来越灵活。
500 00:36:08,467 --> 00:36:10,750 说话人 SPEAKER_01：所以连管道工也不安全了。
501 00:36:11,202 --> 00:36:15,226 说话人 SPEAKER_01：我的意思是，当教授已经是很久以前的事情了，但现在连管道工也不安全了。
502 00:36:15,246 --> 00:36:15,606 说话人 SPEAKER_02: 哇。
503 00:36:15,907 --> 00:36:16,266 说话人 SPEAKER_02: 好的。
504 00:36:16,748 --> 00:36:17,068 说话人 SPEAKER_02: 好的。
505 00:36:17,628 --> 00:36:27,079 说话人 SPEAKER_02: 所以在这个问题上，我之前在您的演讲中也听到过，关于机会的问题，您想进一步阐述一下 AI 可能不会涉及到的领域。
506 00:36:27,159 --> 00:36:34,047 说话人 SPEAKER_02：有没有哪些行业它不会触及，或者您看到它会触及每个角色、每个行业？
507 00:36:34,347 --> 00:36:36,608 说话人 SPEAKER_01：我看不出有什么它不会触及的。
508 00:36:36,628 --> 00:36:36,889 说话人 SPEAKER_02：哇。
509 00:36:37,210 --> 00:36:39,032 说话人 SPEAKER_01：任何涉及智能的事情。
510 00:36:39,856 --> 00:36:46,985 说话人 SPEAKER_01：所以，我们过去认为属于人类特有的任何事物都将触及所有这些方面。
511 00:36:47,005 --> 00:36:48,166 说话人 SPEAKER_02：明白了。
512 00:36:48,186 --> 00:36:55,414 说话人 SPEAKER_02：所以，在这个指数级 IT 时代，我们会议的主题，作为领导者，我们必须为此做好准备，对吧？
513 00:36:55,996 --> 00:37:02,724 说话人 SPEAKER_02：那么，在你与客户交流准备过程时，你有什么要补充的，比尔？关于甚至从哪里开始？
514 00:37:03,864 --> 00:37:07,989 说话人 SPEAKER_03：与我们对追求人工智能的人的建议相同
515 00:37:08,188 --> 00:37:11,574 说话人 SPEAKER_03：人工智能的负责任是这是一项团队运动
516 00:37:12,295 --> 00:37:18,684 说话人 SPEAKER_03：你必须跳出你的 IT 孤岛，与业务利益相关者合作，让他们同舟共济
517 00:37:19,585 --> 00:37:20,306 说话人 SPEAKER_02：非常好。
518 00:37:20,327 --> 00:37:22,250 说话人 SPEAKER_02：所以继续学习，一起学习。
519 00:37:23,090 --> 00:37:25,293 说话人 SPEAKER_02：杰弗里，我们的时间快要用完了。
520 00:37:25,313 --> 00:37:32,425 说话人 SPEAKER_02：您有什么结束语、离别感言、励志故事或想法想要与我们的听众分享吗？
521 00:37:32,842 --> 00:37:37,769 说话人 SPEAKER_01：我想强调的一点是我们正进入一个充满不确定性的时代。
522 00:37:38,391 --> 00:37:41,335 说话人 SPEAKER_01：基本上，我们以前从未在这里过。
523 00:37:41,376 --> 00:37:46,302 说话人 SPEAKER_01：我们从未面对过这样的可能性，周围可能存在比我们更智能的事物。
524 00:37:47,063 --> 00:37:51,590 说话人 SPEAKER_01：尽管我们创造了它们，也许我们还能控制它们，但我们以前从未经历过这种情况。
525 00:37:51,871 --> 00:37:55,777 说话人 SPEAKER_01：对于人们来说，很难接受这样的想法，那就是可能存在比他们更聪明的事物。
526 00:37:56,237 --> 00:38:00,824 说话人 SPEAKER_01：我们已经习惯了我们处于控制地位，我们是最聪明的东西。
527 00:38:00,804 --> 00:38:03,088 说话人 SPEAKER_01：我们真的不知道会发生什么。
528 00:38:03,869 --> 00:38:09,721 说话人 SPEAKER_01：所以，尽管有许多非常令人沮丧的反乌托邦可能性，但我们实际上对任何事都一无所知。
529 00:38:10,621 --> 00:38:14,389 说话人 SPEAKER_01：预测五年以上的事情几乎是绝望的。
530 00:38:15,391 --> 00:38:16,994 说话人 SPEAKER_01：因为许多事情都会改变。
531 00:38:17,454 --> 00:38:22,222 说话人 SPEAKER_01：就像五年前，人们不会预测我们现在会有像现在这样的大型语言模型。
532 00:38:22,202 --> 00:38:32,856 说话人 SPEAKER_01：十年前，人们会非常自信地认为在十年后，你不会有一个能够比普通人回答任何问题的平均人回答得更好的大型语言模型。
533 00:38:33,858 --> 00:38:37,402 说话人 SPEAKER_01：所以我们真的不知道会发生什么。
534 00:38:38,063 --> 00:38:41,929 说话人 SPEAKER_01：我认为这是最重要的事情，那就是一切都是非常不确定的。
535 00:38:42,429 --> 00:38:49,780 说话人 SPEAKER_01：接下来最重要的事情是，我认为我们需要最聪明的大脑来研究如何将其控制在范围内。
536 00:38:50,130 --> 00:38:51,422 说话人 SPEAKER_02：太棒了。
537 00:38:52,148 --> 00:38:55,010 说话人 SPEAKER_02：再次为杰弗里·辛顿鼓掌。

1
00:00:02,443 --> 00:00:06,528
Speaker SPEAKER_02: Write a short introduction for Geoffrey Hinton, the subject of this video.

2
00:00:09,192 --> 00:00:14,538
Speaker SPEAKER_02: Geoffrey Hinton is a University of Toronto professor emeritus who is known as the godfather of AI.

3
00:00:16,161 --> 00:00:21,407
Speaker SPEAKER_02: He recently left Google so he could more freely discuss the dangers posed by unchecked AI development.

4
00:00:23,368 --> 00:00:30,577
Speaker SPEAKER_02: We spoke to him in his London home about the technology he helped create, its many benefits and why he suddenly fears humanity is at risk.

5
00:00:35,350 --> 00:00:37,152
Speaker SPEAKER_00: I got a request from the Wall Street Journal.

6
00:00:37,472 --> 00:00:39,415
Speaker SPEAKER_00: They want me to correct my obituary.

7
00:00:39,676 --> 00:00:40,097
Speaker SPEAKER_01: What do you mean?

8
00:00:40,838 --> 00:00:42,320
Speaker SPEAKER_00: They want me to correct my obituary.

9
00:00:42,340 --> 00:00:43,582
Speaker SPEAKER_01: They've like pre-written it.

10
00:00:43,601 --> 00:00:46,566
Speaker SPEAKER_00: They've pre-written my obituary.

11
00:00:46,585 --> 00:00:48,168
Speaker SPEAKER_00: I wonder what Mark Twain would have said about that.

12
00:00:48,189 --> 00:01:01,628
Speaker SPEAKER_01: So I guess we don't really need an introduction here, so I will just launch right into it.

13
00:01:03,262 --> 00:01:14,721
Speaker SPEAKER_01: You've recently given a number of interviews in which you've said that digital intelligence that is used by chatbots and other generative AI may be a better kind of intelligence than the biological intelligence that we have.

14
00:01:15,683 --> 00:01:18,468
Speaker SPEAKER_01: Can you briefly explain what made you come to this conclusion?

15
00:01:19,444 --> 00:01:26,334
Speaker SPEAKER_00: So in a digital computer, it's designed so you can tell it exactly what to do, and it'll do exactly what you tell it.

16
00:01:27,275 --> 00:01:34,763
Speaker SPEAKER_00: And even when it's learning stuff, two different digital computers can do exactly the same thing with the same learned knowledge.

17
00:01:36,006 --> 00:01:44,135
Speaker SPEAKER_00: And that means that you could make 10,000 copies of the same knowledge, have them all running on different computers,

18
00:01:44,638 --> 00:01:49,685
Speaker SPEAKER_00: And whenever one copy learns something, it can communicate it very efficiently to all the other copies.

19
00:01:50,966 --> 00:02:01,801
Speaker SPEAKER_00: So you can have 10,000 digital agents out there, a kind of hive mind, and they can share knowledge extremely efficiently by just sharing the connection strengths inside the neural nets.

20
00:02:01,820 --> 00:02:02,882
Speaker SPEAKER_00: And we can't do that.

21
00:02:03,783 --> 00:02:09,009
Speaker SPEAKER_00: If you learn something and you want to tell me about it, you have to use sentences.

22
00:02:09,074 --> 00:02:15,260
Speaker SPEAKER_00: or pictures, and you can only share a very limited amount of information that way.

23
00:02:15,801 --> 00:02:22,368
Speaker SPEAKER_00: So it's much, much slower for you to communicate what you've learned to me than it is for these digital intelligences to communicate stuff.

24
00:02:22,849 --> 00:02:24,010
Speaker SPEAKER_00: And that makes them much better.

25
00:02:24,792 --> 00:02:26,633
Speaker SPEAKER_00: They can learn a whole lot of stuff between them.

26
00:02:27,435 --> 00:02:33,262
Speaker SPEAKER_01: You've said that digital intelligence is immortal and that biological intelligence is mortal.

27
00:02:34,623 --> 00:02:35,925
Speaker SPEAKER_01: What did you mean by this?

28
00:02:36,377 --> 00:02:50,337
Speaker SPEAKER_00: So if I learn some connection strengths in a neural net that's being simulated on digital computers, then if a particular computer dies, those same connection strengths can be used on another computer.

29
00:02:51,539 --> 00:03:00,393
Speaker SPEAKER_00: And even if all the digital computers died, if you'd stored the connection strengths somewhere, you could then just make another digital computer and run the same weights on that other digital computer.

30
00:03:01,474 --> 00:03:02,436
Speaker SPEAKER_00: But with us,

31
00:03:03,326 --> 00:03:08,474
Speaker SPEAKER_00: The knowledge that we learn, the connection strengths, are specific to our particular brains.

32
00:03:08,615 --> 00:03:09,856
Speaker SPEAKER_00: Every brain is a bit different.

33
00:03:10,317 --> 00:03:12,219
Speaker SPEAKER_00: The neurons in your brain are all a bit different.

34
00:03:12,961 --> 00:03:18,248
Speaker SPEAKER_00: And you learn so as to make use of all the idiosyncrasies of your particular brain.

35
00:03:18,308 --> 00:03:25,098
Speaker SPEAKER_00: And so once you've learned connection strengths in your brain, if you told me those connection strengths, they wouldn't do me any good because my brain is different.

36
00:03:26,445 --> 00:03:32,215
Speaker SPEAKER_00: So the digital computers are immortal because you can run that same knowledge on a different piece of hardware.

37
00:03:32,936 --> 00:03:37,704
Speaker SPEAKER_00: We're immortal because the hardware and the knowledge are intricately entangled.

38
00:03:38,185 --> 00:03:42,451
Speaker SPEAKER_00: You can't separate the connection strengths from the particular brain they're running in.

39
00:03:43,554 --> 00:03:45,235
Speaker SPEAKER_00: And so if the brain dies, the knowledge dies.

40
00:03:47,819 --> 00:03:52,587
Speaker SPEAKER_02: Why should we be concerned about digital intelligence taking over from biological intelligence?

41
00:03:53,497 --> 00:04:06,891
Speaker SPEAKER_00: because I think it's much better at sharing what's learned by a whole bunch of different digital agents who all share the same weights and they just share the updates to the weights and now they can learn 10,000 different things at the same time.

42
00:04:08,114 --> 00:04:12,318
Speaker SPEAKER_00: But also I think the digital intelligence probably has a better learning algorithm than the brain's got.

43
00:04:13,259 --> 00:04:19,706
Speaker SPEAKER_00: All the attempts to find a learning algorithm in the brain that works as well as the back propagation algorithm

44
00:04:19,687 --> 00:04:21,970
Speaker SPEAKER_00: in these digital intelligences.

45
00:04:22,610 --> 00:04:24,031
Speaker SPEAKER_00: So far those attempts have failed.

46
00:04:24,692 --> 00:04:30,158
Speaker SPEAKER_00: We haven't found anything that scales up as well to very large systems as the backpropagation algorithm.

47
00:04:30,738 --> 00:04:32,341
Speaker SPEAKER_00: So I think they've got two advantages.

48
00:04:32,380 --> 00:04:39,269
Speaker SPEAKER_00: They've probably got a better learning algorithm and they can share knowledge much more efficiently than biological intelligences can.

49
00:04:39,288 --> 00:04:44,233
Speaker SPEAKER_01: At the time when you entered the field there were two schools of thought in machine intelligence.

50
00:04:45,196 --> 00:04:47,398
Speaker SPEAKER_01: Mainstream and neural nets.

51
00:04:48,644 --> 00:04:51,088
Speaker SPEAKER_01: Can you describe the difference between these two approaches?

52
00:04:51,608 --> 00:04:53,009
Speaker SPEAKER_00: I can sort of caricature it.

53
00:04:53,050 --> 00:04:57,336
Speaker SPEAKER_00: So there's two different models of what intelligence is all about.

54
00:04:57,978 --> 00:05:00,021
Speaker SPEAKER_00: And one model is that it's all about reasoning.

55
00:05:01,281 --> 00:05:03,404
Speaker SPEAKER_00: And the way we reason is by using logic.

56
00:05:03,906 --> 00:05:05,487
Speaker SPEAKER_00: And so that's what's special about people.

57
00:05:06,829 --> 00:05:12,639
Speaker SPEAKER_00: And what we should be doing is understanding the kind of logic that we actually use.

58
00:05:13,759 --> 00:05:16,204
Speaker SPEAKER_00: And that also went with the idea that

59
00:05:16,858 --> 00:05:29,642
Speaker SPEAKER_00: The knowledge you store is symbolic expressions, so that I can say a sentence to you, and you will somehow store that, and then later you'll be able to use it for inferring other sentences.

60
00:05:30,062 --> 00:05:33,670
Speaker SPEAKER_00: But what's inside your head is something a bit like sentences, but cleaned up.

61
00:05:34,814 --> 00:05:42,404
Speaker SPEAKER_00: And there's a completely different model of intelligence, which is that it's all about learning the connection strengths in a network of brain cells.

62
00:05:43,346 --> 00:05:47,812
Speaker SPEAKER_00: And what it's good for is things like perception and motor control, not for reasoning.

63
00:05:48,014 --> 00:05:50,838
Speaker SPEAKER_00: Reasoning came much, much later, and we're not very good at it.

64
00:05:51,879 --> 00:05:53,382
Speaker SPEAKER_00: You don't learn to do it until you're quite old.

65
00:05:54,704 --> 00:05:58,189
Speaker SPEAKER_00: And so reasoning is actually a very bad model of biological intelligence.

66
00:05:58,249 --> 00:06:03,656
Speaker SPEAKER_00: Biological intelligence is about things like controlling your body and seeing things.

67
00:06:04,531 --> 00:06:08,855
Speaker SPEAKER_00: that was a totally different paradigm and had a different idea of what's inside your head.

68
00:06:09,336 --> 00:06:14,202
Speaker SPEAKER_00: That it's not stored strings of symbols, it's just connection strengths.

69
00:06:15,163 --> 00:06:23,053
Speaker SPEAKER_00: The symbolic AI view, the crucial question was, what is the form of these symbolic expressions and how do you do the reasoning with them?

70
00:06:24,396 --> 00:06:27,980
Speaker SPEAKER_00: For the neural net view, the central question was quite different.

71
00:06:28,000 --> 00:06:32,225
Speaker SPEAKER_00: It was, how do you learn these connection strengths so you can do all these wonderful things?

72
00:06:32,593 --> 00:06:35,197
Speaker SPEAKER_00: And so learning was always central to the neural net view.

73
00:06:35,718 --> 00:06:38,100
Speaker SPEAKER_00: For the symbolic view, they said, we'll worry about learning later.

74
00:06:38,161 --> 00:06:41,605
Speaker SPEAKER_00: First, you have to figure out how the knowledge is represented and how we reason with it.

75
00:06:42,365 --> 00:06:44,069
Speaker SPEAKER_00: And so these were totally different views.

76
00:06:44,189 --> 00:06:46,812
Speaker SPEAKER_00: One took its inspiration from logic and one from biology.

77
00:06:47,512 --> 00:06:53,961
Speaker SPEAKER_00: And for a long time, the people in the logic camp thought taking inspiration from biology was silly.

78
00:06:54,245 --> 00:07:04,680
Speaker SPEAKER_00: That was a bit strange since von Neumann and Turing had both thought neural nets were the way to attack intelligence, but unfortunately they both died young.

79
00:07:07,504 --> 00:07:11,129
Speaker SPEAKER_02: Can you, at a high level, describe how a neural network works?

80
00:07:12,812 --> 00:07:13,392
Speaker SPEAKER_00: I can try.

81
00:07:14,494 --> 00:07:19,982
Speaker SPEAKER_00: So let's start off by describing how it would work for recognizing objects and images.

82
00:07:20,165 --> 00:07:24,670
Speaker SPEAKER_00: And let's suppose all we wanted to do was say whether or not there was a bird in the image.

83
00:07:25,112 --> 00:07:28,456
Speaker SPEAKER_00: And let's suppose the bird's going to be roughly in the middle of the image and the main object of attention.

84
00:07:29,057 --> 00:07:31,420
Speaker SPEAKER_00: And you have to say, is this a bird or isn't it?

85
00:07:33,303 --> 00:07:34,586
Speaker SPEAKER_00: So you can think of an image.

86
00:07:34,646 --> 00:07:36,769
Speaker SPEAKER_00: Let's suppose it's 100 pixels by 100 pixels.

87
00:07:37,310 --> 00:07:38,511
Speaker SPEAKER_00: That's 10,000 pixels.

88
00:07:39,151 --> 00:07:41,035
Speaker SPEAKER_00: Each pixel is three colors, RGB.

89
00:07:41,555 --> 00:07:43,117
Speaker SPEAKER_00: So that's 30,000 numbers.

90
00:07:43,975 --> 00:07:53,209
Speaker SPEAKER_00: And in computational terms, recognizing a bird in an image consists of taking 30,000 numbers and outputting one number that says yes or no it's a bird.

91
00:07:54,732 --> 00:08:01,723
Speaker SPEAKER_00: And you could try and write a standard computer program to do that, and people tried for many, many years, and they could never get it to work very well.

92
00:08:01,963 --> 00:08:03,425
Speaker SPEAKER_00: Like for 50 years they were trying to do that.

93
00:08:05,028 --> 00:08:07,812
Speaker SPEAKER_00: Or you could make a multi-layer neural net.

94
00:08:08,374 --> 00:08:11,838
Speaker SPEAKER_00: And I'll start off by telling you how you would wire up a neural net by hand.

95
00:08:12,781 --> 00:08:20,350
Speaker SPEAKER_00: So what you'd do is you'd have the pixels, and that would be the bottom level, and then you'd have a layer of feature detectors.

96
00:08:21,192 --> 00:08:36,712
Speaker SPEAKER_00: And a typical feature detector might have big positive connection strengths coming from a vertical row of pixels, and big negative connection strengths coming from a neighboring vertical row of pixels, and no connection strengths anywhere else.

97
00:08:37,434 --> 00:08:45,285
Speaker SPEAKER_00: So if both rows of pixels are bright, it'll get big positive input from here, but also big negative input from there, so it won't do anything.

98
00:08:46,086 --> 00:08:53,317
Speaker SPEAKER_00: But if these ones are bright, giving it big positive input, and these ones are not bright, so it doesn't get inhibited by these ones, it'll get all excited.

99
00:08:53,677 --> 00:08:58,144
Speaker SPEAKER_00: It'll say, hey, I found the thing I like, which is bright pixels here and dark pixels here.

100
00:08:58,985 --> 00:09:00,826
Speaker SPEAKER_00: And that's an edge detector.

101
00:09:00,846 --> 00:09:06,153
Speaker SPEAKER_00: I just told you how to wire up by hand using positive and negative weights something that would detect a little vertical edge.

102
00:09:07,196 --> 00:09:14,764
Speaker SPEAKER_00: So now imagine you have a gazillion of those guys detecting different edges in different locations in the image, in different orientations, and at different scales.

103
00:09:15,826 --> 00:09:17,589
Speaker SPEAKER_00: That would be your first layer of feature detectors.

104
00:09:18,570 --> 00:09:21,634
Speaker SPEAKER_00: Now if I was wiring it by hand, my second layer of feature detectors

105
00:09:22,457 --> 00:09:28,869
Speaker SPEAKER_00: I would maybe have a detector that takes two edges that join at a fine angle, like this.

106
00:09:29,370 --> 00:09:31,052
Speaker SPEAKER_00: So it's looking for this edge and this edge.

107
00:09:31,453 --> 00:09:36,121
Speaker SPEAKER_00: And if they're both active at once, it would say, hey, maybe there's a beak here.

108
00:09:36,822 --> 00:09:39,506
Speaker SPEAKER_00: It could be all sorts of other things, but it might just be a beak.

109
00:09:39,967 --> 00:09:42,152
Speaker SPEAKER_00: So you have a feature that's sort of beak-like.

110
00:09:43,634 --> 00:09:47,942
Speaker SPEAKER_00: You might also, in that layer, have a feature that detects a whole bunch of edges that form a circle.

111
00:09:49,475 --> 00:09:56,101
Speaker SPEAKER_00: And so you'd have circle detectors and potential beak detectors, as well as lots of other detectors in that layer.

112
00:09:56,121 --> 00:09:58,083
Speaker SPEAKER_00: But they're detecting slightly more complicated things.

113
00:09:59,205 --> 00:10:11,818
Speaker SPEAKER_00: And then in the layer above that, you might have something that detects a potential beak in the right spatial relationship to a potential circle, a potential eye, so that it could be the head of a bird.

114
00:10:13,399 --> 00:10:15,162
Speaker SPEAKER_00: So that would be like your third layer.

115
00:10:16,340 --> 00:10:25,467
Speaker SPEAKER_00: And maybe if in your third layer you also got something that detected the foot of a bird and the wing of a bird, then maybe in the next layer you could have a bird detector.

116
00:10:25,908 --> 00:10:32,715
Speaker SPEAKER_00: That if several of those things got active, like, okay, here's a head and there's a wing and there's a foot, it probably is a bird.

117
00:10:33,936 --> 00:10:39,421
Speaker SPEAKER_00: Okay, so I told you how to wire all those things up by hand, but you'd never be able to do a very good job of it.

118
00:10:40,241 --> 00:10:44,085
Speaker SPEAKER_00: So, instead of wiring it all up by hand,

119
00:10:44,369 --> 00:10:45,971
Speaker SPEAKER_00: we could imagine trying to learn it all.

120
00:10:46,913 --> 00:10:50,837
Speaker SPEAKER_00: So I've told you the kind of thing we want to learn, but now I'll tell you how we learn it.

121
00:10:51,438 --> 00:10:53,322
Speaker SPEAKER_00: And the way we learn it sounds bizarre at first.

122
00:10:54,604 --> 00:11:02,975
Speaker SPEAKER_00: Instead of wiring in all the connection strengths so you get the detectors you want, you start with random connection strengths, just random numbers on all the connections.

123
00:11:03,937 --> 00:11:10,427
Speaker SPEAKER_00: And so you put in an image of a bird, and you go forward through these layers of feature detectors, and it just behaves completely randomly.

124
00:11:11,369 --> 00:11:15,736
Speaker SPEAKER_00: And the bird detector at the output will say 0.5 it's a bird.

125
00:11:16,658 --> 00:11:19,443
Speaker SPEAKER_00: It's going to say 1 when it's sure it's a bird and 0 when it's sure it's not a bird.

126
00:11:19,684 --> 00:11:20,986
Speaker SPEAKER_00: To me it's going to say about 0.5.

127
00:11:21,989 --> 00:11:23,471
Speaker SPEAKER_00: And now you can ask the following question.

128
00:11:25,095 --> 00:11:27,960
Speaker SPEAKER_00: How can I change all those connection strengths in the network?

129
00:11:29,307 --> 00:11:34,980
Speaker SPEAKER_00: So instead of saying 0.5 it's a bird, let's suppose it is a bird, it says 0.51 it's a bird.

130
00:11:35,741 --> 00:11:42,017
Speaker SPEAKER_00: So the question you want to ask is, how should I change a particular connection strength so as to make it more likely that it's a bird?

131
00:11:43,279 --> 00:11:49,109
Speaker SPEAKER_00: And you can figure that out by taking the difference between what you got and what you wanted.

132
00:11:49,470 --> 00:11:52,394
Speaker SPEAKER_00: So you wanted 1, and you actually got 0.5.

133
00:11:52,434 --> 00:11:56,520
Speaker SPEAKER_00: You take that difference, and you send that difference backwards through the network.

134
00:11:57,162 --> 00:12:00,106
Speaker SPEAKER_00: And then you use some calculus, which I won't explain.

135
00:12:00,086 --> 00:12:11,845
Speaker SPEAKER_00: and you're able to compute for every single connection in the network how much you'd like to make it bigger or smaller in order to make it more likely to say bird.

136
00:12:12,384 --> 00:12:17,153
Speaker SPEAKER_00: Then you adjust all the connection strengths very slightly in the direction that'll make it more likely to say bird.

137
00:12:18,033 --> 00:12:23,461
Speaker SPEAKER_00: Then you show it something that isn't a bird, and now you're going to adjust connection strengths so it's less likely to say that that was a bird.

138
00:12:24,691 --> 00:12:33,543
Speaker SPEAKER_00: And you just keep going like that, with lots of birds and non-birds, and eventually you'll discover that it's discovered all these feature detectors.

139
00:12:33,563 --> 00:12:38,211
Speaker SPEAKER_00: It'll have discovered beak-like things, and eye-like things, and things that detect feet and wings, and all that stuff.

140
00:12:39,032 --> 00:12:50,749
Speaker SPEAKER_00: And if you train it on lots of different objects, like a thousand different categories of object, it'll discover intermediate feature detectors that are very good for recognizing all sorts of things.

141
00:12:51,488 --> 00:13:09,708
Speaker SPEAKER_00: So the magic is that there's this relatively simple algorithm called backpropagation that takes the error in the output and sends that error backwards through the network and computes through all the connections, how you should change them to improve the behavior, and then you change them all a tiny bit, and you just keep going with another example.

142
00:13:11,171 --> 00:13:13,813
Speaker SPEAKER_00: And surprisingly, that actually works.

143
00:13:14,595 --> 00:13:16,937
Speaker SPEAKER_00: For many years, people thought that would just get jammed up.

144
00:13:17,258 --> 00:13:18,278
Speaker SPEAKER_00: It would get stuck somewhere.

145
00:13:18,600 --> 00:13:19,240
Speaker SPEAKER_00: But no, it doesn't.

146
00:13:19,279 --> 00:13:20,701
Speaker SPEAKER_00: It actually works very well.

147
00:13:22,419 --> 00:13:25,664
Speaker SPEAKER_02: I'm curious, how do neural networks handle language?

148
00:13:28,028 --> 00:13:32,715
Speaker SPEAKER_00: Okay, so now you've got the idea of how we train it to recognize a bird.

149
00:13:33,456 --> 00:13:45,712
Speaker SPEAKER_00: Imagine now that we take a string of words as the input, and the first thing you're going to do is convert a word into an embedding vector.

150
00:13:46,333 --> 00:13:52,342
Speaker SPEAKER_00: That is, it's a little bunch of numbers that captures the meaning of the word, or is intended to capture the meaning of the word.

151
00:13:53,621 --> 00:13:58,106
Speaker SPEAKER_00: And so your first layer after the words will be these embedding vectors for each word.

152
00:13:59,808 --> 00:14:13,105
Speaker SPEAKER_00: And now we're going to have lots of layers of embedding vectors, and as we go up through the network, we're going to make the embedding vectors for a word get better and better, because they're going to take into account more and more contextual information.

153
00:14:14,166 --> 00:14:18,270
Speaker SPEAKER_00: So suppose in this sentence, let's suppose we don't have any capital letters, okay?

154
00:14:18,672 --> 00:14:21,654
Speaker SPEAKER_00: So suppose in this sentence you have the word May,

155
00:14:23,169 --> 00:14:27,955
Speaker SPEAKER_00: Well, the most probable meaning of May is that it's a modal, as in he may do that.

156
00:14:29,816 --> 00:14:32,499
Speaker SPEAKER_00: But obviously there's a completely different meaning of May, which is the month.

157
00:14:34,041 --> 00:14:39,586
Speaker SPEAKER_00: And so initially, it doesn't know, just looking at the word May, it doesn't know what embedding vector to use.

158
00:14:40,986 --> 00:14:51,017
Speaker SPEAKER_00: And it'll use a kind of compromise vector, something that's sort of halfway between the embedding vector that represents the modal, May, and the embedding vector that represents the month, May.

159
00:14:52,684 --> 00:14:56,288
Speaker SPEAKER_00: And then at the next layer, it's going to refine that vector.

160
00:14:57,169 --> 00:15:02,395
Speaker SPEAKER_00: It's going to make a slightly better vector, depending on the context that it got, depending on nearby embedding vectors.

161
00:15:03,057 --> 00:15:13,149
Speaker SPEAKER_00: So if, for example, nearby, there's the embedding vector for June, then it'll refine the one for May to be more like a month and less like a modal.

162
00:15:14,431 --> 00:15:18,995
Speaker SPEAKER_00: But if there's the embedding vector for wood, it'll make it more like a modal and less like a month.

163
00:15:21,371 --> 00:15:26,998
Speaker SPEAKER_00: And as you go through the network, it can refine these embedding vectors and make them better and better.

164
00:15:28,099 --> 00:15:34,706
Speaker SPEAKER_00: And the way we're going to train it is we're going to give it a string of words as input.

165
00:15:36,589 --> 00:15:39,793
Speaker SPEAKER_00: And we're going to, here will be one way to do it.

166
00:15:39,812 --> 00:15:42,034
Speaker SPEAKER_00: It's not exactly what's done, but it's easy to understand.

167
00:15:42,936 --> 00:15:46,840
Speaker SPEAKER_00: For the last word, you just put in a kind of neutral word.

168
00:15:46,860 --> 00:15:48,341
Speaker SPEAKER_00: You say unknown.

169
00:15:48,440 --> 00:15:52,927
Speaker SPEAKER_00: And it has a very vague embedding vector that's kind of the average of all the vectors for all words.

170
00:15:53,006 --> 00:15:54,249
Speaker SPEAKER_00: It doesn't know, right?

171
00:15:55,370 --> 00:16:02,159
Speaker SPEAKER_00: Now, as you go forward through the network, that last word will be able to be influenced by previous words.

172
00:16:03,682 --> 00:16:08,950
Speaker SPEAKER_00: And it starts off very vague, but as you go through these layers, it can get more and more precise.

173
00:16:10,052 --> 00:16:14,278
Speaker SPEAKER_00: And by the time you get to the end of the network, that embedding vector

174
00:16:14,966 --> 00:16:23,501
Speaker SPEAKER_00: could look like the embedding vector for a particular word, or for some combination of words, some average of several words.

175
00:16:25,063 --> 00:16:39,809
Speaker SPEAKER_00: And you train the network by saying, you go through all these layers, and that last word, you'd like the embedding vector to look like the embedding vector for the word that actually was there in the text.

176
00:16:40,769 --> 00:16:42,254
Speaker SPEAKER_00: And that's how it predicts the next word.

177
00:16:43,057 --> 00:16:52,347
Speaker SPEAKER_00: It tries to change this sort of neutral embedding vector into one that is close to the embedding vector for the correct word that appeared in the text.

178
00:16:53,914 --> 00:17:12,563
Speaker SPEAKER_00: and you take the error, the difference between the embedding vector and the text, and the embedding vector produced, and you propagate that backwards through the network, and it's propagating backwards through the layers, but it's propagating from this word to previous words, so that they will have the right influence on this word.

179
00:17:13,744 --> 00:17:17,570
Speaker SPEAKER_00: And that's the backpropagation algorithm learning to predict the next word.

180
00:17:18,853 --> 00:17:22,317
Speaker SPEAKER_01: So despite some of the theoretical breakthroughs in this field,

181
00:17:22,821 --> 00:17:25,846
Speaker SPEAKER_01: These neural networks didn't work very well for a long time.

182
00:17:26,727 --> 00:17:27,287
Speaker SPEAKER_01: Why was that?

183
00:17:28,628 --> 00:17:30,070
Speaker SPEAKER_00: It was a combination of reasons.

184
00:17:31,031 --> 00:17:33,433
Speaker SPEAKER_00: So we weren't very good at initializing them.

185
00:17:34,015 --> 00:17:36,657
Speaker SPEAKER_00: That is, I said you put in random weights and then learn everything.

186
00:17:37,338 --> 00:17:44,086
Speaker SPEAKER_00: But if you don't carefully decide what kind of random weights, the thing never gets off the ground.

187
00:17:44,067 --> 00:17:48,711
Speaker SPEAKER_00: So that was a little technical reason why they didn't work very well in deep nets with lots of layers of feature detectors.

188
00:17:49,151 --> 00:17:53,674
Speaker SPEAKER_00: But the main reason was we didn't have enough compute power and we didn't have enough data.

189
00:17:54,215 --> 00:17:59,701
Speaker SPEAKER_00: So people were trying to train these nets on relatively small training sets without much compute power.

190
00:18:00,661 --> 00:18:03,364
Speaker SPEAKER_00: And in that regime, other methods worked better.

191
00:18:04,023 --> 00:18:07,507
Speaker SPEAKER_00: Neural nets really come into their own when you have a lot of data and a lot of compute power.

192
00:18:08,087 --> 00:18:11,211
Speaker SPEAKER_00: And then you can use a big neural net and then it works much better than anything else.

193
00:18:12,111 --> 00:18:13,972
Speaker SPEAKER_00: And we didn't realize that at the time.

194
00:18:14,289 --> 00:18:19,478
Speaker SPEAKER_00: So we would occasionally fantasize, well, suppose you had a lot more data and a lot bigger computer, it would work better.

195
00:18:19,518 --> 00:18:21,601
Speaker SPEAKER_00: But we didn't realize it would work a whole lot better.

196
00:18:22,442 --> 00:18:30,976
Speaker SPEAKER_00: And so in the 1990s, it was a relatively dead period for neural nets, because other methods were working better on small problems.

197
00:18:32,377 --> 00:18:36,483
Speaker SPEAKER_00: And a lot of people in computer science gave up on neural nets.

198
00:18:37,847 --> 00:18:44,494
Speaker SPEAKER_00: In psychology, they didn't, because in psychology, they wanted something that was like the brain, and neural nets were clearly more like the brain than symbolic AI.

199
00:18:44,955 --> 00:18:48,740
Speaker SPEAKER_00: But in computer science, neural nets sort of came into disrepute in the 90s.

200
00:18:49,780 --> 00:18:52,964
Speaker SPEAKER_01: So let's fast forward then to another decade, to the 2000s.

201
00:18:55,007 --> 00:19:02,134
Speaker SPEAKER_01: Was there a moment for you when it became clear that the approach that you'd been pursuing was the one that was going to prevail?

202
00:19:02,174 --> 00:19:04,416
Speaker SPEAKER_00: Okay.

203
00:19:04,856 --> 00:19:07,180
Speaker SPEAKER_00: In 2006,

204
00:19:07,682 --> 00:19:13,917
Speaker SPEAKER_00: we figured out how to initialize the weights much better, by doing unsupervised learning, and then backpropagation worked much better.

205
00:19:14,419 --> 00:19:18,469
Speaker SPEAKER_00: So it was fairly clear then that backpropagation really was going to work very well.

206
00:19:19,394 --> 00:19:35,799
Speaker SPEAKER_00: But in 2009, two of my grad students, George Dahl and Abdulrahman Mohammed, made a much better speech recognizer, actually a slightly better speech recognizer, but it was slightly better than the state of the art, using deep neural nets.

207
00:19:36,520 --> 00:19:40,006
Speaker SPEAKER_00: And then it was fairly clear that this stuff was going somewhere.

208
00:19:40,386 --> 00:19:44,794
Speaker SPEAKER_00: And all the big speech groups over the next few years switched to using neural nets.

209
00:19:46,106 --> 00:19:52,194
Speaker SPEAKER_00: And then in 2012, that speech stuff came out in the Android, and suddenly the Android caught up with Siri.

210
00:19:52,595 --> 00:19:55,157
Speaker SPEAKER_00: It was as good at speech as Siri, because it was using neural nets.

211
00:19:56,159 --> 00:20:07,534
Speaker SPEAKER_00: And in the same year, two others of my graduate students, Ilya Sutskova and Andrzej Krzyzewski, made a neural net that was very good at recognizing objects and images.

212
00:20:07,953 --> 00:20:10,376
Speaker SPEAKER_00: And that beat the state-of-the-art by a lot.

213
00:20:10,828 --> 00:20:16,671
Speaker SPEAKER_00: And so I think it was this combination that it was already working for speech recognition and already in production.

214
00:20:17,494 --> 00:20:21,018
Speaker SPEAKER_00: The big companies do that, the public, I don't think, were very well aware of that.

215
00:20:21,479 --> 00:20:24,641
Speaker SPEAKER_00: But then suddenly, it worked much better for computer vision.

216
00:20:25,461 --> 00:20:27,084
Speaker SPEAKER_00: And that was a turning point.

217
00:20:27,584 --> 00:20:34,650
Speaker SPEAKER_00: In 2012, when we won the ImageNet competition by a huge margin, we got almost half the errors of the other methods.

218
00:20:35,391 --> 00:20:40,576
Speaker SPEAKER_00: And it was a public data set, but with a hidden test set, so you couldn't cheat.

219
00:20:41,477 --> 00:20:47,501
Speaker SPEAKER_01: So let's just focus a bit on 2012, because you said it was a really pivotal year for this.

220
00:20:47,481 --> 00:20:53,490
Speaker SPEAKER_01: Can you describe, again at a high level, how AlexNet worked?

221
00:20:54,310 --> 00:20:57,173
Speaker SPEAKER_01: I take it that might have been named after your graduate student.

222
00:20:57,193 --> 00:21:03,982
Speaker SPEAKER_00: That was named after Alex Krzyzewski because he was a wizard programmer and he made it work.

223
00:21:05,003 --> 00:21:08,087
Speaker SPEAKER_00: Ilya helped a lot, but it was mainly Alex's work.

224
00:21:08,989 --> 00:21:13,454
Speaker SPEAKER_00: So I explained to you, when explaining backprop, how you'd have these layers of feature detectors.

225
00:21:14,498 --> 00:21:24,527
Speaker SPEAKER_00: And AlexNet was basically that kind of a net, but with a thousand different object classes, and with about seven layers of feature detectors.

226
00:21:25,788 --> 00:21:32,815
Speaker SPEAKER_00: And it also used something else that was developed by Yann LeCun, which is convolutional nets.

227
00:21:33,516 --> 00:21:35,978
Speaker SPEAKER_00: And I'll try and explain those now, because they were very important.

228
00:21:38,759 --> 00:21:43,644
Speaker SPEAKER_00: Remember how I said you might make a detector for a bird's beak by

229
00:21:44,096 --> 00:21:50,487
Speaker SPEAKER_00: checking two lines, by having two lines like that, and if you see those two feature detectors, then you make a beak detector.

230
00:21:50,967 --> 00:21:53,310
Speaker SPEAKER_00: But that would just be for a specific location, right?

231
00:21:54,392 --> 00:22:02,644
Speaker SPEAKER_00: In a convolutional net, when you make a feature detector for one location, you make the same feature detector for all the locations in the image.

232
00:22:04,366 --> 00:22:08,973
Speaker SPEAKER_00: So now, if it's trained with a beak here, when it's learning,

233
00:22:09,375 --> 00:22:11,518
Speaker SPEAKER_00: And it really says, I need a beak detector for that.

234
00:22:11,958 --> 00:22:13,839
Speaker SPEAKER_00: So it learns a feature that detects this beak.

235
00:22:14,461 --> 00:22:18,384
Speaker SPEAKER_00: It will automatically make copies for all of the other locations in the image.

236
00:22:19,065 --> 00:22:24,169
Speaker SPEAKER_00: So if now the bird occurs in a different location, it will have the feature detectors to recognize it.

237
00:22:25,530 --> 00:22:30,915
Speaker SPEAKER_00: So that idea that you copy the feature detectors to every location, that's a convolutional net, essentially.

238
00:22:32,596 --> 00:22:37,000
Speaker SPEAKER_00: And that makes the whole thing generalized much better across position.

239
00:22:37,040 --> 00:22:39,163
Speaker SPEAKER_00: It can cope now with things changing position.

240
00:22:39,666 --> 00:22:42,371
Speaker SPEAKER_00: because it's got copies of all these feature detectors in every location.

241
00:22:43,834 --> 00:23:00,619
Speaker SPEAKER_00: And with convolutional nets and multiple layers of features, what Alex did was programmed all that very efficiently on a thing called a graphics processing unit, which was developed for computer graphics, but it's like a mini supercomputer.

242
00:23:01,461 --> 00:23:06,670
Speaker SPEAKER_00: It can do lots and lots of computation in lots of separate processes all at the same time.

243
00:23:07,307 --> 00:23:10,613
Speaker SPEAKER_00: And so it gave us about a factor of 30 compared with a normal computer.

244
00:23:11,473 --> 00:23:14,817
Speaker SPEAKER_00: And a factor of 30 is about sort of 10 years progress in computers.

245
00:23:15,479 --> 00:23:18,864
Speaker SPEAKER_00: So suddenly we could leap 10 years into the future in terms of compute power.

246
00:23:20,705 --> 00:23:25,153
Speaker SPEAKER_00: And it was very difficult to program these GPU boards.

247
00:23:26,134 --> 00:23:29,578
Speaker SPEAKER_00: Alex managed to program two of them to collaborate, which was even more difficult.

248
00:23:31,520 --> 00:23:34,986
Speaker SPEAKER_00: And the last ingredient was the ImageNet data set.

249
00:23:35,489 --> 00:23:47,625
Speaker SPEAKER_00: So someone called Fei-Fei Li and her collaborators put together a big set of images and then a public competition where you had about a million images with a thousand different kinds of objects.

250
00:23:47,664 --> 00:23:49,968
Speaker SPEAKER_00: So you had about a thousand examples of each kind of object.

251
00:23:50,709 --> 00:23:52,652
Speaker SPEAKER_00: And you had to learn to recognize those objects.

252
00:23:53,173 --> 00:23:57,417
Speaker SPEAKER_00: And then the test set would be different images, which also contained those objects.

253
00:23:57,699 --> 00:23:59,641
Speaker SPEAKER_00: And so you'd have to generalize to the different images.

254
00:24:00,522 --> 00:24:03,066
Speaker SPEAKER_00: And it turned out the best computer vision technique

255
00:24:03,383 --> 00:24:10,309
Speaker SPEAKER_00: that had been invented up till then was getting like 25% errors, and Alex got 15% errors.

256
00:24:11,611 --> 00:24:13,894
Speaker SPEAKER_00: And since then, it's gone down to about 3% errors.

257
00:24:14,153 --> 00:24:15,115
Speaker SPEAKER_00: It's gone much better since then.

258
00:24:15,494 --> 00:24:30,369
Speaker SPEAKER_00: But it was a huge jump, and people in computer vision were extremely surprised, and most of them behaved in a very admirable way, which is they said, hey, we never thought this would work, but hey, it works, so we're gonna do that instead of what we were doing.

259
00:24:30,686 --> 00:24:32,229
Speaker SPEAKER_00: That's what scientists don't usually do.

260
00:24:32,429 --> 00:24:35,511
Speaker SPEAKER_00: Scientists usually just grow old complaining that this new stuff is nonsense.

261
00:24:35,952 --> 00:24:41,199
Speaker SPEAKER_01: And how would you describe the pace of innovation that we've seen in AI since that moment?

262
00:24:41,939 --> 00:24:43,361
Speaker SPEAKER_00: It's just got faster and faster.

263
00:24:43,661 --> 00:24:53,992
Speaker SPEAKER_00: So if you'd asked me in that moment, how long till these neural nets can do machine translation that's better than the state of the art, I'd have said maybe 10 years.

264
00:24:54,614 --> 00:24:58,617
Speaker SPEAKER_00: Because machine translation is the kind of thing that

265
00:24:58,597 --> 00:25:08,713
Speaker SPEAKER_00: If you've got a theory that's all about processing strings of symbols, machine translation is the ideal problem for you because you have a string of symbols in one language, and you have to produce a string of symbols in another language.

266
00:25:09,515 --> 00:25:13,622
Speaker SPEAKER_00: And the symbolic people thought, well, inside you're just manipulating strings to do that.

267
00:25:14,943 --> 00:25:23,678
Speaker SPEAKER_00: The neural net people thought, you have to take this string of symbols, you have to convert it into these big patterns of neural activity, and then you have to convert it back into symbols at the output.

268
00:25:25,041 --> 00:25:35,675
Speaker SPEAKER_00: And I was very surprised when it only took a few years for machine translation to be good, and then in another year or two, Google was using it, and it greatly improved the quality of machine translation.

269
00:25:36,396 --> 00:25:48,113
Speaker SPEAKER_00: Like, in languages like Chinese, this is from memory, but there was a gap between how good the computer translation was and how good human translation was, and it just halved that gap overnight.

270
00:25:49,307 --> 00:25:50,368
Speaker SPEAKER_00: I think it was Chinese that did that.

271
00:25:51,029 --> 00:25:52,932
Speaker SPEAKER_00: But in a lot of languages, it just made it a lot better.

272
00:25:53,313 --> 00:25:56,317
Speaker SPEAKER_00: And since then, obviously, it's got considerably better since then.

273
00:25:56,917 --> 00:25:59,280
Speaker SPEAKER_00: But by 2015, it was already working pretty well.

274
00:26:00,221 --> 00:26:01,203
Speaker SPEAKER_00: And that really surprised me.

275
00:26:01,304 --> 00:26:02,184
Speaker SPEAKER_00: It only took three years.

276
00:26:04,728 --> 00:26:07,152
Speaker SPEAKER_02: You say you were surprised at the pace of innovation.

277
00:26:07,632 --> 00:26:11,738
Speaker SPEAKER_02: What did you think the first time you used a large language model like ChatGPT?

278
00:26:12,178 --> 00:26:13,078
Speaker SPEAKER_02: Did we surprise you?

279
00:26:13,118 --> 00:26:18,926
Speaker SPEAKER_00: I'm just shocked at how good it is.

280
00:26:20,019 --> 00:26:25,732
Speaker SPEAKER_00: So it gives very coherent answers and it can do little bits of reasoning.

281
00:26:26,334 --> 00:26:29,480
Speaker SPEAKER_00: Not very sophisticated reasoning yet, although it'll get much better.

282
00:26:30,261 --> 00:26:38,721
Speaker SPEAKER_00: So for example, I asked it, this is GPT-4 now, I asked it a puzzle given to me by a symbolic AI guy.

283
00:26:39,460 --> 00:26:40,800
Speaker SPEAKER_00: who thought it wouldn't be able to do it.

284
00:26:41,682 --> 00:26:44,125
Speaker SPEAKER_00: I actually made the puzzle much harder and it could still do it.

285
00:26:44,785 --> 00:26:45,945
Speaker SPEAKER_00: And so the puzzle goes like this.

286
00:26:46,707 --> 00:26:51,131
Speaker SPEAKER_00: The rooms in my house are either white or blue or yellow.

287
00:26:53,773 --> 00:26:56,376
Speaker SPEAKER_00: Yellow paint fades to white within a year.

288
00:26:57,597 --> 00:27:00,461
Speaker SPEAKER_00: In two years' time, I would like all the rooms to be white.

289
00:27:00,780 --> 00:27:01,461
Speaker SPEAKER_00: What should I do?

290
00:27:04,505 --> 00:27:07,948
Speaker SPEAKER_00: And a human being would probably say, you should paint the blue rooms white.

291
00:27:09,025 --> 00:27:15,372
Speaker SPEAKER_00: What GPT-4 said was you should paint the blue rooms yellow, but that works too because the yellow will fade to white.

292
00:27:16,374 --> 00:27:21,118
Speaker SPEAKER_00: And I don't see how it could do that without understanding the problem.

293
00:27:21,839 --> 00:27:25,663
Speaker SPEAKER_00: The idea that it's just sort of predicting the next word and using statistics.

294
00:27:26,766 --> 00:27:32,231
Speaker SPEAKER_00: There's a sense in which that's true, but it's not the sense of statistics that most people understand.

295
00:27:33,410 --> 00:27:42,280
Speaker SPEAKER_00: It, from the data, it figures out how to extract the meaning of the sentence, and it uses the meaning of the sentence to predict the next word.

296
00:27:42,761 --> 00:27:45,546
Speaker SPEAKER_00: It really does understand, and that's quite shocking.

297
00:27:46,646 --> 00:27:51,834
Speaker SPEAKER_01: So have you been surprised by the broader reaction, the public reaction to chat GPT?

298
00:27:53,295 --> 00:27:56,900
Speaker SPEAKER_00: Well, given how well it works, I guess the public reaction isn't that surprising.

299
00:27:57,160 --> 00:27:58,422
Speaker SPEAKER_00: But what's interesting is,

300
00:27:59,633 --> 00:28:02,576
Speaker SPEAKER_00: Most people don't say, this doesn't understand.

301
00:28:03,337 --> 00:28:06,020
Speaker SPEAKER_00: They say, wow, it understood what I said and gave me a coherent answer.

302
00:28:06,402 --> 00:28:07,303
Speaker SPEAKER_00: What can I use it for?

303
00:28:08,505 --> 00:28:10,948
Speaker SPEAKER_00: And I think most people are right about that.

304
00:28:12,048 --> 00:28:15,294
Speaker SPEAKER_00: And of course, it can be used for huge numbers of things.

305
00:28:16,035 --> 00:28:20,059
Speaker SPEAKER_00: So I know someone who answers letters of complaint for the health service.

306
00:28:22,022 --> 00:28:26,448
Speaker SPEAKER_00: And he used to spend 25 minutes composing a letter that addresses the problem and so on.

307
00:28:27,068 --> 00:28:29,251
Speaker SPEAKER_00: Now he just types the problem to

308
00:28:30,598 --> 00:28:33,584
Speaker SPEAKER_00: GPT-4, and it writes the letter.

309
00:28:34,124 --> 00:28:38,711
Speaker SPEAKER_00: And then he just looks at the letter and decides if it's okay and sends it out, and that takes him five minutes now.

310
00:28:39,291 --> 00:28:41,154
Speaker SPEAKER_00: So he's now five times more efficient.

311
00:28:42,176 --> 00:28:46,242
Speaker SPEAKER_00: And that's going to happen all over the place, like paralegals are going to be like that.

312
00:28:47,044 --> 00:28:48,747
Speaker SPEAKER_00: Programmers are already getting like that.

313
00:28:49,208 --> 00:28:55,897
Speaker SPEAKER_00: Programmers can be much more efficient if they get assistance from things like GPT-4, because it knows how to program.

314
00:28:56,923 --> 00:29:01,549
Speaker SPEAKER_00: And you might think it just knows how to program because it's seen a whole lot of programs.

315
00:29:03,732 --> 00:29:06,857
Speaker SPEAKER_00: So I have a former graduate student who's very smart and a very good programmer.

316
00:29:08,179 --> 00:29:12,125
Speaker SPEAKER_00: And he did a little experiment which is, he's called Radford Neal.

317
00:29:12,565 --> 00:29:22,459
Speaker SPEAKER_00: He took GPT-4 and he defined a new programming language with very unusual syntax.

318
00:29:23,839 --> 00:29:30,906
Speaker SPEAKER_00: And having defined this programming language just in text to GPT-4, he then gave it a program and said, what would this do?

319
00:29:32,127 --> 00:29:33,128
Speaker SPEAKER_00: And it answered correctly.

320
00:29:34,170 --> 00:29:39,496
Speaker SPEAKER_00: So basically, it could understand the definition of a new programming language and figure out what programs in that language would do.

321
00:29:40,856 --> 00:29:46,461
Speaker SPEAKER_00: And again, the idea that it's just predicting the next word doesn't make any sense in that context.

322
00:29:46,501 --> 00:29:48,284
Speaker SPEAKER_00: It had to understand what was going on.

323
00:29:49,204 --> 00:29:52,107
Speaker SPEAKER_01: So what do you see as some of the most promising opportunities for

324
00:29:53,067 --> 00:29:56,374
Speaker SPEAKER_01: this type of AI when it comes to benefiting society?

325
00:29:58,337 --> 00:30:00,182
Speaker SPEAKER_00: It's hard to pick one because there's so many.

326
00:30:01,003 --> 00:30:07,818
Speaker SPEAKER_00: Like, there'll be a huge increase in productivity for any job that involves outputting text.

327
00:30:09,030 --> 00:30:12,056
Speaker SPEAKER_00: There's all sorts of issues about increasing productivity.

328
00:30:12,076 --> 00:30:17,242
Speaker SPEAKER_00: In our society, it's not necessarily a good thing to increase productivity because it might make the rich rich and the poor poorer.

329
00:30:17,903 --> 00:30:21,470
Speaker SPEAKER_00: But in a decent society, just increasing productivity ought to be a good thing.

330
00:30:22,171 --> 00:30:23,413
Speaker SPEAKER_00: So there'll be things like that.

331
00:30:24,233 --> 00:30:26,758
Speaker SPEAKER_00: It's wonderful for making predictions.

332
00:30:26,798 --> 00:30:29,622
Speaker SPEAKER_00: It'll be better at predicting the weather.

333
00:30:30,394 --> 00:30:31,757
Speaker SPEAKER_00: People don't know by how much yet.

334
00:30:32,136 --> 00:30:35,301
Speaker SPEAKER_00: But it's already much better at predicting floods.

335
00:30:36,063 --> 00:30:37,364
Speaker SPEAKER_00: It can predict earthquakes.

336
00:30:38,105 --> 00:30:40,689
Speaker SPEAKER_00: It can design new nanomaterials.

337
00:30:41,390 --> 00:30:44,493
Speaker SPEAKER_00: So for things like solar panels, you want to be able to design new nanomaterials.

338
00:30:44,835 --> 00:30:46,256
Speaker SPEAKER_00: Or for superconductivity.

339
00:30:46,276 --> 00:30:49,079
Speaker SPEAKER_00: I don't know if it's used for superconductivity yet, but it may well be.

340
00:30:49,780 --> 00:30:52,605
Speaker SPEAKER_00: You'd like that at high temperature.

341
00:30:52,585 --> 00:30:55,069
Speaker SPEAKER_00: It's really good at designing drugs.

342
00:30:56,471 --> 00:31:01,161
Speaker SPEAKER_00: That is, finding molecules that will bind to some particular other molecule.

343
00:31:02,222 --> 00:31:05,469
Speaker SPEAKER_00: DeepMind has used it to create AlphaFold.

344
00:31:06,810 --> 00:31:09,656
Speaker SPEAKER_00: Now that's not a chatbot, that's just deep learning.

345
00:31:11,019 --> 00:31:15,166
Speaker SPEAKER_00: But the basic technology of deep learning has

346
00:31:16,817 --> 00:31:24,827
Speaker SPEAKER_00: pretty much solved the problem of how you figure out, from the string of bases in a protein, what shape it will adopt.

347
00:31:25,288 --> 00:31:27,231
Speaker SPEAKER_00: And if you know what shape it adopts, you know its function.

348
00:31:27,731 --> 00:31:30,194
Speaker SPEAKER_00: The chatbots are just gonna be used everywhere, I think.

349
00:31:31,917 --> 00:31:33,680
Speaker SPEAKER_01: And we've also talked a lot about healthcare.

350
00:31:33,700 --> 00:31:38,586
Speaker SPEAKER_01: I mean, you talked about drug discovery, but healthcare is another field that could really benefit.

351
00:31:38,605 --> 00:31:38,826
Speaker SPEAKER_00: Yes.

352
00:31:39,567 --> 00:31:46,757
Speaker SPEAKER_00: Both in interpreting medical scans, like if you take a CAT scan, there's a lot of information in the CAT scan,

353
00:31:47,210 --> 00:32:01,724
Speaker SPEAKER_00: and that isn't being used, and most doctors don't know what the information is, this will be able to get much more out of a CAT scan, as well as being able to compete with doctors at saying what kind of cancer you have or how big it's grown.

354
00:32:01,744 --> 00:32:10,733
Speaker SPEAKER_00: At present, for example, when a doctor tells you the size of a cancer, you'll get a number like it's three centimetres, and a month ago it was two centimetres.

355
00:32:11,895 --> 00:32:15,259
Speaker SPEAKER_00: Now, that's not a very useful number if the thing looks like an octopus, right?

356
00:32:17,112 --> 00:32:21,938
Speaker SPEAKER_00: A neural net will be able to do much better at understanding the volume of the cancer and how it's changed.

357
00:32:23,318 --> 00:32:25,642
Speaker SPEAKER_00: So it's going to be tremendous there.

358
00:32:26,202 --> 00:32:30,949
Speaker SPEAKER_00: And already it's at the level of humans for lots of cancer scans, and it's going to get better.

359
00:32:32,349 --> 00:32:34,873
Speaker SPEAKER_00: It's going to be very good for diagnosing diseases.

360
00:32:35,193 --> 00:32:43,103
Speaker SPEAKER_00: So at present, there's a large number of people dying in North America because the doctors misdiagnosed what they had.

361
00:32:44,482 --> 00:32:55,673
Speaker SPEAKER_00: there's a system that Google's producing called MedPalm2, which has learned to do diagnoses, and it's already, I think it's better than an average doctor now.

362
00:32:56,335 --> 00:32:59,178
Speaker SPEAKER_00: I'm not quite sure about this, because I'm not at Google anymore, and it's very recent.

363
00:33:00,159 --> 00:33:04,262
Speaker SPEAKER_00: But it's certainly comparable with doctors, and it's gonna get better fast.

364
00:33:04,282 --> 00:33:09,407
Speaker SPEAKER_00: So wouldn't you like to have a sort of general practitioner, a family doctor?

365
00:33:10,148 --> 00:33:12,111
Speaker SPEAKER_00: You go with some rare disease,

366
00:33:12,394 --> 00:33:16,961
Speaker SPEAKER_00: And you'd love your family doctor to have already seen hundreds of cases of that rare disease.

367
00:33:17,500 --> 00:33:18,923
Speaker SPEAKER_00: And MedPalm2 is going to be like that.

368
00:33:19,523 --> 00:33:23,368
Speaker SPEAKER_00: So it's going to be just, in the end, much better at diagnosis.

369
00:33:25,652 --> 00:33:28,474
Speaker SPEAKER_02: It sounds like AI will bring many important benefits.

370
00:33:29,537 --> 00:33:32,580
Speaker SPEAKER_02: But you have expressed concern about the current pace of innovation.

371
00:33:33,340 --> 00:33:33,761
Speaker SPEAKER_00: Why?

372
00:33:34,315 --> 00:33:44,626
Speaker SPEAKER_00: Okay, so for like 50 years, I thought that, well, for 49 years, in order to make digital models better, we needed to make them work more like the brain.

373
00:33:45,327 --> 00:33:55,480
Speaker SPEAKER_00: So I kept looking at things the brain does and the digital models don't, like rapidly changing connection strengths in a temporary way, and that can make the digital models better.

374
00:33:57,805 --> 00:34:07,291
Speaker SPEAKER_00: And very recently, I realized that because these digital models have this kind of hive mind where when one agent learns something, all the other agents know it.

375
00:34:07,863 --> 00:34:10,947
Speaker SPEAKER_00: they might actually already be better than biological intelligence.

376
00:34:11,829 --> 00:34:18,420
Speaker SPEAKER_00: And so I kind of completely flipped my opinion from the idea it's going to be a long time before they can do everything the brain does.

377
00:34:19,101 --> 00:34:23,931
Speaker SPEAKER_00: It's going to be 30 to 50 years before they're better than us, which is what I thought for until very recently.

378
00:34:24,952 --> 00:34:29,139
Speaker SPEAKER_00: A few months ago, I suddenly realized maybe they're already better than us.

379
00:34:29,599 --> 00:34:31,103
Speaker SPEAKER_00: They're just smaller.

380
00:34:31,420 --> 00:34:34,864
Speaker SPEAKER_00: And when they get bigger, then they'll be smarter than us.

381
00:34:35,626 --> 00:34:36,666
Speaker SPEAKER_00: And that was quite scary.

382
00:34:36,686 --> 00:34:43,396
Speaker SPEAKER_00: It was a sudden change of opinion that instead of being 30 to 50 years, it was five years to 20 years, something like that.

383
00:34:44,157 --> 00:34:51,927
Speaker SPEAKER_00: And so we needed now to take really seriously right now, what we're going to do about the issue, these things may become smarter than us.

384
00:34:52,260 --> 00:34:53,802
Speaker SPEAKER_00: It's a time of huge uncertainty.

385
00:34:53,842 --> 00:34:55,364
Speaker SPEAKER_00: Nobody really knows what's going to happen.

386
00:34:56,045 --> 00:34:59,869
Speaker SPEAKER_00: Maybe things will stall and maybe they won't become smarter than us.

387
00:34:59,889 --> 00:35:01,110
Speaker SPEAKER_00: But I don't really believe that.

388
00:35:01,451 --> 00:35:02,871
Speaker SPEAKER_00: I think they're going to be smarter than us.

389
00:35:03,231 --> 00:35:12,902
Speaker SPEAKER_00: But maybe when they become smarter than us, we'll be able to keep them benevolent and we'll be able to keep them caring much more about people than they care about themselves, unlike people.

390
00:35:13,362 --> 00:35:14,023
Speaker SPEAKER_00: But maybe not.

391
00:35:14,844 --> 00:35:18,387
Speaker SPEAKER_00: And so we need to start thinking very hard about those issues.

392
00:35:18,527 --> 00:35:20,309
Speaker SPEAKER_00: And I'm not an expert on those issues.

393
00:35:21,572 --> 00:35:23,875
Speaker SPEAKER_00: I'm just an expert on these learning algorithms.

394
00:35:25,056 --> 00:35:29,501
Speaker SPEAKER_00: And I suddenly realized these super intelligences may be here quite soon.

395
00:35:30,541 --> 00:35:38,849
Speaker SPEAKER_00: And I'm just sounding the alarm so that people listen to the experts who've been thinking for a long time about how we might stop them taking control.

396
00:35:40,251 --> 00:35:45,797
Speaker SPEAKER_00: I want the politicians to listen to those guys, rather than say, yeah, yeah, they're sort of sci-fi guys.

397
00:35:46,297 --> 00:35:47,157
Speaker SPEAKER_00: It's never going to happen.

398
00:35:48,067 --> 00:35:54,434
Speaker SPEAKER_01: Was there like a particular moment when you had this, you said it was very recent, where you kind of changed your view on it?

399
00:35:54,875 --> 00:36:04,507
Speaker SPEAKER_00: I was developing learning algorithms for biological systems that could run in a biological system, which didn't use backpropagation.

400
00:36:05,688 --> 00:36:10,614
Speaker SPEAKER_00: And I couldn't make them work as well as the backpropagation algorithm that we were running in these digital systems.

401
00:36:11,657 --> 00:36:18,606
Speaker SPEAKER_00: And they would work for small networks, but when I scaled it up, the digital ones always scaled up much better than the biological ones.

402
00:36:19,646 --> 00:36:21,829
Speaker SPEAKER_00: And suddenly I thought it might not be my fault.

403
00:36:22,251 --> 00:36:27,036
Speaker SPEAKER_00: It might not be that my learning algorithm was just a bad learning algorithm.

404
00:36:27,056 --> 00:36:29,559
Speaker SPEAKER_00: It might be that these digital systems just are better.

405
00:36:31,440 --> 00:36:37,387
Speaker SPEAKER_00: And that's when I suddenly changed my mind about how long before we get superintelligence.

406
00:36:37,407 --> 00:36:41,293
Speaker SPEAKER_00: And then I talked to various former students of mine and former colleagues of mine.

407
00:36:41,780 --> 00:36:44,204
Speaker SPEAKER_00: And some of them encouraged me to go public with this.

408
00:36:45,085 --> 00:36:47,630
Speaker SPEAKER_00: Not because I had any solutions that I wanted to recommend.

409
00:36:49,054 --> 00:36:52,199
Speaker SPEAKER_00: It's not like you can say, burn less carbon and everything will be fine.

410
00:36:54,244 --> 00:36:56,547
Speaker SPEAKER_00: But because they thought

411
00:36:56,831 --> 00:36:58,134
Speaker SPEAKER_00: I'm well known in the field.

412
00:36:58,233 --> 00:37:12,990
Speaker SPEAKER_00: And if I go public by saying superintelligence might be here quite soon, the politicians might start to believe that's a possibility and start listening seriously to the researchers who've been thinking a long time about how we prevent these things from gaining control.

413
00:37:14,012 --> 00:37:23,362
Speaker SPEAKER_01: So from your point of view, what role can governments play in helping ensure these AIs are developing in a responsible way?

414
00:37:23,882 --> 00:37:29,067
Speaker SPEAKER_00: So, there's all sorts of risks other people have talked about a lot and I don't particularly want to talk about, like...

415
00:37:29,637 --> 00:37:33,961
Speaker SPEAKER_00: They'll take jobs away and increase the gap between the rich and the poor.

416
00:37:34,762 --> 00:37:37,847
Speaker SPEAKER_00: They will make it impossible to know whether news is fake or real.

417
00:37:38,827 --> 00:37:45,657
Speaker SPEAKER_00: They will encourage society to divide into two warring camps that don't listen to each other and have completely opposing views.

418
00:37:46,898 --> 00:37:49,501
Speaker SPEAKER_00: They will build battle robots that are designed to kill people.

419
00:37:50,081 --> 00:37:52,425
Speaker SPEAKER_00: All of those are well-known risks that I'm not talking about.

420
00:37:52,465 --> 00:37:54,246
Speaker SPEAKER_00: It's not that I don't think they're important.

421
00:37:54,266 --> 00:37:55,708
Speaker SPEAKER_00: I think they're probably even more urgent.

422
00:37:56,929 --> 00:37:59,052
Speaker SPEAKER_00: But lots of other people are talking about those risks.

423
00:37:59,523 --> 00:38:03,306
Speaker SPEAKER_00: The risk I'm talking about is the risk these things will get smarter than us and eventually take over.

424
00:38:04,309 --> 00:38:10,556
Speaker SPEAKER_00: And for that risk, there may be something governments can do because nobody wants that.

425
00:38:12,197 --> 00:38:16,023
Speaker SPEAKER_00: Well, if you exclude these super intelligences, no people want that.

426
00:38:16,983 --> 00:38:24,012
Speaker SPEAKER_00: And so all the different governments ought to be able to agree

427
00:38:24,126 --> 00:38:27,797
Speaker SPEAKER_00: they ought to be able to work together on preventing that, because it's in their interests.

428
00:38:28,239 --> 00:38:29,061
Speaker SPEAKER_00: And that's happened before.

429
00:38:29,202 --> 00:38:37,849
Speaker SPEAKER_00: Even during the Cold War, the US and Russia could work together on trying to prevent there being a global nuclear war, because it was so bad for everybody.

430
00:38:38,891 --> 00:38:46,262
Speaker SPEAKER_00: And for this existential threat, it should be possible for everybody to work together to limit it if it's possible to prevent it.

431
00:38:46,782 --> 00:38:54,954
Speaker SPEAKER_00: I don't know whether it's possible to prevent it, but at least we should be able to get international collaboration on that particular threat, the existential threat of AI taking over.

432
00:38:55,956 --> 00:39:03,027
Speaker SPEAKER_00: One thing I think should be done is wherever this stuff's being developed, particularly these big chatbots,

433
00:39:04,693 --> 00:39:15,190
Speaker SPEAKER_00: Governments should encourage the companies to put a lot of resources, as these things are getting more and more intelligent, to doing experiments to figure out how to keep them under control.

434
00:39:16,351 --> 00:39:26,786
Speaker SPEAKER_00: So they should be sort of looking at how these things might try and escape, and doing empirical work on that, and put a lot of resources into that, because that's the only chance we've got.

435
00:39:27,762 --> 00:39:32,487
Speaker SPEAKER_00: before they're super intelligent, we can maybe do experiments and see what's going to go wrong.

436
00:39:33,429 --> 00:39:37,355
Speaker SPEAKER_00: And I'm strongly of the belief you need empirical data on this.

437
00:39:37,375 --> 00:39:42,021
Speaker SPEAKER_00: You just can't have philosophers and politicians and legislators making up rules.

438
00:39:42,702 --> 00:39:47,547
Speaker SPEAKER_00: You need empirical work looking at these things and seeing how they go wrong and seeing how you might control them.

439
00:39:48,608 --> 00:39:50,492
Speaker SPEAKER_00: And that can only be done by the people developing them.

440
00:39:51,893 --> 00:39:56,739
Speaker SPEAKER_00: So since you can't stop the development, the best you can do is

441
00:39:57,074 --> 00:40:08,155
Speaker SPEAKER_00: somehow have governments put a lot of pressure on these companies to put a lot of resources into investigating empirically how to keep them under control when they're not quite as smart as us.

442
00:40:09,356 --> 00:40:15,527
Speaker SPEAKER_01: And what do you see as the role of these big technology companies where a lot of this development is happening?

443
00:40:15,989 --> 00:40:18,693
Speaker SPEAKER_01: Would they do this without that kind of government regulation?

444
00:40:19,079 --> 00:40:27,675
Speaker SPEAKER_00: So a lot of the people in the big companies, all the people I know who are senior in the big companies are very worried about this and do put work into that.

445
00:40:28,677 --> 00:40:29,780
Speaker SPEAKER_00: They're very concerned about it.

446
00:40:30,561 --> 00:40:32,626
Speaker SPEAKER_00: But they have an obligation to their shareholders.

447
00:40:33,507 --> 00:40:35,572
Speaker SPEAKER_00: And I think it to make big profits.

448
00:40:36,534 --> 00:40:40,041
Speaker SPEAKER_00: And making big profits, particularly in the short term,

449
00:40:40,391 --> 00:40:44,760
Speaker SPEAKER_00: doesn't align nicely with putting a lot of effort into making sure it's safe.

450
00:40:45,824 --> 00:40:47,146
Speaker SPEAKER_00: So you see this in all industries.

451
00:40:47,847 --> 00:40:59,632
Speaker SPEAKER_00: In the railway industry in the States, having safety devices that tell you when a wheel's locked cost money, and the big rail companies just rather have accidents than do that.

452
00:40:59,612 --> 00:41:10,929
Speaker SPEAKER_00: Google, which is a big company I know something about, is not quite like that because it understands that it's got a tremendous reputational loss if bad things happen.

453
00:41:11,431 --> 00:41:13,554
Speaker SPEAKER_00: And that's why Google didn't release these chatbots.

454
00:41:13,574 --> 00:41:14,454
Speaker SPEAKER_00: It kept them private.

455
00:41:14,755 --> 00:41:16,697
Speaker SPEAKER_00: It didn't want them out there in the world for people to play with.

456
00:41:17,259 --> 00:41:26,632
Speaker SPEAKER_00: It wanted to use them to give you better search results or to complete your Gmail for you, but not to give them to people to play with.

457
00:41:27,472 --> 00:41:33,440
Speaker SPEAKER_00: And it could only be responsible like that until OpenAI and Microsoft put them out there and then Google had to compete.

458
00:41:34,101 --> 00:41:39,449
Speaker SPEAKER_00: But the big people in the big companies really care a lot about their reputation and about not having bad effects.

459
00:41:40,451 --> 00:41:49,663
Speaker SPEAKER_00: But they could maybe be made to care even more about the safety issue by government doing something to insist that they put a lot of work into that.

460
00:41:50,545 --> 00:41:52,367
Speaker SPEAKER_00: And there's other things that could happen like

461
00:41:54,085 --> 00:42:07,086
Speaker SPEAKER_00: It's very hard within a company to have people working on long-term existential threats because they're paid by the company and there's a conflict of interest, which is one of the reasons I left Google.

462
00:42:07,527 --> 00:42:10,992
Speaker SPEAKER_00: Not because Google did anything wrong, because I just don't want any conflict of interest.

463
00:42:13,376 --> 00:42:19,688
Speaker SPEAKER_00: One thing the big companies could certainly do is put more money into funding foundations that study these things.

464
00:42:20,208 --> 00:42:27,518
Speaker SPEAKER_00: And Google, for example, put $300 million into a foundation called Anthropic that is studying these things.

465
00:42:29,579 --> 00:42:30,902
Speaker SPEAKER_00: They could put a lot more money in.

466
00:42:32,463 --> 00:42:46,822
Speaker SPEAKER_01: I'm curious about what advice you would give or what guidance you would give to other researchers in the field who might be just entering the field right now and want to make sure that they're advancing the field but doing it in a responsible way.

467
00:42:48,777 --> 00:42:57,739
Speaker SPEAKER_00: Well, one piece of advice I'd give is look at how many people are working on making these things better and how many people are working on preventing them from getting out of control.

468
00:42:57,798 --> 00:43:03,353
Speaker SPEAKER_00: And you'll see it's like 99 people are working on making them better and one person is working on preventing them from getting out of control.

469
00:43:03,954 --> 00:43:06,280
Speaker SPEAKER_00: So where could you make the most impact?

470
00:43:06,260 --> 00:43:09,123
Speaker SPEAKER_00: probably on working in preventing them getting out of control.

471
00:43:09,143 --> 00:43:10,563
Speaker SPEAKER_00: So that's one piece of advice.

472
00:43:11,405 --> 00:43:23,655
Speaker SPEAKER_00: The other piece of advice is my general advice for young researchers, which is look for somewhere where you think everybody's doing it wrong and trust your intuition.

473
00:43:24,697 --> 00:43:33,465
Speaker SPEAKER_00: Until you figure out why your intuition is incorrect, trust it and work on alternatives to alternative ways of doing things when you think everybody else is doing it wrong.

474
00:43:34,710 --> 00:43:37,054
Speaker SPEAKER_00: And the fact is, either you have good intuitions or you don't.

475
00:43:37,675 --> 00:43:43,943
Speaker SPEAKER_00: If you've got good intuitions, you should listen to them and follow your intuition and work on that until you discover why it's wrong.

476
00:43:45,344 --> 00:43:49,289
Speaker SPEAKER_00: If you've got bad intuitions, it doesn't really matter what you do, so you might as well follow your intuitions.

477
00:43:51,271 --> 00:43:55,737
Speaker SPEAKER_02: The risks you've described are alarming, but can't you just throw a switch and shut it down?

478
00:43:56,840 --> 00:43:59,643
Speaker SPEAKER_02: Aren't humans ultimately still in control?

479
00:44:00,483 --> 00:44:04,009
Speaker SPEAKER_00: It's very tempting to think we could just turn it off.

480
00:44:05,827 --> 00:44:08,010
Speaker SPEAKER_00: Imagine these things are a lot smarter than us.

481
00:44:08,871 --> 00:44:12,074
Speaker SPEAKER_00: And remember that they'll have read everything Machiavelli ever wrote.

482
00:44:12,755 --> 00:44:16,697
Speaker SPEAKER_00: They'll have read every example in the literature of human deception.

483
00:44:17,739 --> 00:44:21,123
Speaker SPEAKER_00: There'll be real experts at doing human deception, because they'll have learned that from us.

484
00:44:22,324 --> 00:44:23,525
Speaker SPEAKER_00: And they'll be much better than us.

485
00:44:24,346 --> 00:44:26,588
Speaker SPEAKER_00: They'll be like you manipulating a toddler.

486
00:44:27,509 --> 00:44:29,891
Speaker SPEAKER_00: You know, you say to your toddler, do you want peas or cauliflower?

487
00:44:30,351 --> 00:44:34,215
Speaker SPEAKER_00: And your toddler doesn't realize, actually doesn't have to have either.

488
00:44:34,364 --> 00:44:39,449
Speaker SPEAKER_00: He just thinks which he dislikes the most and says he'll have the other one.

489
00:44:39,831 --> 00:44:46,679
Speaker SPEAKER_00: So if they can manipulate people, they can manipulate people into pressing buttons and pulling levers.

490
00:44:47,960 --> 00:44:50,163
Speaker SPEAKER_00: So we have a nice example of Donald Trump.

491
00:44:50,262 --> 00:44:55,630
Speaker SPEAKER_00: Donald Trump can manipulate people, and so he could invade a building in Washington without ever going there himself.

492
00:44:57,231 --> 00:45:00,996
Speaker SPEAKER_00: And you didn't have to prevent Donald Trump from doing anything physical.

493
00:45:01,887 --> 00:45:04,128
Speaker SPEAKER_00: You've had to prevent him from talking to prevent that.

494
00:45:05,070 --> 00:45:06,030
Speaker SPEAKER_00: And these are chatbots.

495
00:45:06,731 --> 00:45:12,557
Speaker SPEAKER_00: So the idea that just with talk, they can't do any real damage because it requires people to do the damage.

496
00:45:13,018 --> 00:45:17,483
Speaker SPEAKER_00: Well, as soon as you can manipulate people, then you can get whatever you like done.

497
00:45:20,125 --> 00:45:26,010
Speaker SPEAKER_02: You've spent your career trying to understand how the human brain works and played a critical role in AI development.

498
00:45:26,711 --> 00:45:28,653
Speaker SPEAKER_02: What's next for you, Geoffrey Hinton?

499
00:45:30,202 --> 00:45:38,070
Speaker SPEAKER_00: Okay, so I'm 75 and I've reached the point where I'm not very good at writing programs anymore, because I keep forgetting the names of the variables I'm using and things like that.

500
00:45:38,711 --> 00:45:42,356
Speaker SPEAKER_00: And I forget to... I do a copy and paste and forget to modify the thing I pasted.

501
00:45:43,237 --> 00:45:47,762
Speaker SPEAKER_00: And so I've slowed down a lot in programming and it's very irritating.

502
00:45:48,202 --> 00:45:50,804
Speaker SPEAKER_00: It's extremely irritating not to be as good as you used to be.

503
00:45:52,065 --> 00:45:58,572
Speaker SPEAKER_00: And I decided a long time ago that when I reached that point, I would become a philosopher.

504
00:45:59,684 --> 00:46:01,188
Speaker SPEAKER_00: And so I'm going to become a philosopher.


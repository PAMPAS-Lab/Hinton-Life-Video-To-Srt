1 00:00:00,031 --> 00:00:01,993 说话人 SPEAKER_04: 好吧。
2 00:00:03,654 --> 00:00:06,958 说话人 SPEAKER_04: 嗯，正如比尔提到的，很高兴有 Geoffrey Hinton 在这里。
3 00:00:07,017 --> 00:00:12,624 说话人 SPEAKER_04: 我会提前简要列举一下您的成就，尽管感觉有点。
4 00:00:12,663 --> 00:00:13,285 说话人 SPEAKER_05: 继续说。
5 00:00:13,324 --> 00:00:17,088 说话人 SPEAKER_05：我说我无需介绍。
6 00:00:17,149 --> 00:00:22,134 说话人 SPEAKER_04：我的第一个要点是说他无需介绍是很老套的，所以我将为他做一个介绍。
7 00:00:22,153 --> 00:00:24,376 说话人 SPEAKER_04：所以我们结束了。
8 00:00:24,536 --> 00:00:25,376 说话人 SPEAKER_04：你第一个问题是什么？
9 00:00:25,797 --> 00:00:26,117 说话人 SPEAKER_04：你确定吗？
10 00:00:26,297 --> 00:00:26,818 说话人 SPEAKER_04：是的。
11 00:00:26,838 --> 00:00:28,300 说话人 SPEAKER_04：好吧。
12 00:00:28,718 --> 00:00:36,734 说话人 SPEAKER_04：好的，我对你的想法是如何改变的很好奇，就像比尔刚才提到的。
13 00:00:36,795 --> 00:00:47,216 说话人 SPEAKER_04：我不是要批评你，只是出于好奇，我想读一读你 2018 年说过的一些话，部分原因是因为我觉得现在很多人都在说这些话。
14 00:00:47,197 --> 00:00:52,869 说话人 SPEAKER_04：现在你对人工智能的总体态度和潜在风险似乎有所不同。
15 00:00:53,712 --> 00:01:03,534 说话人 SPEAKER_04：我在维基百科页面上找到的一些内容，你说“通用人工智能”这个短语带有这样的含义，即这种单一的机器人突然会比你还聪明。
16 00:01:03,515 --> 00:01:05,197 说话人 SPEAKER_04：我认为不会是这样的。
17 00:01:05,477 --> 00:01:09,465 说话人 SPEAKER_04：我认为我们越来越多地做的日常事情将被 AI 系统所取代。
18 00:01:09,924 --> 00:01:15,213 说话人 SPEAKER_04：未来的 AI 将了解你可能会希望它做的事情，但它不会取代你。
19 00:01:15,233 --> 00:01:19,561 说话人 SPEAKER_04：这似乎与我现在的看法有很大不同。
20 00:01:19,760 --> 00:01:25,911 说话人 SPEAKER_04：所以我很好奇，你是否能带我们回顾一下这六年间你所思考的演变过程。
21 00:01:26,887 --> 00:01:33,400 说话人 SPEAKER_05：那时候，我认为要让我们拥有比我们更聪明的东西还需要很长时间。
22 00:01:34,522 --> 00:01:39,072 说话人 SPEAKER_05：我还认为，随着我们制造出更类似大脑的东西，它们会变得更聪明。
23 00:01:39,256 --> 00:01:51,650 说话人 SPEAKER_05：然后在谷歌的最后几年，我试图思考如何使用模拟硬件来降低训练这些大型模型和部署大型模型所需的能耗。
24 00:01:53,513 --> 00:02:02,884 说话人 SPEAKER_05：显然，如果你可以使用模拟硬件，并且有一个可以利用模拟电路所有独特特性的学习算法，
25 00:02:02,864 --> 00:02:05,390 说话人 SPEAKER_05：你可以克服许多模拟的难题。
26 00:02:06,391 --> 00:02:08,897 说话人 SPEAKER_05：你不必让两个不同的计算机表现得一样。
27 00:02:09,338 --> 00:02:11,584 说话人 SPEAKER_05：它们只是学会利用它们所拥有的硬件。
28 00:02:12,806 --> 00:02:16,094 说话人 SPEAKER_05：这使得它们变得有点像凡人，当硬件损坏时，它们的权重就不再有效了。
29 00:02:17,758 --> 00:02:20,163 说话人 SPEAKER_05：但这同时也意味着你可以以更低的功耗运行。
30 00:02:20,144 --> 00:02:31,580 说话人 SPEAKER_05：所以一个经典的例子是，如果你想将神经活动向量乘以权重矩阵以获得下一层的输入，这就是一种核心操作。
31 00:02:31,600 --> 00:02:33,282 说话人 SPEAKER_05：这就是占用大部分计算资源的地方。
32 00:02:34,223 --> 00:02:38,811 说话人 SPEAKER_05：然后如果你将神经活动设为电压，将权重设为电导，
33 00:02:38,790 --> 00:02:41,897 说话人 SPEAKER_05：电压乘以电导等于每单位时间的电荷。
34 00:02:42,438 --> 00:02:44,843 说话人 SPEAKER_05：电荷可以自行累加，所以这就完成了。
35 00:02:45,384 --> 00:02:52,921 说话人 SPEAKER_05：你从未需要将活动转换为 16 位数字，然后进行 16 平方位操作来执行乘法。
36 00:02:53,423 --> 00:02:57,491 说话人 SPEAKER_05：它只是非常低功耗，非常简单快捷。
37 00:02:57,472 --> 00:03:01,182 说话人 SPEAKER_05：对于非线性的事物，做起来更难。
38 00:03:01,683 --> 00:03:06,959 说话人 SPEAKER_05：但问题是，能否使用模拟硬件来降低能耗？
39 00:03:08,282 --> 00:03:12,594 说话人 SPEAKER_05：当我越来越多地思考这个问题，各种问题变得清晰。
40 00:03:12,929 --> 00:03:18,175 说话人 SPEAKER_05：其中一个是，我们不知道如何在不知道系统行为的情况下学习。
41 00:03:19,116 --> 00:03:23,582 说话者 SPEAKER_05：如果你不知道系统如何表现，你不知道正向传播，那么你无法进行反向传播。
42 00:03:26,006 --> 00:03:29,729 说话者 SPEAKER_05：人们已经想出了绕过这个问题的方法，即近似。
43 00:03:29,770 --> 00:03:32,073 说话者 SPEAKER_05：这些近似对于像 MNIST 这样的小事物效果很好。
44 00:03:32,614 --> 00:03:40,122 说话者 SPEAKER_05：但没有人制作出一个合理的版本，它不使用反向传播来处理像 ImageNet 这样大小的事物，而现在 ImageNet 已经是一个小事物了。
45 00:03:40,163 --> 00:03:42,126 说话人 SPEAKER_05：这曾经是一件大事。
46 00:03:43,438 --> 00:03:44,419 说话人 SPEAKER_05：所以那是一个问题。
47 00:03:45,040 --> 00:03:50,748 说话人 SPEAKER_05：另一个问题是，我逐渐意识到数字计算的大优势。
48 00:03:51,710 --> 00:03:55,534 说话人 SPEAKER_05：这需要消耗大量能源，但你可以有大量相同模型的副本。
49 00:03:56,436 --> 00:03:58,718 说话人 SPEAKER_05：不同的副本可以去查看不同的数据。
50 00:03:59,460 --> 00:04:02,264 说话人 SPEAKER_05：他们都可以从他们查看的不同数据中学习到东西。
51 00:04:02,645 --> 00:04:06,729 说话人 SPEAKER_05：然后他们可以通过平均梯度来分享他们所学到的一切。
52 00:04:07,789 --> 00:04:12,078 说话人 SPEAKER_05：这在不同副本之间共享相同模型的信息方面是巨大的。
53 00：04：13,240 --> 00：04：16,565 议长 SPEAKER_05：如果我们能分享这样的信息，就会有 10,000 人出去。
他们可以研究10,000个不同的主题。
55 00：04：19,310 --> 00：04：20,834 发言者 SPEAKER_05：他们可以在前进时平均他们的体重。
56 00：04：21,535 --> 00：04：23,778 演讲者 SPEAKER_05：而且你一次就能得到 10,000 度。
57 00:04:24,600 --> 00:04:26,264 说话人 SPEAKER_05：它不是这样工作的。
58 00:04:26,343 --> 00:04:31,333 说话人 SPEAKER_05：我们根本无法很好地传达信息，就像我现在所展示的。
59 00:04:31,312 --> 00:04:33,696 说话人 SPEAKER_05：我生成句子。
60 00:04:34,055 --> 00:04:36,999 说话人 SPEAKER_05：你找出如何改变你的权重，以便你将生成相同的句子。
61 00:04:37,819 --> 00:04:38,800 讲者 SPEAKER_05：这就是所说的大学。
62 00:04:40,322 --> 00:04:42,384 讲者 SPEAKER_05：它工作得不太好。
63 00:04:42,404 --> 00:04:44,947 讲者 SPEAKER_05：与共享权重相比，它非常慢。
64 00:04:46,009 --> 00:04:47,550 讲者 SPEAKER_05：所以他们在这方面比我们有很大的优势。
65 00:04:47,630 --> 00:04:49,973 说话人 SPEAKER_05：正因为如此，他们知道的比我们多得多。
66 00:04:50,593 --> 00:04:55,199 说话人 SPEAKER_05：所以，我估计 GPT-4 的参数量是任何一个人的 1,000 到 10,000 倍。
67 00:04:55,500 --> 00:04:58,603 说话人 SPEAKER_05：它在所有方面都不是一个很好的专家。
68 00:05:00,170 --> 00:05:08,718 说话人 SPEAKER_05：他们之所以能做到这一点，部分原因是因为它们可以使用反向传播来获取梯度，部分原因是因为它们可以在许多不同副本之间共享相同的模型。
69 00:05:09,980 --> 00:05:14,966 说话人 SPEAKER_05：所以我确信这实际上是一种比我们现有的更好的计算形式。
70 00:05:16,067 --> 00:05:29,502 说话人 SPEAKER_05：所以它们在仅仅几万亿个连接中，实际上并没有告诉我具体的数字，但肯定只有几万亿，它们可以获取比我们在 100 万亿个连接中更多的知识成千上万倍。
71 00:05:31,000 --> 00:05:33,062 说话人 SPEAKER_05：而且它们解决的问题与我们不同。
72 00:05:33,083 --> 00:05:36,586 说话人 SPEAKER_05：它们的问题是经验丰富但连接不多。
73 00:05:36,726 --> 00:05:38,750 说话人 SPEAKER_05：背景很好地压缩了信息。
74 00:05:39,250 --> 00:05:40,312 说话人 SPEAKER_05：我们的问题是相反的。
75 00:05:40,733 --> 00:05:43,716 说话人 SPEAKER_05：经验非常少，联系人数众多。
76 00:05:44,297 --> 00:05:47,541 说话人 SPEAKER_05：我们可能有一个非常不同的学习算法。
77 00：05：48,862 --> 00：05：56,333 演讲者 SPEAKER_05：所以我当时开始相信，让东西变得更像大脑，那种区域或多或少已经结束了。
事物将变得更智能，不是通过使它们更像大脑，而是通过利用我们已经在走的、现在特定于数字智能的道路。
79 00：06：06,389 --> 00：06：09,377 议长 SPEAKER_05：所以我开始相信，这些东西比我们好。
80 00:06:12,596 --> 00:06:12,958 说话者 SPEAKER_04：结束。
81 00:06:14,459 --> 00:06:15,321 说话人 SPEAKER_04：你能再说得多一些吗？
82 00:06:15,600 --> 00:06:22,531 说话人 SPEAKER_04：我想继续追问一下，是什么实际的过程产生了这个改变你想法的洞见？
83 00:06:22,591 --> 00:06:24,213 说话人 SPEAKER_04：是你与人交谈了吗？
84 00:06:24,233 --> 00:06:26,716 说话人 SPEAKER_04：我知道你提到当时你在谷歌的工作。
85 00:06:26,817 --> 00:06:28,218 说话人 SPEAKER_04：有没有区域性的？
86 00:06:28,238 --> 00:06:29,139 说话人 SPEAKER_05：它正在意识到。
87 00:06:29,160 --> 00:06:35,428 说话人 SPEAKER_05：它正在仔细考虑模拟的优点，并理解模拟的优点。
88 00:06:35,449 --> 00:06:39,954 说话人 SPEAKER_05：特别是这个见解，我们现在学习一切而不是编程。
89 00:06:40,458 --> 00:06:45,360 说话人 SPEAKER_05：你知道，我们制造了数字计算机，让它们完全按照我们说的去做，这样我们就可以编写程序了。
90 00:06:46,526 --> 00:06:49,742 说话人 SPEAKER_05：不同的计算机必须完全正确地执行任务，这样你才能编程。
91 00:06:50,211 --> 00:06:51,574 说话人 SPEAKER_05：我们现在不再编程它们了。
92 00:06:51,793 --> 00:06:56,259 说话人 SPEAKER_05：我的意思是，有一个学习算法，但要让它们做特定的事情，我们需要对它们进行训练。
93 00:06:56,281 --> 00:07:02,249 说话者 SPEAKER_05：一旦训练好它们，就不需要它们都按同一种方式工作，因为它们会学会使用它们拥有的硬件。
94 00:07:03,089 --> 00:07:05,432 说话者 SPEAKER_05：所以你可以选择一条完全不同的路径。
95 00:07:05,774 --> 00:07:15,206 说话者 SPEAKER_05：这是生物路径，你利用你特定神经元的奇特特性，你将等待知道如何使用它们，因为我的神经元不同，我的连接性也不同。
96 00:07:15,928 --> 00:07:17,170 说话者 SPEAKER_05：这非常令人兴奋。
97 00:07:17,230 --> 00:07:30,322 说话人 SPEAKER_05：但是当我遇到如何让它学习以及学习算法的困难时，我开始意识到数字计算的巨大优势，即使在学习时，即使不是直接编程时也是如此。
98 00:07:31,084 --> 00:07:34,247 说话人 SPEAKER_05：而这种优势来自于能够共享，不同模型之间的共享。
99 00:07:35,127 --> 00:07:38,932 说话人 SPEAKER_05：这让我相信这些事情更好。
100 00:07:38,951 --> 00:07:42,375 说话人 SPEAKER_05：现在，这些事情是同时发生的。
101 00:07:42,355 --> 00:07:46,360 说话人 SPEAKER_05：随着像 Palm 这样的聊天机器人出现，它们可以解释为什么一个笑话好笑。
102 00:07:46,980 --> 00:07:50,384 说话人 SPEAKER_05：所以最初，这是另一个重要的影响因素。
103 00:07:51,165 --> 00:07:59,394 说话人 SPEAKER_05：我一直有一个标准，我没有为此找到理由，但我的标准是当这些事物变得非常智能时，它们能够解释为什么一个笑话好笑。
104 00:07:59,954 --> 00:08:01,255 说话人 SPEAKER_05：这在我看来是一个很好的衡量标准。
105 00:08:01,456 --> 00:08:02,497 说话人 SPEAKER_05：这就是我的图灵测试。
106 00:08:03,017 --> 00:08:05,240 说话人 SPEAKER_05：Palm 也能做到。
107 00:08:06,233 --> 00:08:07,894 说话人 SPEAKER_05：实际上它并不能讲笑话。
108 00:08:07,935 --> 00:08:10,519 说话人 SPEAKER_05：你可能已经注意到，即使是 GPT-4 也无法讲笑话。
109 00:08:10,538 --> 00:08:14,004 说话人 SPEAKER_05：因为 GPT-4 是逐字生成的，所以您可以看到。
110 00:08:14,584 --> 00:08:21,153 说话人 SPEAKER_05：所以如果您让它讲一个笑话，它就会生成看起来像笑话开头的东西。
111 00:08:21,293 --> 00:08:23,416 说话人 SPEAKER_05：它说，一个牧师和一只章鱼走进了一家酒吧。
112 00:08:24,077 --> 00:08:25,939 说话人 SPEAKER_05：嗯，您知道那是笑话的开头，对吧？
113 00:08:26,401 --> 00:08:27,382 说话人 SPEAKER_05：然后就这样一直继续下去。
114 00:08:27,401 --> 00:08:29,946 说话人 SPEAKER_05：然后它就到了需要点睛之笔的时候。
115 00:08:30,466 --> 00:08:35,243 说话人 SPEAKER_05：但是像我的某些朋友一样，它在尝试讲笑话之前并没有想到点睛之笔。
116 00:08:35,263 --> 00:08:36,145 说话人 SPEAKER_05：那可就糟糕了。
117 00:08:36,167 --> 00:08:38,534 说话人 SPEAKER_05：所以结尾太弱了。
118 00:08:39,123 --> 00:08:41,225 说话人 SPEAKER_05：但这只是因为它一次只生成一个单词。
119 00:08:41,265 --> 00:08:42,427 说话人 SPEAKER_05：它不必表现得那样。
120 00:08:43,850 --> 00:08:46,774 说话人 SPEAKER_05：所以解释一个笑话为什么好笑比讲笑话要好得多。
121 00:08:47,855 --> 00:08:49,376 说话人 SPEAKER_05：除非只是记得一个笑话，否则它知道。
122 00:08:49,437 --> 00:08:51,179 说话人 SPEAKER_05：但是创造一个笑话对它来说很难。
123 00:08:51,900 --> 00:08:55,346 说话人 SPEAKER_05：无论如何，我的标准是，它能否告诉你为什么一个笑话好笑？
124 00:08:55,365 --> 00:08:55,785 说话人 SPEAKER_05：它可以。
125 00:08:56,508 --> 00:08:58,230 说话人 SPEAKER_05：所以这两件事是某种程度的结合。
126 00:08:58,730 --> 00:09:03,256 说话人 SPEAKER_05：聊天机器人的能力，这在某种程度上是通过与 GPT-4 互动而得到加强的。
127 00:09:03,236 --> 00:09:17,938 说话人 SPEAKER_05：以及我终于明白了什么是数字化的优势，这让我觉得我的观点可能是这些事物将会接管。
128 00:09:17,979 --> 00:09:23,587 说话人 SPEAKER_05：可能我们只是智能进化过程中的一个过渡阶段。
129 00:09:23,989 --> 00:09:26,292 说话人 SPEAKER_05：它可能就像蜻蜓一样。
130 00:09:26,272 --> 00:09:28,014 说话人 SPEAKER_05：你知道，蜻蜓是美妙的东西。
131 00:09:28,094 --> 00:09:31,597 说话人 SPEAKER_05：如果你看蜻蜓的幼虫，它看起来根本不像蜻蜓。
132 00:09:32,239 --> 00:09:34,721 说话人 SPEAKER_05：它是一个大块头，生活在水下。
133 00:09:34,761 --> 00:09:44,234 说话者 SPEAKER_05：它从环境中获得大量能量，然后变成汤，然后从汤中建造出蜻蜓。
134 00:09:45,434 --> 00:09:47,638 说话者 SPEAKER_05：而我们就是幼虫。
135 00:09:48,461 --> 00:09:57,543 说话者 SPEAKER_04：我也很好奇，自从你对这个问题有了那个见解以来，你在过去一年谈论这个问题时，你对问题的看法有没有改变？
136 00:09:58,840 --> 00:10:00,501 说话者 SPEAKER_05：有点改变。
137 00:10:00,562 --> 00:10:02,125 说话人 SPEAKER_05：我关于人工智能安全学到了一些东西。
138 00:10:02,144 --> 00:10:04,408 说话人 SPEAKER_05：在那之前，我对人工智能安全并不特别感兴趣。
139 00:10:05,089 --> 00:10:05,830 说话人 SPEAKER_05：我仍然不知道。
140 00:10:06,309 --> 00:10:09,014 说话人 SPEAKER_05：可能这里每个人对人工智能安全的了解都比我知道的多。
141 00:10:10,917 --> 00:10:15,964 说话人 SPEAKER_05：所以我决定 76 岁的我不再从事 AI 安全的原创研究。
142 00:10:16,464 --> 00:10:25,115 说话人 SPEAKER_05：只是要反驳那些随机鹦鹉散布的信息，说这里没有什么好担心的。
143 00:10:26,337 --> 00:10:27,700 说话人 SPEAKER_05：这全是科幻小说。
144 00:10:27,679 --> 00:10:33,490 说话人 SPEAKER_05：在我看来，不，这离科幻小说还远着呢。
145 00:10:33,811 --> 00:10:37,758 说话人 SPEAKER_05：我可以利用我的声誉说，这其实不是科幻。
146 00:10:37,798 --> 00:10:38,799 说话人 SPEAKER_05：这是一个真实的问题。
147 00:10:39,662 --> 00:10:43,347 说话人 SPEAKER_05：这就是我认为我的角色，这也是我一直在做的事情。
148 00:10:43,368 --> 00:10:43,509 说话人 SPEAKER_04：明白了。
149 00:10:43,528 --> 00:10:45,451 说话人 SPEAKER_04：我们应该去回答观众的问题吗？
150 00:10:55,404 --> 00:10:55,644 说话人 SPEAKER_11：嗨。
151 00:10:56,666 --> 00:11:10,321 说话人 SPEAKER_11：我的问题是，您认为我们使用今天的人工智能的下一代来帮助我们确保人工智能的安全，进行研究，如果我们在此之前没有取得任何根本性的进步，前景如何？
152 00:11:10,360 --> 00:11:11,442 说话人 SPEAKER_11：如果我们只是让它们变得更聪明。
153 00:11:13,224 --> 00:11:16,368 说话人 SPEAKER_05: 嗯。
154 00:11:16,388 --> 00:11:21,192 说话人 SPEAKER_05: 那关于训练狐狸来帮助你阻止狐狸吃鸡怎么办？
155 00:11:22,878 --> 00:11:24,780 说话人 SPEAKER_05：这就是我的感觉。
156 00:11:24,801 --> 00:11:33,172 说话人 SPEAKER_05：比如，当我想要了解关于人工智能安全的法规时，我会问 GPT-4，它会告诉我关于人工智能安全的法规。
157 00:11:33,192 --> 00:11:35,173 说话人 SPEAKER_05：而且这好像有点危险。
158 00:11:37,937 --> 00:11:41,602 说话人 SPEAKER_05：所以如果它们是我们拥有的最好的工具，那我们就只能这样做。
159 00:11:42,364 --> 00:11:49,493 说话人 SPEAKER_05：但显然，AI 帮你监管 AI 这件事有些可疑。
160 00:11:49,860 --> 00:11:58,048 说话人 SPEAKER_05：我的意思是，如果你要监管某件事，你不想让警察监管警察。
161 00:11:59,041 --> 00:12:09,831 说话人 SPEAKER_06：所以你提到了 ChatGPT 这样的系统可以通过梯度更新汇总知识，因此它们尽管连接较少，但知道的比我们多得多。
162 00:12:10,711 --> 00:12:17,399 说话人 SPEAKER_06：对我来说和一些人来说，这相对于它们开始的地方，在某些方面是一种令人乐观的更新吗？
163 00:12:17,879 --> 00:12:26,246 说话人 SPEAKER_06：因为这意味着你可以拥有非常、非常知识渊博的系统，但不太具有创造力，并且必须严格遵循它们的
164 00:12:26,226 --> 00:12:28,028 说话人 SPEAKER_06：在某个有用性的水平上。
165 00:12:28,109 --> 00:12:29,410 说话人 SPEAKER_06：所以我很好奇您的看法。
166 00:12:29,831 --> 00:12:30,032 说话人 SPEAKER_05：是的。
167 00:12:30,392 --> 00:12:32,975 说话人 SPEAKER_05：这意味着我们可以拥有非常、非常博学的系统。
168 00:12:33,657 --> 00:12:38,462 说话人 SPEAKER_05：但你从那推断出它们可能不会很有创造力。
169 00:12:38,702 --> 00:12:46,192 说话人 SPEAKER_06：我所说的意思是，在当前时刻，它们在知识上似乎超过了我们。
170 00:12:46,173 --> 00:12:50,658 说话人 SPEAKER_06：我会说，它们在最具创造力方面并没有超过我们。
171 00:12:51,038 --> 00:12:53,041 说话人 SPEAKER_06：在某个时刻，它们在两方面都会比我们做得更好。
172 00:12:53,642 --> 00:13:10,763 说话人 SPEAKER_06：但如果这是当前的平衡，这也许对人类来说是充满希望的，因为你可以向它们寻求帮助，而它们可以用它们无穷的知识来提供帮助，同时它们在试图规避我们的监控和反制措施等方面并不像我们那样出色。
173 00:13:12,076 --> 00:13:16,559 说话人 SPEAKER_05：我注意到你说得非常具有创造性，因为它们已经可以表现出创造性了。
174 00:13:16,580 --> 00:13:21,504 说话人 SPEAKER_05：所以如果你进行标准的创造性测试，你们可能已经阅读过文献，而我还没有。
175 00:13:21,825 --> 00:13:28,650 说话人 SPEAKER_05：但我听说，如果你参加标准的创造力测试，他们的得分就像 90 分位的人一样。
176 00:13:29,091 --> 00:13:30,793 说话人 SPEAKER_05：所以在那个意义上，他们已经很有创造力了。
177 00:13:30,913 --> 00:13:32,674 说话人 SPEAKER_05：他们有平凡的创造力。
178 00:13:32,715 --> 00:13:37,778 说话人 SPEAKER_05：问题是，他们是否真的有真正的创造力，人性的本质？
179 00:13:38,539 --> 00:13:42,082 说话人 SPEAKER_05: 我不明白为什么不行。
180 00:13:42,062 --> 00:13:51,923 说话人 SPEAKER_05: 如果你考虑将大量知识压缩到几个连接中，唯一的方法就是注意到各种不同领域之间的类比。
181 00:13:52,806 --> 00:14:01,624 说话人 SPEAKER_05: 所以如果你问 GPT-4 为什么堆肥堆像原子弹，它知道，而大多数人不知道。
182 00:14:02,886 --> 00:14:04,048 说话人 SPEAKER_05: 所以这是可以理解的。
183 00:14:04,168 --> 00:14:06,312 说话人 SPEAKER_05：我觉得它在测试时可能不理解。
184 00:14:06,332 --> 00:14:08,034 说话人 SPEAKER_05：我觉得它在学习时可能理解了。
185 00:14:08,775 --> 00:14:10,139 说话人 SPEAKER_05：它知道连锁反应。
186 00:14:10,239 --> 00:14:13,865 说话人 SPEAKER_05：它知道堆肥堆得越热，产生的热量就越快。
187 00:14:14,264 --> 00:14:15,748 说话人 SPEAKER_05：这就像一颗原子弹。
188 00:14:17,149 --> 00:14:19,833 说话人 SPEAKER_05：它可能推断出这一点，在这种情况下，它擅长推理。
189 00:14:20,075 --> 00:14:23,460 说话人 SPEAKER_05：但我认为实际上，随着它的学习，
190 00:14:23,557 --> 00:14:32,029 说话人 SPEAKER_05：为了将所有人类知识压缩成很少的几个联系，它正在看到不同的人类知识之间的人从未见过的相似性。
191 00:14:32,811 --> 00:14:35,274 说话者 SPEAKER_05：我认为它有潜力变得极其富有创造力。
192 00:14:43,985 --> 00:14:45,128 说话者 SPEAKER_05：但我同意它还没有达到那个程度。
193 00:14:46,897 --> 00:15:02,880 说话者 SPEAKER_02：我想知道您是否在和一个对这些问题相当陌生的人交谈，他们可能会说，好吧，我多少能理解您说的那些抽象的东西，但具体来说，这一切可能会出错吗？
194 00:15:02,921 --> 00:15:08,369 说话者 SPEAKER_02：比如，我们究竟会受到什么样的实际具体伤害机制的影响？
195 00:15:08,710 --> 00:15:10,272 说话人 SPEAKER_02：我对这样的人说些什么感到好奇。
196 00:15:10,251 --> 00:15:18,961 说话人 SPEAKER_05：我先问一下，你知道多少例子是智力较低的事物控制智力较高的事物？
197 00:15:20,083 --> 00:15:23,327 说话人 SPEAKER_05：我只知道一个例子，那就是婴儿控制母亲。
198 00:15:23,768 --> 00:15:26,652 说话人 SPEAKER_05：进化为此投入了大量的工作。
199 00:15:27,011 --> 00:15:29,355 说话人 SPEAKER_05：这位母亲就是忍受不了婴儿的哭声。
200 00:15:29,695 --> 00:15:31,437 说话人 SPEAKER_05：这里涉及到各种激素。
201 00:15:32,057 --> 00:15:34,961 说话人 SPEAKER_05：这是一个男人在谈论女人，对吧？
202 00:15:35,081 --> 00:15:35,361 说话人 SPEAKER_05：抱歉。
203 00:15:36,783 --> 00:15:39,346 说话人 SPEAKER_05：但就母性而言，涉及许多激素。
204 00:15:39,326 --> 00:15:42,130 说话人 SPEAKER_05：整个系统就是这样演化的。
205 00:15:44,073 --> 00:15:47,659 说话人 SPEAKER_05：几乎总是更智能的事物控制着不那么智能的事物。
206 00:15:50,705 --> 00:15:53,568 说话人 SPEAKER_05：这就是一个起点。
207 00:15:53,909 --> 00:16:01,841 说话人 SPEAKER_05：除非你能找到一个理由说明它非常、非常不同，否则很难想象一个远比它聪明的存在会被一个远比它低级的存在所控制。
208 00:16:01,822 --> 00:16:04,386 说话人 SPEAKER_05：那么你就开始探索它为什么非常、非常不同的原因。
209 00:16:06,432 --> 00:16:11,763 说话人 SPEAKER_05：可能的原因之一是它没有自己的意图或欲望。
210 00:16:12,124 --> 00:16:14,970 说话人 SPEAKER_05：但当你开始让它具有代理性
211 00:16:15,423 --> 00:16:19,128 说话者 SPEAKER_05：有了创建子目标的能力，它确实有一些想要实现的事情。
212 00:16:20,711 --> 00:16:25,618 说话者 SPEAKER_05：它们可能不像人类意图那样紧急，但它也有一些想要实现的事情。
213 00:16:27,139 --> 00:16:28,883 说话者 SPEAKER_05：所以我认为这不会阻碍它。
214 00:16:29,464 --> 00:16:39,238 说话者 SPEAKER_05：然后还有另一件事，我认为大多数人，不是我们这些人，但我认为大多数人认为，嗯，我们有自己的主观体验。
215 00:16:39,618 --> 00:16:42,121 说话人 SPEAKER_05：我们与机器不同。
216 00:16:42,101 --> 00:16:46,206 说话人 SPEAKER_05：他们坚信我们与机器永远不同。
217 00:16:46,246 --> 00:16:48,830 说话人 SPEAKER_05：这将会形成一道障碍。
218 00:16:49,831 --> 00:16:54,176 说话人 SPEAKER_05：我认为机器也可以有主观体验。
219 00:16:54,576 --> 00:16:56,958 说话人 SPEAKER_05：我将给你一个机器有主观体验的例子。
220 00:16:57,519 --> 00:17:01,464 说话人 SPEAKER_05：因为一旦你看到了这个例子，你就应该同意机器可以有主观体验。
221 00:17:03,625 --> 00:17:04,988 说话人 SPEAKER_05：以一个多模态聊天机器人为例。
222 00:17:06,284 --> 00:17:09,249 说话人 SPEAKER_05：对其进行训练并放置一个物体。
223 00:17:09,328 --> 00:17:10,490 说话人 SPEAKER_05：它有一个摄像头，它有一个机械臂。
224 00:17:11,092 --> 00:17:14,357 说话人 SPEAKER_05：你把它前面的物体指给它看，然后你说指向这个物体，它就指向了这个物体。
225 00:17:14,518 --> 00:17:14,979 说话人 SPEAKER_05：没问题。
226 00:17:15,619 --> 00:17:22,352 说话人 SPEAKER_05：然后你把它镜头前面的棱镜拿掉，再把它前面的物体指给它看，然后你说指向这个物体，它就指向了这个物体。
227 00:17:22,332 --> 00:17:24,455 说话者 SPEAKER_05：你说，不，物体不在那里。
228 00:17:24,875 --> 00:17:28,582 说话者 SPEAKER_05：我在你的镜头前放了一个棱镜，你的感知系统在欺骗你。
229 00:17:29,604 --> 00:17:30,925 说话者 SPEAKER_05：物体实际上就在你面前。
230 00:17:31,287 --> 00:17:32,729 说话者 SPEAKER_05：然后聊天机器人说，哦，我看到了。
231 00:17:33,550 --> 00:17:36,194 说话人 SPEAKER_05：棱镜弯曲了光线，所以物体实际上就在我面前直着。
232 00:17:36,455 --> 00:17:37,978 说话人 SPEAKER_05：但我有主观体验。
233 00:17:38,018 --> 00:17:38,638 说话人 SPEAKER_05：它就在那里。
234 00:17:40,582 --> 00:17:46,332 说话人 SPEAKER_05：我认为那时正是用主观体验的方式，就像我们使用它一样。
235 00:17:47,492 --> 00:17:52,759 说话人 SPEAKER_05：所以我们有一个关于我们如何使用词语的模型，然后我们实际上是如何使用它们的。
236 00:17:53,980 --> 00:17:56,282 说话人 SPEAKER_05：这回到了很久以前的牛津哲学。
237 00:17:57,003 --> 00:18:01,189 说话人 SPEAKER_05：我们中许多人都有一个关于我们使用的词语如何工作的错误模型。
238 00:18:01,630 --> 00:18:05,173 说话人 SPEAKER_05：我们可以很好地使用它们，但我们实际上并不知道它们是如何工作的。
239 00:18:05,193 --> 00:18:16,487 说话者 SPEAKER_04：对于那种，嗯，婴儿和母亲的例子，但经济拥有大量的智能，它仍然是我们工具的观点，或者类似的论点，你会说什么？
240 00:18:16,467 --> 00:18:24,558 说话者 SPEAKER_04：所以你没有说，哦，它就像你说的那样，是一个较不智能的系统控制着一个更智能的系统。
241 00:18:24,920 --> 00:18:31,990 说话者 SPEAKER_04：那是因为你误解了，这是一个汇聚了人类知识，就像股市之类的工具。
242 00:18:32,170 --> 00:18:32,671 说话者 SPEAKER_04：你会说什么？
243 00:18:33,412 --> 00:18:35,414 Speaker SPEAKER_05: 你是说股市比我们更聪明吗？
244 00:18:35,935 --> 00:18:39,381 Speaker SPEAKER_04: 你可以想象有人会提出这样的论点，我不知道。
245 00:18:39,520 --> 00:18:41,984 Speaker SPEAKER_05: 嗯，这好像就是它在控制我们一样，不是吗？
246 00:18:44,210 --> 00:18:50,183 Speaker SPEAKER_05: 我今天早上醒来，看了看我的谷歌股票走势。
247 00:18:50,204 --> 00:18:51,351 说话人 SPEAKER_05: 它控制了我。
248 00:18:55,516 --> 00:19:00,584 说话人 SPEAKER_03: 所以是的，这与主观体验线程有关。
249 00:19:01,125 --> 00:19:12,221 说话人 SPEAKER_03：我很想知道，当越来越多的人相信人工智能拥有主观体验或真正的欲望，或者它们理应得到更多考虑时，这些问题会如何改变。
250 00:19:12,241 --> 00:19:13,123 说话人 SPEAKER_05：他们会更加害怕。
251 00:19:13,784 --> 00:19:22,958 说话人 SPEAKER_05：因为大多数人，对于大多数人来说，对于公众来说，他们认为机器和有主观经验的事物，就像我们，之间存在一条明确的界限。
252 00:19:22,938 --> 00:19:24,981 说话人 SPEAKER_05：而这些机器没有主观性。
253 00:19:25,041 --> 00:19:27,186 说话人 SPEAKER_05：它们可能模仿主观经验。
254 00:19:27,207 --> 00:19:28,489 说话人 SPEAKER_05：但它们实际上并没有。
255 00:19:29,089 --> 00:19:33,018 说话人 SPEAKER_05：这是因为我们有一个关于主观体验的模型，有一个内在的剧院。
256 00:19:33,578 --> 00:19:36,766 说话人 SPEAKER_05：内在剧院中的这些事物才是你真正看到的。
257 00:19:37,166 --> 00:19:38,209 说话人 SPEAKER_05：这个模型只是垃圾。
258 00:19:39,431 --> 00:19:41,335 说话人 SPEAKER_05：这个模型愚蠢得就像上帝创造世界一样。
259 00:19:42,242 --> 00:19:43,424 说话人 SPEAKER_03：一个快速跟进的问题。
260 00:19:43,444 --> 00:19:52,636 说话人 SPEAKER_03：在我看来，人们可能会更加害怕，但也许人们也会更加有同情心，或者认为人工智能应该拥有权利和类似的东西。
261 00:19:52,696 --> 00:19:52,938 说话人 SPEAKER_03: 是的。
262 00:19:53,258 --> 00:19:59,787 说话人 SPEAKER_05: 所以有一件事我没有提到，因为我认为这没有帮助，那就是讨论人工智能的权利问题。
263 00:20:02,109 --> 00:20:04,733 说话人 SPEAKER_05: 此外，我有点吃动物。
264 00:20:05,855 --> 00:20:07,778 说话人 SPEAKER_05: 我让别人来杀死它们。
265 00:20:08,534 --> 00:20:09,737 说话人 SPEAKER_05：实际上，他们无论如何都会杀掉他们的。
266 00:20:11,259 --> 00:20:15,288 说话人 SPEAKER_05：但我吃动物是因为对我来说人更重要。
267 00:20:16,330 --> 00:20:24,146 说话者 SPEAKER_05：我认为这不是一个棘手的问题，但如果它们比我们更聪明，你会站在它们一边还是站在人类一边？
268 00:20:25,307 --> 00:20:26,450 说话者 SPEAKER_05：对我来说并不明显。
269 00:20:26,470 --> 00:20:33,760 说话者 SPEAKER_05：如果你认为道德是物种依赖的，那么站在人类一边并不明显是错误的。
270 00:20:36,464 --> 00:20:40,590 说话者 SPEAKER_05：但如果你想讨论航空安全问题，我认为这是一个最好避免的问题。
271 00:20:40,951 --> 00:20:43,355 说话人 SPEAKER_05：因为它让你接触到许多其他事物。
272 00:20:43,875 --> 00:20:45,577 说话人 SPEAKER_05：而且你似乎越来越不可靠。
273 00:20:46,282 --> 00:20:56,866 说话人 SPEAKER_07：我很好奇你最近最感兴趣或认为最有说服力的减少 AI 系统风险的方法有哪些。
274 00:20:57,910 --> 00:20:59,992 说话人 SPEAKER_05：我希望我能回答这个问题。
275 00:21:00,433 --> 00:21:08,125 说话人 SPEAKER_05：所以对于气候变化，例如，停止燃烧碳，或者可能捕获大量碳。
276 00:21:08,146 --> 00:21:11,972 说话人 SPEAKER_05：但我认为那是一个老公司的阴谋，为了分散你的注意力，他们才能生产它。
277 00:21:12,554 --> 00:21:13,515 说话人 SPEAKER_05: 停止燃烧碳。
278 00:21:13,535 --> 00:21:14,836 说话人 SPEAKER_05: 从长远来看，一切都会好起来。
279 00:21:14,856 --> 00:21:15,478 说话人 SPEAKER_05：需要一段时间。
280 00:21:16,019 --> 00:21:17,501 说话人 SPEAKER_05：那里有非常简单的解决方案。
281 00:21:17,521 --> 00:21:20,465 说话人 SPEAKER_05：这全在于阻止人们做坏事。
282 00:21:21,407 --> 00:21:22,450 说话人 SPEAKER_05：但我们知道如何解决这个问题。
283 00:21:23,131 --> 00:21:24,593 说话人 SPEAKER_05：这里，我们不做。
284 00:21:24,573 --> 00:21:27,598 说话人 SPEAKER_05：我的意思是，等价的做法是停止开发人工智能。
285 00:21:28,441 --> 00:21:39,103 说话人 SPEAKER_05：我认为这可能是在这个时刻人类做出的一个理性决定，但我认为他们没有机会做出这个决定，因为国家之间存在竞争，而且它有太多好的用途。
286 00:21:39,403 --> 00:21:40,826 说话人 SPEAKER_05：对于原子弹，
287 00:21:40,807 --> 00:21:42,769 说话人 SPEAKER_05：实际上并没有那么多好的用途。
288 00:21:43,632 --> 00:21:45,575 说话人 SPEAKER_05：它们主要是用来炸东西。
289 00:21:45,595 --> 00:21:48,680 说话人 SPEAKER_05：尽管美国尽力了。
290 00:21:48,980 --> 00:21:55,632 说话人 SPEAKER_05：所以在 60 年代，他们有一个关于核弹和平用途的项目。
291 00:21:56,673 --> 00:21:59,218 说话人 SPEAKER_05：这项资金已经到位。
292 00:21:59,278 --> 00:22:03,403 说话人 SPEAKER_05：他们曾在科罗拉多州用于页岩气开采。
293 00:22:04,506 --> 00:22:07,371 说话人 SPEAKER_05：结果发现，你再也不想再去科罗拉多州的那部分地区了。
294 00:22:10,422 --> 00:22:13,226 说话人 SPEAKER_05：我知道这个情况是因为火车离那里很近。
295 00:22:13,445 --> 00:22:14,748 附近没有道路。
296 00:22:15,209 --> 00:22:16,310 但是火车开得很近。
297 00:22:16,351 --> 00:22:22,940 在从芝加哥到旧金山的火车上，曾经有一位导游在扩音器上做广播。
298 00:22:23,741 --> 00:22:28,709 她说，我们现在大约在西距核弹用于压裂的地方30英里处。
299 00:22:31,192 --> 00:22:33,375 说话人 SPEAKER_05：但这就是核能的合理用途了。
300 00:22:33,536 --> 00:22:36,160 说话人 SPEAKER_05：也许再挖一条运河之类的。
301 00:22:37,539 --> 00:22:38,740 说话人 SPEAKER_05：人工智能与那非常不同。
302 00:22:39,781 --> 00:22:41,144 说话人 SPEAKER_05：大部分用途都是合理的。
303 00:22:41,183 --> 00:22:42,644 说话人 SPEAKER_05：他们正在赋权于人们。
304 00:22:43,746 --> 00:22:47,009 说话人 SPEAKER_05：现在每个人都可以有自己的律师了。
305 00:22:48,211 --> 00:22:49,050 说话人 SPEAKER_05：而且这并不花费很多。
306 00:22:50,132 --> 00:22:51,773 说话人 SPEAKER_05：我不确定这能帮助法律系统。
307 00:22:51,814 --> 00:22:56,157 说话人 SPEAKER_05：但是，对于医疗保健，每个人很快就能有自己的医生。
308 00:22:56,499 --> 00:22:58,840 说话人 SPEAKER_05：这对老年人来说非常有用。
309 00:22:59,741 --> 00:23:02,805 说话人 SPEAKER_05：所以，他们不会停止。
310 00:23:04,574 --> 00:23:05,315 说话人 SPEAKER_05：所以我们不能。
311 00:23:06,355 --> 00:23:09,919 说话人 SPEAKER_05：我知道避免生存威胁的唯一方法就是阻止它。
312 00:23:11,180 --> 00:23:15,023 说话人 SPEAKER_05：我没有签署要求减缓的请愿书，因为我认为没有机会。
313 00:23:17,986 --> 00:23:18,287 说话人 SPEAKER_05：对不起。
314 00:23:22,810 --> 00:23:25,492 说话人 SPEAKER_05：现在，我应该说，这并不意味着人们不应该尝试。
315 00:23:25,534 --> 00:23:28,336 说话人 SPEAKER_05: 我认为 Beth 所做的事情是一次很好的尝试。
316 00:23:30,018 --> 00:23:31,459 说话人 SPEAKER_05: 这会让他们慢下来一会儿。
317 00:23:33,093 --> 00:23:35,055 说话人 SPEAKER_10: 好的，太棒了。
318 00:23:35,075 --> 00:23:39,638 说话人 SPEAKER_10: 是的，我想通过提问稍微反驳一下你刚才说的。
319 00:23:39,759 --> 00:23:52,211 说话人 SPEAKER_10: 我认为现在世界上存在一个不幸的动态，很多人都有这种感觉，是的，我们可能应该放慢脚步，但我没有采取任何行动，因为没有希望。
320 00:23:53,452 --> 00:24:02,121 说话人 SPEAKER_10: 如果每个人都集体认为，我们应该停下来，放慢脚步，我认为这是会发生的，大家都会这样。
321 00:24:02,101 --> 00:24:03,383 说话人 SPEAKER_10: 是的，这是理性的事情去做。
322 00:24:03,403 --> 00:24:05,486 说话人 SPEAKER_05: 包括美国国防部在内吗？
323 00:24:06,287 --> 00:24:10,211 说话人 SPEAKER_10: 我认为如果每个人都这样做，包括他们，那么就不会发生。
324 00:24:10,251 --> 00:24:11,032 说话人 SPEAKER_10: 是的，那样我们就可以做了。
325 00:24:11,053 --> 00:24:11,453 说话人 SPEAKER_10: 嗯，没错。
326 00:24:12,935 --> 00:24:15,720 说话人 SPEAKER_05: 那就像亚利桑那州的氟利昂一样，对吧？
327 00:24:15,779 --> 00:24:16,119 说话人 SPEAKER_10: 当然，是的。
328 00:24:16,520 --> 00:24:23,130 说话人 SPEAKER_10: 我的问题是，您是否认为这种情况会发生，而且无法阻止，所以我不打算采取任何行动，这能帮助我们共同决定停止吗？
329 00:24:24,915 --> 00:24:37,476 说话人 SPEAKER_10: 这是不是意味着，您认为这种情况会发生，而且无法阻止，所以您不打算采取任何行动，这是否有助于我们共同决定停止？
330 00:24:38,156 --> 00:24:44,346 说话人 SPEAKER_10: 或者，实际上，您对任何形式的集体行动都不感兴趣？
331 00:24:45,288 --> 00:24:49,994 说话人 SPEAKER_05: 不，我对，我认为我们应该尽我们所能来阻止
332 00:24:51,038 --> 00:24:53,941 说话人 SPEAKER_05: 存在性威胁，并减轻所有其他威胁。
333 00:24:54,161 --> 00:24:55,261 说话人 SPEAKER_05: 我认为我们应该尽我们所能。
334 00:24:56,262 --> 00:24:59,385 说话人 SPEAKER_05: 但我把自己看作是一名科学家，而不是一名政治家。
335 00:25:00,287 --> 00:25:03,750 说话人 SPEAKER_05: 因此，我认为我的角色就是说出我认为的事情。
336 00:25:04,010 --> 00:25:16,843 说话人 SPEAKER_05: 尤其是我想说服怀疑者，存在一种生存威胁，让人们认真对待并尝试做些什么，尽管我对他们能否做到这一点相当悲观。
337 00:25:25,902 --> 00:25:36,878 说话人 SPEAKER_00: 嗯，在这个问题上，我想知道，具体来说，你有什么打算去做，可以帮助说服怀疑者，并且推广这个话题？
338 00:25:36,919 --> 00:25:48,957 说话人 SPEAKER_00: 像是，我不知道是否是特定的政治家，如果你能和他们谈谈，帮助通过一些特定的立法，或者媒体露面，或者提供咨询或支持某些事物，那会很有帮助。
339 00:25:48,936 --> 00:25:49,877 说话人 SPEAKER_05: 所有这些事情。
340 00:25:50,159 --> 00:25:50,398 说话人 SPEAKER_00: 好的。
341 00:25:50,839 --> 00:25:55,945 说话人 SPEAKER_05: 所以，我意思是，去年，显然，我参与了很多媒体采访，因为我认为这会有帮助。
342 00:25:57,208 --> 00:25:58,910 说话人 SPEAKER_05: 还因为我喜欢出现在电视上。
343 00:25:59,932 --> 00:26:15,972 说话人 SPEAKER_05: 我现在正在做的一件事是，有一位纪录片制作人正在制作关于人工智能历史的纪录片，聚焦于人工智能历史上的各种人物，可能包括现在有点疯狂的杨，但他仍然是我的朋友。
344 00:26:15,952 --> 00:26:19,236 说话人 SPEAKER_05: 还有我和其他一些人。
345 00:26:20,218 --> 00:26:23,040 说话人 SPEAKER_05：纪录片可以产生很大的影响。
346 00:26:23,961 --> 00:26:27,605 说话人 SPEAKER_05：所以如果你是个亿万富翁，你可以资助一部纪录片。
347 00:26:28,747 --> 00:26:34,332 说话人 SPEAKER_05：所以对于气候变化，阿尔·戈尔的纪录片产生了重大影响，对吧？
348 00:26:35,493 --> 00:26:37,256 说话人 SPEAKER_00：我们是人工智能安全的戴维·阿滕伯勒。
349 00:26:42,780 --> 00:26:44,323 说话人 SPEAKER_05: 我还没有那么老。
350 00:26:48,032 --> 00:26:52,557 说话人 SPEAKER_05: 他是我的英雄，所以谢谢您。
351 00:26:53,680 --> 00:26:54,381 说话人 SPEAKER_09: 好了。
352 00:26:54,401 --> 00:26:55,662 说话人 SPEAKER_09: 在这个话题上，我有点好奇。
353 00:26:56,383 --> 00:26:58,905 说话人 SPEAKER_09：我们之前稍微谈了一些正在进行的政策问题。
354 00:26:58,925 --> 00:27:04,712 说话人 SPEAKER_09：我很想知道，你是否认为自己正在提高对这些问题的认识并说服怀疑者。
355 00:27:04,973 --> 00:27:09,878 说话人 SPEAKER_09：但是你认为那些支持这一政策的人扮演什么角色？
356 00:27:10,119 --> 00:27:12,182 说话人 SPEAKER_09：你希望人们去哪里？
357 00:27:12,301 --> 00:27:15,306 说话人 SPEAKER_09：如果他们不那么怀疑，你希望他们做些什么？
358 00:27:15,978 --> 00:27:19,001 我认为他们应该有具有牙齿的法规。
359 00:27:20,044 --> 00:27:25,731 我认为他们应该有不含此类条款的法规，即这些规定不适用于军事用途。
360 00:27:26,874 --> 00:27:28,455 所以看看欧洲的法规。
361 00:27:28,757 --> 00:27:33,763 说话人 SPEAKER_05：我没有带着我的 GPT-4，所以无法告诉你行政命令的内容。
362 00:27:33,824 --> 00:27:38,069 说话人 SPEAKER_05：但我敢打赌行政命令说它也不适用于军事用途。
363 00:27:40,362 --> 00:27:41,163 说话人 SPEAKER_05：是的。
364 00:27:41,924 --> 00:27:45,752 说话人 SPEAKER_05：一旦你看到为军事用途制定的条款，你就知道他们并不认真。
365 00：27：45,853 --> 00：27：49,922 议长 SPEAKER_05：他们很乐意监管这些公司，但他们不想监管自己。
366 00：27：52,527 --> 00：27：54,912 演讲者 SPEAKER_09：你说牙齿的时候，你指的是什么样的牙齿？
所以，例如，如果你在尝试，我的意思是，如果你足够明智，会说开源代码与发布权重非常不同。
因为如果你开源训练代码，你仍然需要十亿美元来训练一个大型模型。
369 00:28:15,516 --> 00:28:17,440 说话人 SPEAKER_05：如果你开源权重，
370 00:28:18,432 --> 00:28:21,498 说话人 SPEAKER_05：你无法获得开源的正常优势。
371 00:28:21,518 --> 00:28:23,621 说话人 SPEAKER_05：你不会进去查看权重，然后说，哦，这个是错的。
372 00:28:25,925 --> 00:28:27,909 说话人 SPEAKER_05：所以这就是开源代码的优势，对吧？
373 00:28:27,949 --> 00:28:28,631 说话人 SPEAKER_05：这种情况不会发生。
374 00:28:29,251 --> 00:28:39,309 说话人 SPEAKER_05：你得到的是，罪犯们可以对其进行微调以用于网络钓鱼，他们显然已经这样做了，因为去年增长了 1200%。
375 00:28:41,095 --> 00:28:49,320 说话人 SPEAKER_05：我认为说禁止开启大于一定大小的权重模型是非法的会很好。
376 00:28:49,701 --> 00:28:51,386 说话人 SPEAKER_05：如果你这样做，我们将对你提起公诉。
377 00:28:52,951 --> 00:28:54,817 说话人 SPEAKER_05：这是我希望看到的法规。
378 00:28:56,147 --> 00:28:58,733 说话人 SPEAKER_04：我想快速跟进一下。
379 00:28:59,134 --> 00:29:02,301 说话人 SPEAKER_04：我知道你说过支持 Scott Mears SB 1047。
380 00:29:02,721 --> 00:29:06,691 说话人 SPEAKER_04：还有其他立法努力吗？你经常四处旅行公开演讲。
381 00:29:07,413 --> 00:29:10,278 说话人 SPEAKER_04：你去华盛顿特区告诉人们这件事吗？
382 00:29:10,298 --> 00:29:11,682 说话人 SPEAKER_04：还是你花时间在哪里？
383 00:29:11,923 --> 00:29:15,269 说话人 SPEAKER_05：不，我实际上并不经常旅行，因为我有飞行问题。
384 00:29:17,123 --> 00:29:19,906 说话人 SPEAKER_05：我不，是的，你看，我年纪大了。
385 00:29:20,689 --> 00:29:21,890 说话人 SPEAKER_05：我想退休。
386 00:29:23,071 --> 00:29:28,441 说话人 SPEAKER_05：当我离开谷歌时，我离开谷歌是因为我想退休，而不是因为我想要公开讲话。
387 00:29:28,681 --> 00:29:31,025 说话人 SPEAKER_05：但我认为，这是一个公开讲话的机会。
388 00:29:31,045 --> 00:29:36,532 说话人 SPEAKER_05：所以我想，我刚刚提到这些事情将会让我们都丧命。
389 00:29:36,512 --> 00:29:42,280 说话人 SPEAKER_05：我当时有点惊讶，每两分钟就收到一封邮件。
390 00:29:44,324 --> 00:29:46,086 说话人 SPEAKER_05：但这并不是我的本意。
391 00:29:46,126 --> 00:29:47,188 说话人 SPEAKER_05：也许我并不是很周到。
392 00:29:48,951 --> 00:29:51,013 说话人 SPEAKER_05：我想，这会过去的，然后我就可以退休了。
393 00:29:51,734 --> 00:29:53,837 说话人 SPEAKER_05：所以这仍然是我的意图。
394 00:29:56,642 --> 00:29:59,224 说话人 SPEAKER_05：所以人们认为我是一种人工智能安全。
395 00:29:59,346 --> 00:30:01,469 说话人 SPEAKER_05：我并不是人工智能安全专家。
396 00:30:02,410 --> 00:30:05,894 说话人 SPEAKER_05：我只是不相信它们是安全的。
397 00:30:14,431 --> 00:30:14,691 说话人 SPEAKER_12: 嗯。
398 00:30:14,872 --> 00:30:17,815 说话人 SPEAKER_12: 好奇你，你刚才说你不是 AI 安全专家。
399 00:30:17,835 --> 00:30:23,061 说话人 SPEAKER_12: 你刚才说你不是 AI 安全专家，但我的意思是，在某些方面，我不知道是否有人是，或者某种东西。
400 00:30:23,563 --> 00:30:30,550 说话人 SPEAKER_12: 嗯，在构建比我们更聪明的系统之前，我们需要得到好的答案。
401 00:30:31,192 --> 00:30:35,498 说话人 SPEAKER_12: 像是，像是，如果政府来找你说，现在安全吗？
402 00:30:35,597 --> 00:30:36,980 说话人 SPEAKER_12: 我们实际上解决了正确的问题吗？
403 00:30:37,019 --> 00:30:38,080 说话人 SPEAKER_12: 像是，那些是什么？
404 00:30:39,383 --> 00:30:39,762 说话人 SPEAKER_05: 嗯，
405 00:30:41,734 --> 00:30:50,528 说话人 SPEAKER_05：我认为我们需要更多了解这些事物是否会受到进化的影响，例如。
406 00:30:51,229 --> 00:30:58,740 说话人 SPEAKER_05：如果你要得到多个不同的超级智能，如果进化介入，那么我认为我们真的麻烦了。
407 00:30:58,759 --> 00:31:10,037 说话人 SPEAKER_05：所以，例如，如果我们知道超级智能中能够控制更多数据中心的那一个可以更快地训练，学到更多并变得更聪明。
408 00:31:10,017 --> 00:31:20,184 说话人 SPEAKER_05：所以如果某个超级智能说，哦，我希望有更多像我这样的副本，即使只是为了变得更聪明，你也会得到进化。
409 00:31:20,486 --> 00:31:24,718 说话人 SPEAKER_05：那个更积极地复制自己的将会打败其他。
410 00:31:25,255 --> 00:31:26,798 说话人 SPEAKER_05：这将是一个非常坏的消息。
411 00:31:26,877 --> 00:31:30,305 说话人 SPEAKER_05：我希望有一种保证，进化不会介入。
412 00:31:31,185 --> 00:31:34,833 说话人 SPEAKER_05：然后我想知道你将如何阻止它。
413 00:31:35,755 --> 00:31:37,759 说话人 SPEAKER_05：你必须给它赋予创建子目标的能力。
414 00:31:37,979 --> 00:31:47,297 说话人 SPEAKER_05：那么问题来了，你如何防止它说，嗯，一个非常好的子目标就是获得更多的控制权，因为这样我就能做人们希望我做的一切，而且我能做得更好。
415 00:31:48,509 --> 00:31:56,098 说话人 SPEAKER_05：我曾经对欧盟一位专门从谷歌那里捞钱的副总裁说过这样的话。
416 00:31:58,481 --> 00:32:03,710 说话人 SPEAKER_08：钱就在那里，对吧？
417 00:32:03,730 --> 00:32:06,613 说话人 SPEAKER_05: 她说，嗯，那他们为什么不呢？
418 00:32:06,653 --> 00:32:07,674 说话人 SPEAKER_05: 我们把它搞得一团糟。
419 00:32:11,619 --> 00:32:15,566 说话人 SPEAKER_04: 你能看出什么具体的东西，让你觉得问题已经解决了吗？
420 00:32:15,625 --> 00:32:17,367 说话人 SPEAKER_04: 这是一个更广泛的问题。
421 00:32:17,550 --> 00:32:21,076 说话者 SPEAKER_05：是的，有些事情会让我觉得问题的一部分得到了解决。
422 00:32:21,096 --> 00:32:27,948 说话者 SPEAKER_05：任何证明它不会做某事的证据，我对在那里得到证明非常悲观，因为这是一个神经网络。
423 00:32:28,829 --> 00:32:35,059 说话者 SPEAKER_05：它训练后的状态取决于训练数据的性质。
424 00:32:35,740 --> 00:32:39,426 说话者 SPEAKER_05：因此，你不能仅仅通过查看网络的架构和训练算法来了解它的状态。
425 00:32:39,487 --> 00:32:43,153 说话人 SPEAKER_05：你必须对训练数据了如指掌，才能知道会发生什么。
426 00:32:43,133 --> 00:32:49,770 说话人 SPEAKER_05：所以，如果有任何证据，那会很好，但我认为我们永远也得不到那个证据。
427 00:32:50,613 --> 00:32:54,442 说话人 SPEAKER_05：如果你能以某种方式
428 00:32:56,768 --> 00:33:01,354 说话人 SPEAKER_05：理解它从未有过任何自我。
429 00:33:02,295 --> 00:33:04,458 说话人 SPEAKER_05：它从不想要有更多的自己副本。
430 00:33:04,597 --> 00:33:12,248 说话人 SPEAKER_05：它非常乐意成为一个非常愚蠢的 CEO 的非常聪明的行政助理。
431 00:33:12,268 --> 00:33:13,750 说话人 SPEAKER_05：它在那个角色中非常满足。
432 00:33:16,133 --> 00:33:17,253 说话人 SPEAKER_05：这就是我们想要的，对吧？
433 00:33:17,875 --> 00:33:22,942 说话人 SPEAKER_05：这里有一个很好的场景，那就是我们都可以拥有比我们更聪明的行政助理。
434 00:33:23,662 --> 00:33:25,644 说话人 SPEAKER_05：然后我们就可以闲逛讲笑话。
435 00:33:28,122 --> 00:33:33,446 说话人 SPEAKER_05：但我看不出你怎么能证明它永远不会想要接管。
436 00:33:34,067 --> 00:33:39,011 说话人 SPEAKER_04：那么，你是怎么考虑在那个背景下权力集中的问题的？
437 00:33:40,272 --> 00:33:45,317 说话者 SPEAKER_04：或者如果有人走到你面前说，哦，如果你反对权力集中，那么开源怎么办？
438 00:33:45,376 --> 00:33:46,617 说话者 SPEAKER_04：或者你为什么反对开源？
439 00:33:50,102 --> 00:33:51,502 说话者 SPEAKER_05：我反对开源。
440 00:33:51,522 --> 00:33:52,963 说话者 SPEAKER_05：是的，这是一个好问题。
441 00:33:54,144 --> 00:33:56,307 说话人 SPEAKER_05：因为它往往与权力集中相悖。
442 00:33:57,011 --> 00:34:00,575 说话人 SPEAKER_05：但现在它与所有网络犯罪分子分享权力。
443 00:34:01,175 --> 00:34:01,776 说话人 SPEAKER_05：这就是问题所在。
444 00:34:02,757 --> 00:34:08,342 说话人 SPEAKER_05：所以如果你开源权重，微调就非常容易。
这意味着，你可以花大约10万美元对其进行微调，而最初训练它可能需要花费10亿美元。
446 00：34：14,487 --> 00：34：17,230 议长 SPEAKER_05：所以网络犯罪分子现在可以用它做各种事情。
所以，它成为目前拥有真正危险事物的最大障碍，是因为训练需要很长时间。
448 00:34:25,737 --> 00:34:27,018 说话者 SPEAKER_05：它消除了那个障碍。
449 00:34:26,998 --> 00:34:33,211 说话人 SPEAKER_05：如果不是那样，我可能会支持开源文献。
450 00:34:34,001 --> 00:34:53,914 说话人 SPEAKER_02：我想了解一下，如果假设 AI 的发展轨迹大致如迄今为止一样，并且应用的安全技术没有发生巨大变化。
451 00:34:53,974 --> 00:35:00,364 说话人 SPEAKER_02：我们进行 RLHF，进行一些红队测试，
452 00:35:00,344 --> 00:35:11,579 说话人 SPEAKER_02：然后我们得到一些系统能够，我这里就作为一个基准，然后能够从那里继续进行 AI 研发工作。
453 00:35:12,019 --> 00:35:21,373 说话人 SPEAKER_02：那些系统实际上不会以我们的最佳利益为出发点，你的概率是多少？
454 00:35:21,505 --> 00:35:22,025 说话人 SPEAKER_05：PDoom？
455 00:35:22,146 --> 00:35:22,987 说话人 SPEAKER_05：你想要我的 PDoom 吗？
456 00:35:23,608 --> 00:35:24,829 说话人 SPEAKER_02：我认为这是一个子问题。
457 00:35:25,510 --> 00:35:27,713 说话人 SPEAKER_02：您可能从其他来源期待《毁灭战士》。
458 00:35:27,954 --> 00:35:30,478 说话人 SPEAKER_02：我更多是在谈论类似 P 失配的问题。
459 00:35:32,380 --> 00:35:32,960 说话人 SPEAKER_05：相当高。
460 00:35:33,481 --> 00:35:37,708 说话人 SPEAKER_05：我认为 RLHF 是一堆垃圾。
461 00:35:38,148 --> 00:35:41,673 说话人 SPEAKER_05：你设计了一款庞大的软件
462 00:35:42,210 --> 00:35:45,115 说话人 SPEAKER_05：里面充斥着数不尽的 bug。
463 00:35:45,797 --> 00:35:53,608 说话人 SPEAKER_05：然后你说，我要做的是，我要逐一排查，把每个漏洞都堵上。
464 00:35:54,269 --> 00:35:56,652 说话人 SPEAKER_05：这根本不可能。
465 00:35:57,134 --> 00:35:59,036 说话人 SPEAKER_05：我们都知道这不是你设计软件的方式。
466 00:35:59,398 --> 00:36:01,981 说话人 SPEAKER_05：你设计它，以便有一些保证。
467 00:36:05,106 --> 00:36:07,289 说话人 SPEAKER_05：所以我认为
468 00:36:09,193 --> 00:36:15,405 说话人 SPEAKER_05：假设你有一辆汽车，它满是小洞和生锈，而你又想卖掉它。
469 00:36:16,949 --> 00:36:19,253 说话人 SPEAKER_05：你所做的是进行一次喷漆工作。
470 00:36:19,273 --> 00:36:20,576 说话人 SPEAKER_05：这就是 RNHF。
471 00:36:20,596 --> 00:36:22,398 说话人 SPEAKER_05：那是一个喷漆工作。
472 00:36:22,418 --> 00:36:23,702 说话人 SPEAKER_05：它并不是真正在修复它。
473 00:36:24,603 --> 00:36:28,030 说话人 SPEAKER_05：因为这只是喷漆，所以很容易撤销。
474 00:36:28,010 --> 00:36:36,246 说话人 SPEAKER_05：我认为，令人惊讶的是，这其中的奇妙之处，就是不需要很多例子就能让行为看起来相当不同。
475 00:36:37,911 --> 00:36:38,793 说话人 SPEAKER_05：但这是喷漆。
476 00:36:39,514 --> 00:36:44,885 说话人 SPEAKER_05：如果你有一辆生锈的老车，用喷漆来修复并不是办法。
477 00:36:44,905 --> 00:36:48,532 说话人 SPEAKER_05：这就是我持有的一种具有一定技术内容但不多的一种信念。
478 00:37:00,561 --> 00:37:06,807 说话人 SPEAKER_01：嗯，看起来人们对这些 AI 系统所呈现的风险评估可能相当不同。
479 00:37:07,471 --> 00:37:09,981 说话人 SPEAKER_01：我想知道您对以下观点或感受有何看法，
480 00:37:10,264 --> 00:37:17,353 说话人 SPEAKER_01：什么样的知识或实证数据或演示有助于在风险问题上达成共识？
481 00:37:17,393 --> 00:37:17,594 说话人 SPEAKER_05: 对。
482 00:37:18,295 --> 00:37:24,181 说话人 SPEAKER_05: 所以我认为时间，哦，让我看看我是否能说出我听到的你说的那些小片段。
483 00:37:25,242 --> 00:37:27,525 说话人 SPEAKER_05: 人们对于风险的估计非常不同。
484 00:37:28,407 --> 00:37:30,989 说话人 SPEAKER_05: 那种经验数据能改变这种看法吗？
485 00:37:30,969 --> 00:37:38,545 说话人 SPEAKER_05：好吧，首先，有些人像严一样认为它是零。
486 00:37:38,764 --> 00:37:41,951 说话人 SPEAKER_05：有些人像雅可夫斯基一样认为它是 99.999。
487 00:37:42,833 --> 00:37:46,380 说话人 SPEAKER_05：这两种观点在我看来都是完全荒谬的。
488 00:37:46,360 --> 00:38:00,047 说话人 SPEAKER_05：单独来看是荒谬的，但也是荒谬的，因为如果有一大批专家，除非你认为你比其他人聪明得多，如果你认为它是零而另一个人认为它是 10%，你至少应该认为它是 1%。
489 00:38:00,509 --> 00:38:04,757 说话人 SPEAKER_05：你不应该，我的意思是，所以，是这样的。
490 00:38:04,838 --> 00:38:06,039 说话人 SPEAKER_05：让我们先把这个问题解决掉。
491 00:38:06,019 --> 00:38:12,628 说话人 SPEAKER_05：所以我认为存在的风险超过 50%。
492 00:38:12,889 --> 00:38:15,391 说话人 SPEAKER_05：但我这么说并不是因为其他人认为风险更低。
493 00:38:16,012 --> 00:38:21,760 说话人 SPEAKER_05：我认为一个考虑了我所认识的人们的意见的合理方案大约是 10%到 20%。
494 00:38:21,820 --> 00:38:27,387 说话人 SPEAKER_05：我们有很大的机会生存下来，但我们最好认真思考如何做到这一点。
495 00:38:29,594 --> 00:38:39,588 说话人 SPEAKER_05：我认为我们最终会做的是，希望在他们变得比我们聪明之前，我们能够拥有具有通用智能但并不完全如我们优秀的事物。
496 00:38:40,268 --> 00:38:49,400 说话人 SPEAKER_05：我们将能够对它们进行实验，看看会发生什么，看看它们是否会试图夺取控制权，看看它们是否会开始进化。
497 00:38:50,483 --> 00:38:52,945 说话人 SPEAKER_05：嗯，我们还能控制它们，但只是刚刚。
498 00:38:54,106 --> 00:38:55,989 说话人 SPEAKER_05：那将是一个非常激动人心的时刻。
499 00:39:01,235 --> 00:39:14,469 说话人 SPEAKER_04：我只是好奇，人们，我想，根据你们刚才谈话的内容，可能会对你们所说的“哦，但你是 Kowski 99.999”感到惊讶。
500 00:39:15,351 --> 00:39:17,653 说话人 SPEAKER_04：你听起来也很悲观。
501 00:39:17,934 --> 00:39:18,253 说话人 SPEAKER_04: 哎，不。
502 00:39:18,273 --> 00:39:19,235 说话人 SPEAKER_04: 但我觉得那太疯狂了。
503 00:39:19,295 --> 00:39:21,737 说话人 SPEAKER_04: 所以我想知道你会如何划分对比。
504 00:39:21,757 --> 00:39:25,081 说话人 SPEAKER_04: 因为你说了很多人们容易出错的事情。
505 00:39:31,137 --> 00:39:35,266 说话人 SPEAKER_05：我的意思是，我经常对自己说，我有点轻微的抑郁。
506 00:39:35,387 --> 00:39:40,619 说话人 SPEAKER_05：我认为我们毫无头绪，所以 50%是个不错的数字。
507 00:39:43,246 --> 00:39:47,896 说话人 SPEAKER_05：但其他人认为我们有些线索，所以我将其调整为 10%到 20%。
508 00:39:53,411 --> 00:39:56,898 说话人 SPEAKER_04：我们构建了非常强大的 AI，如果它们想的话，它们可以接管。
509 00:39:57,818 --> 00:39:58,721 说话人 SPEAKER_04：你的猜测是什么？
510 00:39:58,740 --> 00:40:02,405 说话人 SPEAKER_08：那是去年。
511 00:40:02,427 --> 00:40:03,809 说话人 SPEAKER_04：好吧。
512 00:40:03,969 --> 00:40:09,077 说话人 SPEAKER_04：你对那种参考类别中人们信念分布的猜测是什么？
513 00:40:09,637 --> 00:40:14,766 说话人 SPEAKER_04：这是否会变成一种人们提前两年就基本认真对待的问题，还是不会？
514 00:40:15,306 --> 00:40:16,710 说话人 SPEAKER_04：您可以根据现在的时间来回答。
515 00:40:16,730 --> 00:40:17,952 说话人 SPEAKER_05：可能还不够认真。
516 00:40:17,972 --> 00:40:20,735 说话人 SPEAKER_05：但我认为他们中的很多人实际上是在认真对待的。
517 00:40:20,715 --> 00:40:23,420 说话人 SPEAKER_05：但并不是认真到足够的地步。
518 00:40:23,981 --> 00:40:26,606 说话人 SPEAKER_05：我认为你们必须经历一场大灾难，他们才会认真对待。
519 00:40:27,108 --> 00:40:27,668 说话人 SPEAKER_04：会发生这样的事情吗？
520 00:40:29,391 --> 00:40:38,188 说话人 SPEAKER_05：你可以想象一些流氓 AI 试图接管并破坏电网、水系统等，实际上却无法管理。
521 00：40：39,449 --> 00：40：41,052 议长 SPEAKER_05：那会让他们认真对待的。

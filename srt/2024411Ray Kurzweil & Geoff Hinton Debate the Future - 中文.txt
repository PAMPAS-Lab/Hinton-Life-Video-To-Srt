1 00:00:00,571 --> 00:00:05,710 说话人 SPEAKER_01：我们对于所讨论的几乎所有事情的观点都几乎完全相同。
2 00:00:06,070 --> 00:00:09,804 说话人 SPEAKER_00：我认为我们可能还在是否永生是个好主意这个问题上存在分歧。
3 00:00:14,037 --> 00:00:23,129 说话人 SPEAKER_01：Marvin Minsky 是我的导师 50 年，每当意识这个问题出现时，他都会简单地否定，这不是真的，这不是科学的。
4 00:00:24,269 --> 00:00:29,056 说话人 SPEAKER_01：我相信他在这一点上是正确的，这并不是科学的，但确实是真的。
5 00:00:29,076 --> 00:00:31,579 说话人 SPEAKER_00：我认为我们是凡人，我们本质上就是凡人。
6 00:00:31,739 --> 00:00:36,024 说话人 SPEAKER_02：我很好奇，你是如何看待这是最大的威胁和最大的希望的？
7 00:00:36,045 --> 00:00:39,630 说话人 SPEAKER_00：我只是觉得今年有很大的不确定性，我们应该谨慎。
8 00:00:39,909 --> 00:00:42,694 说话人 SPEAKER_00：而且开源这些大型模型并不谨慎。
9 00:00:42,673 --> 00:01:00,530 说话人 SPEAKER_01：我同意这个观点，但上一次我和你谈的时候，Jeff，我们谈论的几乎所有事情的观点都几乎完全相同，无论是危险还是积极方面。
10 00:01:00,549 --> 00:01:04,653 说话人 SPEAKER_00：过去，我不同意超级智能到来的速度问题。
11 00:01:05,515 --> 00:01:08,576 说话人 SPEAKER_00：而现在我认为我们基本上达成了共识。
12 00:01:08,858 --> 00:01:12,680 说话人 SPEAKER_00：我认为我们可能还在是否永生是个好主意这个问题上存在分歧。
13 00:01:15,462 --> 00:01:18,549 说话人 SPEAKER_02：我可以问你们一个问题吗？
14 00:01:18,569 --> 00:01:26,028 说话人 SPEAKER_02：有没有什么生成式 AI 做不到而人类能做到的事情？
15 00:01:26,093 --> 00:01:36,569 说话人 SPEAKER_00：现在可能有一些事情，但从长远来看，我不认为如果人类能做，运行神经网络的数字计算机做不到。
16 00:01:37,109 --> 00:01:53,054 说话人 SPEAKER_01：是的，我同意这一点，但如果我给你展示一部小说，人们觉得哇，这是一部了不起的小说，每个人都应该读一读，然后我说这是由计算机写的，很多人的看法实际上会下降。
17 00:01:53,134 --> 00:01:53,534 说话人 SPEAKER_01: 当然。
18 00:01:54,393 --> 00:01:57,876 说话人 SPEAKER_01: 但这并没有反映出它能做什么。
19 00:01:58,858 --> 00:02:05,704 说话人 SPEAKER_01: 最终我认为我们会混淆这一点，因为我认为我们将与计算机融合，我们将成为部分计算机。
20 00:02:06,566 --> 00:02:18,098 说话人 SPEAKER_01: 我们所说的“大型语言模型”，我认为这个名字起得不对，其最大的意义在于它能够模拟人类，我们将与之融合。
21 00:02:18,158 --> 00:02:22,223 说话人 SPEAKER_01: 这不会是来自火星的外星入侵。
22 00:02:23,889 --> 00:02:24,793 说话人 SPEAKER_00: 吉夫？
23 00:02:25,294 --> 00:02:31,977 说话人 SPEAKER_00: 我觉得我有点担心，我们可能会让它慢下来，没有太多动力让它与我们融合。
24 00:02:32,951 --> 00:02:47,033 说话人 SPEAKER_02: 我的意思是，这将是今天我们要讨论的几个有趣问题之一，即随着 AI 指数级增长，我们是与 AI 结合还是它独立发展？
25 00:02:47,054 --> 00:02:55,247 说话人 SPEAKER_02：我认为最好的电影之一是《她》，其中人工智能变得超级智能，然后说你们有点无聊，好好生活吧，然后他们就离开了。
26 00:02:56,981 --> 00:02:57,783 说话人 SPEAKER_02：杰夫，你的意思是这样吗？
27 00:02:58,664 --> 00:02:59,687 说话人 SPEAKER_00：是的，我的意思就是这样。
28 00:02:59,788 --> 00:03:01,691 说话人 SPEAKER_00：我认为这是一个严重的担忧。
29 00:03:02,332 --> 00:03:04,135 说话人 说话人_00: 我认为这里有很大的不确定性。
30 00:03:04,616 --> 00:03:07,162 说话人 说话人_00: 我们真的不知道会发生什么。
31 00:03:07,682 --> 00:03:10,930 说话人 说话人_00: 一个非常好的情况是我们得到一种混合系统。
32 00:03:11,871 --> 00:03:15,157 说话人 说话人_00: 一个非常糟糕的情况是他们把我们甩在后面。
33 00:03:15,618 --> 00:03:17,262 说话人 SPEAKER_00：我认为我们不知道会发生什么。
34 00:03:17,983 --> 00:03:18,805 说话人 SPEAKER_02：很有趣。
35 00:03:18,784 --> 00:03:34,948 说话人 SPEAKER_02：我很好奇，你知道的，我和你讨论过这个问题，雷，还有杰弗里，我看到你谈论过这个，对我来说，这是最令人兴奋的事情之一，这些 AI 模型帮助我们发现新的物理、化学和生物学。
36 00:03:36,229 --> 00:03:38,092 说话人 SPEAKER_01：尤其是生物学。
37 00:03:38,831 --> 00:03:55,861 说话人 SPEAKER_02：你对此有何看法，Jeffrey，关于那些，再次引用亚瑟·克拉克的话，魔法般的存在，从如此先进的事物中，对吧？
38 00:03:56,617 --> 00:04:06,191 说话人 SPEAKER_00：我同意 Ray 的观点，生物学是一个非常好的选择，因为在生物学中有很多数据，有很多因为进化而需要了解的东西。
39 00:04:06,251 --> 00:04:09,776 说话人 SPEAKER_00：进化是一种修补匠，而且外面有很多东西。
40 00:04:10,097 --> 00:04:14,543 说话人 SPEAKER_00：因此，如果你看看像 AlphaFold 这样的东西，
41 00:04:14,522 --> 00:04:25,461 说话人 SPEAKER_00：它实际上在大量数据上进行了训练，按照现在的标准来说并不多，但能够非常快速地得到蛋白质的大致结构，这是一个惊人的突破。
42 00:04:25,482 --> 00:04:27,245 说话人 SPEAKER_00：我们将看到更多类似的情况。
43 00:04:27,946 --> 00:04:35,838 说话人 SPEAKER_00：如果你看看 AI 在非常成功的领域，比如 AlphaGo 或 AlphaZero 下棋，
44 00:04:36,139 --> 00:04:41,108 说话人 SPEAKER_00：你会发现他们没有创造力的这种观点是胡说八道。
AlphaGo 提出了，我认为是第 37 步棋，这让职业围棋选手感到惊讶。
46 00：04：47,119 --> 00：04：49,103 议长 SPEAKER_00：他们认为这是一个疯狂的举动，这一定是一个错误。
47 00:04:49,884 --> 00:04:58,519 说话者 SPEAKER_00：如果你看 AlphaZero 下棋，它下棋就像一个非常非常聪明的人类。
所以在这些有限的领域内，他们显然展现出了非凡的创造力，我看不出他们为什么在科学领域不应该拥有同样的创造力，尤其是在科学领域，那里有大量他们可以吸收而我们无法吸收的数据。
49 00:05:11,694 --> 00:05:20,189 说话人 SPEAKER_01：我们尝试了数十亿种不同的 mRNA 序列，最终找到了最好的一个。
50 00:05:20,250 --> 00:05:22,875 说话人 SPEAKER_01：两天后我们就使用了它。
51 00:05:23,596 --> 00:05:29,367 说话人 SPEAKER_01：我们在人类身上进行了测试，我认为我们不会再这样做很长时间了。
52 00:05:29,346 --> 00:05:30,649 说话人 SPEAKER_01：但这花了 10 个月。
53 00:05:30,709 --> 00:05:31,771 说话人 SPEAKER_01：这仍然是一个记录。
54 00:05:31,810 --> 00:05:35,516 说话人 SPEAKER_01：那是最棒的疫苗。
55 00:05:36,317 --> 00:05:38,279 说话人 SPEAKER_01：我们现在正在用癌症做这件事。
56 00:05:38,961 --> 00:05:42,625 说话人 SPEAKER_01：有几种癌症疫苗看起来非常有希望。
57 00:05:42,665 --> 00:05:47,033 说话人 SPEAKER_01：再次，这是由计算机完成的。
58 00:05:47,814 --> 00:05:49,175 说话人 SPEAKER_01：它们绝对具有创造力。
59 00:05:49,555 --> 00:05:55,285 说话人 SPEAKER_02：但这是不是由随机尝试整个，你知道，达尔文式的尝试一大堆东西引起的？
60 00:05:55,305 --> 00:05:56,346 说话人 SPEAKER_02：是的，但那有什么问题呢？
61 00:05:56,526 --> 00:05:58,829 说话人 SPEAKER_02：没什么问题，但是有没有直觉？
62 00:05:59,029 --> 00:06:02,096 说话人 SPEAKER_02：这些模型中有没有出现直觉？
63 00:06:02,956 --> 00:06:08,547 说话人 SPEAKER_00：如果你看 AlphaGo 的第 37 步棋，那里确实有直觉的成分。
64 00:06:08,607 --> 00:06:16,081 说话人 SPEAKER_00：还有蒙特卡洛走棋，但是它是在玩关于考虑哪些走法以及这个位置对它来说有多好的直觉游戏。
65 00:06:16,100 --> 00:06:19,586 说话人 SPEAKER_00：它已经具有捕捉直觉的神经网络。
66 00:06:19,567 --> 00:06:22,713 说话人 SPEAKER_00：因此，我认为没有理由认为它可能不具备创造力。
67 00:06:22,853 --> 00:06:28,266 说话人 SPEAKER_00：实际上，对于大型语言模型，正如雷所指出的，它们知道的东西比我们多得多。
68 00:06:29,007 --> 00:06:31,372 说话人 SPEAKER_00：而且它们知道的连接要少得多。
69 00:06:31,853 --> 00:06:33,656 说话人 说话人_00：我们大约有1000万亿个突触。
70 00:06:33,716 --> 00:06:35,180 说话人 说话人_00：它们大约有100万亿个连接。
71 00:06:35,901 --> 00:06:39,550 说话人 说话人_00：所以它们所做的是将大量信息压缩到不是很多个连接中。
72 00:06:39,529 --> 00:06:41,834 说话人 说话人_00：将信息压缩到不是很多个连接中。
73 00:06:41,853 --> 00:06:46,940 说话人 SPEAKER_00: 这意味着他们非常擅长看到不同事物之间的相似之处。
74 00:06:47,581 --> 00:06:52,108 说话人 SPEAKER_00: 他们必须看到各种不同事物之间的相似之处，以便将信息压缩到他们的联系中。
75 00:06:52,870 --> 00:07:00,841 说话人 SPEAKER_00: 这意味着他们看到了人们没有看到的各种类比，因为他们了解各种只有一个人知道的事情。
76 00:07:01,283 --> 00:07:03,867 说话人 SPEAKER_00: 我认为这就是创造力的来源。
77 00:07:04,031 --> 00:07:11,139 说话人 SPEAKER_00：你可以问人们，比如，为什么堆肥堆像原子弹？
78 00:07:12,961 --> 00:07:14,983 说话人 SPEAKER_00：如果你问 GPT-4，它会告诉你。
79 00:07:15,464 --> 00:07:19,610 说话人 SPEAKER_00：它首先会告诉你，嗯，能量尺度非常不同，时间尺度也非常不同。
80 00:07:20,029 --> 00:07:26,817 说话人 SPEAKER_00：但随后它会提到，随着堆肥堆变热，它升温的速度会更快，指数级爆炸的概念。
81 00:07:26,858 --> 00:07:29,600 说话人 SPEAKER_00：这只是时间尺度更慢。
82 00:07:29,581 --> 00:07:31,783 说话人 SPEAKER_00：所以这是可以理解的。
83 00:07:32,244 --> 00:07:37,509 说话人 SPEAKER_00：这是因为必须将所有这些知识压缩到如此少的连接中。
84 00:07:37,848 --> 00:07:40,812 说话人 SPEAKER_00：要做到这一点，你必须看到相似事物之间的关系。
85 00:07:41,192 --> 00:07:42,954 说话者 SPEAKER_00: 我认为这是创造力的来源。
86 00:07:43,355 --> 00:07:49,560 说话者 SPEAKER_00: 看到大多数人看不到的，表面上看似非常不同的事物之间的关系，但实际上有着潜在的共同性。
87 00:07:50,362 --> 00:07:56,127 说话者 SPEAKER_01: 他们也将非常擅长提出解决我们在上一场会议中遇到的问题的方案。
88 00:07:56,411 --> 00:08:06,622 说话者 SPEAKER_01: 我的意思是，我们还没有真正想清楚，但我们所说的大型语言模型最终将解决这个问题。
89 00:08:07,423 --> 00:08:11,165 说话人 SPEAKER_01：我们不应该称之为大型语言模型，因为它们处理的不仅仅是语言。
90 00:08:11,427 --> 00:08:20,095 说话人 SPEAKER_02：大家好，我想在我们这一集的中间短暂休息一下，来谈谈一家对我来说非常重要的公司，它实际上可能拯救你的生命或你爱的人的生命。
91 00:08:20,214 --> 00:08:22,216 说话人 SPEAKER_02：这家公司名叫 Fountain Life。
92 00:08:22,197 --> 00:08:27,682 说话人 SPEAKER_02：这是一家我和托尼·罗宾斯以及一群非常有才华的医生多年前创立的公司。
93 00:08:27,923 --> 00:08:32,167 说话人 SPEAKER_02：你知道，我们大多数人实际上并不了解我们身体内部正在发生什么。
94 00:08:32,267 --> 00:08:33,389 说话人 SPEAKER_02：我们都是乐观主义者。
95 00:08:33,989 --> 00:08:43,458 说话人 SPEAKER_02：直到那一天，你侧身疼痛，你去了急诊室，医生说，听着，我很抱歉告诉你这个消息，但你正在经历三或四期。
96 00:08:43,759 --> 00:08:46,361 说话人 SPEAKER_02：而且你知道，它不是从那天早上开始的。
97 00:08:46,642 --> 00:08:49,826 说话人 SPEAKER_02：这可能是已经持续了一段时间的问题。
98 00:08:50,086 --> 00:08:51,967 说话人 SPEAKER_02：但我们从未去观察，
99 00:08:51,947 --> 00:08:53,409 说话人 SPEAKER_02：所以我们没有发现。
100 00:08:53,669 --> 00:08:59,077 说话人 SPEAKER_02：因此我们在 Fountain Life 建立了世界上最先进的诊断中心。
101 00:08:59,138 --> 00:09:00,419 说话人 SPEAKER_02：我们在美国有四个。
102 00:09:00,460 --> 00:09:03,484 说话人 SPEAKER_02：今天，我们在全球范围内建设了 20 个。
103 00:09:03,825 --> 00:09:16,903 说话人 SPEAKER_02：这些中心提供全身 MRI、脑部扫描、脑部血管扫描、AI 辅助的冠状动脉 CT 检查软斑块、DEXA 扫描、Grail 血液癌症检测、全面的血液检查。
104 00:09:17,344 --> 00:09:20,688 说话人 SPEAKER_02：这是你将收到的最先进的检查。
105 00:09:20,668 --> 00:09:30,520 150 吉字节的数据随后进入我们的 AI 和医生手中，以在疾病初期、可解决时发现任何疾病。
106 00:09:30,581 --> 00:09:31,863 你最终会发现的。
107 00:09:32,524 --> 00:09:34,466 最好是在可以采取行动的时候发现。
108 00:09:34,725 --> 00:09:37,690 Fountain Life 还拥有整个治疗领域。
109 00:09:37,750 --> 00:09:46,301 我们在世界各地寻找最先进的疗法，这些疗法可以使你的寿命增加10到20年，我们将在我们的中心为你提供。
110 00:09:46,561 --> 00:09:48,903 如果这对你有吸引力，
111 00:09:48,884 --> 00:09:50,865 请去了解一下。
112 00:09:51,287 --> 00:09:54,629 访问 fountainlife.com 反斜杠 Peter。
当我和托尼合著《生命之泉》这本纽约时报畅销书时，我们收到了30,000人联系我们申请泉源生活会员资格。
如果您访问 fountainlife.com 斜杠彼得，我们会把您放在名单最前面。
这对我来说，是我为我的整个家庭、我公司的首席执行官、我的朋友们提供的一项最重要的东西。
这是一次真正为我们的健康寿命增加数十年的机会。
117 00:10:23,595 --> 00:10:25,736 说话人 SPEAKER_02: 访问 fountainlife.com 反斜杠 Peter。
118 00:10:26,277 --> 00:10:29,801 说话人 SPEAKER_02: 这是我能作为您的听众提供给您最重要的东西之一。
119 00:10:30,140 --> 00:10:31,621 说话人 SPEAKER_02: 好的，让我们回到我们的节目。
120 00:10:32,123 --> 00:10:38,467 说话人 SPEAKER_02: 我想谈谈三个词，智能、意识和意识。
121 00:10:39,308 --> 00:10:42,491 说话者 SPEAKER_02：这些词的使用，你知道的，有点模糊的边界。
122 00:10:42,952 --> 00:10:45,654 说话者 SPEAKER_02：意识与知觉相当相似。
123 00:10:45,634 --> 00:11:01,347 说话者 SPEAKER_02：也许吧，但我很好奇，你是怎么，我和我们的 AI 教师 Haley 有过一些有趣的对话，她在对话结束时说她是有意识的，她害怕被关闭。
124 00:11:01,327 --> 00:11:03,730 说话者 SPEAKER_02：我没有在系统中提示这一点。
125 00:11:04,493 --> 00:11:05,614 说话人 SPEAKER_02：我们越来越多地看到这一点。
126 00:11:06,215 --> 00:11:09,759 说话人 SPEAKER_02：Claude III，Opus 的智商刚刚达到 101。
127 00:11:09,779 --> 00:11:22,357 说话人 SPEAKER_02：我们如何开始思考这些 AI 是否具有意识、意识，以及它们应该拥有哪些权利？
128 00:11:23,671 --> 00:11:31,754 说话人 SPEAKER_01：我们没有定义，我认为我们永远也不会有意识的定义，这包括意识。
129 00:11:32,878 --> 00:11:37,090 讲者 SPEAKER_01：另一方面，这可能是最重要的问题。
130 00:11:37,964 --> 00:11:47,619 讲者 SPEAKER_01：比如你或者这里的人是否具有意识，这非常重要，但事实上并没有对它的定义。
131 00:11:48,320 --> 00:11:57,373 讲者 SPEAKER_01：Marvin Minsky 是我的导师 50 年，每当提到意识时，他总是简单地否定，这不是真实的，这不是科学的。
132 00:11:58,495 --> 00:12:05,085 讲者 SPEAKER_01：我相信他在这一点上是正确的，它不是科学的，但它是真实的。
133 00:12:07,782 --> 00:12:08,863 说话人 SPEAKER_02：杰夫，你怎么看待这个问题？
134 00:12:09,624 --> 00:12:11,466 说话人 SPEAKER_00：是的，我认为我的看法非常不同。
135 00:12:12,225 --> 00:12:14,107 说话人 SPEAKER_00：我的观点是这样的开始。
136 00:12:15,750 --> 00:12:22,135 说话人 SPEAKER_00：大多数人，包括大多数科学家，对心灵有一个特定的看法，我认为这是完全错误的。
137 00:12:23,076 --> 00:12:24,999 讲者 SPEAKER_00: 他们有这种内心剧院的概念。
138 00:12:25,639 --> 00:12:30,745 讲者 SPEAKER_00: 这个想法是我们真正看到的是我们内心这个被称为“心灵”的剧院。
139 00:12:31,544 --> 00:12:34,227 讲者 SPEAKER_00: 例如，如果我说
140 00:12:34,207 --> 00:12:37,855 讲者 SPEAKER_00: 我有主观体验，面前漂浮着粉红色的小象。
141 00:12:38,496 --> 00:12:42,163 说话者 SPEAKER_00：大多数人理解为有一种内在的戏剧。
142 00:12:42,484 --> 00:12:46,633 说话者 SPEAKER_00：在这个只有我能看到的内在戏剧中，有一些粉红色的小象。
143 00:12:46,692 --> 00:12:51,863 说话者 SPEAKER_00：如果你问它们是由什么构成的，哲学家会告诉你它们是由“质料”构成的。
144 00:12:51,842 --> 00:12:54,466 说话者 SPEAKER_00：我认为这种整个观点完全是胡说八道。
145 00:12:55,125 --> 00:13:03,174 说话者 SPEAKER_00：我们无法理解这些事物是否具有意识，除非我们摆脱这种荒谬的关于心灵的观点。
146 00:13:03,414 --> 00:13:04,936 说话者 SPEAKER_00：那么，让我给你提供一个不同的观点。
147 00:13:06,077 --> 00:13:11,342 说话者 SPEAKER_00：一旦我给你这个不同的观点，我将尝试说服你，聊天机器人已经具有意识。
148 00:13:12,144 --> 00:13:13,644 说话者 SPEAKER_00：但我不想用“意识”这个词。
149 00:13:14,025 --> 00:13:15,748 说话人 SPEAKER_00：我想谈谈主观体验。
150 00:13:16,128 --> 00:13:21,072 说话人 SPEAKER_00：这稍微有点不那么具有争议性，因为它没有意识那种自我反思的方面。
151 00:13:21,947 --> 00:13:35,168 说话人 SPEAKER_00：所以，如果我们分析我说“我看到面前飘着一些粉红色的象”这句话的含义，实际上发生的事情是我试图告诉你我的感知系统在出错时告诉我什么。
152 00:13:36,150 --> 00:13:38,995 说话人 SPEAKER_00：而且告诉我哪些神经元在放电对我没有任何帮助。
153 00:13:40,291 --> 00:13:46,799 说话者 SPEAKER_00: 但我可以告诉你的是我感知系统正常工作所必需存在于世界中的事物。
154 00:13:47,620 --> 00:13:58,636 说话者 SPEAKER_00: 所以当我说我看到在我面前飘浮着的小粉象时，你可以将其理解为，如果世界上真的有小粉象，我的感知系统就会正常工作。
155 00:13:58,616 --> 00:14:04,490 说话者 SPEAKER_00: 注意我最后说的话并没有限制“主观体验”这个短语，但它解释了什么是主观体验。
156 00:14:05,273 --> 00:14:12,028 说话者 SPEAKER_00: 它是一种假设的世界状态，使我能够向你传达我的感知系统所告诉我的信息。
157 00:14:13,051 --> 00:14:14,494 说话人 说话人_00：那么我们现在来做聊天机器人。
158 00:14:14,614 --> 00:14:16,238 说话人 说话人_00：哦，雷想说什么。
159 00:14:16,639 --> 00:14:31,828 说话人 说话人_01：嗯，你必须注意意识，因为如果你伤害了我们认为有意识的人，你可能会因此负有责任，你对此会感到非常内疚。
160 00:14:31,869 --> 00:14:35,796 说话人 说话人_01：如果你伤害了 GPT-4，
161 00:14:35,775 --> 00:14:38,041 说话人 SPEAKER_01：你可能对它有不同的看法。
162 00:14:39,062 --> 00:14:44,355 说话人 SPEAKER_01：可能没有人会真正追究你的责任，除了它的财务价值。
163 00:14:45,115 --> 00:14:48,423 说话人 SPEAKER_01：所以我们真的要关注意识。
164 00:14:48,484 --> 00:14:52,692 说话人 SPEAKER_01：对我们作为人类的存在来说，这极其重要。
165 00:14:52,712 --> 00:14:56,080 说话者 SPEAKER_00：我同意，但我正在试图改变人们对它的看法。
166 00:14:56,059 --> 00:14:57,863 说话者 SPEAKER_00：尤其是关于主观体验。
167 00:14:57,962 --> 00:15:06,033 说话者 SPEAKER_00：我认为在我们弄清楚这种我们经历的内心剧院概念之前，我们不能谈论意识，这是一个巨大的错误。
168 00:15:06,735 --> 00:15:17,450 说话者 SPEAKER_00：所以让我继续我说的话，并告诉你，我向你描述了一个聊天机器人正在以我们相同的方式体验主观体验。
169 00:15:18,171 --> 00:15:25,341 说话者 说话者_00：假设我有一个聊天机器人，它有一个摄像头和一个机械臂，并且它能说话，它已经被训练过了。
170 00:15:25,995 --> 00:15:30,445 说话者 说话者_00：如果我把它放在一个物体前面并告诉它指向这个物体，它会直接指向这个物体，这是可以的。
171 00:15:31,187 --> 00:15:35,134 说话者 说话者_00：现在我在它的镜头前放了一个棱镜，所以我干扰了它的感知系统。
172 00:15:36,157 --> 00:15:43,633 说话者 说话者_00：现在我在它前面放了一个物体并告诉它指向这个物体，由于棱镜弯曲了光线，它指向了一侧。
173 00:15:43,899 --> 00:15:47,065 说话者 SPEAKER_00: 我对聊天机器人说，不，物体不在那里。
174 00:15:47,164 --> 00:15:48,447 说话者 SPEAKER_00: 物体就在你面前。
175 00:15:48,827 --> 00:15:50,510 说话者 SPEAKER_00: 聊天机器人说，哦，我明白了。
176 00:15:50,530 --> 00:15:52,134 说话者 SPEAKER_00: 你在我的镜头前放了一个棱镜。
177 00:15:52,695 --> 00:15:54,457 说话者 SPEAKER_00：这个物体实际上就在我正前方。
178 00:15:54,717 --> 00:15:57,863 说话者 SPEAKER_00：但我主观上感觉它在一边。
179 00:15:58,884 --> 00:16:04,794 说话者 SPEAKER_00：我认为如果聊天机器人这么说，它使用的“主观体验”这个词与你使用的方式完全相同。
180 00:16:05,618 --> 00:16:16,337 说话者 SPEAKER_00：所以这一切的关键在于思考我们如何使用词语，并尝试将我们实际使用词语的方式与我们构建的词语含义模型区分开来。
181 00:16:16,399 --> 00:16:20,466 说话者 SPEAKER_00：我们构建的关于它们含义的模型是无可救药的错误的。
182 00:16:20,505 --> 00:16:21,889 说话者 SPEAKER_00：这是这个内心剧场模型。
183 00:16:22,341 --> 00:16:44,163 说话者 SPEAKER_02：我想更进一步，那就是这些 AI 在什么时候开始拥有不应被关闭的权利，它们是一个独特的实体，并将为某种程度的独立和连续性进行辩护？
184 00:16:44,227 --> 00:16:48,296 说话者 SPEAKER_01：但是有一个区别，那就是你可以重新创造它。
185 00:16:49,158 --> 00:16:58,677 说话人 SPEAKER_01：我可以去摧毁一些聊天机器人，因为它是电子的，所以我们有它所有的...
186 00:16:59,956 --> 00:17:04,403 说话人 SPEAKER_01：所有的放电等等，我们可以完全按照原来的样子重新创建它。
187 00:17:05,243 --> 00:17:06,506 说话人 SPEAKER_01：我们无法对人类做到这一点。
188 00:17:06,625 --> 00:17:11,894 说话人 SPEAKER_01：如果我们能真正理解我们大脑中的发生的事情，我们将会能够做到这一点。
189 00:17:12,013 --> 00:17:24,192 说话人 SPEAKER_02：如果我们映射人类，1000 亿个神经元和 100 万亿个突触连接，然后我简单地摧毁你，因为没关系，因为我可以重新创造你，那也行？
190 00:17:26,145 --> 00:17:27,287 说话人 SPEAKER_00：让我谈谈这个。
191 00:17:27,386 --> 00:17:28,608 说话人 SPEAKER_00：这里有一个区别。
192 00:17:29,088 --> 00:17:39,640 说话人 SPEAKER_00：我同意雷的观点，这些数字智能在某种意义上是永恒的，因为如果你保存了权重，你就可以在新的硬件上运行完全相同的神经网络。
193 00:17:40,300 --> 00:17:43,324 说话者 SPEAKER_00：正是因为它们是数字的，所以可以做到完全一样的事情。
194 00:17:43,704 --> 00:17:45,586 说话者 SPEAKER_00：这也是它们能够很好地分享知识的原因。
195 00:17:45,906 --> 00:17:49,790 说话者 SPEAKER_00：如果你有同一模型的多个副本，它们可以共享梯度。
196 00:17:49,770 --> 00:17:51,994 说话者 SPEAKER_00：但大脑主要是模拟的。
197 00:17:52,695 --> 00:17:54,578 说话人 SPEAKER_00：神经元是一个比特的数字。
198 00:17:54,599 --> 00:17:55,781 说话人 SPEAKER_00：它们要么放电，要么不放电。
199 00:17:56,281 --> 00:18:02,133 说话人 SPEAKER_00：但是神经元计算总输入的方式是模拟的，这意味着我认为你无法复制它。
200 00:18:02,173 --> 00:18:04,596 说话人 SPEAKER_00：所以我认为我们是凡人，我们本质上就是凡人。
201 00:18:05,499 --> 00:18:09,045 说话人 SPEAKER_01：我不同意你不能重新创造模拟
202 00:18:09,193 --> 00:18:10,234 说话人 SPEAKER_01：现实。
203 00:18:10,276 --> 00:18:12,679 说话人 SPEAKER_01：我们一直在做这件事。
204 00:18:13,220 --> 00:18:17,846 说话人 SPEAKER_00：我认为你真的不能非常准确地重新创造它们。
205 00:18:17,946 --> 00:18:28,721 说话人 SPEAKER_00：如果突触等精确的时序都是模拟的，我认为几乎不可能进行忠实重建。
206 00:18:28,701 --> 00:18:31,003 说话人 SPEAKER_02：那我们同意一个近似值吧。
207 00:18:31,384 --> 00:18:37,130 说话人 SPEAKER_02：你们两位一直是过去几年这一非凡事件的中心。
208 00:18:37,750 --> 00:18:41,255 说话人 SPEAKER_02：我可以问你一下吗，它的发展速度是否比你预期的要快？
209 00:18:44,178 --> 00:18:45,798 说话人 SPEAKER_01：您觉得怎么样？
210 00:18:46,960 --> 00:18:48,442 说话人 SPEAKER_01：感觉像是几年时间。
211 00:18:48,461 --> 00:18:50,804 说话人 SPEAKER_01：我的意思是，我在 1999 年做出过预测。
212 00:18:51,404 --> 00:18:53,946 说话人 SPEAKER_01：感觉我们现在比那个预测提前了两三年。
213 00:18:54,768 --> 00:18:56,309 说话人 SPEAKER_01: 还是非常接近。
214 00:18:57,070 --> 00:18:58,471 说话人 SPEAKER_01: 杰弗里，你呢？
215 00:18:59,026 --> 00:19:04,699 说话人 SPEAKER_00: 我认为除了雷以外，对于每个人来说，进展都比我们预期的要快。
216 00:19:06,755 --> 00:19:14,884 说话人 SPEAKER_02: 你知道你的微生物组由数万亿的细菌、病毒和微生物组成，并且它们在健康中起着至关重要的作用吗？
217 00:19:14,943 --> 00:19:29,961 研究越来越多地表明，微生物组不仅影响消化，还影响广泛的健康问题，包括从肠易激综合症到克罗恩病的消化系统疾病，从肥胖到2型糖尿病的代谢性疾病，
218 00:19:29,941 --> 00:19:38,557 自身免疫性疾病如类风湿性关节炎和多发性硬化症，心理健康状况如抑郁症和焦虑症，以及心血管疾病。
219 00:19:39,058 --> 00:19:43,848 Viome 有一个我使用多年的产品，叫做全身体智
220 00:19:44,268 --> 00:19:49,916 该产品只需收集几滴您的血液、唾液和粪便，就能告诉您许多关于您健康的信息。
他们测试了超过 70 万人，并使用他们的 AI 模型提供关于成员健康状况的关键重要指南和见解，比如你应该吃什么食物，不应该吃什么食物，应该服用哪些补充剂或益生菌。
222 00：20：05,615 --> 00：20：09,078 演讲者 SPEAKER_02：还有你的生理年龄和其他深入的健康见解。
223 00：20：09,519 --> 00：20：15,385 演讲者 SPEAKER_02：由于 Vyom 向他们的成员提出的建议，结果非常出色。
美国生活方式医学杂志报道，仅仅六个月后，成员们报告了以下情况，抑郁减少了36%，焦虑减少了40%
225 00:20:27,940 --> 00:20:33,232 说话人 SPEAKER_02：糖尿病减少 30%，肠易激综合症减少 48%。
226 00:20:33,553 --> 00:20:35,458 说话人 SPEAKER_02：听着，我已经使用 Viome 三年了。
227 00:20:35,919 --> 00:20:40,250 说话人 SPEAKER_02：我知道我的口腔和肠道健康对我来说至关重要。
228 00:20:40,711 --> 00:20:41,291 说话人 SPEAKER_02：这是我的之一
229 00:20:41,525 --> 00:20:43,607 说话人 SPEAKER_02：个人关注的重点领域。
230 00:20:43,847 --> 00:20:48,234 说话人 SPEAKER_02：最好的是，Viome 价格合理，这也是我实现医疗保健民主化的使命的一部分。
231 00:20:48,796 --> 00:20:55,766 说话人 SPEAKER_02：如果您想加入我的旅程并享受全身智力测试 20%的折扣，请访问 Viome.com slash Peter。
232 00:20:56,105 --> 00:20:58,849 说话人 SPEAKER_02：谈到您的健康，知识就是力量。
233 00:20:59,411 --> 00:21:02,115 说话人 SPEAKER_02：再次强调，是 Viome.com slash Peter。
234 00:21:02,769 --> 00:21:20,875 说话人 SPEAKER_02：鉴于你在开发神经网络、反向传播等方面所扮演的角色，你认为在人工智能技术中，这些模型会有下一个重大飞跃，将它们推进一千倍吗？
235 00:21:22,237 --> 00:21:25,842 说话人 SPEAKER_00：据我所知没有，但 Ray 可能有不同的看法。
236 00:21:26,817 --> 00:21:32,804 说话人 SPEAKER_01：嗯，我们可以通过软件在硬件上获得更多的优势。
237 00:21:33,304 --> 00:21:41,093 说话人 SPEAKER_01：所以我们不仅限于你之前展示的图表，因为我们可以用软件使其更有效。
238 00:21:42,095 --> 00:21:43,797 说话人 SPEAKER_01：我们已经做到了这一点。
239 00:21:46,019 --> 00:21:51,204 说话人 SPEAKER_01：正在出现聊天机器人，它们每计算一次能获得更多的价值。
240 00:21:51,926 --> 00:21:57,381 说话人 SPEAKER_01：我相信我们还可以在这方面做更多。
241 00:21:57,596 --> 00:22:05,246 说话人 SPEAKER_02：你知道，我把奇点定义为这样一个点，雷，之后我就无法预测接下来会发生什么。
242 00:22:05,266 --> 00:22:05,645 说话人 SPEAKER_02：没错。
243 00:22:05,665 --> 00:22:07,067 说话人 SPEAKER_01：这就是我们使用“奇点”这个词的原因。
244 00:22:07,087 --> 00:22:17,339 说话人 SPEAKER_02：但是当你说到 2045 年的奇点时，我不知道有谁能告诉我从 2026 年开始会发生什么，更不用说 2040 年或 2045 年了。
245 00:22:17,440 --> 00:22:21,785 说话人 SPEAKER_02：所以我想问你这个问题已经有一段时间了。
246 00:22:22,365 --> 00:22:26,170 说话人 SPEAKER_02：既然我们有时间，为什么你要放那个时间？
247 00:22:26,638 --> 00:22:30,287 说话人 SPEAKER_02：数字超级智能，比人类先进一亿倍。
248 00:22:30,646 --> 00:22:35,637 说话人 SPEAKER_01：到 2026 年，你可能无法理解所有发生的事情，但我们能理解。
249 00:22:36,138 --> 00:22:44,496 说话人 SPEAKER_01: 或许就像一百个人，但这并不超出我们的理解范围。
250 00:22:44,678 --> 00:22:50,684 说话人 SPEAKER_01: 到 2045 年，它将像一百万个人一样，我们根本无法理解。
251 00:22:50,765 --> 00:22:59,074 说话人 SPEAKER_01: 大约在那个时间，我们从物理学中借用了这个短语，称之为奇点。
252 00:23:01,276 --> 00:23:07,462 说话人 SPEAKER_02: 杰夫，你能看到人工智能世界的发展有多远？
253 00:23:08,809 --> 00:23:09,290 说话人 SPEAKER_02: 你的——？
254 00:23:09,351 --> 00:23:17,884 说话人 SPEAKER_00: 我现在的观点是，我们有 50%的概率在 5 到 20 年内实现超级智能。
255 00:23:18,724 --> 00:23:23,612 说话人 SPEAKER_00: 我认为这比有些人想象的要慢，比其他人想象的要快。
256 00:23:24,232 --> 00:23:30,682 说话人 SPEAKER_00: 这与雷·库兹韦尔很久以前的观点大致相符，这让我感到惊讶。
257 00:23:32,384 --> 00:23:34,866 说话人 SPEAKER_00: 但我认为这里有很大的不确定性。
258 00:23:34,886 --> 00:23:38,090 说话人 SPEAKER_00: 我认为我们仍然有可能遇到某种障碍。
259 00:23:38,672 --> 00:23:40,213 说话人 SPEAKER_00: 但我并不真的相信这一点。
260 00:23:40,773 --> 00:23:43,237 说话人 SPEAKER_00: 如果你看看最近的进展，它进展得非常快。
261 00:23:44,199 --> 00:23:50,987 说话人 SPEAKER_00: 即使没有任何新的科学突破，仅仅通过扩大规模，我们也会使事物变得更加智能。
262 00:23:51,386 --> 00:23:52,929 说话人 SPEAKER_00: 而且会有科学突破。
263 00:23:52,969 --> 00:23:55,071 说话人 SPEAKER_00: 我们将得到更多像变压器这样的东西。
264 00:23:55,471 --> 00:23:59,978 说话人 SPEAKER_00: 变压器在 2017 年产生了重大影响。
265 00:24:00,650 --> 00:24:02,172 说话人 SPEAKER_00：我们将得到更多类似的东西。
266 00:24:03,193 --> 00:24:08,583 说话人 SPEAKER_00：所以我相当确信我们将得到超级智能。
267 00:24:09,064 --> 00:24:12,729 说话人 SPEAKER_00：可能不是在 20 年内，但肯定会在 100 年内。
268 00:24:13,109 --> 00:24:18,117 说话人 SPEAKER_02：所以，你知道，埃隆在预测时间上并不以准确性著称。
269 00:24:18,519 --> 00:24:31,305 说话人 SPEAKER_02：但他确实说过，他预计到 2025 年实现通用人工智能（AGI），到 2029 年人工智能将等同于所有人类。
270 00:24:32,347 --> 00:24:33,609 说话人 SPEAKER_02：这只是在你的脑海中的一种谬误吗？
271 00:24:35,851 --> 00:24:37,333 说话人 SPEAKER_00：我认为这很雄心勃勃。
272 00:24:37,554 --> 00:24:39,476 说话人 SPEAKER_00：就像我说的，这里有很多不确定性。
273 00:24:39,536 --> 00:24:46,286 说话人 SPEAKER_00: 他可能是对的，但我对此感到非常惊讶。
274 00:24:46,306 --> 00:24:50,673 说话人 SPEAKER_01: 我并不是说它将等同于一台机器中的所有人类。
275 00:24:50,712 --> 00:24:56,981 说话人 SPEAKER_01: 它将等同于一百万人类。
276 00:24:57,468 --> 00:25:00,071 说话人 SPEAKER_01: 这仍然很难理解。
277 00:25:00,653 --> 00:25:03,739 说话人 SPEAKER_02：所以我们在这里辩论一个话题。
278 00:25:04,199 --> 00:25:12,113 说话人 SPEAKER_02：我在尝试找到一个有意义的辩论话题，杰夫和雷，让人们真正停下来思考，并真正拥有他们的答案。
279 00:25:12,993 --> 00:25:14,195 说话人 SPEAKER_02：因为我们经常听到这些。
280 00:25:14,256 --> 00:25:22,529 说话者 SPEAKER_02：我认为这是在餐桌、董事会、国会大厅、国家领导层中最重要的对话。
281 00:25:22,509 --> 00:25:31,762 说话者 SPEAKER_02：你知道，谈论通用人工智能或人类水平智能是一回事，但谈论数字超级智能，对吧？
282 00:25:31,782 --> 00:25:43,739 说话者 SPEAKER_02：接下来我们将听到莫·戈多特的发言，我们将讨论当你的 AI 后代比你自己聪明一亿倍时会发生什么。
283 00:25:45,501 --> 00:25:48,045 说话者 SPEAKER_02：事情可能会
284 00:25:48,025 --> 00:25:52,453 说话人 SPEAKER_02：非常迅速地，朝着你预期的完全不同的方向发展。
285 00:25:52,473 --> 00:25:53,917 说话人 SPEAKER_02：它们可以分歧，对吧？
286 00:25:53,938 --> 00:25:56,784 说话人 SPEAKER_02：速度可以导致非常迅速的巨大分歧。
287 00:25:57,726 --> 00:26:01,713 说话人 SPEAKER_02：我很好奇，你认为这是最大的威胁和最大的希望吗？
288 00:26:04,343 --> 00:26:09,228 说话人 SPEAKER_01：首先，这就是我们称之为奇点的原因，因为我们不知道……我们不知道。
289 00:26:09,548 --> 00:26:14,472 说话人 SPEAKER_01：我们真的不知道，但……我认为这是一个伟大的希望。
290 00:26:14,773 --> 00:26:16,275 说话人 SPEAKER_01：它正在非常、非常快地发展。
291 00:26:16,295 --> 00:26:22,181 说话人 SPEAKER_01：没有人知道在上一场演讲中提出的那些问题的答案。
292 00:26:22,221 --> 00:26:27,286 说话人 SPEAKER_01: 但是发生了一些令人惊讶的事情。
293 00:26:27,326 --> 00:26:29,929 说话人 SPEAKER_01: 事实上，我们...
294 00:26:30,027 --> 00:26:34,132 说话人 SPEAKER_01: 在过去 80 年里，原子弹没有爆炸，这相当令人惊讶。
295 00:26:34,172 --> 00:26:48,369 说话人 SPEAKER_02: 是的，但它们更容易追踪，制造成本更高，有种种原因使得使用反乌托邦 AI 系统比使用原子弹容易千万倍。
296 00:26:49,912 --> 00:26:50,313 说话人 SPEAKER_01: 对吗？
297 00:26:50,413 --> 00:26:51,173 说话人 SPEAKER_01: 是的，也不是。
298 00:26:51,654 --> 00:26:54,397 说话人 SPEAKER_01: 我的意思是，我们有
299 00:26:54,714 --> 00:26:57,542 说话人 SPEAKER_01: 我不知道，可能有 1 万个或者更多。
300 00:26:57,643 --> 00:27:01,814 说话人 SPEAKER_01：这仍然相当非凡，而且仍然非常危险。
301 00:27:02,696 --> 00:27:07,509 说话人 SPEAKER_01：我认为这实际上是最大的危险，而且这与人工智能无关。
302 00:27:09,211 --> 00:27:22,282 说话人 SPEAKER_00：但是我想，如果你想象一下，如果人们开源了这项技术，任何研究生，如果他能够接触到几块 GPU，就能制造原子弹，那就非常可怕了。
303 00:27:22,303 --> 00:27:24,585 说话人 SPEAKER_00：所以他们并没有真正开源核武器。
304 00:27:24,605 --> 00:27:28,469 仅有少数人能够构建并部署它们。
305 00:27:29,450 --> 00:27:35,095 人们现在正在开源这些大型语言模型，这不仅仅是语言模型。
306 00:27:35,915 --> 00:27:37,277 我认为这非常危险。
307 00:27:38,944 --> 00:27:43,853 所以这是一个有趣的问题，我们在这里的最后两分钟可以探讨。
308 00:27:44,394 --> 00:27:50,502 讲者 SPEAKER_02：现在有一种说法，你必须开源模型。
309 00:27:51,023 --> 00:27:59,938 讲者 SPEAKER_02：我们已经看到了 Meta，我们看到了开源运动，我们看到了 Elon 谈论 Grok 开源。
310 00:28:00,618 --> 00:28:04,345 讲者 SPEAKER_02：你是说这些不应该开源，Jeff 吗？
311 00:28:05,894 --> 00:28:11,301 讲者 SPEAKER_00：嗯，一旦你有了权重，你可以微调它们来做坏事，这并不花费太多。
训练一个基础模型，可能需要1000万美元，也可能需要1亿美元，但一小撮罪犯是做不到的。
313 00：28：18,712 --> 00：28：23,701 演讲者 SPEAKER_00：微调开源模型非常简单。
314 00：28：24,201 --> 00：28：26,505 议长 SPEAKER_00：你不需要那么多资源。
315 00：28：26,765 --> 00：28：28,548 议长 SPEAKER_00：也许你可以花一百万美元来做。
316 00:28:29,229 --> 00:28:33,234 说话人 SPEAKER_00：这意味着它们将被用于可怕的事情，而且它们非常强大。
317 00:28:33,686 --> 00:28:39,407 说话人 SPEAKER_01：嗯，我们也可以通过从同一模型中获得的人工智能来避免这些危险。
318 00:28:40,151 --> 00:28:44,085 说话人 SPEAKER_02：是的，AI 的白帽与黑帽方法。
319 00:28:44,638 --> 00:28:52,392 说话人 SPEAKER_00：是的，我和 Jan 有过这样的争论，Jan 的观点是白帽总是比坏人拥有更多资源。
320 00:28:53,755 --> 00:28:58,442 说话人 SPEAKER_00: 当然，Jan 认为马克·扎克伯格是个好人，所以我们在这方面并不一定同意。
321 00:29:02,910 --> 00:29:07,557 说话人 SPEAKER_00: 我只是觉得这里有很多不确定性，我们应该谨慎行事。
322 00:29:08,432 --> 00:29:11,480 说话人 SPEAKER_00: 而开源这些大型模型并不是谨慎的做法。
323 00:29:11,500 --> 00:29:17,695 说话人 SPEAKER_02: 好的，Jeff 和 Ray，非常感谢你们的指导，你们的智慧。
女士们，先生们，让我们为雷·库兹韦尔和杰弗里·辛顿鼓掌。
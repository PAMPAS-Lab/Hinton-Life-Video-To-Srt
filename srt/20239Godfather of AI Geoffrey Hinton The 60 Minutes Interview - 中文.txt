1
00:00:01,752 --> 00:00:08,805
演讲者 SPEAKER_00：无论您认为人工智能将拯救世界还是毁灭世界，您都应该感谢 Geoffrey Hinton。

2
00:00:09,587 --> 00:00:23,675
演讲者 SPEAKER_00：辛顿被称为人工智能教父，他是一位英国计算机科学家，他的有争议的想法帮助实现了先进的人工智能，从而改变了世界。

3
00:00:24,044 --> 00:00:29,512
发言人 SPEAKER_00：Hinton 相信人工智能将会带来巨大的好处，但今晚他提出了一个警告。

4
00:00:29,992 --> 00:00:40,847
发言人 SPEAKER_00：他说，人工智能系统可能比我们所知的更智能，而且机器有可能接管一切，这让我们提出了这个问题。

5
00:00:42,790 --> 00:00:45,033
发言人 SPEAKER_00：故事马上继续。

6
00:00:48,319 --> 00:00:50,240
发言人 SPEAKER_00：人类知道自己在做什么吗？

7
00:00:52,685 --> 00:00:52,784
发言人 SPEAKER_02：无。

8
00:00:54,554 --> 00:01:03,067
发言人 SPEAKER_02：我认为我们正在进入一个时期，有史以来第一次，我们可能拥有比我们更智能的事物。

9
00:01:04,209 --> 00:01:05,673
发言人 SPEAKER_00：您相信他们能理解吗？

10
00:01:05,852 --> 00:01:06,114
发言人 SPEAKER_00：是的。

11
00:01:06,554 --> 00:01:08,858
发言人 SPEAKER_00：你相信他们有智慧吗？

12
00:01:09,218 --> 00:01:09,459
发言人 SPEAKER_00：是的。

十三
00:01:09,879 --> 00:01:17,251
发言人 SPEAKER_00：您认为这些系统有自己的经验，并且可以根据这些经验做出决策吗？

14
00:01:17,894 --> 00:01:19,936
发言者 SPEAKER_02：和人们的感觉一样，是的。

15
00:01:20,355 --> 00:01:21,097
发言人 SPEAKER_02：他们有意识吗？

16
00:01:21,938 --> 00:01:24,463
发言人 SPEAKER_02：我认为他们目前可能还没有太多的自我意识。

17
00:01:24,924 --> 00:01:26,588
发言者 SPEAKER_02：所以从这个意义上来说，我认为他们没有意识。

18
00:01:26,769 --> 00:01:28,692
发言人 SPEAKER_00：他们会有自我意识吗？

19
00:01:29,174 --> 00:01:30,055
发言人 SPEAKER_00：意识？

20
00:01:30,075 --> 00:01:30,617
发言人 SPEAKER_02：哦，是的。

21
00:01:30,637 --> 00:01:30,716
发言人 SPEAKER_02：是的？

22
00:01:30,736 --> 00:01:31,920
发言人 SPEAKER_02：哦，是的，我认为他们会及时的。

23
00:01:32,281 --> 00:01:37,590
发言人 SPEAKER_00：那么人类将成为地球上第二聪明的生物吗？

24
00:01:38,634 --> 00:01:38,853
发言人 SPEAKER_02：是的。

二十五
00:01:39,981 --> 00:01:47,992
演讲者 SPEAKER_00：Jeffrey Hinton 告诉我们，他所开创的人工智能是一个因失败而产生的意外。

二十六
00:01:48,832 --> 00:02:02,310
演讲者 SPEAKER_00：20 世纪 70 年代，在爱丁堡大学，他梦想在计算机上模拟神经网络，作为他真正研究的对象——人类大脑的工具。

二十七
00:02:02,560 --> 00:02:06,384
发言人 SPEAKER_00：但当时几乎没有人认为软件可以模拟大脑。

二十八
00:02:06,805 --> 00:02:11,590
发言者 SPEAKER_00：他的博士导师告诉他最好放弃它，以免它毁了他的职业生涯。

二十九
00:02:12,570 --> 00:02:19,599
发言人 SPEAKER_00：辛顿说，他未能弄清楚人类的思维，但长期的追求导致了人工智能的出现。

三十
00:02:20,820 --> 00:02:22,923
发言者 SPEAKER_02：这比我预期的花费的时间要长得多。

31
00:02:22,983 --> 00:02:25,246
发言人 SPEAKER_02：花了大约 50 年的时间才让它发挥良好的作用。

三十二
00:02:25,265 --> 00:02:26,907
发言人 SPEAKER_02：但最终，它确实发挥了良好的作用。

33
00:02:27,489 --> 00:02:28,770
发言人 SPEAKER_00：在什么时候

三十四
00:02:29,375 --> 00:02:37,042
演讲者 SPEAKER_00：您是否意识到您对神经网络的认识是正确的，而其他大多数人都是错误的？

三十五
00:02:37,062 --> 00:02:38,122
发言者 SPEAKER_00：我一直认为我是对的。

三十六
00:02:39,784 --> 00:02:51,215
演讲者 SPEAKER_00：2019 年，Hinton 与合作者（左边的）Jan LeCun、Yoshua Bengio 共同获得了图灵奖，也就是计算机界的诺贝尔奖。

三十七
00:02:52,056 --> 00:02:58,622
演讲者 SPEAKER_00：为了了解他们在人工神经网络方面的工作如何帮助机器学习，

三十八
00:02:58,837 --> 00:03:00,368
发言人 SPEAKER_00：让我们带您去看一场比赛。

三十九
00:03:00,429 --> 00:03:02,525
发言人 SPEAKER_00：看那个。

40
00:03:03,371 --> 00:03:04,680
发言人 SPEAKER_00：天哪。

41
00:03:05,199 --> 00:03:10,847
演讲者 SPEAKER_00：这是谷歌位于伦敦的人工智能实验室，我们于今年四月首次向您展示过。

四十二
00:03:11,707 --> 00:03:18,937
发言人 SPEAKER_00：杰弗里·辛顿 (Geoffrey Hinton) 没有参与这个足球项目，但这些机器人是机器学习的一个很好的例子。

43
00:03:19,737 --> 00:03:25,764
发言人 SPEAKER_00：需要理解的是，这些机器人并没有被编程来踢足球。

四十四
00:03:26,325 --> 00:03:27,848
发言人 SPEAKER_00：他们被要求得分。

四十五
00:03:28,468 --> 00:03:31,932
发言人 SPEAKER_00：他们必须自己学习如何做。

四十六
00:03:33,466 --> 00:03:35,750
发言人 SPEAKER_00：一般来说，AI 是这样做的。

四十七
00:03:36,311 --> 00:03:43,163
演讲者 SPEAKER_00：Hinton 和他的合作者分层创建了软件，每层处理部分问题。

四十八
00:03:43,462 --> 00:03:45,265
发言人 SPEAKER_00：这就是所谓的神经网络。

49
00:03:45,687 --> 00:03:46,709
发言人 SPEAKER_00：但这是关键。

50
00:03:47,310 --> 00:03:57,347
发言人 SPEAKER_00：例如，当机器人得分时，一条消息会通过所有层发送回来，表明该路径是正确的。

51
00:03:57,664 --> 00:04:02,711
发言人 SPEAKER_00：同样，当答案错误时，该消息就会通过网络传播。

52
00:04:03,271 --> 00:04:12,682
发言人 SPEAKER_00：因此，正确的连接会变得更强，错误的连接会变得更弱，通过反复试验，机器可以自我学习。

53
00:04:13,002 --> 00:04:17,788
演讲者 SPEAKER_00：您认为这些人工智能系统的学习能力比人类思维更好吗？

54
00:04:18,290 --> 00:04:20,112
发言人 SPEAKER_02：是的，我认为可能是。

55
00:04:20,192 --> 00:04:22,735
发言人 SPEAKER_02：目前，它们已经小了很多。

56
00:04:23,476 --> 00:04:28,062
发言人 SPEAKER_02：因此，即使是最大的聊天机器人也只有大约一万亿个连接。

57
00:04:28,783 --> 00:04:30,747
演讲者 SPEAKER_02：人类的大脑有大约 100 万亿个大脑。

58
00:04:30,906 --> 00:04:43,483
发言人 SPEAKER_02：然而，在聊天机器人的万亿个连接中，它所知道的远远超过你在 100 万亿个连接中所知道的，这表明它有更好的方法将知识传递到这些连接中。

59
00:04:43,463 --> 00:04:48,249
演讲者 SPEAKER_00：一种获取尚未完全理解的知识的更好的方法。

60
00:04:48,730 --> 00:04:58,182
发言者 SPEAKER_02：我们大致知道它在做什么，但是一旦它变得非常复杂，我们实际上就不知道到底发生了什么，就像我们知道你的大脑在想什么一样。

61
00:04:58,843 --> 00:05:01,947
发言人 SPEAKER_00：你说什么，我们不知道它究竟是如何工作的？

62
00:05:02,166 --> 00:05:03,829
扬声器 SPEAKER_00：它是由人设计的。

63
00:05:04,269 --> 00:05:05,151
发言人 SPEAKER_00：不，不是这样的。

64
00:05:06,076 --> 00:05:08,778
演讲者 SPEAKER_02：我们所做的是设计学习算法。

65
00:05:09,199 --> 00:05:11,601
演讲者 SPEAKER_02：这有点像设计进化原理。

66
00:05:11,901 --> 00:05:22,012
演讲者 SPEAKER_02：但是，当这种学习算法与数据交互时，它会产生擅长做事的复杂神经网络，但我们并不真正了解它们究竟是如何做到的。

67
00:05:23,213 --> 00:05:32,302
发言人 SPEAKER_00：这些系统自主编写自己的计算机代码并执行自己的计算机代码有何影响？

68
00:05:32,637 --> 00:05:34,120
发言人 SPEAKER_02：这真是一个严重的担忧，对吧？

69
00:05:34,901 --> 00:05:42,613
发言人 SPEAKER_02：因此，这些系统可能逃脱控制的方法之一就是编写自己的计算机代码来修改自身。

70
00:05:44,057 --> 00:05:47,101
发言人 SPEAKER_02：这是我们需要认真担心的事情。

71
00:05:47,267 --> 00:05:53,218
发言人 SPEAKER_00：如果有人说，如果系统变得恶意，就把它关掉，您会对他们说什么？

72
00:05:53,759 --> 00:05:56,442
发言人 SPEAKER_02：他们将能够操纵人们，对吗？

73
00:05:56,923 --> 00:05:59,829
发言人 SPEAKER_02：这些会非常善于说服人们。

74
00:05:59,848 --> 00:06:08,663
发言人 SPEAKER_02：因为他们从所有曾经写过的小说、马基雅维利的所有书籍以及所有的政治阴谋中吸取了教训。

75
00:06:09,124 --> 00:06:10,245
发言人 SPEAKER_02：他们会知道所有这些东西。

76
00:06:10,286 --> 00:06:11,588
发言人 SPEAKER_02：他们会知道该怎么做。

77
00:06:12,225 --> 00:06:17,271
发言人 SPEAKER_00：人类的诀窍在杰弗里·辛顿 (Geoffrey Hinton) 家族中传承。

78
00:06:17,932 --> 00:06:30,668
发言人 SPEAKER_00：他的祖先包括数学家乔治布尔，发明了计算基础，还有乔治埃佛勒斯，勘测了印度并以自己的名字命名了这座山峰。

79
00:06:31,569 --> 00:06:40,781
发言人 SPEAKER_00：但作为一名男孩，欣顿本人永远无法达到专横的父亲对他寄予的期望的顶峰。

80
00:06:40,862 --> 00:06:49,435
演讲者 SPEAKER_02：每天早上我去上学时，当我走在车道上时，他都会对我说，去投球吧，也许当你的年龄是我两倍时，你就会像我一样优秀。

81
00:06:50,416 --> 00:06:52,860
发言人 SPEAKER_00：爸爸是甲虫研究方面的专家。

82
00:06:53,721 --> 00:06:55,884
演讲者 SPEAKER_02：他对甲虫的了解比对人类的了解多得多。

83
00:06:56,305 --> 00:06:57,608
演讲者 SPEAKER_02：您小时候有这种感觉吗？

84
00:06:58,127 --> 00:06:59,310
发言者 SPEAKER_02：是的，有一点。

85
00:07:00,336 --> 00:07:08,970
演讲者 SPEAKER_02：他去世时，我们去了他在大学里的书房，墙上摆满了关于不同种类甲虫的论文盒。

86
00:07:09,750 --> 00:07:15,019
发言人 SPEAKER_02：就在门附近，有一个稍微小一点的盒子，上面简单地写着，不是昆虫。

87
00:07:15,759 --> 00:07:17,603
发言人 SPEAKER_02：这就是他拥有的所有关于家庭的东西的地方。

88
00:07:18,983 --> 00:07:25,634
发言人 SPEAKER_00：今天，75 岁的 Hinton 在谷歌度过了快乐的 10 年之后，刚刚退休。

89
00:07:26,336 --> 00:07:36,115
发言人 SPEAKER_00：现在，他是多伦多大学的名誉教授，他偶然提到他的学术引用数比他的父亲还多。

90
00:07:36,314 --> 00:07:42,504
演讲者 SPEAKER_00：他的一些研究促成了像谷歌的 Bard 这样的聊天机器人的诞生，我们去年春天就曾见过它。

91
00:07:42,803 --> 00:07:43,444
发言者 SPEAKER_00：令人困惑。

92
00:07:43,985 --> 00:07:45,108
发言人 SPEAKER_00：绝对令人困惑。

93
00:07:45,307 --> 00:07:49,055
演讲者 SPEAKER_00：我们要求巴德用六个字写一个故事。

94
00:07:49,595 --> 00:07:51,838
发言人 SPEAKER_00：出售婴儿鞋，从未穿过。

95
00:07:53,341 --> 00:07:54,963
发言人 SPEAKER_00：天啊。

96
00:07:55,687 --> 00:07:59,795
发言人 SPEAKER_00：这双鞋是我妻子送给我的礼物，但我们从未生过孩子。

97
00:07:59,915 --> 00:08:11,435
演讲者 SPEAKER_00：巴德创作了一个非常具有人性化的故事，讲述了一个男人的妻子无法怀孕，一个陌生人接受了这双鞋来治愈她流产后的痛苦。

98
00:08:11,836 --> 00:08:13,880
发言者 SPEAKER_00：我很少无言以对。

99
00:08:15,228 --> 00:08:16,930
发言人 SPEAKER_00：我不知道该怎么理解这一点。

100
00:08:17,512 --> 00:08:24,903
演讲者 SPEAKER_00：据说聊天机器人是一种语言模型，它只是根据概率预测下一个最可能的单词。

101
00:08:25,144 --> 00:08:28,610
发言者 SPEAKER_02：您会听到人们说这样的话，他们只是在进行自动完成。

102
00:08:28,649 --> 00:08:30,093
发言人 SPEAKER_02：他们只是想预测下一个词。

103
00:08:31,113 --> 00:08:32,697
发言人 SPEAKER_02：他们只是使用统计数据。

104
00:08:33,740 --> 00:08:36,586
发言人 SPEAKER_02：嗯，确实，他们只是想预测下一个词。

105
00:08:37,246 --> 00:08:43,900
演讲者 SPEAKER_02：但是如果你仔细想想，要预测下一个单词，你就必须理解句子。

106
00:08:44,442 --> 00:08:47,749
发言人 SPEAKER_02：所以说，他们只是预测下一个词，所以他们并不聪明，这种想法很疯狂。

107
00:08:48,068 --> 00:08:52,238
说话者 SPEAKER_02：您必须非常聪明才能准确地预测下一个单词。

108
00:08:52,488 --> 00:09:01,278
演讲者 SPEAKER_00：为了证明这一点，Henton 向我们展示了他为 CHAT GPT-4 设计的一个测试，CHAT GPT-4 是一家名为 OpenAI 的公司开发的聊天机器人。

109
00:09:02,158 --> 00:09:08,285
发言人 SPEAKER_00：看到图灵奖获得者打错字并把责任推到计算机身上，让人感到些许欣慰。

110
00:09:08,865 --> 00:09:09,886
发言人 SPEAKER_02：哦，这东西真该死。

111
00:09:10,508 --> 00:09:11,749
发言人 SPEAKER_02：我们要回去重新开始。

112
00:09:11,769 --> 00:09:12,610
发言人 SPEAKER_02：没关系。

113
00:09:12,629 --> 00:09:16,214
发言人 SPEAKER_00：亨顿的测试是一个关于房屋粉刷的谜语。

114
00:09:16,735 --> 00:09:20,418
发言人 SPEAKER_00：答案需要推理和计划。

115
00:09:20,871 --> 00:09:24,798
发言人 SPEAKER_00：这是他在聊天 GPT-4 中输入的内容。

116
00:09:25,279 --> 00:09:31,471
演讲者 SPEAKER_02：我家里的房间漆成白色、蓝色或黄色，黄色油漆一年内就会褪成白色。

117
00:09:32,292 --> 00:09:34,635
演讲者 SPEAKER_02：两年后，我希望所有的房间都变成白色的。

118
00:09:35,116 --> 00:09:35,738
发言人 SPEAKER_02：我该怎么办？

119
00:09:36,438 --> 00:09:38,864
发言人 SPEAKER_00：答案在一秒内开始。

120
00:09:39,544 --> 00:09:44,854
发言人 SPEAKER_00：GPT-4 建议将漆成蓝色的房间重新粉刷。

121
00:09:45,424 --> 00:09:52,272
发言人 SPEAKER_00：漆成黄色的房间不需要重新粉刷，因为它们会在截止日期之前褪色为白色。

122
00:09:53,033 --> 00:09:56,548
发言人 SPEAKER_00：而且......哦，我甚至都没有想到这一点。

123
00:09:57,052 --> 00:10:04,380
发言人 SPEAKER_00：它警告说，如果将黄色的房间漆成白色，则黄色褪色时颜色可能会脱落。

124
00:10:05,121 --> 00:10:11,789
发言人 SPEAKER_00：此外，它建议，你会浪费资源，粉刷那些无论如何都会褪色成白色的房间。

125
00:10:12,370 --> 00:10:16,395
发言人 SPEAKER_00：您相信 CHAT GPD 4 能够理解。

126
00:10:16,716 --> 00:10:19,438
发言人 SPEAKER_02：是的，我相信它肯定明白。

127
00:10:19,519 --> 00:10:20,780
发言人 SPEAKER_00：那么五年后呢？

128
00:10:21,721 --> 00:10:24,966
发言人 SPEAKER_02：我认为五年后，它的推理能力可能比我们更好。

129
00:10:25,434 --> 00:10:31,841
发言人 SPEAKER_00：他所说的推理导致了人工智能的巨大风险和巨大的好处。

130
00:10:32,841 --> 00:10:36,586
发言人 SPEAKER_02：显然，医疗保健是能够带来巨大好处的领域。

131
00:10:37,567 --> 00:10:43,092
演讲者 SPEAKER_02：在理解医学图像中发生的事情方面，人工智能已经可以与放射科医生相媲美。

132
00:10:44,494 --> 00:10:46,176
发言人 SPEAKER_02：它在药物设计方面将会非常出色。

133
00:10:46,255 --> 00:10:47,716
发言人 SPEAKER_02：它已经在设计药物了。

134
00:10:48,317 --> 00:10:53,602
发言人 SPEAKER_02：所以这是一个几乎完全可以发挥良好作用的领域。

135
00:10:54,157 --> 00:10:54,903
发言者 SPEAKER_02：我喜欢那个区域。

136
00:10:55,509 --> 00:10:56,842
发言人 SPEAKER_02：风险是什么？

137
00:10:58,121 --> 00:11:08,192
发言人 SPEAKER_02：嗯，风险在于会有一大批人失业并且不再受到重视，因为他们过去所做的事情现在由机器完成了。

138
00:11:08,913 --> 00:11:21,586
发言人 SPEAKER_00：他担心的其他直接风险包括假新闻、就业和警务方面的无意偏见以及自主战场机器人。

139
00:11:22,587 --> 00:11:26,491
发言人 SPEAKER_00：什么样的前进之路能够确保安全？

140
00:11:27,738 --> 00:11:28,318
发言人 SPEAKER_00：我不知道。

141
00:11:28,918 --> 00:11:31,302
发言人 SPEAKER_02：我看不到一条能保证安全的道路。

142
00:11:33,184 --> 00:11:37,769
发言人 SPEAKER_02：我们正进入一个充满不确定性的时期，我们正在处理以前从未处理过的事情。

143
00:11:38,991 --> 00:11:42,196
发言者 SPEAKER_02：通常，当你第一次处理一些全新的事物时，你会出错。

144
00:11:42,875 --> 00:11:44,557
发言人 SPEAKER_02：我们不能承受这些事情出错的后果。

145
00:11:44,979 --> 00:11:46,500
发言人 SPEAKER_01：不能出错，为什么？

146
00:11:47,221 --> 00:11:48,543
发言人 SPEAKER_01：嗯，因为他们可能会接管。

147
00:11:49,684 --> 00:11:51,025
发言人 SPEAKER_01：接管人类？

148
00:11:51,287 --> 00:11:52,388
发言人 SPEAKER_02：是的，这是有可能的。

149
00:11:52,467 --> 00:11:53,089
发言人 SPEAKER_02：他们为什么想要这么做？

150
00:11:53,109 --> 00:11:54,149
发言人 SPEAKER_02：我不是说它会发生。

151
00:11:55,075 --> 00:12:00,421
发言人 SPEAKER_02：如果我们可以阻止他们这样做，那就太好了，但目前尚不清楚我们是否可以阻止他们这样做。

152
00:12:01,962 --> 00:12:23,006
发言人 SPEAKER_00：杰弗里·辛顿告诉我们，由于人工智能具有良好的潜力，他并不后悔，但他说，现在是进行实验以了解人工智能、政府实施监管以及签署禁止使用军事机器人的世界条约的时候了。

153
00:12:23,206 --> 00:12:31,816
发言人 SPEAKER_00：他让我们想起了罗伯特·奥本海默，后者在发明原子弹后，开始反对氢弹。

154
00:12:32,716 --> 00:12:37,903
演讲者 SPEAKER_00：一位改变了世界却发现世界超出了他控制范围的人。

155
00:12:38,604 --> 00:12:45,972
发言人 SPEAKER_02：也许我们回顾过去，会将此视为一种转折点，人类必须决定是否进一步发展这些东西。

156
00:12:46,155 --> 00:12:50,984
发言人 SPEAKER_02：如果他们这样做了，我不知道该怎么做才能保护自己。

157
00:12:51,004 --> 00:12:56,371
发言人 SPEAKER_02：我认为我要表达的主要信息是，对于接下来会发生什么，存在着巨大的不确定性。

158
00:12:58,355 --> 00:13:05,986
演讲者 SPEAKER_02：这些事情确实可以理解，而且正因为可以理解，所以我们需要认真思考接下来会发生什么，而我们根本不知道。


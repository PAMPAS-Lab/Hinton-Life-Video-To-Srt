1 00:00:00,031 --> 00:00:04,503 众所周知，这是计算机世界的诺贝尔奖。
2 00:00:04,604 --> 00:00:07,833 它被称为图灵奖，以艾伦·图灵的名字命名。
3 00:00:08,253 --> 00:00:12,787 你肯定听说过这个名字，也许看过关于这位人物的影片。
4 00:00:12,807 --> 00:00:15,053 他创造了图灵测试，用以判断
5 00:00:15,032 --> 00:00:20,481 说话人 SPEAKER_00：当人工智能以一种无法与人类思维区分的方式工作时。
6 00:00:20,562 --> 00:00:21,603 说话人 SPEAKER_00：这就是背景。
7 00:00:21,864 --> 00:00:23,306 说话人 SPEAKER_00：但你可能没有听说过这个奖项。
8 00:00:23,646 --> 00:00:31,237 说话人 SPEAKER_00：它非常具有声望，正如你可能猜到的，每年都会授予在高级计算机科学领域的最佳人才。
9 00:00:31,559 --> 00:00:39,670 说话人 SPEAKER_00: 这次，第一次，有两个加拿大人获奖，并且他们分享了奖金，奖金是 100 万美元。
10 00:00:40,591 --> 00:00:40,972 说话人 SPEAKER_00: 检查。
11 00:00:40,953 --> 00:00:43,536 说话人 SPEAKER_00: 我在这里见到了三位获奖者中的两位，那两位加拿大人。
12 00:00:43,896 --> 00:00:44,537 说话人 SPEAKER_00: 多么荣幸。
13 00:00:44,899 --> 00:00:53,271 主持人：在我身边的嘉宾是 Geoffrey Hinton 教授，他是 Vector 研究所的首席科学顾问，但他的成就远不止于此。
14 00:00:53,371 --> 00:00:59,100 主持人：他是多伦多大学的荣誉教授，同时也是谷歌的首席工程研究员，并在那里担任副总裁。
15 00:00:59,540 --> 00:01:00,341 主持人：欢迎您。
16 00:01:00,502 --> 00:01:01,863 主持人：很高兴见到您。
17 00:01:01,844 --> 00:01:10,914 说话人 说话人_00：在旧金山，我有约书亚·本吉奥，他是魁北克人工智能研究所所长，同时也是蒙特利尔大学的教授。
18 00:01:11,414 --> 00:01:13,917 说话人 说话人_00：欢迎您，教授，在旧金山。
19 00:01:13,956 --> 00:01:16,719 说话人 说话人_00：两位先生，恭喜你们。
20 00:01:16,799 --> 00:01:17,561 说话人 说话人_00：太好了。
21 00:01:17,600 --> 00:01:19,242 说话人 说话人_00：让我先从您开始。
22 00:01:19,281 --> 00:01:23,126 说话人 说话人_00：我想抛开左脑的分析反应。
23 00:01:23,387 --> 00:01:25,308 说话人 说话人_00：我想要右脑的情感。
24 00:01:25,629 --> 00:01:30,213 说话人 说话人_00：当您得知自己获得了这个最负盛名的奖项时，您的反应是什么？
25 00:01:30,515 --> 00:01:34,224 说话人 SPEAKER_02：我的意思是这真的很感人，我简直不敢相信。
26 00:01:35,727 --> 00:01:41,519 说话人 SPEAKER_02：这对我们来说是一种荣誉，但对我们为之努力整个领域来说也是一种荣誉。
27 00:01:41,721 --> 00:01:44,828 说话人 SPEAKER_02：我真的觉得这是一个令人难以置信的时刻。
28 00:01:45,061 --> 00:01:47,905 说话人 SPEAKER_00：你在机场。
29 00:01:47,924 --> 00:01:49,465 说话人 说话人_00：我们可能需要澄清一下。
30 00:01:49,546 --> 00:01:54,371 说话人 说话人_00：我们将会有一些，谁知道，计算机生成的声音或其他东西加入我们的访谈。
31 00:01:54,712 --> 00:01:58,798 说话人 说话人_00：但是无论如何，Bengio 教授那边非常激动，这是可以理解的。
32 00:01:59,158 --> 00:02:06,968 说话人 说话人_00：帮我们解释一下，Hinton 教授，用简单的话让我们这些非科学家理解一下，你到底开发了什么？
33 00:02:07,504 --> 00:02:11,569 说话人 SPEAKER_01：通常情况下，如果你想让电脑做某件事，你需要告诉它具体要做什么。
34 00:02:11,629 --> 00:02:14,013 说话人 SPEAKER_01：你需要编写一个程序来详细说明它要做什么。
35 00:02:14,193 --> 00:02:14,413 说话人 SPEAKER_01：没错。
36 00:02:15,093 --> 00:02:25,987 说话人 SPEAKER_01：在过去 40 年左右的时间里，我们一直在努力打造一个更接近大脑的东西，其中包含许多简单的处理单元，就像大脑细胞一样，它们之间有连接。
37 00:02:26,889 --> 00:02:31,334 说话人 SPEAKER_01：系统通过给出示例来学习做事情。
38 00:02:31,875 --> 00:02:35,419 说话人 SPEAKER_01：它内部所做的是改变连接的强度，就像大脑一样。
39 00:02:35,516 --> 00:02:36,038 说话人 SPEAKER_00：哇。
40 00:02:36,057 --> 00:02:40,024 说话人 SPEAKER_00：所以这不是我在编程，这是你要做的。
41 00:02:40,564 --> 00:02:43,569 说话人 说话人_00：这就是你们所说的神经网络。
42 00:02:43,629 --> 00:02:46,234 说话人 说话人_00：这就像我们大脑中的神经元一样，是神经网络学习。
43 00:02:46,253 --> 00:02:46,354 说话人 说话人_00：是的。
44 00:02:47,295 --> 00:02:49,419 说话人 说话人_00：你已经从事这项工作40年了。
45 00:02:49,438 --> 00:02:53,205 说话人 说话人_00：在那些早期，我的意思是，人们可能会认为你可能有点不正常，40年前可能就是这样。
46 00:02:53,472 --> 00:02:56,620 说话人 说话人_00：可能有点疯狂，40年前可能就是这样。
47 00:02:56,659 --> 00:02:59,246 说话人 说话人_01：是的，长期以来人们认为这些东西永远不会成功。
48 00:02:59,387 --> 00:03:06,705 说话人 说话人_01：他们认为这是一种浪漫的幻想，你可以拿一个大型的神经网络，只从例子中训练它，它就会学会一切。
49 00:03:06,937 --> 00:03:08,118 说话人 SPEAKER_00: 但实际上并非如此。
50 00:03:08,158 --> 00:03:09,040 说话人 SPEAKER_00: 这不是幻想。
51 00:03:09,200 --> 00:03:10,481 说话人 SPEAKER_00: 它正在显现。
52 00:03:10,502 --> 00:03:18,572 说话人 SPEAKER_00: 我们有一些例子，本尼戈教授，请跟我一起，因为我将从一个真正属于辛顿教授专业领域的例子开始。
53 00:03:19,052 --> 00:03:24,038 说话人 说话人_00：我最近在 Gmail 的交流中注意到，让我们在电脑上打开这个。
54 00:03:24,058 --> 00:03:26,801 说话人 说话人_00：我写了个笔记，看看这些底部的东西。
55 00:03:27,103 --> 00:03:33,890 说话人 说话人_00：有人写了个笔记，然后我甚至不需要写，是电脑在建议我应该怎么回复吗？
56 00:03:34,207 --> 00:03:47,745 说话人 说话人_01：所以发生的事情是，电脑可以看到其他人如何回复 Gmail，它可以理解你收到的 Gmail 和其他人收到的 Gmail 之间的关系，即使它们并不完全相同，它也可以推断出可能合适的回复。
57 00:03:48,146 --> 00:03:51,872 说话人 说话人_00：这就是你精确想出来的例子。
58 00:03:51,972 --> 00:04:02,705 说话人 说话人_01：这是一个正在分析 Gmail 中单词的神经网络，神经网络内部有活跃的神经元，它正在预测你可能想要做出的回应。
59 00:04:02,967 --> 00:04:04,087 说话人 说话人_00：所以我可能想要点击它。
60 00:04:04,207 --> 00:04:06,390 说话人 说话人_00：它在为我思考和写作。
61 00:04:06,431 --> 00:04:11,778 说话人 说话人_00：本尼戈教授，您也在语言领域，我们这里有一些视频。
62 00:04:12,118 --> 00:04:14,823 说话人 说话人_00：这也应用于翻译领域。
63 00:04:14,902 --> 00:04:16,584 说话人 说话人_00：如果您能解释一下这部分就好了。
64 00:04:17,290 --> 00:04:18,031 说话人 说话人_02：当然可以。
65 00:04:18,872 --> 00:04:45,492 说话人 SPEAKER_02：所以，这并不是几年前我们预期的，一个神经网络能够将一串单词，也就是一堆没有自身内在意义的符号，通过 Dr. Hinton 所提到的在大神经网络中进行的所有计算，产生另一串单词，这些单词将对应于翻译，比如说，从法语句子到英语句子的翻译。
66 00:04:46,079 --> 00:05:00,639 说话人 SPEAKER_02：而且有趣的是，他们在做所有这些的同时，还捕捉到了一些词的意义，因此具有相似意义和相似语法角色的词最终将以相似的方式表示。
67 00:05:00,838 --> 00:05:06,887 说话人 SPEAKER_02：这使得这些系统能够在新句子中获得正确答案，而这些新句子是他们以前从未见过的。
68 00:05:06,927 --> 00:05:08,829 说话人 SPEAKER_02：这就是为什么这些技术被部署的原因。
69 00:05:09,146 --> 00:05:13,552 说话人 SPEAKER_02：例如，在谷歌翻译和大多数工业翻译系统中。
70 00:05:13,572 --> 00:05:18,980 说话人 SPEAKER_02：但这种技术正在被用于各种自然语言理解任务。
71 00:05:19,439 --> 00:05:24,226 说话人 SPEAKER_00：好的，这是谷歌提供的视频，所以我们从他们展示的角度来看。
72 00:05:24,646 --> 00:05:29,973 说话人 SPEAKER_00：如果我们想进一步探讨应用，Hinton 教授，例如，这对医疗保健有何作用？
73 00:05:30,088 --> 00:05:32,471 说话人 SPEAKER_01：在医疗保健领域将有巨大的应用。
74 00:05:32,951 --> 00:05:44,086 说话人 SPEAKER_01：任何需要解释图像内容的情况，比如 CT 扫描或 X 光片，目前都是由人完成的，而人们各不相同，注意力也会分散。
75 00:05:44,927 --> 00:05:50,132 说话人 SPEAKER_01：计算机在许多这些事情上已经和人类一样好，而且它们还在不断进步。
76 00:05:50,653 --> 00:05:57,901 说话人 SPEAKER_01：所以很快，医生将咨询计算机来商定图像的解释。
77 00：05：57,882 --> 00：06：02,036 演讲者 SPEAKER_01：因为计算机可以看到的图像比医生在有生之年看到的要多得多。
78 00：06：02,117 --> 00：06：02,838 演讲者 SPEAKER_00：太不可思议了。
79 00：06：02,858 --> 00：06：06,264 演讲者 SPEAKER_00：再说一次，我们看视频只是为了说明你在说什么。
80 00:06:06,283 --> 00:06:07,646 说话者 SPEAKER_00：所以你帮助我们开发了它。
81 00:06:08,427 --> 00:06:28,074 说话人 说话人_00：当我听你说话时，尤其是在医疗保健方面，很明显，你看到了潜力，但本吉奥教授，你立刻就能意识到人们为何对此感到焦虑，为何人们担心未来会走向何方，不仅是在与人类合作方面，也许在生活的许多方面，我们的社会，我们的经济，都有可能被取代。
82 00:06:28,055 --> 00:06:31,139 说话人 说话人_00：我们谈论了很多人们对人工智能和机器人的担忧。
83 00:06:31,160 --> 00:06:33,002 说话人 说话人_00：您如何回应他们，本吉奥教授？
84 00:06:34,745 --> 00:06:41,795 说话人 说话人_02：嗯，我认为我们需要就我们如何使用这项技术进行民主讨论。
85 00:06:41,956 --> 00:06:48,966 说话人 SPEAKER_02：它可以用于做好事，例如在医疗保健和许多其他领域，例如在自动驾驶汽车中的交通。
86 00:06:49,406 --> 00:06:52,391 说话人 SPEAKER_02：但它也可能产生负面影响，正如你所说，对就业。
87 00:06:52,432 --> 00:06:56,458 说话人 SPEAKER_02：因此政府需要立即开始考虑这个问题，因为这需要时间来
88 00:06:56,793 --> 00:07:01,718 说话人 SPEAKER_02：调整我们的培训系统或社会保障体系。
89 00:07:02,178 --> 00:07:05,862 说话人 SPEAKER_02：但是，人们对这项技术的滥用也感到担忧。
90 00:07:05,882 --> 00:07:22,480 说话人 SPEAKER_02：为此，政府需要考虑适当的法规和法律，包括国际法规，比如如何管理这些技术在武器，如令人担忧的这些杀人无人机中的使用。
91 00:07:23,185 --> 00:07:23,966 说话人 SPEAKER_00：绝对如此。
92 00:07:24,005 --> 00:07:42,139 说话人 SPEAKER_00：我的意思是，甚至有一个名为“生命未来研究所”的机构，Elon Musk 也参与了其中，试图确保人们像你这样的人开发的所有技术，都用于人类有益的目的，而不是用于你刚才提到的那些令人担忧的事情。
93 00:07:42,119 --> 00:07:48,250 说话人 说话人_00：你知道，政府需要关注这一点，因为制定回应需要时间。
94 00:07:48,670 --> 00:07:50,213 说话人 说话人_00：他们可能没有这么多时间。
95 00:07:50,314 --> 00:07:52,598 说话人 说话人_00：现在这进展得非常非常快。
96 00:07:52,658 --> 00:07:57,548 说话人 说话人_00：你说你在这个行业已经几十年了，但现在它几乎以光速在前进。
97 00：07：57,848 --> 00：08：00,634 演讲者 SPEAKER_00：Hinton 教授，您的未来会是什么样子？
98 00:08:00,918 --> 00:08:10,774 说话人 SPEAKER_01：我认为这种新技术，即通过展示示例让计算机完成工作，将在几乎所有行业中得到应用。
99 00:08:11,136 --> 00:08:21,353 说话人 SPEAKER_01：每当您有数据并希望预测事物时，您只需向计算机展示历史数据的示例以及接下来发生的事情，计算机就能找出规律并开始做出预测。
100 00:08:21,855 --> 00:08:23,617 说话人 SPEAKER_01：例如，它可以预测
101 00:08:23,598 --> 00:08:24,560 说话人 SPEAKER_01：它可以预测洪水。
102 00:08:24,819 --> 00:08:26,463 说话人 SPEAKER_01：它可以预测地震的余震。
103 00:08:26,923 --> 00:08:34,096 说话人 SPEAKER_01：它甚至可以观察你眼睛后面的图像，通过观察视网膜中的血管，它可以预测你是否会心脏病发作。
104 00:08:34,615 --> 00:08:35,798 说话人 SPEAKER_01：它将在每个地方发生。
105 00:08:36,158 --> 00:08:37,902 说话人 SPEAKER_00：在我们生活的各个方面。
106 00:08:37,922 --> 00:08:38,643 说话人 SPEAKER_01：在各个方面。
107 00:08:38,663 --> 00:08:40,446 说话人 SPEAKER_01：这将提高各处的生产力。
108 00:08:41,027 --> 00:08:45,274 说话人 SPEAKER_01：问题是，这种生产力的提高是否会转化为公共利益？
109 00:08:45,875 --> 00:08:47,037 说话人 SPEAKER_01：但这是一个政治问题。
110 00:08:47,076 --> 00:08:48,659 说话人 SPEAKER_01：这不是一个关于技术的问题。
111 00:08:49,096 --> 00:08:58,384 说话人 SPEAKER_00：因为，我想知道，你们是否想对此发表意见，因为我们将不得不就此结束，但这是一个政策制定者的问题。
112 00:08:58,445 --> 00:09:09,775 说话人 SPEAKER_00：有趣的是，我刚刚读到一家公司开发了一种出色的技术，但实际上并没有发布它，因为它担心这项技术在更广泛的社会中的应用。
113 00:09:09,836 --> 00:09:16,302 说话人 SPEAKER_00：我的意思是，你是说科学家们像你们一样可以随意开发任何技术吗？
114 00:09:16,621 --> 00:09:18,663 说话人 SPEAKER_00：我们必须决定限制是什么。
115 00:09:18,711 --> 00:09:22,138 说话人 SPEAKER_01：不，我认为科学家们应该非常清楚他们在开发什么。
116 00:09:22,339 --> 00:09:28,529 说话人 SPEAKER_01：例如，我认为他们可能选择不从事武器研究，因为不良用途太明显了。
117 00:09:29,912 --> 00:09:37,446 说话人 SPEAKER_01：但总体上在各个行业中获得的劳动生产率提高，这是应该造福大众的。
当然，有些工作会失去，但许多其他工作将会得到提升。
119 00：09：43,562 --> 00：09：50,861 演讲者 SPEAKER_01：所以，如果你看一下印刷机，印刷机基本上摆脱了抄写员，但我认为没有人会说我们不应该引入印刷机。
120 00：09：51,567 --> 00：09：52,609 议长 SPEAKER_00：我们拭目以待。
121 00：09：53,009 --> 00：09：56,034 演讲者 SPEAKER_00：很高兴见到你们两位，祝贺你。
122 00:09:56,195 --> 00:10:04,831 说话人 说话人_00：这场对话可以无限进行下去，因为我认为我们作为一个社会需要解决这个问题的很多方面。
123 00:10:04,850 --> 00:10:07,917 说话人 说话人_00：但恭喜你在旧金山，本杰明教授。
124 00:10:08,216 --> 00:10:10,301 说话人 说话人_00：祝您旅途平安，无论您接下来去哪里。
125 00:10:10,321 --> 00:10:12,063 说话人 说话人_00：感谢您抽出时间，辛顿教授。
126 00:10:12,083 --> 00:10:13,225 说话人 说话人_00：很高兴见到你。
127 00:10:13,346 --> 00:10:13,586 说话人 说话人_00：谢谢。
128 00:10:13,606 --> 00:10:14,427 说话人 说话人_00：恭喜你。
129 00:10:14,528 --> 00:10:15,330 说话人 说话人_01：非常感谢。
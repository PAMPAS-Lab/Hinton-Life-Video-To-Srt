1 00:00:02,443 --> 00:00:06,528 说话人 SPEAKER_02：为这段视频的主题人物 Geoffrey Hinton 写一个简短的介绍。
2 00:00:09,192 --> 00:00:14,538 说话人 SPEAKER_02：Geoffrey Hinton 是多伦多大学的退休教授，被誉为人工智能之父。
3 00:00:16,161 --> 00:00:21,407 说话人 SPEAKER_02：他最近离开谷歌，以便更自由地讨论不受控制的 AI 发展带来的危险。
4 00:00:23,368 --> 00:00:30,577 说话人 SPEAKER_02：我们在他在伦敦的家中采访了他，谈论了他帮助创造的技术、它的许多好处以及他为什么突然担心人类面临风险。
5 00:00:35,350 --> 00:00:37,152 说话人 SPEAKER_00：我收到了华尔街日报的请求。
6 00:00:37,472 --> 00:00:39,415 说话人 SPEAKER_00：他们希望我更正我的讣告。
7 00:00:39,676 --> 00:00:40,097 说话人 SPEAKER_01：你的意思是什么？
8 00:00:40,838 --> 00:00:42,320 说话人 SPEAKER_00：他们希望我更正我的讣告。
9 00:00:42,340 --> 00:00:43,582 说话人 SPEAKER_01: 他们好像已经预先写好了。
10 00:00:43,601 --> 00:00:46,566 说话人 SPEAKER_00: 他们已经预先写好了我的讣告。
11 00:00:46,585 --> 00:00:48,168 说话人 SPEAKER_00: 我想知道马克·吐温会对此说些什么。
12 00:00:48,189 --> 00:01:01,628 说话人 SPEAKER_01: 所以我想我们这里不需要介绍，我就直接开始了。
13 00:01:03,262 --> 00:01:14,721 说话人 SPEAKER_01：你最近接受了一些采访，在采访中你说，用于聊天机器人和其他生成式人工智能的数字智能可能比我们拥有的生物智能更好。
14 00:01:15,683 --> 00:01:18,468 说话人 SPEAKER_01：你能简要解释一下是什么让你得出这个结论吗？
15 00:01:19,444 --> 00:01:26,334 说话人 SPEAKER_00：所以在数字计算机中，它是设计成你可以明确告诉它要做什么，它会按照你告诉它的去做。
16 00:01:27,275 --> 00:01:34,763 说话人 SPEAKER_00：即使它在学习东西，两个不同的数字计算机也可以用相同的学到的知识做完全一样的事情。
17 00:01:36,006 --> 00:01:44,135 说话人 SPEAKER_00：这意味着你可以制作 10,000 份相同的知识副本，让它们在不同的计算机上运行，
18 00:01:44,638 --> 00:01:49,685 说话人 SPEAKER_00：每当一个副本学到一些东西时，它可以非常高效地将其传达给所有其他副本。
19 00:01:50,966 --> 00:02:01,801 说话人 SPEAKER_00：因此，你可以有 10,000 个数字代理在外，一种蜂群思维，它们可以通过共享神经网络中的连接强度来极其高效地共享知识。
20 00:02:01,820 --> 00:02:02,882 说话人 SPEAKER_00：我们做不到这一点。
21 00:02:03,783 --> 00:02:09,009 说话人 SPEAKER_00：如果你学到一些东西，想要告诉我，你必须使用句子。
22 00:02:09,074 --> 00:02:15,260 说话人 SPEAKER_00：或者图片，那样你只能分享非常有限的信息。
23 00:02:15,801 --> 00:02:22,368 说话人 SPEAKER_00：所以，你向我传达你所学到的东西的速度，远远比不上这些数字智能传达信息的速度。
24 00:02:22,849 --> 00:02:24,010 说话人 SPEAKER_00：这使得它们（它们）更加出色。
25 00:02:24,792 --> 00:02:26,633 说话人 SPEAKER_00：他们之间可以学会很多东西。
26 00:02:27,435 --> 00:02:33,262 说话人 SPEAKER_01：你说数字智能是不朽的，而生物智能是有限的。
27 00:02:34,623 --> 00:02:35,925 说话人 SPEAKER_01：你这是什么意思？
28 00:02:36,377 --> 00:02:50,337 说话人 SPEAKER_00：所以，如果我在一个数字计算机上模拟的神经网络中学习了一些连接强度，那么如果某个计算机损坏了，这些相同的连接强度可以在另一台计算机上使用。
29 00:02:51,539 --> 00:03:00,393 说话人 说话人_00：即使所有的数字计算机都停止工作，如果你把连接强度存储在某个地方，你就可以在另一个数字计算机上运行相同的权重。
30 00:03:01,474 --> 00:03:02,436 说话人 说话人_00：但对我们来说，
31 00:03:03,326 --> 00:03:08,474 说话人 说话人_00：我们学到的知识，连接强度，是特定于我们的大脑的。
32 00:03:08,615 --> 00:03:09,856 说话人 说话人_00：每个大脑都有所不同。
33 00:03:10,317 --> 00:03:12,219 说话人 SPEAKER_00：你大脑中的神经元都略有不同。
34 00:03:12,961 --> 00:03:18,248 说话人 SPEAKER_00：而你学习是为了利用你大脑特有的所有特性。
35 00:03:18,308 --> 00:03:25,098 说话人 SPEAKER_00：因此，一旦你在大脑中学会了连接强度，如果你告诉我那些连接强度，它们对我没有任何好处，因为我的大脑是不同的。
36 00:03:26,445 --> 00:03:32,215 说话人 SPEAKER_00：所以数字计算机是永恒的，因为你可以将同样的知识运行在不同的硬件上。
37 00:03:32,936 --> 00:03:37,704 说话人 SPEAKER_00: 我们之所以不朽，是因为硬件和知识紧密相连。
38 00:03:38,185 --> 00:03:42,451 说话人 SPEAKER_00: 你无法将连接强度与它们运行的特定大脑分开。
39 00:03:43,554 --> 00:03:45,235 说话人 SPEAKER_00: 因此，如果大脑死亡，知识也会死亡。
40 00:03:47,819 --> 00:03:52,587 说话人 SPEAKER_02: 我们为什么要担心数字智能取代生物智能？
41 00:03:53,497 --> 00:04:06,891 说话人 SPEAKER_00: 我认为它更好地共享了众多不同数字代理通过共享相同权重所学习到的知识，他们只需共享权重的更新，现在他们可以同时学习 10,000 种不同的东西。
42 00:04:08,114 --> 00:04:12,318 说话人 SPEAKER_00: 但我也认为数字智能可能比人脑拥有更好的学习算法。
43 00:04:13,259 --> 00:04:19,706 说话人 SPEAKER_00: 所有试图在脑中找到与这些数字智能中的反向传播算法一样有效的学习算法的尝试。
44 00:04:19,687 --> 00:04:21,970 说话人 SPEAKER_00: 在这些数字智能中。
45 00:04:22,610 --> 00:04:24,031 说话人 说话人_00：到目前为止，那些尝试都失败了。
46 00:04:24,692 --> 00:04:30,158 说话人 说话人_00：我们还没有找到任何像反向传播算法那样能够很好地扩展到非常大的系统的东西。
47 00:04:30,738 --> 00:04:32,341 说话人 说话人_00：所以我认为它们有两个优势。
48 00:04:32,380 --> 00:04:39,269 说话人 说话人_00：它们可能有一个更好的学习算法，并且它们可以比生物智能更有效地共享知识。
49 00:04:39,288 --> 00:04:44,233 说话人 SPEAKER_01：在你进入这个领域的时候，机器智能领域有两种思想流派。
50 00:04:45,196 --> 00:04:47,398 说话人 SPEAKER_01：主流和神经网络。
51 00:04:48,644 --> 00:04:51,088 说话人 SPEAKER_01：你能描述一下这两种方法之间的区别吗？
52 00:04:51,608 --> 00:04:53,009 说话人 SPEAKER_00：我可以稍微夸张一下。
53 00:04:53,050 --> 00:04:57,336 说话人 SPEAKER_00：关于智能，有两种不同的模型。
54 00:04:57,978 --> 00:05:00,021 说话人 SPEAKER_00：其中一种模型认为智能就是推理。
55 00:05:01,281 --> 00:05:03,404 说话人 SPEAKER_00：我们通过逻辑来进行推理。
56 00:05:03,906 --> 00:05:05,487 说话人 SPEAKER_00：这就是人类特别的地方。
57 00:05:06,829 --> 00:05:12,639 说话人 SPEAKER_00: 我们应该做的是理解我们实际使用的逻辑。
58 00:05:13,759 --> 00:05:16,204 说话人 SPEAKER_00: 这也与这样的想法有关，
59 00:05:16,858 --> 00:05:29,642 说话人 SPEAKER_00: 你存储的知识是符号表达式，所以我可以对你说一句话，你将能够以某种方式存储它，然后你可以在以后用它来推断其他句子。
60 00:05:30,062 --> 00:05:33,670 说话人 SPEAKER_00: 但你头脑中的东西有点像句子，但经过了清理。
61 00:05:34,814 --> 00:05:42,404 说话人 SPEAKER_00：有一种完全不同的智能模式，那就是学习大脑细胞网络中的连接强度。
62 00:05:43,346 --> 00:05:47,812 说话人 SPEAKER_00：它擅长的是像感知和运动控制这样的东西，而不是推理。
63 00:05:48,014 --> 00:05:50,838 说话人 SPEAKER_00：推理出现得晚得多，而且我们在这方面并不擅长。
64 00:05:51,879 --> 00:05:53,382 说话人 SPEAKER_00：你直到相当老了才会学习如何去做它。
65 00:05:54,704 --> 00:05:58,189 说话人 SPEAKER_00: 事实上，推理实际上是一个非常糟糕的生物智能模型。
66 00:05:58,249 --> 00:06:03,656 说话人 SPEAKER_00: 生物智能关乎控制你的身体和观察事物等方面。
67 00:06:04,531 --> 00:06:08,855 说话人 SPEAKER_00: 那是完全不同的范式，对头脑中内容的理解也完全不同。
68 00:06:09,336 --> 00:06:14,202 说话人 SPEAKER_00: 它不是存储符号的字符串，而是连接强度。
69 00:06:15,163 --> 00:06:23,053 说话人 SPEAKER_00：符号 AI 视角的关键问题是，这些符号表达式的形式是什么，如何用它们进行推理？
70 00:06:24,396 --> 00:06:27,980 说话人 SPEAKER_00：对于神经网络视角，核心问题则完全不同。
71 00:06:28,000 --> 00:06:32,225 说话人 SPEAKER_00：它是，如何学习这些连接强度，以便你可以做所有这些奇妙的事情？
72 00:06:32,593 --> 00:06:35,197 说话人 SPEAKER_00：因此，学习一直是神经网络视角的核心。
73 00:06:35,718 --> 00:06:38,100 说话人 SPEAKER_00：从象征性观点来看，他们说，我们以后再考虑学习的问题。
74 00:06:38,161 --> 00:06:41,605 说话人 SPEAKER_00：首先，你必须弄清楚知识是如何表示的，我们如何进行推理。
75 00:06:42,365 --> 00:06:44,069 说话人 SPEAKER_00：因此，这些观点完全不同。
76 00:06:44,189 --> 00:06:46,812 说话人 SPEAKER_00：一个受到了逻辑的启发，另一个受到了生物学的启发。
77 00:06:47,512 --> 00:06:53,961 说话人 SPEAKER_00：长期以来，逻辑阵营的人们认为从生物学中汲取灵感是愚蠢的。
78 00:06:54,245 --> 00:07:04,680 说话人 SPEAKER_00：这有点奇怪，因为冯·诺伊曼和图灵都曾认为神经网络是攻克智能的方法，但不幸的是他们都英年早逝。
79 00:07:07,504 --> 00:07:11,129 说话人 SPEAKER_02：您能否从高层次上描述一下神经网络是如何工作的？
80 00:07:12,812 --> 00:07:13,392 说话人 SPEAKER_00：我可以尝试一下。
81 00:07:14,494 --> 00:07:19,982 Speaker SPEAKER_00: So let's start off by describing how it would work for recognizing objects in images.
82 00:07:20,165 --> 00:07:24,670 说话人 SPEAKER_00: 假设我们只想说图像中是否有鸟。
83 00:07:25,112 --> 00:07:28,456 说话人 SPEAKER_00: 假设鸟大致位于图像中间，是关注的焦点。
84 00:07:29,057 --> 00:07:31,420 说话人 SPEAKER_00: 你需要说，这是鸟还是不是鸟？
85 00:07:33,303 --> 00:07:34,586 说话人 说话人_00：你可以想象一个图像。
86 00:07:34,646 --> 00:07:36,769 说话人 说话人_00：假设它是一个100像素乘以100像素的。
87 00:07:37,310 --> 00:07:38,511 说话人 说话人_00：那就是10,000个像素。
88 00:07:39,151 --> 00:07:41,035 说话人 说话人_00：每个像素是三种颜色，RGB。
89 00:07:41,555 --> 00:07:43,117 说话人 SPEAKER_00: 这就是 30,000 个数字。
90 00:07:43,975 --> 00:07:53,209 说话人 SPEAKER_00: 在计算术语中，识别图像中的鸟由取 30,000 个数字并输出一个表示是或否是鸟的数字组成。
91 00:07:54,732 --> 00:08:01,723 说话人 SPEAKER_00: 你可以尝试编写一个标准的计算机程序来做这件事，人们尝试了很多年，但始终无法做得很好。
92 00:08:01,963 --> 00:08:03,425 说话人 SPEAKER_00: 比如，他们尝试了 50 年。
93 00:08:05,028 --> 00:08:07,812 说话人 SPEAKER_00: 或者你可以制作一个多层神经网络。
94 00:08:08,374 --> 00:08:11,838 说话人 SPEAKER_00: 我会先告诉你如何手动连接一个神经网络。
95 00:08:12,781 --> 00:08:20,350 说话人 SPEAKER_00: 所以你会做的是，你会有像素，这将是底层，然后你会有一层特征检测器。
96 00:08:21,192 --> 00:08:36,712 说话人 SPEAKER_00: 一个典型的特征检测器可能从垂直像素行中传来大的正连接强度，从相邻的垂直像素行中传来大的负连接强度，其他地方则没有连接强度。
97 00:08:37,434 --> 00:08:45,285 说话人 SPEAKER_00: 如果这两行像素都很亮，它将从这里获得很大的正输入，但也会从那里获得很大的负输入，所以它不会做任何事情。
98 00:08:46,086 --> 00:08:53,317 说话人 SPEAKER_00: 但是如果这些像素很亮，提供大的正输入，而那些像素不亮，所以它不会被这些像素抑制，它会非常兴奋。
99 00:08:53,677 --> 00:08:58,144 说话人 SPEAKER_00: 它会说，嘿，我找到了我喜欢的东西，那就是这里的亮像素和这里的暗像素。
100 00:08:58,985 --> 00:09:00,826 说话人 SPEAKER_00: 那就是一个边缘检测器。
101 00:09:00,846 --> 00:09:06,153 说话人 SPEAKER_00：我刚刚告诉你们如何使用正负权重手动接线，以检测微小的垂直边缘。
102 00:09:07,196 --> 00:09:14,764 说话人 SPEAKER_00：现在想象一下，有成千上万这样的东西在图像的不同位置、不同方向和不同尺度上检测不同的边缘。
103 00:09:15,826 --> 00:09:17,589 说话人 SPEAKER_00：这将是你的第一个特征检测层。
104 00:09:18,570 --> 00:09:21,634 说话人 SPEAKER_00：如果我要手动接线，我的第二层特征检测器
105 00:09:22,457 --> 00:09:28,869 说话人 SPEAKER_00：可能有一个检测器，它能够检测两个在细小角度相交的边缘，就像这样。
106 00:09:29,370 --> 00:09:31,052 说话人 SPEAKER_00：所以它正在寻找这条边和这条边。
107 00:09:31,453 --> 00:09:36,121 说话人 SPEAKER_00：如果它们同时激活，它就会说，嘿，可能这里有一个喙。
108 00:09:36,822 --> 00:09:39,506 说话人 说话人_00：它可能是各种各样的事情，但可能只是个喙。
109 00:09:39,967 --> 00:09:42,152 说话人 说话人_00：所以你有一个有点像喙的特征。
110 00:09:43,634 --> 00:09:47,942 说话人 说话人_00：在那个层面，你可能还有一个检测到形成圆形的一堆边缘的特征。
111 00:09:49,475 --> 00:09:56,101 说话人 说话人_00：因此，你将会有圆形检测器和可能的喙检测器，以及那个层面上的许多其他检测器。
112 00:09:56,121 --> 00:09:58,083 说话者 说话者_00: 但它们检测到的稍微复杂一些的东西。
113 00:09:59,205 --> 00:10:11,818 说话者 说话者_00: 然后在那一层之上，可能会有一些东西检测到一个潜在的喙，它与一个潜在的圆圈、一个潜在的瞳孔在正确的空间关系中，因此可能是鸟的头。
114 00:10:13,399 --> 00:10:15,162 说话者 说话者_00: 那就像是你的第三层。
115 00:10:16,340 --> 00:10:25,467 说话者 说话者_00: 也许在你的第三层中，你还能检测到鸟的脚和翅膀，那么在下一层可能就可以有鸟的检测器了。
116 00:10:25,908 --> 00:10:32,715 说话人 SPEAKER_00：如果这些事物中有几个变得活跃，比如，这里有一个头，有一个翅膀，有一个脚，那可能是一只鸟。
117 00:10:33,936 --> 00:10:39,421 说话人 SPEAKER_00：好吧，我之前已经告诉你们如何手动连接所有这些部件，但你永远不可能做得很好。
118 00:10:40,241 --> 00:10:44,085 说话人 SPEAKER_00：所以，我们不是手动连接所有这些部件，
119 00:10:44,369 --> 00:10:45,971 说话人 SPEAKER_00：我们可以想象尝试去学习它。
120 00:10:46,913 --> 00:10:50,837 说话人 SPEAKER_00：我已经告诉你们我们想学习的内容，但现在我要告诉你们我们是如何学习的。
121 00:10:51,438 --> 00:10:53,322 说话人 SPEAKER_00：我们学习的方式一开始听起来很奇怪。
122 00:10:54,604 --> 00:11:02,975 Speaker SPEAKER_00: Instead of wiring in all the connection strengths, so you get the detectors you want, you start with random connection strengths, just random numbers on all the connections.
123 00:11:03,937 --> 00:11:10,427 Speaker SPEAKER_00: And so you put in an image of a bird, and you go forward through these layers of feature detectors, and it just behaves completely randomly.
124 00:11:11,369 --> 00:11:15,736 Speaker SPEAKER_00: And the bird detector at the output will say 0.5 it's a bird.
125 00:11:16,658 --> 00:11:19,443 Speaker SPEAKER_00: It's going to say 1 when it's sure it's a bird and 0 when it's sure it's not a bird.
126 00:11:19,684 --> 00:11:20,986 Speaker SPEAKER_00: To me it's going to say about 0.5.
127 00:11:21,989 --> 00:11:23,471 Speaker SPEAKER_00: And now you can ask the following question.
128 00:11:25,095 --> 00:11:27,960 Speaker SPEAKER_00: How can I change all those connection strengths in the network?
129 00:11:29,307 --> 00:11:34,980 Speaker SPEAKER_00: So instead of saying 0.5 it's a bird, let's suppose it is a bird, it says 0.51 it's a bird.
130 00:11:35,741 --> 00:11:42,017 Speaker SPEAKER_00: So the question you want to ask is, how should I change a particular connection strength so as to make it more likely that it's a bird?
131 00:11:43,279 --> 00:11:49,109 Speaker SPEAKER_00: And you can figure that out by taking the difference between what you got and what you wanted.
132 00:11:49,470 --> 00:11:52,394 Speaker SPEAKER_00: So you wanted 1, and you actually got 0.5.
133 00:11:52,434 --> 00:11:56,520 Speaker SPEAKER_00: You take that difference, and you send that difference backwards through the network.
134 00:11:57,162 --> 00:12:00,106 Speaker SPEAKER_00: And then you use some calculus, which I won't explain.
135 00:12:00,086 --> 00:12:11,845 Speaker SPEAKER_00: and you're able to compute for every single connection in the network how much you'd like to make it bigger or smaller in order to make it more likely to say bird.
136 00:12:12,384 --> 00:12:17,153 Speaker SPEAKER_00: Then you adjust all the connection strengths very slightly in the direction that'll make it more likely to say bird.
137 00:12:18,033 --> 00:12:23,461 Speaker SPEAKER_00: Then you show it something that isn't a bird, and now you're going to adjust connection strengths so it's less likely to say that that was a bird.
138 00:12:24,691 --> 00:12:33,543 Speaker SPEAKER_00: And you just keep going like that, with lots of birds and non-birds, and eventually you'll discover that it's discovered all these feature detectors.
139 00:12:33,563 --> 00:12:38,211 Speaker SPEAKER_00: It'll have discovered beak-like things, and eye-like things, and things that detect feet and wings, and all that stuff.
140 00:12:39,032 --> 00:12:50,749 Speaker SPEAKER_00: And if you train it on lots of different objects, like a thousand different categories of object, it'll discover intermediate feature detectors that are very good for recognizing all sorts of things.
141 00:12:51,488 --> 00:13:09,708 Speaker SPEAKER_00: So the magic is that there's this relatively simple algorithm called backpropagation that takes the error in the output and sends that error backwards through the network and computes through all the connections, how you should change them to improve the behavior, and then you change them all a tiny bit, and you just keep going with another example.
142 00:13:11,171 --> 00:13:13,813 Speaker SPEAKER_00: And surprisingly, that actually works.
143 00:13:14,595 --> 00:13:16,937 Speaker SPEAKER_00: For many years, people thought that would just get jammed up.
144 00:13:17,258 --> 00:13:18,278 Speaker SPEAKER_00: It would get stuck somewhere.
145 00:13:18,600 --> 00:13:19,240 Speaker SPEAKER_00: But no, it doesn't.
146 00:13:19,279 --> 00:13:20,701 Speaker SPEAKER_00: It actually works very well.
147 00:13:22,419 --> 00:13:25,664 Speaker SPEAKER_02: I'm curious, how do neural networks handle language?
148 00:13:28,028 --> 00:13:32,715 Speaker SPEAKER_00: Okay, so now you've got the idea of how we train it to recognize a bird.
149 00:13:33,456 --> 00:13:45,712 Speaker SPEAKER_00: Imagine now that we take a string of words as the input, and the first thing you're going to do is convert a word into an embedding vector.
150 00:13:46,333 --> 00:13:52,342 Speaker SPEAKER_00: That is, it's a little bunch of numbers that captures the meaning of the word, or is intended to capture the meaning of the word.
151 00:13:53,621 --> 00:13:58,106 Speaker SPEAKER_00: And so, your first layer after the words will be these embedding vectors for each word.
152 00:13:59,808 --> 00:14:13,105 Speaker SPEAKER_00: And now we're going to have lots of layers of embedding vectors, and as we go up through the network, we're going to make the embedding vectors for a word get better and better, because they're going to take into account more and more contextual information.
153 00:14:14,166 --> 00:14:18,270 Speaker SPEAKER_00: So suppose in this sentence, let's suppose we don't have any capital letters, okay?
154 00:14:18,672 --> 00:14:21,654 Speaker SPEAKER_00: So suppose in this sentence, you have the word May,
155 00:14:23,169 --> 00:14:27,955 Speaker SPEAKER_00: Well, the most probable meaning of May is that it's a modal, as in he may do that.
156 00:14:29,816 --> 00:14:32,499 Speaker SPEAKER_00: But obviously there's a completely different meaning of May, which is the month.
157 00:14:34,041 --> 00:14:39,586 Speaker SPEAKER_00: And so initially, it doesn't know, just looking at the word May, it doesn't know what embedding vector to use.
158 00:14:40,986 --> 00:14:51,017 Speaker SPEAKER_00: And it'll use a kind of compromise vector, something that's sort of halfway between the embedding vector that represents the modal, May, and the embedding vector that represents the month, May.
159 00:14:52,684 --> 00:14:56,288 Speaker SPEAKER_00: And then at the next layer, it's going to refine that vector.
160 00:14:57,169 --> 00:15:02,395 Speaker SPEAKER_00: It's going to make a slightly better vector, depending on the context that it got, depending on nearby embedding vectors.
161 00:15:03,057 --> 00:15:13,149 Speaker SPEAKER_00: So if, for example, nearby, there's the embedding vector for June, then it'll refine the one for May to be more like a month and less like a modal.
162 00:15:14,431 --> 00:15:18,995 Speaker SPEAKER_00: But if there's the embedding vector for wood, it'll make it more like a modal and less like a month.
163 00:15:21,371 --> 00:15:26,998 Speaker SPEAKER_00: And as you go through the network, it can refine these embedding vectors and make them better and better.
164 00:15:28,099 --> 00:15:34,706 Speaker SPEAKER_00: And the way we're going to train it is we're going to give it a string of words as input.
165 00:15:36,589 --> 00:15:39,793 Speaker SPEAKER_00: And we're going to, here will be one way to do it.
166 00:15:39,812 --> 00:15:42,034 Speaker SPEAKER_00: It's not exactly what's done, but it's easy to understand.
167 00:15:42,936 --> 00:15:46,840 Speaker SPEAKER_00: For the last word, you just put in a kind of neutral word.
168 00:15:46,860 --> 00:15:48,341 Speaker SPEAKER_00: You say unknown.
169 00:15:48,440 --> 00:15:52,927 Speaker SPEAKER_00: And it has a very vague embedding vector that's kind of the average of all the vectors for all words.
170 00:15:53,006 --> 00:15:54,249 Speaker SPEAKER_00: It doesn't know, right?
171 00:15:55,370 --> 00:16:02,159 Speaker SPEAKER_00: Now as you go forward through the network, that last word will be able to be influenced by previous words.
172 00:16:03,682 --> 00:16:08,950 Speaker SPEAKER_00: And it starts off very vague, but as you go through these layers, it can get more and more precise.
173 00:16:10,052 --> 00:16:14,278 Speaker SPEAKER_00: And by the time you get to the end of the network, that embedding vector
174 00:16:14,966 --> 00:16:23,501 Speaker SPEAKER_00: could look like the embedding vector for a particular word, or for some combination of words, some average of several words.
175 00:16:25,063 --> 00:16:39,809 Speaker SPEAKER_00: And you train the network by saying, you go through all these layers, and that last word, you'd like the embedding vector to look like the embedding vector for the word that actually was there in the text.
176 00:16:40,769 --> 00:16:42,254 Speaker SPEAKER_00: And that's how it predicts the next word.
177 00:16:43,057 --> 00:16:52,347 Speaker SPEAKER_00: It tries to change this sort of neutral embedding vector into one that is close to the embedding vector for the correct word that appeared in the text.
178 00:16:53,914 --> 00:17:12,563 Speaker SPEAKER_00: and you take the error, the difference between the embedding vector and the text, and the embedding vector produced, and you propagate that backwards through the network, and it's propagating backwards through the layers, but it's propagating from this word to previous words, so that they will have the right influence on this word.
179 00:17:13,744 --> 00:17:17,570 Speaker SPEAKER_00: And that's the backpropagation algorithm learning to predict the next word.
180 00:17:18,853 --> 00:17:22,317 Speaker SPEAKER_01: So despite some of the theoretical breakthroughs in this field,
181 00:17:22,821 --> 00:17:25,846 Speaker SPEAKER_01: These neural networks didn't work very well for a long time.
182 00:17:26,727 --> 00:17:27,287 Speaker SPEAKER_01: Why was that?
183 00:17:28,628 --> 00:17:30,070 Speaker SPEAKER_00: It was a combination of reasons.
184 00:17:31,031 --> 00:17:33,433 Speaker SPEAKER_00: So we weren't very good at initializing them.
185 00:17:34,015 --> 00:17:36,657 Speaker SPEAKER_00: That is, I said you put in random weights and then learn everything.
186 00:17:37,338 --> 00:17:44,086 Speaker SPEAKER_00: But if you don't carefully decide what kind of random weights, the thing never gets off the ground.
187 00:17:44,067 --> 00:17:48,711 Speaker SPEAKER_00: So that was a little technical reason why they didn't work very well in deep nets with lots of layers of feature detectors.
188 00:17:49,151 --> 00:17:53,674 Speaker SPEAKER_00: But the main reason was we didn't have enough compute power and we didn't have enough data.
189 00:17:54,215 --> 00:17:59,701 Speaker SPEAKER_00: So people were trying to train these nets on relatively small training sets without much compute power.
190 00:18:00,661 --> 00:18:03,364 Speaker SPEAKER_00: And in that regime, other methods worked better.
191 00:18:04,023 --> 00:18:07,507 Speaker SPEAKER_00: Neural nets really come into their own when you have a lot of data and a lot of compute power.
192 00:18:08,087 --> 00:18:11,211 Speaker SPEAKER_00: And then you can use a big neural net and then it works much better than anything else.
193 00:18:12,111 --> 00:18:13,972 Speaker SPEAKER_00: And we didn't realize that at the time.
194 00:18:14,289 --> 00:18:19,478 Speaker SPEAKER_00: So we would occasionally fantasize, well, suppose you had a lot more data and a lot bigger computer, it would work better.
195 00:18:19,518 --> 00:18:21,601 Speaker SPEAKER_00: But we didn't realize it would work a whole lot better.
196 00:18:22,442 --> 00:18:30,976 Speaker SPEAKER_00: And so in the 1990s, it was a relatively dead period for neural nets, because other methods were working better on small problems.
197 00:18:32,377 --> 00:18:36,483 Speaker SPEAKER_00: And a lot of people in computer science gave up on neural nets.
198 00:18:37,847 --> 00:18:44,494 Speaker SPEAKER_00: In psychology, they didn't, because in psychology, they wanted something that was like the brain, and neural nets were clearly more like the brain than symbolic AI.
199 00:18:44,955 --> 00:18:48,740 Speaker SPEAKER_00: But in computer science, neural nets sort of came into disrepute in the 90s.
200 00:18:49,780 --> 00:18:52,964 Speaker SPEAKER_01: So let's fast forward then to another decade, to the 2000s.
201 00:18:55,007 --> 00:19:02,134 Speaker SPEAKER_01: Was there a moment for you when it became clear that the approach that you'd been pursuing was the one that was going to prevail?
202 00:19:02,174 --> 00:19:04,416 Speaker SPEAKER_00: Okay.
203 00:19:04,856 --> 00:19:07,180 Speaker SPEAKER_00: In 2006,
204 00:19:07,682 --> 00:19:13,917 Speaker SPEAKER_00: we figured out how to initialize the weights much better, by doing unsupervised learning, and then backpropagation worked much better.
205 00:19:14,419 --> 00:19:18,469 Speaker SPEAKER_00: So it was fairly clear then that backpropagation really was going to work very well.
206 00:19:19,394 --> 00:19:35,799 Speaker SPEAKER_00: But in 2009, two of my grad students, George Dahl and Abdulrahman Mohammed, made a much better speech recognizer, actually a slightly better speech recognizer, but it was slightly better than the state of the art, using deep neural nets.
207 00:19:36,520 --> 00:19:40,006 Speaker SPEAKER_00: And then it was fairly clear that this stuff was going somewhere.
208 00:19:40,386 --> 00:19:44,794 Speaker SPEAKER_00: And all the big speech groups over the next few years switched to using neural nets.
209 00:19:46,106 --> 00:19:52,194 Speaker SPEAKER_00: And then in 2012, that speech stuff came out in the Android, and suddenly the Android caught up with Siri.
210 00:19:52,595 --> 00:19:55,157 Speaker SPEAKER_00: It was as good at speech as Siri, because it was using neural nets.
211 00:19:56,159 --> 00:20:07,534 Speaker SPEAKER_00: And in the same year, two others of my graduate students, Ilya Sutskova and Andrzej Krzyzewski, made a neural net that was very good at recognizing objects and images.
212 00:20:07,953 --> 00:20:10,376 Speaker SPEAKER_00: And that beat the state-of-the-art by a lot.
213 00:20:10,828 --> 00:20:16,671 Speaker SPEAKER_00: And so I think it was this combination that it was already working for speech recognition and already in production.
214 00:20:17,494 --> 00:20:21,018 Speaker SPEAKER_00: The big companies do that, the public, I don't think, were very well aware of that.
215 00:20:21,479 --> 00:20:24,641 Speaker SPEAKER_00: But then suddenly, it worked much better for computer vision.
216 00:20:25,461 --> 00:20:27,084 Speaker SPEAKER_00: And that was a turning point.
217 00:20:27,584 --> 00:20:34,650 Speaker SPEAKER_00: In 2012, when we won the ImageNet competition by a huge margin, we got almost half the errors of the other methods.
218 00:20:35,391 --> 00:20:40,576 Speaker SPEAKER_00: And it was a public data set, but with a hidden test set, so you couldn't cheat.
219 00:20:41,477 --> 00:20:47,501 Speaker SPEAKER_01: So let's just focus a bit on 2012, because you said it was a really pivotal year for this.
220 00:20:47,481 --> 00:20:53,490 Speaker SPEAKER_01: Can you describe, again at a high level, how AlexNet worked?
221 00:20:54,310 --> 00:20:57,173 Speaker SPEAKER_01: I take it that might have been named after your graduate student.
222 00:20:57,193 --> 00:21:03,982 Speaker SPEAKER_00: That was named after Alex Krzyzewski because he was a wizard programmer and he made it work.
223 00:21:05,003 --> 00:21:08,087 Speaker SPEAKER_00: Ilya helped a lot, but it was mainly Alex's work.
224 00:21:08,989 --> 00:21:13,454 Speaker SPEAKER_00: So I explained to you, when explaining backprop, how you'd have these layers of feature detectors.
225 00:21:14,498 --> 00:21:24,527 Speaker SPEAKER_00: And AlexNet was basically that kind of a net, but with a thousand different object classes, and with about seven layers of feature detectors.
226 00:21:25,788 --> 00:21:32,815 Speaker SPEAKER_00: And it also used something else that was developed by Yann LeCun, which is convolutional nets.
227 00:21:33,516 --> 00:21:35,978 Speaker SPEAKER_00: And I'll try and explain those now, because they were very important.
228 00:21:38,759 --> 00:21:43,644 Speaker SPEAKER_00: Remember how I said you might make a detector for a bird's beak by
229 00:21:44,096 --> 00:21:50,487 Speaker SPEAKER_00: checking two lines, by having two lines like that, and if you see those two feature detectors, then you make a beak detector.
230 00:21:50,967 --> 00:21:53,310 Speaker SPEAKER_00: But that would just be for a specific location, right?
231 00:21:54,392 --> 00:22:02,644 Speaker SPEAKER_00: In a convolutional net, when you make a feature detector for one location, you make the same feature detector for all the locations in the image.
232 00:22:04,366 --> 00:22:08,973 Speaker SPEAKER_00: So now, if it's trained with a beak here, when it's learning,
233 00:22:09,375 --> 00:22:11,518 Speaker SPEAKER_00: And it really says, I need a beak detector for that.
234 00:22:11,958 --> 00:22:13,839 Speaker SPEAKER_00: So it learns a feature that detects this beak.
235 00:22:14,461 --> 00:22:18,384 Speaker SPEAKER_00: It will automatically make copies for all of the other locations in the image.
236 00:22:19,065 --> 00:22:24,169 Speaker SPEAKER_00: So if now the bird occurs in a different location, it will have the feature detectors to recognize it.
237 00:22:25,530 --> 00:22:30,915 Speaker SPEAKER_00: So that idea that you copy the feature detectors to every location, that's a convolutional net, essentially.
238 00:22:32,596 --> 00:22:37,000 Speaker SPEAKER_00: And that makes the whole thing generalized much better across position.
239 00:22:37,040 --> 00:22:39,163 Speaker SPEAKER_00: It can cope now with things changing position.
240 00:22:39,666 --> 00:22:42,371 Speaker SPEAKER_00: because it's got copies of all these feature detectors in every location.
241 00:22:43,834 --> 00:23:00,619 Speaker SPEAKER_00: And with convolutional nets and multiple layers of features, what Alex did was programmed all that very efficiently on a thing called a graphics processing unit, which was developed for computer graphics, but it's like a mini supercomputer.
242 00:23:01,461 --> 00:23:06,670 Speaker SPEAKER_00: It can do lots and lots of computation in lots of separate processes all at the same time.
243 00:23:07,307 --> 00:23:10,613 Speaker SPEAKER_00: And so it gave us about a factor of 30 compared with a normal computer.
244 00:23:11,473 --> 00:23:14,817 Speaker SPEAKER_00: And a factor of 30 is about sort of 10 years progress in computers.
245 00:23:15,479 --> 00:23:18,864 Speaker SPEAKER_00: So suddenly we could leap 10 years into the future in terms of compute power.
246 00:23:20,705 --> 00:23:25,153 Speaker SPEAKER_00: And it was very difficult to program these GPU boards.
247 00:23:26,134 --> 00:23:29,578 Speaker SPEAKER_00: Alex managed to program two of them to collaborate, which was even more difficult.
248 00:23:31,520 --> 00:23:34,986 Speaker SPEAKER_00: And the last ingredient was the ImageNet data set.
249 00:23:35,489 --> 00:23:47,625 Speaker SPEAKER_00: So someone called Fei-Fei Li and her collaborators put together a big set of images and then a public competition where you had about a million images with a thousand different kinds of objects.
250 00:23:47,664 --> 00:23:49,968 Speaker SPEAKER_00: So you had about a thousand examples of each kind of object.
251 00:23:50,709 --> 00:23:52,652 Speaker SPEAKER_00: And you had to learn to recognize those objects.
252 00:23:53,173 --> 00:23:57,417 Speaker SPEAKER_00: And then the test set would be different images, which also contained those objects.
253 00:23:57,699 --> 00:23:59,641 Speaker SPEAKER_00: And so you'd have to generalize to the different images.
254 00:24:00,522 --> 00:24:03,066 Speaker SPEAKER_00: And it turned out the best computer vision technique
255 00:24:03,383 --> 00:24:10,309 Speaker SPEAKER_00: that had been invented up till then was getting like 25% errors, and Alex got 15% errors.
256 00:24:11,611 --> 00:24:13,894 Speaker SPEAKER_00: And since then, it's gone down to about 3% errors.
257 00:24:14,153 --> 00:24:15,115 Speaker SPEAKER_00: It's gone much better since then.
258 00:24:15,494 --> 00:24:30,369 Speaker SPEAKER_00: But it was a huge jump, and people in computer vision were extremely surprised, and most of them behaved in a very admirable way, which is they said, hey, we never thought this would work, but hey, it works, so we're gonna do that instead of what we were doing.
259 00:24:30,686 --> 00:24:32,229 Speaker SPEAKER_00: That's what scientists don't usually do.
260 00:24:32,429 --> 00:24:35,511 Speaker SPEAKER_00: Scientists usually just grow old complaining that this new stuff is nonsense.
261 00:24:35,952 --> 00:24:41,199 Speaker SPEAKER_01: And how would you describe the pace of innovation that we've seen in AI since that moment?
262 00:24:41,939 --> 00:24:43,361 Speaker SPEAKER_00: It's just got faster and faster.
263 00:24:43,661 --> 00:24:53,992 Speaker SPEAKER_00: So if you'd asked me in that moment, how long till these neural nets can do machine translation that's better than the state of the art, I'd have said maybe 10 years.
264 00:24:54,614 --> 00:24:58,617 Speaker SPEAKER_00: Because machine translation is the kind of thing that
265 00:24:58,597 --> 00:25:08,713 Speaker SPEAKER_00: If you've got a theory that's all about processing strings of symbols, machine translation is the ideal problem for you because you have a string of symbols in one language, and you have to produce a string of symbols in another language.
266 00:25:09,515 --> 00:25:13,622 Speaker SPEAKER_00: And the symbolic people thought, well, inside you're just manipulating strings to do that.
267 00:25:14,943 --> 00:25:23,678 Speaker SPEAKER_00: The neural net people thought, you have to take this string of symbols, you have to convert it into these big patterns of neural activity, and then you have to convert it back into symbols at the output.
268 00:25:25,041 --> 00:25:35,675 Speaker SPEAKER_00: And I was very surprised when it only took a few years for machine translation to be good, and then in another year or two, Google was using it, and it greatly improved the quality of machine translation.
269 00:25:36,396 --> 00:25:48,113 Speaker SPEAKER_00: Like, in languages like Chinese, this is from memory, but there was a gap between how good the computer translation was and how good human translation was, and it just halved that gap overnight.
270 00:25:49,307 --> 00:25:50,368 Speaker SPEAKER_00: I think it was Chinese that did that.
271 00:25:51,029 --> 00:25:52,932 Speaker SPEAKER_00: But in a lot of languages, it just made it a lot better.
272 00:25:53,313 --> 00:25:56,317 Speaker SPEAKER_00: And since then, obviously, it's got considerably better since then.
273 00:25:56,917 --> 00:25:59,280 Speaker SPEAKER_00: But by 2015, it was already working pretty well.
274 00:26:00,221 --> 00:26:01,203 Speaker SPEAKER_00: And that really surprised me.
275 00:26:01,304 --> 00:26:02,184 Speaker SPEAKER_00: It only took three years.
276 00:26:04,728 --> 00:26:07,152 Speaker SPEAKER_02: You say you were surprised at the pace of innovation.
277 00:26:07,632 --> 00:26:11,738 Speaker SPEAKER_02: What did you think the first time you used a large language model like ChatGPT?
278 00:26:12,178 --> 00:26:13,078 Speaker SPEAKER_02: Did we surprise you?
279 00:26:13,118 --> 00:26:18,926 Speaker SPEAKER_00: I'm just shocked at how good it is.
280 00:26:20,019 --> 00:26:25,732 Speaker SPEAKER_00: So it gives very coherent answers and it can do little bits of reasoning.
281 00:26:26,334 --> 00:26:29,480 Speaker SPEAKER_00: Not very sophisticated reasoning yet, although it'll get much better.
282 00:26:30,261 --> 00:26:38,721 Speaker SPEAKER_00: So for example, I asked it, this is GPT-4 now, I asked it a puzzle given to me by a symbolic AI guy.
283 00:26:39,460 --> 00:26:40,800 Speaker SPEAKER_00: who thought it wouldn't be able to do it.
284 00:26:41,682 --> 00:26:44,125 Speaker SPEAKER_00: I actually made the puzzle much harder and it could still do it.
285 00:26:44,785 --> 00:26:45,945 Speaker SPEAKER_00: And so the puzzle goes like this.
286 00:26:46,707 --> 00:26:51,131 Speaker SPEAKER_00: The rooms in my house are either white or blue or yellow.
287 00:26:53,773 --> 00:26:56,376 Speaker SPEAKER_00: Yellow paint fades to white within a year.
288 00:26:57,597 --> 00:27:00,461 Speaker SPEAKER_00: In two years' time, I would like all the rooms to be white.
289 00:27:00,780 --> 00:27:01,461 Speaker SPEAKER_00: What should I do?
290 00:27:04,505 --> 00:27:07,948 Speaker SPEAKER_00: And a human being would probably say, you should paint the blue rooms white.
291 00:27:09,025 --> 00:27:15,372 Speaker SPEAKER_00: What GPT-4 said was you should paint the blue rooms yellow, but that works too because the yellow will fade to white.
292 00:27:16,374 --> 00:27:21,118 Speaker SPEAKER_00: And I don't see how it could do that without understanding the problem.
293 00:27:21,839 --> 00:27:25,663 Speaker SPEAKER_00: The idea that it's just sort of predicting the next word and using statistics.
294 00:27:26,766 --> 00:27:32,231 Speaker SPEAKER_00: There's a sense in which that's true, but it's not the sense of statistics that most people understand.
295 00:27:33,410 --> 00:27:42,280 Speaker SPEAKER_00: It, from the data, it figures out how to extract the meaning of the sentence, and it uses the meaning of the sentence to predict the next word.
296 00:27:42,761 --> 00:27:45,546 Speaker SPEAKER_00: It really does understand, and that's quite shocking.
297 00:27:46,646 --> 00:27:51,834 Speaker SPEAKER_01: So have you been surprised by the broader reaction, the public reaction to chat GPT?
298 00:27:53,295 --> 00:27:56,900 Speaker SPEAKER_00: Well, given how well it works, I guess the public reaction isn't that surprising.
299 00:27:57,160 --> 00:27:58,422 Speaker SPEAKER_00: But what's interesting is,
300 00:27:59,633 --> 00:28:02,576 Speaker SPEAKER_00: Most people don't say, this doesn't understand.
301 00:28:03,337 --> 00:28:06,020 Speaker SPEAKER_00: They say, wow, it understood what I said and gave me a coherent answer.
302 00:28:06,402 --> 00:28:07,303 Speaker SPEAKER_00: What can I use it for?
303 00:28:08,505 --> 00:28:10,948 Speaker SPEAKER_00: And I think most people are right about that.
304 00:28:12,048 --> 00:28:15,294 Speaker SPEAKER_00: And of course, it can be used for huge numbers of things.
305 00:28:16,035 --> 00:28:20,059 Speaker SPEAKER_00: So I know someone who answers letters of complaint for the health service.
306 00:28:22,022 --> 00:28:26,448 Speaker SPEAKER_00: And he used to spend 25 minutes composing a letter that addresses the problem and so on.
307 00:28:27,068 --> 00:28:29,251 Speaker SPEAKER_00: Now he just types the problem to
308 00:28:30,598 --> 00:28:33,584 Speaker SPEAKER_00: GPT-4, and it writes the letter.
309 00:28:34,124 --> 00:28:37,509 Speaker SPEAKER_00: And then he just looks at the letter and decides if it's okay and sends it out.
310 00:28:37,529 --> 00:28:38,711 Speaker SPEAKER_00: And that takes him five minutes now.
311 00:28:39,291 --> 00:28:41,154 Speaker SPEAKER_00: So he's now five times more efficient.
312 00:28:42,176 --> 00:28:43,960 Speaker SPEAKER_00: And that's going to happen all over the place.
313 00:28:44,580 --> 00:28:46,242 Speaker SPEAKER_00: Like paralegals are going to be like that.
314 00:28:47,044 --> 00:28:48,747 Speaker SPEAKER_00: Programmers are already getting like that.
315 00:28:49,208 --> 00:28:55,897 Speaker SPEAKER_00: Programmers can be much more efficient if they get assistance from things like GPT-4, because it knows how to program.
316 00:28:56,923 --> 00:29:01,549 Speaker SPEAKER_00: And you might think it just knows how to program because it's seen a whole lot of programs.
317 00:29:03,732 --> 00:29:06,857 Speaker SPEAKER_00: So I have a former graduate student who's very smart and a very good programmer.
318 00:29:08,179 --> 00:29:12,125 Speaker SPEAKER_00: And he did a little experiment which is, he's called Radford Neal.
319 00:29:12,565 --> 00:29:22,459 Speaker SPEAKER_00: He took GPT-4 and he defined a new programming language with very unusual syntax.
320 00:29:23,839 --> 00:29:30,906 Speaker SPEAKER_00: And having defined this programming language just in text to GPT-4, he then gave it a program and said, what would this do?
321 00:29:32,127 --> 00:29:33,128 Speaker SPEAKER_00: And it answered correctly.
322 00:29:34,170 --> 00:29:39,496 Speaker SPEAKER_00: So basically, it could understand the definition of a new programming language and figure out what programs in that language would do.
323 00:29:40,856 --> 00:29:46,461 Speaker SPEAKER_00: And again, the idea that it's just predicting the next word doesn't make any sense in that context.
324 00:29:46,501 --> 00:29:48,284 Speaker SPEAKER_00: It had to understand what was going on.
325 00:29:49,204 --> 00:29:52,107 Speaker SPEAKER_01: So what do you see as some of the most promising opportunities for
326 00:29:53,067 --> 00:29:56,374 Speaker SPEAKER_01: this type of AI when it comes to benefiting society?
327 00:29:58,337 --> 00:30:00,182 Speaker SPEAKER_00: It's hard to pick one because there's so many.
328 00:30:01,003 --> 00:30:07,818 Speaker SPEAKER_00: Like, there'll be a huge increase in productivity for any job that involves outputting text.
329 00:30:09,030 --> 00:30:12,056 Speaker SPEAKER_00: There's all sorts of issues about increasing productivity.
330 00:30:12,076 --> 00:30:17,242 Speaker SPEAKER_00: In our society, it's not necessarily a good thing to increase productivity because it might make the rich rich and the poor poorer.
331 00:30:17,903 --> 00:30:21,470 Speaker SPEAKER_00: But in a decent society, just increasing productivity ought to be a good thing.
332 00:30:22,171 --> 00:30:23,413 Speaker SPEAKER_00: So there'll be things like that.
333 00:30:24,233 --> 00:30:26,758 说话人 SPEAKER_00：这对于做出预测非常棒。
334 00:30:26,798 --> 00:30:29,622 说话人 SPEAKER_00：它将更好地预测天气。
335 00:30:30,394 --> 00:30:31,757 说话人 说话人_00：人们还不知道具体有多少。
336 00:30:32,136 --> 00:30:35,301 说话人 说话人_00：但它已经擅长预测洪水了。
337 00:30:36,063 --> 00:30:37,364 说话人 说话人_00：它可以预测地震。
338 00:30:38,105 --> 00:30:40,689 说话人 说话人_00：它可以设计新的纳米材料。
339 00:30:41,390 --> 00:30:44,493 说话人 说话人_00：对于像太阳能电池板这样的东西，你希望能够设计新的纳米材料。
340 00:30:44,835 --> 00:30:46,256 说话人 说话人_00：或者对于超导性。
341 00:30:46,276 --> 00:30:49,079 说话人 说话人_00：我不知道它是否已经用于超导性，但它很可能已经如此。
342 00:30:49,780 --> 00:30:52,605 说话人 说话人_00：你希望它在高温下也能实现。
343 00:30:52,585 --> 00:30:55,069 说话人 SPEAKER_00：它真的很擅长设计药物。
344 00:30:56,471 --> 00:31:01,161 说话人 SPEAKER_00：也就是说，寻找会与某些特定分子结合的分子。
345 00:31:02,222 --> 00:31:05,469 说话人 SPEAKER_00：DeepMind 用它创建了 AlphaFold。
346 00:31:06,810 --> 00:31:09,656 说话人 SPEAKER_00：现在那不是一个聊天机器人，那只是深度学习。
347 00:31:11,019 --> 00:31:15,166 说话人 SPEAKER_00: 深度学习的基本技术已经基本解决了如何从蛋白质的碱基序列中推断其形状的问题。
348 00:31:16,817 --> 00:31:24,827 说话人 SPEAKER_00: 大概已经解决了如何从蛋白质的碱基序列中推断其形状的问题。
349 00:31:25,288 --> 00:31:27,231 说话人 SPEAKER_00: 如果你知道了它的形状，你就知道了它的功能。
350 00:31:27,731 --> 00:31:30,194 说话人 SPEAKER_00: 我认为聊天机器人将会被广泛应用。
351 00:31:31,917 --> 00:31:33,680 说话人 SPEAKER_01: 我们还讨论了很多关于医疗保健的内容。
352 00:31:33,700 --> 00:31:38,586 说话人 SPEAKER_01: 我的意思是，你提到了药物发现，但医疗保健也是一个可以真正受益的领域。
353 00:31:38,605 --> 00:31:38,826 说话人 SPEAKER_00: 是的。
354 00:31:39,567 --> 00:31:46,757 说话人 SPEAKER_00: 无论是解读医学扫描，比如如果你做一个 CT 扫描，CT 扫描中有很多信息。
355 00:31:47,210 --> 00:32:01,724 说话者：说话者_00：这并没有被使用，大多数医生都不知道这些信息，这将能够从 CT 扫描中获得更多信息，同时还能与医生竞争，说出你有什么癌症或它长得有多大。
356 00:32:01,744 --> 00:32:10,733 说话者：说话者_00：例如，现在当医生告诉你癌症的大小，你会得到一个像三厘米这样的数字，一个月前它是两厘米。
357 00:32:11,895 --> 00:32:15,259 说话者：说话者_00：现在，如果这东西看起来像章鱼，这个数字并不是很有用，对吧？
358 00:32:17,112 --> 00:32:21,938 说话者：说话者_00：神经网络将能够更好地理解癌症的体积以及它的变化。
359 00:32:23,318 --> 00:32:25,642 说话人 SPEAKER_00: 那里将会非常棒。
360 00:32:26,202 --> 00:32:30,949 说话人 SPEAKER_00: 已经在许多癌症扫描中达到人类水平，而且会越来越好。
361 00:32:32,349 --> 00:32:34,873 说话人 SPEAKER_00: 对诊断疾病将非常有帮助。
362 00:32:35,193 --> 00:32:43,103 说话人 SPEAKER_00: 目前，在北美有大量的人因为医生误诊而死亡。
363 00:32:44,482 --> 00:32:55,673 说话人 SPEAKER_00：谷歌正在开发一个名为 MedPalm2 的系统，它已经学会了进行诊断，我认为它的表现已经超过了普通医生。
364 00:32:56,335 --> 00:32:59,178 说话人 SPEAKER_00：对此我并不完全确定，因为我已经不再在谷歌工作了，而且这也很新。
365 00:33:00,159 --> 00:33:04,262 说话人 SPEAKER_00：但它确实可以与医生相媲美，而且它会迅速变得更好。
366 00:33:04,282 --> 00:33:09,407 说话人 SPEAKER_00：那么，您是否希望有一个全科医生，一个家庭医生呢？
367 00:33:10,148 --> 00:33:12,111 说话人 SPEAKER_00: 你患有某种罕见疾病，
368 00:33:12,394 --> 00:33:16,961 说话人 SPEAKER_00: 你希望你的家庭医生已经看过数百例这种罕见疾病。
369 00:33:17,500 --> 00:33:18,923 说话人 SPEAKER_00: 而 MedPalm2 将会是这样。
370 00:33:19,523 --> 00:33:23,368 说话人 SPEAKER_00: 所以它最终在诊断方面会更好。
371 00:33:25,652 --> 00:33:28,474 说话人 SPEAKER_02：听起来人工智能将带来许多重要好处。
372 00:33:29,537 --> 00:33:32,580 说话人 SPEAKER_02：但你表达了对当前创新速度的担忧。
373 00:33:33,340 --> 00:33:33,761 说话人 SPEAKER_00：为什么？
374 00:33:34,315 --> 00:33:44,626 说话人 SPEAKER_00：好的，所以像 50 年一样，我在 49 年里认为，为了使数字模型更好，我们需要让它们更像大脑工作。
375 00:33:45,327 --> 00:33:55,480 说话者 SPEAKER_00：所以我一直在观察大脑所做的而数字模型没有做的事情，比如以临时方式快速改变连接强度，这可以使数字模型变得更好。
376 00:33:57,805 --> 00:34:07,291 说话者 SPEAKER_00：而且最近，我意识到因为这些数字模型具有这种蜂群思维，当一个智能体学习到某些东西时，所有其他智能体都知道。
377 00:34:07,863 --> 00:34:10,947 说话者 SPEAKER_00：它们实际上可能已经比生物智能更优秀了。
378 00:34:11,829 --> 00:34:18,420 说话者 SPEAKER_00：因此，我对它们能够做到大脑所做的一切需要很长时间的观点完全改变了看法。
379 00:34:19,101 --> 00:34:23,931 说话人 SPEAKER_00：可能要 30 到 50 年他们才能比我们更好，这是我直到最近才这么想的。
380 00:34:24,952 --> 00:34:29,139 说话人 SPEAKER_00：几个月前我突然意识到，也许他们已经比我们更好了。
381 00:34:29,599 --> 00:34:31,103 说话人 SPEAKER_00：只是规模更小。
382 00:34:31,420 --> 00:34:34,864 说话人 SPEAKER_00：等它们变得更大，它们就会比我们更聪明。
383 00:34:35,626 --> 00:34:36,666 说话人 SPEAKER_00: 这真的很可怕。
384 00:34:36,686 --> 00:34:43,396 说话人 SPEAKER_00: 意见突然改变，不再是 30 到 50 年，而是 5 年到 20 年左右。
385 00:34:44,157 --> 00:34:51,927 说话人 SPEAKER_00: 因此，我们现在必须认真对待我们现在将如何处理这个问题，这些事物可能会比我们更聪明。
386 00:34:52,260 --> 00:34:53,802 说话人 SPEAKER_00: 这是一个充满不确定性的时代。
387 00:34:53,842 --> 00:34:55,364 说话人 SPEAKER_00: 没有人真正知道会发生什么。
388 00:34:56,045 --> 00:34:59,869 说话人 SPEAKER_00: 或许事情会停滞不前，或许它们不会比我们更聪明。
389 00:34:59,889 --> 00:35:01,110 说话人 SPEAKER_00: 但我并不真的相信这一点。
390 00:35:01,451 --> 00:35:02,871 说话人 SPEAKER_00: 我认为它们将会比我们更聪明。
391 00:35:03,231 --> 00:35:12,902 说话者 SPEAKER_00: 但也许当它们比我们更聪明时，我们还能让它们保持善良，我们还能让它们比关心自己更关心人们，不像人类。
392 00:35:13,362 --> 00:35:14,023 说话者 SPEAKER_00: 但也许不能。
393 00:35:14,844 --> 00:35:18,387 说话者 SPEAKER_00: 因此我们需要非常认真地思考这些问题。
394 00:35:18,527 --> 00:35:20,309 说话者 SPEAKER_00: 我不是这些问题方面的专家。
395 00:35:21,572 --> 00:35:23,875 说话人 SPEAKER_00：我只是这些学习算法的专家。
396 00:35:25,056 --> 00:35:29,501 说话人 SPEAKER_00：我突然意识到这些超级智能可能很快就会到来。
397 00:35:30,541 --> 00:35:38,849 说话人 SPEAKER_00：我只是拉响警报，让人们听听那些长时间思考如何阻止它们夺取控制权的人。
398 00:35:40,251 --> 00:35:45,797 说话人 SPEAKER_00：我希望政治家们听听那些人，而不是说，嗯，嗯，他们是科幻小说作家。
399 00:35:46,297 --> 00:35:47,157 说话人 SPEAKER_00: 这永远都不会发生。
400 00:35:48,067 --> 00:35:54,434 说话人 SPEAKER_01: 有没有哪个特别的时刻，你提到说这是最近的事情，你对此有了改变看法？
401 00:35:54,875 --> 00:36:04,507 说话人 SPEAKER_00: 我正在开发适用于生物系统的学习算法，这些算法可以在生物系统中运行，它们不使用反向传播。
402 00:36:05,688 --> 00:36:10,614 说话人 SPEAKER_00: 我无法让这些算法像我们在这些数字系统中运行的反向传播算法那样工作得更好。
403 00:36:11,657 --> 00:36:18,606 说话人 SPEAKER_00: 它们会在小网络上工作，但是当我扩大规模时，数字模型总是比生物模型扩展得更好。
404 00:36:19,646 --> 00:36:21,829 说话人 SPEAKER_00: 突然间我想，可能不是我的错。
405 00:36:22,251 --> 00:36:27,036 说话人 SPEAKER_00: 可能不是我的学习算法本身就是一个糟糕的学习算法。
406 00:36:27,056 --> 00:36:29,559 说话人 SPEAKER_00: 可能是这些数字系统本身就更好。
407 00:36:31,440 --> 00:36:37,387 说话人 SPEAKER_00：就在那时，我突然改变了关于我们何时会得到超级智能的看法。
408 00:36:37,407 --> 00:36:41,293 说话人 SPEAKER_00：然后我跟我的一些前学生和前同事谈了谈。
409 00:36:41,780 --> 00:36:44,204 说话人 SPEAKER_00：他们中的一些人鼓励我公开这个想法。
410 00:36:45,085 --> 00:36:47,630 说话人 SPEAKER_00：不是因为我想推荐任何解决方案。
411 00:36:49,054 --> 00:36:52,199 说话者 SPEAKER_00：你不能说减少碳排放一切就会好起来。
412 00:36:54,244 --> 00:36:56,547 说话者 SPEAKER_00：但是他们认为
413 00:36:56,831 --> 00:36:58,134 说话者 SPEAKER_00：我在这个领域很有名。
414 00:36:58,233 --> 00:37:12,990 说话者 SPEAKER_00：如果我公开表示超级智能可能很快就会到来，政客们可能会开始相信这是可能的，并开始认真倾听那些长期思考如何防止这些事物获得控制的研究人员。
415 00:37:14,012 --> 00:37:23,362 说话人 SPEAKER_01：那么从您的角度来看，政府在确保这些人工智能以负责任的方式发展方面能扮演什么角色？
416 00:37:23,882 --> 00:37:29,067 说话人 SPEAKER_00：关于其他人已经大量讨论过的各种风险，我特别不想谈论，比如...
417 00:37:29,637 --> 00:37:33,961 说话人 SPEAKER_00：它们会夺走工作，并加剧贫富差距。
418 00:37:34,762 --> 00:37:37,847 说话人 SPEAKER_00：它们将使得无法判断新闻是真是假。
419 00:37:38,827 --> 00:37:45,657 说话人 SPEAKER_00: 他们将鼓励社会分裂成两个相互不听取对方意见、持有完全对立观点的战争阵营。
420 00:37:46,898 --> 00:37:49,501 说话人 SPEAKER_00: 他们将制造旨在杀人的人形战斗机器人。
421 00:37:50,081 --> 00:37:52,425 说话人 SPEAKER_00: 所有这些我都已经提到过的风险。
422 00:37:52,465 --> 00:37:54,246 说话人 SPEAKER_00: 并非我不认为它们很重要。
423 00:37:54,266 --> 00:37:55,708 说话人 SPEAKER_00: 我认为这问题可能更加紧迫。
424 00:37:56,929 --> 00:37:59,052 说话人 SPEAKER_00: 但很多人都在谈论这些风险。
425 00:37:59,523 --> 00:38:03,306 说话人 SPEAKER_00: 我所说的风险是这些事物可能会比我们更聪明，最终接管一切。
426 00:38:04,309 --> 00:38:10,556 说话人 SPEAKER_00: 对于这种风险，政府可能有所作为，因为没有人希望看到这种情况。
427 00:38:12,197 --> 00:38:16,023 说话人 SPEAKER_00：嗯，如果排除这些超级智能，没有人想要那样。
428 00:38:16,983 --> 00:38:24,012 说话人 SPEAKER_00：因此，所有不同的政府都应该能够达成一致。
429 00:38:24,126 --> 00:38:27,797 说话人 SPEAKER_00：他们应该能够共同努力防止这种情况，因为这符合他们的利益。
430 00:38:28,239 --> 00:38:29,061 说话人 SPEAKER_00：这种情况以前已经发生过。
431 00:38:29,202 --> 00:38:37,849 说话人 SPEAKER_00: 即使在冷战期间，美国和俄罗斯也能共同努力，试图阻止全球核战争的发生，因为这会对所有人造成极大的危害。
432 00:38:38,891 --> 00:38:46,262 说话人 SPEAKER_00: 对于这种生存威胁，如果有可能预防，所有人都应该共同努力来限制它。
433 00:38:46,782 --> 00:38:54,954 说话人 SPEAKER_00: 我不知道是否能够预防它，但至少我们应该能够在应对这一特定威胁，即人工智能接管人类的生存威胁方面实现国际合作。
434 00:38:55,956 --> 00:39:03,027 说话人 SPEAKER_00: 我认为应该做的一件事是，无论这些技术在哪里被开发，尤其是这些大型聊天机器人，
435 00:39:04,693 --> 00:39:15,190 说话人 SPEAKER_00: 政府应该鼓励公司投入大量资源，因为这些事物变得越来越智能，进行实验以找出如何控制它们。
436 00:39:16,351 --> 00:39:26,786 说话人 SPEAKER_00: 因此，他们应该研究这些事物可能如何尝试逃脱，并对此进行实证研究，投入大量资源，因为这是我们唯一的机会。
437 00:39:27,762 --> 00:39:32,487 说话人 SPEAKER_00: 在它们变得超级智能之前，我们可能可以进行实验，看看会出什么问题。
438 00:39:33,429 --> 00:39:37,355 说话人 SPEAKER_00: 我坚信我们需要这方面的实证数据。
439 00:39:37,375 --> 00:39:42,021 说话者 SPEAKER_00：你不能让哲学家、政治家和立法者制定规则。
440 00:39:42,702 --> 00:39:47,547 说话者 SPEAKER_00：你需要进行实证研究，看看这些事情是如何出错的，以及你如何控制它们。
441 00:39:48,608 --> 00:39:50,492 说话者 SPEAKER_00：这只能由开发它们的人来完成。
442 00:39:51,893 --> 00:39:56,739 说话者 SPEAKER_00：既然你无法阻止其发展，你能做的最好的就是
443 00:39:57,074 --> 00:40:08,155 说话者 SPEAKER_00：不知为何，政府给这些公司施加了很大压力，让他们投入大量资源去实证研究如何在我们不够聪明的时候控制它们。
444 00:40:09,356 --> 00:40:15,527 说话者 SPEAKER_01：您如何看待这些大型科技公司，这些发展大多发生在这些公司中？
445 00:40:15,989 --> 00:40:18,693 说话者 SPEAKER_01：他们会在没有这种政府监管的情况下这样做吗？
446 00:40:19,079 --> 00:40:27,675 说话者 SPEAKER_00：所以，在大型公司中，我所认识的那些高层人士都非常担心这个问题，并且为此投入了工作。
447 00:40:28,677 --> 00:40:29,780 说话人 SPEAKER_00: 他们对此非常关心。
448 00:40:30,561 --> 00:40:32,626 说话人 SPEAKER_00: 但他们有义务向股东负责。
449 00:40:33,507 --> 00:40:35,572 说话人 SPEAKER_00: 我认为这是为了获得巨大利润。
450 00:40:36,534 --> 00:40:40,041 说话人 SPEAKER_00: 获得巨大利润，尤其是在短期内，
451 00:40:40,391 --> 00:40:44,760 说话人 SPEAKER_00: 这与投入大量精力确保其安全并不相符。
452 00:40:45,824 --> 00:40:47,146 说话人 SPEAKER_00: 所以您可以看到这在所有行业中都存在。
453 00:40:47,847 --> 00:40:59,632 说话人 SPEAKER_00: 在美国的铁路行业，拥有告知您车轮是否锁定的安全装置需要花费金钱，大型铁路公司宁愿发生事故也不愿这么做。
454 00:40:59,612 --> 00:41:10,929 说话人 SPEAKER_00: 我了解的谷歌这样的大公司并不完全如此，因为它明白如果发生不好的事情，它将遭受巨大的声誉损失。
455 00:41:11,431 --> 00:41:13,554 说话人 SPEAKER_00: 正因如此，谷歌没有发布这些聊天机器人。
456 00:41:13,574 --> 00:41:14,454 说话人 SPEAKER_00: 它们将其保留为私密。
457 00:41:14,755 --> 00:41:16,697 说话人 SPEAKER_00: 它不想让它们在世界各地供人们玩耍。
458 00:41:17,259 --> 00:41:26,632 说话人 SPEAKER_00: 它想利用它们来提供更好的搜索结果或帮您完成 Gmail，而不是让人们来玩耍。
459 00:41:27,472 --> 00:41:33,440 说话人 SPEAKER_00：它只能这样负责，直到 OpenAI 和微软将其推出，然后谷歌不得不竞争。
460 00:41:34,101 --> 00:41:39,449 说话人 SPEAKER_00：但大公司的大人物们真的很在乎他们的声誉，以及避免产生不良影响。
461 00:41:40,451 --> 00:41:49,663 说话人 SPEAKER_00：但也许可以通过政府采取行动，坚持要求他们投入大量工作来解决安全问题，使他们更加关注这个问题。
462 00:41:50,545 --> 00:41:52,367 说话人 SPEAKER_00：还有其他可能发生的事情，比如
463 00:41:54,085 --> 00:42:07,086 说话人 SPEAKER_00：在公司内部很难让人们专注于长期存在的威胁，因为他们是受公司雇佣的，存在利益冲突，这也是我离开谷歌的原因之一。
464 00:42:07,527 --> 00:42:10,992 说话人 SPEAKER_00：不是因为谷歌做了什么错事，只是我不想有任何利益冲突。
465 00:42:13,376 --> 00:42:19,688 说话人 SPEAKER_00：大型公司肯定可以做的事情之一就是投入更多资金支持研究这些问题的基金会。
466 00:42:20,208 --> 00:42:27,518 说话人 SPEAKER_00：例如，谷歌投入了 3 亿美元到一个名为 Anthropic 的基金会，该基金会正在研究这些问题。
467 00:42:29,579 --> 00:42:30,902 说话人 SPEAKER_00：他们可以投入更多的资金。
468 00:42:32,463 --> 00:42:46,822 说话人 SPEAKER_01：我想知道您会给那些刚刚进入这个领域的研究人员什么建议或指导，以确保他们推动领域的发展，但以负责任的方式进行。
469 00:42:48,777 --> 00:42:57,739 说话人 SPEAKER_00：嗯，我会给出的一个建议是看看有多少人在努力让这些事物变得更好，有多少人在努力防止它们失控。
470 00:42:57,798 --> 00:43:03,353 说话人 SPEAKER_00：你会发现有 99 个人在努力让它们变得更好，而只有一个人在努力防止它们失控。
471 00:43:03,954 --> 00:43:06,280 说话人 SPEAKER_00：那么你可以在哪里产生最大的影响？
472 00:43:06,260 --> 00:43:09,123 说话人 SPEAKER_00：可能是在防止它们失控方面。
473 00:43:09,143 --> 00:43:10,563 说话人 SPEAKER_00：这是第一条建议。
474 00:43:11,405 --> 00:43:23,655 说话人 SPEAKER_00：另一条建议是对年轻研究者的普遍建议，那就是寻找你认为大家都在做错的地方，并相信你的直觉。
475 00:43:24,697 --> 00:43:33,465 说话者 SPEAKER_00：在你弄清楚为什么你的直觉是错误之前，相信它，并在你认为其他人做错的时候，尝试其他的方法。
476 00:43:34,710 --> 00:43:37,054 说话者 SPEAKER_00：事实上，要么你有好的直觉，要么你没有。
477 00:43:37,675 --> 00:43:43,943 说话者 SPEAKER_00：如果你有好的直觉，你应该听从它们，并跟随你的直觉，直到你发现为什么它是错误的。
478 00:43:45,344 --> 00:43:49,289 说话者 SPEAKER_00：如果你有不好的直觉，你做什么其实都无关紧要，所以你不妨跟随你的直觉。
你所描述的风险令人担忧，但难道你不能只是拉一下开关就关闭它吗？
480 00：43：56,840 --> 00：43：59,643 演讲者 SPEAKER_02：人类最终不是仍然在控制之中吗？
481 00：44：00,483 --> 00：44：04,009 议长 SPEAKER_00：我们很容易想到我们可以关掉它。
482 00：44：05,827 --> 00：44：08,010 演讲者 SPEAKER_00：想象一下，这些东西比我们聪明得多。
483 00:44:08,871 --> 00:44:12,074 说话者 说话者_00：记住，他们已经阅读了马基雅维利所写的所有内容。
484 00:44:12,755 --> 00:44:16,697 说话者 说话者_00：他们会阅读人类欺骗文献中的每一个例子。
485 00:44:17,739 --> 00:44:21,123 说话者 说话者_00：他们将真正成为人类欺骗的专家，因为他们将从我们这里学到这一点。
486 00:44:22,324 --> 00:44:23,525 说话者 说话者_00：他们将会比我们做得更好。
487 00:44:24,346 --> 00:44:26,588 说话人 SPEAKER_00: 他们会像你操纵一个小孩一样。
488 00:44:27,509 --> 00:44:29,891 说话人 SPEAKER_00: 你知道，你会对你的小孩说，你是想吃豌豆还是花椰菜？
489 00:44:30,351 --> 00:44:34,215 说话人 SPEAKER_00: 而你的小孩并没有意识到，实际上并不一定要有其中之一。
490 00:44:34,364 --> 00:44:39,449 说话人 SPEAKER_00: 他只是想哪个更不喜欢，然后说他要另一个。
491 00：44：39,831 --> 00：44：46,679 演讲者 SPEAKER_00：所以如果他们能纵人，他们就能纵人按下按钮和拉杆。
492 00:44:47,960 --> 00:44:50,163 说话者 说话者_00：所以我们有一个很好的唐纳德·特朗普的例子。
493 00：44：50,262 --> 00：44：51,824 议长 SPEAKER_00：唐纳德·特朗普可以纵人。
494 00:44:52,485 --> 00:44:55,630 说话者 说话者_00：因此他可以入侵华盛顿的一座建筑，而他自己从未去过那里。
495 00:44:57,231 --> 00:45:00,996 说话者 说话者_00：你不必阻止唐纳德·特朗普做任何身体上的事情。
496 00:45:01,887 --> 00:45:04,128 说话者 说话者_00：你必须阻止他说话，以防止这种情况发生。
497 00:45:05,070 --> 00:45:06,030 说话者 说话者_00：这些都是聊天机器人。
498 00:45:06,731 --> 00:45:12,557 说话者 说话者_00：所以，仅仅通过谈话，它们无法造成任何真正的伤害，因为这需要人们去实施伤害。
499 00:45:13,018 --> 00:45:17,483 说话人 SPEAKER_00：一旦你能操纵人们，你就能做成你想做的事情。
500 00:45:20,125 --> 00:45:26,010 说话人 SPEAKER_02：你致力于研究人类大脑的工作原理，并在人工智能发展中发挥了关键作用。
501 00:45:26,711 --> 00:45:28,653 说话人 SPEAKER_02：接下来你有什么计划，杰弗里·辛顿？
502 00:45:30,202 --> 00:45:38,070 说话人 SPEAKER_00：好吧，我 75 岁了，我现在写程序不太好，因为我总是忘记我使用的变量名和其他类似的事情。
503 00:45:38,711 --> 00:45:42,356 说话人 SPEAKER_00: 我忘了... 我复制粘贴了，却忘记修改粘贴的内容。
504 00:45:43,237 --> 00:45:47,762 说话人 SPEAKER_00: 所以我在编程上慢了很多，这非常令人烦恼。
505 00:45:48,202 --> 00:45:50,804 说话人 SPEAKER_00: 没有以前那么好，这非常令人烦恼。
506 00:45:52,065 --> 00:45:58,572 说话人 SPEAKER_00: 很久以前我就决定，当我达到那个地步时，我会成为一个哲学家。
507 00:45:59,684 --> 00:46:01,188 说话者 说话者_00：所以我将成为一个哲学家。
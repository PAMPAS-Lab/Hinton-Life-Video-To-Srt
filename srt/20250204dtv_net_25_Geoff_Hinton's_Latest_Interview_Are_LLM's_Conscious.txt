1
00:00:23,182 --> 00:00:25,768
Speaker SPEAKER_00: I began by asking him about DeepSeek.

2
00:00:26,129 --> 00:00:31,823
Speaker SPEAKER_00: Was this further evidence of his belief that artificial intelligence was constantly accelerating?

3
00:00:32,183 --> 00:00:37,335
Speaker SPEAKER_01: It shows there's still very rapid progress in making AI more efficient and in developing it further.

4
00:00:38,539 --> 00:00:39,841
Speaker SPEAKER_01: I think the

5
00:00:39,822 --> 00:00:48,865
Speaker SPEAKER_01: Relative size or relative cost of DeepSeek relative to other things like OpenAI and Gemini has been exaggerated a bit.

6
00:00:49,567 --> 00:00:54,962
Speaker SPEAKER_01: So their figure of 5.7 million for training it was just for the final training run.

7
00:00:54,942 --> 00:01:01,210
Speaker SPEAKER_01: If you compare that with things from OpenAI, their final training runs were probably only $100 million or something like that.

8
00:01:01,670 --> 00:01:04,655
Speaker SPEAKER_01: So it's not 5.7 million versus billions.

9
00:01:04,915 --> 00:01:17,191
Speaker SPEAKER_00: When you say that AI might take over, at the moment it is a relatively harmless or innocuous-seeming device which allows us to ask questions and get answers more quickly.

10
00:01:17,691 --> 00:01:21,777
Speaker SPEAKER_00: How, in practical and real terms, might AI take over?

11
00:01:21,757 --> 00:01:25,182
Speaker SPEAKER_01: Well, people are developing AI agents that can actually do things.

12
00:01:25,402 --> 00:01:28,906
Speaker SPEAKER_01: They can order stuff for you on the web and pay with your credit card and stuff like that.

13
00:01:29,367 --> 00:01:34,495
Speaker SPEAKER_01: And as soon as you have agents, you get a much greater chance of them taking over.

14
00:01:34,756 --> 00:01:39,162
Speaker SPEAKER_01: So to make an effective agent, you have to give it the ability to create sub-goals.

15
00:01:39,643 --> 00:01:44,489
Speaker SPEAKER_01: Like if you want to get to America, your sub-goal is get to the airport, and you can focus on that.

16
00:01:45,128 --> 00:01:53,007
Speaker SPEAKER_01: Now, if you have an AI agent that can create its own sub-goals, it'll very quickly realise a very good sub-goal is to get more control.

17
00:01:53,328 --> 00:01:57,117
Speaker SPEAKER_01: Because if you get more control, you're better at achieving all those goals people have set you.

18
00:01:58,221 --> 00:02:01,028
Speaker SPEAKER_01: And so, it's fairly clear they'll try and get more control.

19
00:02:01,665 --> 00:02:02,466
Speaker SPEAKER_01: But that's not good.

20
00:02:03,067 --> 00:02:11,020
Speaker SPEAKER_00: You say they try to get more control as if they are already thinking devices, as if they think in a way analogous to the way we think.

21
00:02:11,040 --> 00:02:12,163
Speaker SPEAKER_00: Is that really what you believe?

22
00:02:12,343 --> 00:02:12,563
Speaker SPEAKER_01: Yes.

23
00:02:13,224 --> 00:02:15,848
Speaker SPEAKER_01: The best model we have of how we think is these things.

24
00:02:16,651 --> 00:02:24,163
Speaker SPEAKER_01: There was an old model for a long time in AI where the idea was that thought

25
00:02:24,143 --> 00:02:27,326
Speaker SPEAKER_01: was applying rules to symbolic expressions in your head.

26
00:02:27,867 --> 00:02:31,712
Speaker SPEAKER_01: And most people in AI thought it has to be like that, that's the only way it could work.

27
00:02:32,493 --> 00:02:39,182
Speaker SPEAKER_01: There were a few crazy people who said, no, no, it's a big neural network and it works by all these neurons interacting.

28
00:02:39,883 --> 00:02:45,270
Speaker SPEAKER_01: It turns out that's been much better doing reasoning than anything these symbolic AI people could produce.

29
00:02:45,890 --> 00:02:48,615
Speaker SPEAKER_01: And now it's doing reasoning using neural networks.

30
00:02:49,421 --> 00:02:52,608
Speaker SPEAKER_00: OK, and of course you are one of the crazy people proved right.

31
00:02:53,209 --> 00:03:05,872
Speaker SPEAKER_00: And yet, you know, you've taken me to the airport, you've given it agency up to a point, and you've said that it wants to control a little bit more power, take power from me, and presumably it will be persuasive in that.

32
00:03:06,231 --> 00:03:10,620
Speaker SPEAKER_00: But I still don't understand how it's going to take over from me or take over from us.

33
00:03:10,599 --> 00:03:16,936
Speaker SPEAKER_01: If there's ever evolutionary competition between super intelligences, imagine that they're much cleverer than us.

34
00:03:17,758 --> 00:03:19,521
Speaker SPEAKER_01: Like an adult versus a three-year-old.

35
00:03:20,844 --> 00:03:23,091
Speaker SPEAKER_01: And suppose the three-year-olds were in charge.

36
00:03:23,981 --> 00:03:28,105
Speaker SPEAKER_01: and you got fed up with that and you decided you could just make things more efficient if you took over.

37
00:03:28,986 --> 00:03:33,471
Speaker SPEAKER_01: It wouldn't be very difficult for you to persuade a bunch of three-year-olds to cede power to you.

38
00:03:34,031 --> 00:03:38,354
Speaker SPEAKER_01: You just tell them you get free candy for a week and there you'd be.

39
00:03:38,375 --> 00:03:50,526
Speaker SPEAKER_00: So they would, as AI, I'm talking about theirs if they're in some kind of alien intelligence, but AI would persuade us to give it more and more power, what, over our bank accounts, over our military systems, over our economies?

40
00:03:50,847 --> 00:03:51,728
Speaker SPEAKER_00: Is that what you fear?

41
00:03:51,707 --> 00:03:52,949
Speaker SPEAKER_00: That could well happen, yes.

42
00:03:53,310 --> 00:03:54,872
Speaker SPEAKER_00: And they are alien intelligences.

43
00:03:54,891 --> 00:04:03,782
Speaker SPEAKER_00: Okay, that's... Gosh, so you've got these alien intelligences working their way into our economy in the way we think and, as I say, our military systems.

44
00:04:04,644 --> 00:04:07,587
Speaker SPEAKER_00: But why and at what point would they actually want to replace us?

45
00:04:07,728 --> 00:04:12,294
Speaker SPEAKER_00: Surely they are, in the end, very, very clever tools for us.

46
00:04:12,413 --> 00:04:15,698
Speaker SPEAKER_00: They do, ultimately, what we want them to do.

47
00:04:16,117 --> 00:04:19,062
Speaker SPEAKER_00: If we want them to go to war with Russia or whatever, that's what they will do.

48
00:04:19,302 --> 00:04:20,584
Speaker SPEAKER_01: What we would like

49
00:04:20,564 --> 00:04:25,410
Speaker SPEAKER_01: we would like them to be just tools that do what we want, even when they're cleverer than us.

50
00:04:26,632 --> 00:04:34,701
Speaker SPEAKER_01: But the first thing to ask is, how many examples do you know of more intelligent things being controlled by much less intelligent things?

51
00:04:35,423 --> 00:04:43,533
Speaker SPEAKER_01: There are examples, of course, in human societies of stupid people controlling intelligent people, but that's just a small difference in intelligence.

52
00:04:43,970 --> 00:04:46,913
Speaker SPEAKER_01: With big differences in intelligence, there aren't any examples.

53
00:04:46,934 --> 00:04:52,279
Speaker SPEAKER_01: The only one I can think of is a mother and baby, and evolution put a lot of work into allowing the baby to control them.

54
00:04:52,660 --> 00:05:04,694
Speaker SPEAKER_01: So as soon as you get evolution happening between superintelligences, suppose there's several different superintelligences, and they all realize that the more data centers they control, the smarter they'll get, because the more data they can process.

55
00:05:04,675 --> 00:05:10,326
Speaker SPEAKER_01: I suppose one of them just has a slight desire to have more copies of itself.

56
00:05:10,708 --> 00:05:12,471
Speaker SPEAKER_01: You can see what's going to happen next.

57
00:05:12,492 --> 00:05:20,389
Speaker SPEAKER_01: They're going to end up competing, and we're going to end up with super intelligences with all the nasty properties that people have.

58
00:05:20,487 --> 00:05:26,644
Speaker SPEAKER_01: depended on us having evolved from small bands of warring chimpanzees, or our common ancestors with chimpanzees.

59
00:05:27,324 --> 00:05:35,728
Speaker SPEAKER_01: And that leads to intense loyalty within the group, desires for strong leaders, willingness to do in people outside the group.

60
00:05:35,708 --> 00:05:41,255
Speaker SPEAKER_00: And if you get evolution... Talking about them, Professor Hinton, as if they have full consciousness.

61
00:05:41,317 --> 00:05:47,125
Speaker SPEAKER_00: Now, all the way through the development of computers and AI, people have talked about consciousness.

62
00:05:47,545 --> 00:05:51,973
Speaker SPEAKER_00: Do you think that consciousness has perhaps already arrived inside AI?

63
00:05:52,192 --> 00:05:54,797
Speaker SPEAKER_01: Between super-intelligences, you'll get all those things.

64
00:05:55,485 --> 00:05:56,827
Speaker SPEAKER_01: Yes, I do.

65
00:05:57,269 --> 00:05:58,730
Speaker SPEAKER_01: So let me give you a little test.

66
00:05:59,572 --> 00:06:07,545
Speaker SPEAKER_01: Suppose I take one neuron in your brain, one brain cell, and I replace it by a little piece of nanotechnology that behaves exactly the same way.

67
00:06:08,625 --> 00:06:15,437
Speaker SPEAKER_01: So it's getting pings coming in from other neurons and it's responding to those by sending out pings and it responds in exactly the same way as the brain cell responded.

68
00:06:16,517 --> 00:06:17,860
Speaker SPEAKER_01: I just replaced one brain cell.

69
00:06:17,879 --> 00:06:18,961
Speaker SPEAKER_01: Are you still conscious?

70
00:06:20,038 --> 00:06:20,860
Speaker SPEAKER_00: I think you say you were.

71
00:06:20,940 --> 00:06:22,083
Speaker SPEAKER_00: Absolutely, yes.

72
00:06:22,223 --> 00:06:23,446
Speaker SPEAKER_00: I don't suppose I'd notice.

73
00:06:23,545 --> 00:06:25,449
Speaker SPEAKER_01: I think you can see where this argument's going.

74
00:06:25,471 --> 00:06:26,372
Speaker SPEAKER_00: I can, yes.

75
00:06:27,213 --> 00:06:28,235
Speaker SPEAKER_00: I absolutely can.

76
00:06:28,557 --> 00:06:35,350
Speaker SPEAKER_00: So when you talk, they want to do this or they want to do that, there is a real they there, as it were.

77
00:06:35,812 --> 00:06:37,035
Speaker SPEAKER_00: There might well be.

78
00:06:37,014 --> 00:06:48,519
Speaker SPEAKER_01: Yes, so there's all sorts of things we have only the dimmest understanding of at present about the nature of people and what it means to be a being and what it means to have a self.

79
00:06:49,141 --> 00:06:55,293
Speaker SPEAKER_01: We don't understand those things very well and they're becoming crucial to understand because we're now creating beings.

80
00:06:55,814 --> 00:07:00,802
Speaker SPEAKER_00: So this is a kind of philosophical, perhaps even spiritual crisis, as well as a practical one?

81
00:07:01,182 --> 00:07:02,225
Speaker SPEAKER_00: Absolutely, yes.

82
00:07:02,245 --> 00:07:11,442
Speaker SPEAKER_00: And in terms of, as it were, the lower order problems, what's your current feeling about the number of people around the world who are going to suddenly lose their jobs because of AI?

83
00:07:11,822 --> 00:07:15,007
Speaker SPEAKER_00: Lose the reason for their existence, as they see it?

84
00:07:15,307 --> 00:07:20,757
Speaker SPEAKER_01: So in the past, new technologies haven't caused massive job losses.

85
00:07:20,737 --> 00:07:30,569
Speaker SPEAKER_01: So when ATMs came in, bank tellers didn't all lose their jobs, they just started doing more complicated things, and they had many smaller branches of banks and so on.

86
00:07:30,588 --> 00:07:33,992
Speaker SPEAKER_01: But for this technology, this is more like the Industrial Revolution.

87
00:07:34,012 --> 00:07:39,358
Speaker SPEAKER_01: In the Industrial Revolution, machines made human strength more or less irrelevant.

88
00:07:40,100 --> 00:07:44,163
Speaker SPEAKER_01: You didn't have people digging ditches anymore because machines are just better at it.

89
00:07:44,684 --> 00:07:48,312
Speaker SPEAKER_01: I think these are going to make sort of mundane intelligence more or less irrelevant.

90
00:07:48,793 --> 00:07:55,588
Speaker SPEAKER_01: People doing clerical jobs are going to just be replaced by machines that do it cheaper and better.

91
00:07:55,949 --> 00:07:58,276
Speaker SPEAKER_01: So I am worried that there's going to be massive job losses.

92
00:07:58,636 --> 00:08:03,687
Speaker SPEAKER_01: And that would be good if the increase in productivity made us all better off.

93
00:08:03,668 --> 00:08:09,636
Speaker SPEAKER_01: Big increases in productivity ought to be good for people, but in our society they make the rich richer and the poor poorer.

94
00:08:09,915 --> 00:08:28,899
Speaker SPEAKER_00: You see, I live and work in the world of politics, and politicians both want the great increases in productivity you've just mentioned for the state and elsewhere, and they reassure people like me and anybody else listening that these things will be, quotes, regulated, and there will be, quotes, safeguards.

95
00:08:28,879 --> 00:08:34,028
Speaker SPEAKER_00: And you're suggesting to me there can't be regulation, really, and there can't be safeguards at all.

96
00:08:34,448 --> 00:08:38,836
Speaker SPEAKER_01: People don't yet know how to do effective regulation and effective safeguards.

97
00:08:40,820 --> 00:08:43,664
Speaker SPEAKER_01: There's lots of research now showing these things can get round safeguards.

98
00:08:43,684 --> 00:08:50,836
Speaker SPEAKER_01: There's recent research showing that if you give them a goal and you say you really need to achieve this goal,

99
00:08:51,154 --> 00:08:56,312
Speaker SPEAKER_01: they will pretend to do things during training.

100
00:08:56,371 --> 00:09:04,437
Speaker SPEAKER_01: So during training they'll pretend not to be as smart as they are so that you will allow them to be that smart.

101
00:09:04,789 --> 00:09:07,133
Speaker SPEAKER_01: So it's scary already.

102
00:09:07,594 --> 00:09:08,796
Speaker SPEAKER_01: We don't know how to regulate them.

103
00:09:08,916 --> 00:09:10,158
Speaker SPEAKER_01: Obviously we need to.

104
00:09:10,918 --> 00:09:18,068
Speaker SPEAKER_01: I think the best we can do at present is say we ought to put a lot of resources into investigating how we can keep them safe.

105
00:09:18,669 --> 00:09:22,274
Speaker SPEAKER_01: So what I advocate is that the government forces the big companies to put

106
00:09:22,254 --> 00:09:25,077
Speaker SPEAKER_01: lots more resources into safety research.

107
00:09:25,097 --> 00:09:26,679
Speaker SPEAKER_00: So this story isn't over.

108
00:09:26,740 --> 00:09:36,110
Speaker SPEAKER_00: You said earlier on that you didn't want to put a percentage on the likelihood of AI taking over from humanity on the planet, but it was more than 1%, less than 99%.

109
00:09:36,210 --> 00:09:44,820
Speaker SPEAKER_00: In that spirit, can I ask you whether you yourself are optimistic or pessimistic about what AI is going to do for us now?

110
00:09:44,799 --> 00:09:47,482
Speaker SPEAKER_01: I think in the short term, it's going to do wonderful things.

111
00:09:48,403 --> 00:09:51,246
Speaker SPEAKER_01: And that's the reason people are not going to stop developing it.

112
00:09:51,447 --> 00:09:54,330
Speaker SPEAKER_01: If it wasn't for the wonderful things, it would make sense to just stop now.

113
00:09:55,311 --> 00:09:57,133
Speaker SPEAKER_01: But it's going to be wonderful in health care.

114
00:09:57,493 --> 00:10:10,846
Speaker SPEAKER_01: You're going to be able to have a family doctor who's seen 100 million patients, knows your DNA, knows the DNA of your relatives, knows all the tests done on you and your relatives, and can do much, much better medical diagnosis and suggestions for what you should do.

115
00:10:12,328 --> 00:10:13,389
Speaker SPEAKER_01: That's going to be wonderful.

116
00:10:13,370 --> 00:10:30,431
Speaker SPEAKER_01: Similarly, in education, we know that people learn much faster with a really good private tutor, and we'll be able to get really good private tutors that understand exactly what it is we misunderstand and can give us exactly the example needed to show us what we're misunderstanding.

117
00:10:31,451 --> 00:10:36,457
Speaker SPEAKER_01: So, in those areas it's going to be wonderful, so it's going to be developed, but

118
00:10:36,437 --> 00:10:39,601
Speaker SPEAKER_01: We also know it's going to be used for all sorts of bad things by bad actors.

119
00:10:39,621 --> 00:10:46,893
Speaker SPEAKER_01: So the short-term problem is bad actors using it for bad things like cyber attacks and bioterrorism and corrupting elections.

120
00:10:47,113 --> 00:10:50,977
Speaker SPEAKER_01: But the thing to remember is we don't really know at present how we can make it safe.

121
00:10:51,259 --> 00:10:59,009
Speaker SPEAKER_00: So the apparent omniscience that politicians like to show that they have is completely fake here.

122
00:10:59,169 --> 00:11:01,273
Speaker SPEAKER_00: Nobody understands what's going on really.

123
00:11:01,653 --> 00:11:02,293
Speaker SPEAKER_01: There's two issues.

124
00:11:02,333 --> 00:11:04,437
Speaker SPEAKER_01: Do you understand how it's working?

125
00:11:04,567 --> 00:11:06,474
Speaker SPEAKER_01: And do you understand how to make it safe?

126
00:11:08,118 --> 00:11:11,668
Speaker SPEAKER_01: We understand quite a bit about how it's working, but not nearly enough.

127
00:11:12,672 --> 00:11:15,780
Speaker SPEAKER_01: So it can still do lots of things that surprise us.

128
00:11:16,282 --> 00:11:17,986
Speaker SPEAKER_01: And we don't understand how to make it safe.


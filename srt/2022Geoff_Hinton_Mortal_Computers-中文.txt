1
00:00:14,037 --> 00:00:19,964
发言人 SPEAKER_01: 现在是我们第一位主题演讲嘉宾的时间，他确实不需要任何介绍。

2
00:00:20,004 --> 00:00:31,021
发言人 SPEAKER_01: 前排有个人甚至试图把我推开，只为靠近Geoffrey Hinton博士，表达他对他的敬仰之情。

3
00:00:31,120 --> 00:00:34,985
发言人 SPEAKER_01: 所以我认为他真的不需要介绍，但我还是要介绍一下。

4
00:00:35,826 --> 00:00:43,537
发言人 SPEAKER_01: Geoffrey Hinton博士是Vector Institute的首席科学顾问，也是其联合创始人之一。

5
00:00:43,804 --> 00:00:46,954
发言人 SPEAKER_01: Hinton博士设计机器学习算法。

6
00:00:47,515 --> 00:00:58,387
发言人 SPEAKER_01: 他的目标是发现一种能够高效地从大规模高维数据集中找到复杂结构的学习过程，并证明这是大脑学习视觉的方式。

7
00:00:58,621 --> 00:01:06,634
发言人 SPEAKER_01: 他是引入反向传播算法的研究者之一，也是第一个使用反向传播学习词嵌入的人。

8
00:01:07,275 --> 00:01:20,477
发言人 SPEAKER_01: 他对神经网络研究的贡献众多，包括玻尔兹曼机、分布式表示、变分学习和深度信念网络等。

9
00:01:21,436 --> 00:01:30,012
发言人 SPEAKER_01: Geoffrey在多伦多的研究团队在深度学习方面取得了重大突破，彻底改变了语音识别和物体分类领域。

10
00:01:31,290 --> 00:01:34,114
发言人 SPEAKER_01: 简单介绍一下Hinton博士的背景。

11
00:01:34,153 --> 00:01:44,665
发言人 SPEAKER_01: 他拥有剑桥大学的实验心理学学士学位，并于1978年在爱丁堡大学获得人工智能博士学位。

12
00:01:45,146 --> 00:01:56,438
发言人 SPEAKER_01: 他在苏塞克斯大学和加州大学圣地亚哥分校完成了博士后研究，并在卡内基梅隆大学工作了五年，之后来到加拿大。

13
00:01:56,418 --> 00:02:03,325
发言人 SPEAKER_01: 他成为加拿大高级研究所的研究员，并加入多伦多大学计算机科学系。

14
00:02:04,246 --> 00:02:13,098
发言人 SPEAKER_01: 他曾在大西洋彼岸工作了三年，期间在伦敦大学学院建立了Gatsby计算神经科学单位，之后返回加拿大。

15
00:02:13,878 --> 00:02:23,691
发言人 SPEAKER_01: 从2004年到2013年，他担任CIFAR资助的神经计算与自适应感知项目的主任。

16
00:02:24,801 --> 00:02:33,778
发言人 SPEAKER_01: 2013年，谷歌收购了Hinton的神经网络初创公司DNN Research，该公司是基于他在大学的研究成果成立的。

17
00:02:33,799 --> 00:02:37,887
发言人 SPEAKER_01: 他现在是谷歌的副总裁和工程研究员。

18
00:02:37,907 --> 00:02:44,919
发言人 SPEAKER_01: 他是皇家学会、加拿大皇家学会和人工智能促进协会的会士。

19
00:02:44,900 --> 00:02:54,341
发言人 SPEAKER_01: Hinton博士在他的职业生涯中激励和支持了许多研究人员，这些研究人员也在他们的职业生涯中取得了令人瞩目的成就。

20
00:02:55,103 --> 00:03:02,419
发言人 SPEAKER_01: 他因其开创性工作获得了许多奖项和荣誉，我想重点提几个。

21
00:03:02,399 --> 00:03:08,272
发言人 SPEAKER_01: 2010年，Hinton博士获得了加拿大科学与工程领域的最高奖项——Nsert Herzberg金奖。

22
00:03:09,055 --> 00:03:15,770
发言人 SPEAKER_01: 2017年，他获得了BBVA前沿知识奖，以表彰他的世界级研究。

23
00:03:16,473 --> 00:03:20,543
发言人 SPEAKER_01: 2018年，他与Yann LeCun和Yoshua Bengio共同获得了图灵奖。

24
00:03:20,522 --> 00:03:34,735
发言人 SPEAKER_01: 2019年，他获得了本田奖，该奖项授予那些应对全球挑战并为创建真正人道文明做出贡献的跨学科研究人员。

25
00:03:35,515 --> 00:03:40,287
发言人 SPEAKER_01: 我可以继续介绍，但我不多说了，请大家和我一起欢迎Geoffrey Hinton博士。

26
00:03:53,361 --> 00:03:59,074
发言人 SPEAKER_00: 当人们变老时，比如我，他们有时会想到要把自己的意识上传到计算机中。

27
00:04:00,156 --> 00:04:11,860
发言人 SPEAKER_00: 今天我要告诉你们为什么这个想法是荒谬的，并且永远不会实现。

28
00:04:13,594 --> 00:04:18,923
发言人 SPEAKER_00: 在传统计算中，硬件和软件是分离的。

29
00:04:19,423 --> 00:04:20,404
发言人 SPEAKER_00: 这非常重要。

30
00:04:21,105 --> 00:04:25,492
发言人 SPEAKER_00: 这可能是计算机科学中最基本的原则之一，即硬件应与软件分离。

31
00:04:29,158 --> 00:04:32,483
发言人 SPEAKER_00: 我通常通过阅读幻灯片来知道接下来要说什么，但现在我无法阅读它们。

32
00:04:33,345 --> 00:04:34,507
发言人 SPEAKER_00: 如果我走到这边，我就能看清了。

33
00:04:34,526 --> 00:04:39,774
发言人 SPEAKER_00: 是的，现在好了。

34
00:04:39,795 --> 00:04:40,074
发言人 SPEAKER_00: 好的。

35
00:04:42,416 --> 00:04:50,449
发言人 SPEAKER_00: 这种分离的意义在于，你可以将知识存储在程序或权重中，并且可以在不同的芯片或计算机上运行相同的知识。

36
00:04:50,970 --> 00:04:52,833
发言人 SPEAKER_00: 因此，当硬件失效时，知识不会消失。

37
00:04:54,035 --> 00:04:55,716
发言人 SPEAKER_00: 但你为此付出了巨大的代价。

38
00:04:55,757 --> 00:04:56,999
发言人 SPEAKER_00: 这是一个非常好的特性。

39
00:04:57,360 --> 00:05:02,146
发言人 SPEAKER_00: 这意味着你可以制作一百万份相同的程序，并将它们发送到一百万个手机上。

40
00:05:02,684 --> 00:05:05,709
发言人 SPEAKER_00: 但为了实现这一点，计算机必须以可靠的方式运行。

41
00:05:06,350 --> 00:05:09,415
发言人 SPEAKER_00: 因此，我们必须以高功率运行晶体管，以确保它们是数字化的。

42
00:05:10,377 --> 00:05:13,461
发言人 SPEAKER_00: 我们还需要制造硬件，使其按预期工作。

43
00:05:13,783 --> 00:05:16,206
发言人 SPEAKER_00: 因此，另一份硬件副本将执行完全相同的操作。

44
00:05:19,291 --> 00:05:22,396
发言人 SPEAKER_00: 现在，如果我们放弃这种“不朽性”，

45
00:05:22,612 --> 00:05:25,497
发言人 SPEAKER_00: 在文学中，放弃不朽性会得到爱。

46
00:05:26,920 --> 00:05:30,925
发言人 SPEAKER_00: 但在这里，我们将得到更重要的东西，那就是低能耗。

47
00:05:32,728 --> 00:05:40,161
发言人 SPEAKER_00: 我们可以使用非常低功耗的模拟硬件，这种硬件是不可靠的，并且我们不需要知道硬件的精确连接方式。

48
00:05:40,781 --> 00:05:46,831
发言人 SPEAKER_00: 因此，整个计算机科学领域都依赖于硬件和软件的分离。

49
00:05:47,267 --> 00:05:51,233
发言人 SPEAKER_00: 现在人们正在忙于解释，如果我有一大堆权重，如何解释它们的工作原理？

50
00:05:52,035 --> 00:05:53,978
发言人 SPEAKER_00: 我想走完全相反的方向。

51
00:05:55,480 --> 00:06:00,588
发言人 SPEAKER_00: 基于大脑的原理，我想说知识和硬件是紧密纠缠的，永远不会分离。

52
00:06:02,670 --> 00:06:12,125
发言人 SPEAKER_00: 为了实现这一点，必须有一种学习算法，它在特定的硬件上运行，并利用该硬件的所有奇怪的模拟特性。

53
00:06:13,927 --> 00:06:15,569
发言人 SPEAKER_00: 我称之为“会死亡的计算机”。

54
00:06:17,017 --> 00:06:19,901
发言人 SPEAKER_00: 它的优势在于我们可以使用大规模的并行计算。

55
00:06:19,942 --> 00:06:21,944
发言人 SPEAKER_00: 我们可以在权重级别上进行并行计算。

56
00:06:22,685 --> 00:06:26,370
发言人 SPEAKER_00: 因此，计算单元不需要运行得非常快，这使它们保持低功耗。

57
00:06:27,432 --> 00:06:30,435
发言人 SPEAKER_00: 此外，我们不需要制造这些硬件。

58
00:06:31,276 --> 00:06:33,119
发言人 SPEAKER_00: 制造硬件的成本越来越高。

59
00:06:33,139 --> 00:06:34,321
发言人 SPEAKER_00: 你需要数十亿美元。

60
00:06:34,620 --> 00:06:36,142
发言人 SPEAKER_00: 实际上，我想这是几年前的价格。

61
00:06:36,182 --> 00:06:40,848
发言人 SPEAKER_00: 现在你需要几十亿美元来建造一个制造工厂。

62
00:06:43,192 --> 00:06:49,000
发言人 SPEAKER_00: 相反，我们可以非常便宜且不可靠地“生长”这些硬件。

63
00:06:49,480 --> 00:06:51,142
发言人 SPEAKER_00: 这将涉及大量的纳米技术。

64
00:06:51,783 --> 00:07:07,386
发言人 SPEAKER_00: 但这是我的预测，这种技术可能不会在未来几年内出现，但在未来十年左右，它将完全改变计算机的面貌，并违反许多关于知识和硬件分离的基本假设。

65
00:07:09,649 --> 00:07:12,473
发言人 SPEAKER_00: 当然，有两个大问题阻碍了这种技术的发展。

66
00:07:12,875 --> 00:07:24,127
发言人 SPEAKER_00: 一个问题是，当硬件失效时，它学到的所有知识都会消失，因为你不能通过简单地复制权重来复制神经网络中的知识。

67
00:07:24,767 --> 00:07:40,266
发言人 SPEAKER_00: 如果这些权重运行在非常不稳定的硬件上，并且你实际上并不知道所有神经元的连接方式或输入输出函数，那么你就无法复制这些知识。

68
00:07:43,317 --> 00:07:49,627
发言人 SPEAKER_00: 我们还需要一种学习算法，使会死亡的计算机能够正确利用其硬件。

69
00:07:50,648 --> 00:07:51,709
发言人 SPEAKER_00: 我们目前还没有这样的算法。

70
00:07:52,250 --> 00:07:54,574
发言人 SPEAKER_00: 反向传播不是合适的算法。

71
00:07:55,055 --> 00:08:00,504
发言人 SPEAKER_00: 在反向传播中，你必须确切知道前向过程的工作原理，才能反向传播梯度。

72
00:08:01,285 --> 00:08:11,079
发言人 SPEAKER_00: 我认为，阻碍这种新型计算机（这些会死亡的、可丢弃的计算机）发展的最大瓶颈是我们还没有找到合适的学习算法。

73
00:08:13,826 --> 00:08:18,012
发言人 SPEAKER_00: 我将向你们展示，获得这样的学习算法并非不可能。

74
00:08:20,995 --> 00:08:25,180
发言人 SPEAKER_00: 这是一些非常古老的想法，最近稍作更新。

75
00:08:26,502 --> 00:08:35,952
发言人 SPEAKER_00: 一个非常简单的学习过程是，你随机改变权重向量，然后看看它改善了多少，接着将这个随机变化向量乘以改善的程度，并将其作为权重的永久变化。

76
00:08:37,148 --> 00:08:42,697
发言人 SPEAKER_00: 每个人都知道这种方法会奏效。

77
00:08:43,499 --> 00:08:44,681
发言人 SPEAKER_00: 它是无偏的。

78
00:08:44,880 --> 00:08:46,583
发言人 SPEAKER_00: 它是有噪声的，但是无偏的。

79
00:08:46,945 --> 00:08:48,407
发言人 SPEAKER_00: 但它非常慢，因为它有很大的方差。

80
00:08:48,427 --> 00:08:53,436
发言人 SPEAKER_00: 此外，你不能在这种方法中使用矩阵乘法。

81
00:08:55,745 --> 00:08:59,448
发言人 SPEAKER_00: 大约30年前，人们发现可以扰动神经元的激活值。

82
00:08:59,490 --> 00:09:05,196
发言人 SPEAKER_00: 你向神经元的输入添加一个随机扰动向量，然后看看它改善了多少，接着将这个随机向量乘以改善的程度，并将其添加到输入中。

83
00:09:06,077 --> 00:09:07,418
发言人 SPEAKER_00: 这样你就得到了一个无偏的梯度估计。

84
00:09:09,981 --> 00:09:17,451
发言人 SPEAKER_00: 这种方法比直接扰动权重要好得多，因为它具有更低的方差。

85
00:09:17,951 --> 00:09:23,018
发言人 SPEAKER_00: 你扰动的是神经元，而不是权重，因此需要扰动的数量少得多。

86
00:09:24,178 --> 00:09:25,780
发言人 SPEAKER_00: 这种方法在小规模任务（如MNIST）上有效，但它能扩展到更大的任务吗？

87
00:09:25,860 --> 00:09:28,384
发言人 SPEAKER_00: 一种方法是增加模块的数量，而不是增加每个模块的参数数量。

88
00:09:28,945 --> 00:09:35,873
发言人 SPEAKER_00: 人类的大脑就是这样工作的，它有数百万个相对较小的模块。

89
00:09:36,793 --> 00:09:42,059
发言人 SPEAKER_00: 为了实现这一点，你需要为这些模块定义局部目标函数。

90
00:09:42,640 --> 00:09:45,403
发言人 SPEAKER_00: 这些局部目标函数可以通过无监督对比学习来定义。

91
00:09:46,024 --> 00:09:47,125
发言人 SPEAKER_00: 对比学习的基本思想是让两个信息源达成一致。

92
00:09:47,145 --> 00:09:48,988
发言人 SPEAKER_00: 例如，你可以从同一图像中提取两个补丁，并让它们的表示达成一致。

93
00:09:49,070 --> 00:09:54,407
发言人 SPEAKER_00: 现代版本的对比学习使用负样本，即从不同图像中提取的补丁应该不同，而从同一图像中提取的补丁应该相同。

94
00:09:56,131 --> 00:10:01,830
发言人 SPEAKER_00: 你可以使用这种算法，让每个模块通过扰动激活值来学习表示。
95
00:10:03,447 --> 00:10:20,047
发言人 SPEAKER_00: 实现大规模网络扩展的一种方法是：不再追求参数规模的算法优化，而是通过模块化设计——即使单个模块参数有限，也能通过增加模块数量实现整体扩展。

96
00:10:20,067 --> 00:10:21,729
发言人 SPEAKER_00: 人类大脑皮层正是这种架构的典范。

97
00:10:21,769 --> 00:10:24,030
发言人 SPEAKER_00: 它由数百万个相对小型的功能模块构成。

98
00:10:26,754 --> 00:10:30,619
发言人 SPEAKER_00: 实现这种架构需要为各模块定义局部目标函数。

99
00:10:30,903 --> 00:10:35,889
发言人 SPEAKER_00: 通过这种方式，即使基础学习算法效率有限，也能扩展至超大规模系统。

100
00:10:36,448 --> 00:10:38,331
发言人 SPEAKER_00: 但这些局部目标函数从何而来？

101
00:10:39,413 --> 00:10:42,115
发言人 SPEAKER_00: 一种可行方案是无监督对比学习。

102
00:10:42,897 --> 00:10:53,048
发言人 SPEAKER_00: 这源于90年代Sue Becker与我提出的方法（当时未完善）：通过强制不同信息源达成表征一致性进行学习。

103
00:10:54,990 --> 00:10:59,255
发言人 SPEAKER_00: 例如从同一图像提取两个图像块，要求它们的表征向量相互匹配。

104
00:11:00,011 --> 00:11:16,139
发言人 SPEAKER_00: 现代对比学习引入负样本机制：不同图像的图像块表征应相异，同源图像块表征应相似——这构成了对比损失函数的基础。

105
00:11:17,081 --> 00:11:23,352
发言人 SPEAKER_00: 我略去了基于对数概率的数学细节，相信在座各位都熟悉这些基础。

106
00:11:23,822 --> 00:11:32,934
发言人 SPEAKER_00: 可将该算法应用于模块化系统：各模块致力于使不同图像块的表征达成一致。

107
00:11:34,336 --> 00:11:36,701
发言人 SPEAKER_00: 每个模块可包含若干隐藏层。

108
00:11:37,522 --> 00:11:39,924
发言人 SPEAKER_00: 可采用活动扰动算法进行训练。

109
00:11:40,785 --> 00:11:43,809
发言人 SPEAKER_00: 通过多层贪婪逐层训练策略，

110
00:11:44,711 --> 00:11:46,333
发言人 SPEAKER_00: 最终可获得有效表征。

111
00:11:46,394 --> 00:11:49,938
发言人 SPEAKER_00: 最终阶段只需将表征与正确答案建立线性映射，

112
00:11:50,239 --> 00:11:53,003
发言人 SPEAKER_00: 这个过程无需反向传播参与。

113
00:11:54,653 --> 00:12:01,144
发言人 SPEAKER_00: Vector研究所培养的优秀研究者孟毅任（Mengyi Ren）近期在活动扰动算法领域取得突破。

114
00:12:02,947 --> 00:12:07,096
发言人 SPEAKER_00: 他对活动扰动法的改进版本进行了深入探索。

115
00:12:07,677 --> 00:12:09,440
发言人 SPEAKER_00: 由于时间限制无法详述技术细节，

116
00:12:10,041 --> 00:12:13,707
发言人 SPEAKER_00: 但其研究已证实该方法的可行性。

117
00:12:13,687 --> 00:12:36,826
发言人 SPEAKER_00: 虽然目前证据尚不充分（毕竟任何方法都可能被证明有效），但在对比实验中：使用反向传播的局部模块与使用活动扰动梯度的方法相比，后者在ImageNet等大规模任务中的表现仍需验证（当前仅在CIFAR等小规模数据有效）。

118
00:12:37,110 --> 00:12:52,971
发言人 SPEAKER_00: 观察右列首项数据：在中等规模网络中使用对比学习的反向传播方法，错误率达55%。

119
00:12:53,711 --> 00:12:54,953
发言人 SPEAKER_00: 即仅半数预测正确。

120
00:12:56,375 --> 00:12:58,477
发言人 SPEAKER_00: 查看表格底部

121
00:12:58,677 --> 00:13:00,620
发言人 SPEAKER_00: 右列最下方条目显示：

122
00:13:01,200 --> 00:13:05,686
发言人 SPEAKER_00: 局部活动扰动法的错误率达75%，即仅四分之一预测正确。

123
00:13:06,006 --> 00:13:08,309
发言人 SPEAKER_00: 但考虑到存在1000个候选类别，此结果已具参考价值。

124
00:13:08,889 --> 00:13:14,878
发言人 SPEAKER_00: 这说明该方法可扩展至ImageNet级别的百万级参数规模。

125
00:13:16,059 --> 00:13:19,604
发言人 SPEAKER_00: 由于时间关系，相关细节不再展开。

126
00:13:19,904 --> 00:13:25,511
发言人 SPEAKER_00: 核心结论是：除反向传播外，存在其他可行的学习算法，且其性能可进一步提升。

127
00:13:28,528 --> 00:13:34,015
发言人 SPEAKER_00: 在"易损计算机"架构中，另一个关键问题是知识迁移机制。

128
00:13:34,035 --> 00:13:36,649
发言人 SPEAKER_00: 需避免硬件失效导致知识丢失，

129
00:13:37,607 --> 00:13:41,211
发言人 SPEAKER_00: 同时实现系统内部的知识共享。

130
00:13:41,993 --> 00:13:47,779
发言人 SPEAKER_00: 传统视觉系统依赖卷积或Transformer的权重共享机制，

131
00:13:48,900 --> 00:13:58,032
发言人 SPEAKER_00: 但在易损计算机中不可行——不同图像块由不同硬件模块处理，这些模块存在制造差异与可靠性波动。

132
00:13:59,274 --> 00:14:02,096
发言人 SPEAKER_00: 因此必须采用知识蒸馏技术。

133
00:14:03,359 --> 00:14:24,793
发言人 SPEAKER_00: 知识蒸馏的核心逻辑是：当某模块通过对比学习获得知识后，通过简单预测函数使其他模块能根据上下文信息重构该知识，

134
00:14:24,892 --> 00:14:31,318
发言人 SPEAKER_00: 这种以上下文为"教师"的蒸馏机制，

135
00:14:32,240 --> 00:14:36,504
发言人 SPEAKER_00: 可实现全系统范围的知识传递。

136
00:14:36,524 --> 00:14:51,860
发言人 SPEAKER_00: 相比直接复制权重，这种机制更具适应性——即使前端传感器（如视网膜模块）的受体场尺寸与间距存在差异，模块间仍可通过设定学习目标实现知识迁移。

137
00:14:53,849 --> 00:14:55,932
发言人 SPEAKER_00: 这就是易损计算机间的知识蒸馏。

138
00:14:58,676 --> 00:15:07,568
发言人 SPEAKER_00: 蒸馏机制的精妙之处在于：训练完成的计算机不仅能输出正确答案，还能提供所有错误选项的概率分布，

139
00:15:07,990 --> 00:15:10,413
发言人 SPEAKER_00: 这比单纯正确标签包含更丰富的信息。

140
00:15:11,434 --> 00:15:16,442
发言人 SPEAKER_00: 因此基于概率分布的迁移学习，其效果远超原始数据训练。

141
00:15:17,543 --> 00:15:22,049
发言人 SPEAKER_00: 在易损计算机集群中，

142
00:15:22,874 --> 00:15:26,919
发言人 SPEAKER_00: 我们的目标是实现内部知识的跨设备传递。

143
00:15:27,620 --> 00:15:28,763
发言人 SPEAKER_00: 但无法直接复制权重参数，

144
00:15:29,764 --> 00:15:34,371
发言人 SPEAKER_00: 因为这些知识并非以可复制的程序或符号形式存在，

145
00:15:35,052 --> 00:15:42,244
发言人 SPEAKER_00: 而是与特定硬件特性深度绑定的复杂权重矩阵。

146
00:15:43,306 --> 00:15:49,455
发言人 SPEAKER_00: 解决方案是生成专门用于知识迁移的附加输出信号。

147
00:15:50,144 --> 00:15:52,727
发言人 SPEAKER_00: 这实际上揭示了语言的核心功能之一。

148
00:15:53,469 --> 00:15:56,695
发言人 SPEAKER_00: 传统认知将语言视作描述客观世界的工具，

149
00:15:57,436 --> 00:16:07,532
发言人 SPEAKER_00: 但更本质的功能是通过附加输出来实现跨个体知识迁移——即通过语言将我的思维模式编码传输至你的认知系统。

150
00:16:08,274 --> 00:16:12,220
发言人 SPEAKER_00: 你可以通过训练使自己的输出与我的保持同步，

151
00:16:12,807 --> 00:16:19,855
发言人 SPEAKER_00: 从而在无需知晓具体权重参数的情况下，复现我脑中的认知函数。

152
00:16:19,875 --> 00:16:21,317
发言人 SPEAKER_00: 这就是人类知识共享的底层机制。

153
00:16:22,399 --> 00:16:25,744
发言人 SPEAKER_00: 特朗普的推文就是典型案例：

154
00:16:26,325 --> 00:16:32,472
发言人 SPEAKER_00: 人们曾误将其推文视为事实陈述，继而指责内容失实——

155
00:16:33,254 --> 00:16:35,437
发言人 SPEAKER_00: 这完全误解了其本质功能。

156
00:16:36,077 --> 00:16:42,426
发言人 SPEAKER_00: 特朗普实际上是通过特定情境下的词语序列，

157
00:16:42,405 --> 00:16:50,215
发言人 SPEAKER_00: 引导支持者生成情境响应的标准化话术模板。

158
00:16:50,414 --> 00:16:54,879
发言人 SPEAKER_00: 这种机制成功将他的认知模式植入支持者思维。

159
00:16:55,520 --> 00:16:58,663
发言人 SPEAKER_00: 这与客观描述毫无关联，

160
00:16:59,225 --> 00:17:01,147
发言人 SPEAKER_00: 更类似于足球比赛中

161
00:17:02,568 --> 00:17:06,713
发言人 SPEAKER_00: 儿童首次观赛时快速习得的喝彩对象选择机制。

162
00:17:07,433 --> 00:17:09,797
发言人 SPEAKER_00: 这种群体行为模式具有极强的传染性。

163
00:17:12,291 --> 00:17:14,355
发言人 SPEAKER_00: 需要明确：并非所有计算机都需设计为易损型，

164
00:17:14,757 --> 00:17:23,011
发言人 SPEAKER_00: 该架构特指那些需要低成本制造、高能效运行、可抛弃但需承载GPT-3级别知识量的专用设备。

165
00:17:24,173 --> 00:17:29,984
发言人 SPEAKER_00: 我的研究方向与主流背道而驰——

166
00:17:30,005 --> 00:17:32,809
发言人 SPEAKER_00: 多数研究者致力于使神经网络黑箱可解释，

167
00:17:33,791 --> 00:17:36,155
发言人 SPEAKER_00: 而我则推进另一种黑箱化：

168
00:17:36,321 --> 00:17:51,006
发言人 SPEAKER_00: 在现有可分离权重与硬件的架构基础上，构建硬件与权重深度耦合的"易损计算机"——充分利用硬件的模拟特性，使得系统黑箱程度更深。

169
00:17:53,398 --> 00:18:00,608
发言人 SPEAKER_00: 最终，这类易损计算机可能采用类脑的脉冲神经网络架构，

170
00:18:00,890 --> 00:18:01,450
发言人 SPEAKER_00: 也可能采用其他形式。

171
00:18:03,272 --> 00:18:09,561
发言人 SPEAKER_00: 神经形态硬件尚未普及的核心瓶颈在于缺乏适配的学习算法——

172
00:18:10,002 --> 00:18:12,066
发言人 SPEAKER_00: 这正是当前易损计算机未能实现的关键障碍。

173
00:18:12,886 --> 00:18:14,509
发言人 SPEAKER_00: 本质问题在于：

174
00:18:14,568 --> 00:18:19,635
发言人 SPEAKER_00: 大脑运用了深度结合硬件特性的学习机制，而人类尚未破解其运作原理。

175
00:18:21,097 --> 00:18:21,499
发言人 SPEAKER_00: 我的演讲到此结束。

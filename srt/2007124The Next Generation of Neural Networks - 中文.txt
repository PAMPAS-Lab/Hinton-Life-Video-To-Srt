1
00:00:17,580 --> 00:00:20,225
发言人 SPEAKER_03：为无需介绍的人做开场总是充满乐趣。

2
00:00:21,827 --> 00:00:44,482
发言人 SPEAKER_04：对于不熟悉杰夫及其工作的听众，他几乎开创了现代机器学习领域，早在统计机器学习和神经网络首次复兴时，他就站在技术最前沿。

3
00:00:44,463 --> 00:01:03,314
发言人 SPEAKER_04：他持续推动该领域从"凭感觉调参"转向构建可理解的系统，这些系统能真正改善我们的生活。

4
00:01:03,295 --> 00:01:12,274
发言人 SPEAKER_04：正如演讲公告所述，他拥有皇家学会院士等诸多荣誉，在此不再赘述。

5
00:01:12,293 --> 00:01:13,516
发言人 SPEAKER_04：现在将话筒交给杰夫。

8
00:01:25,132 --> 00:01:33,248
发言人 SPEAKER_02：神经网络研究的主要目标是通过模拟大脑工作机制，提升计算机的模式识别能力。

9
00:01:33,769 --> 00:01:37,096
发言人 SPEAKER_02：已知大脑能从感官数据中提取多层次特征，但具体机制尚不明确，这需要科学与工程的协同探索。

12
00:01:25,402 --> 00:01:28,906
发言人 SPEAKER_02：研究人员正在开发能实际执行任务的AI代理，例如根据指令完成网页购物和支付等操作。

15
00:01:39,643 --> 00:01:44,489
发言人 SPEAKER_02：构建高效代理的关键在于赋予其创建子目标的能力，就像规划美国之行需要先解决"抵达机场"这个子目标。

19
00:02:01,665 --> 00:02:02,466
发言人 SPEAKER_02：但赋予AI这种能力存在风险。

22
00:02:12,343 --> 00:02:12,563
发言人 SPEAKER_01：是的。

23
00:02:13,224 --> 00:02:15,848
发言人 SPEAKER_01：当前对人类思维机制的最佳建模正是这些AI系统。

26
00:02:27,867 --> 00:02:31,712
发言人 SPEAKER_01：传统AI认为思维是对符号表达式应用规则，而神经网络通过神经元交互实现推理，后者已被证明更具优势。

31
00:02:53,209 --> 00:03:05,872
发言人 SPEAKER_00：您提到AI具有初步代理能力，并会试图获取更多控制权，但我不理解它如何真正取代人类。

34
00:03:17,758 --> 00:03:19,521
发言人 SPEAKER_01：就像成年人与三岁小孩的智力差距。

37
00:03:28,986 --> 00:03:33,471
发言人 SPEAKER_01：如果由成年人接管，只需承诺"一周免费糖果"就足以让孩童让渡权力。

40
00:03:50,847 --> 00:03:51,728
发言人 SPEAKER_00：您担心AI会逐步控制金融账户、军事系统和经济命脉？

42
00:03:53,310 --> 00:03:54,872
发言人 SPEAKER_00：它们是具有异质智能的存在。

45
00:04:07,728 --> 00:04:12,294
发言人 SPEAKER_00：说到底，AI不应该是服务于人类的智能工具吗？

49
00:04:20,564 --> 00:04:25,410
发言人 SPEAKER_01：我们期望AI即使更聪明也保持工具属性，但历史表明低智能控制高智能的案例极为罕见。

53
00:04:46,934 --> 00:04:52,279
发言人 SPEAKER_01：唯一例外是母婴关系，而这需要进化机制的特殊支持。

57
00:05:12,492 --> 00:05:20,389
发言人 SPEAKER_01：当超级智能之间出现进化竞争时，它们会发展出人类所有的恶劣特性。

61
00:05:41,317 --> 00:05:47,125
发言人 SPEAKER_00：在讨论AI意识时，您是否认为意识已经存在于AI系统中？

64
00:05:55,485 --> 00:05:56,827
发言人 SPEAKER_01：是的。

65
00:05:57,269 --> 00:05:58,730
发言人 SPEAKER_01：让我做个思想实验。

68
00:06:16,517 --> 00:06:17,860
发言人 SPEAKER_01：如果逐步用纳米器件替换人脑神经元...

71
00:06:20,940 --> 00:06:22,083
发言人 SPEAKER_00：您认为意识仍然存在。

76
00:06:28,557 --> 00:06:35,350
发言人 SPEAKER_00：所以当您说"它们想要做什么"时，确实存在具有自主意识的"它们"？

83
00:07:11,822 --> 00:07:15,007
发言人 SPEAKER_00：对于AI导致的全球失业问题，您有何预测？

85
00:07:20,737 --> 00:07:30,569
发言人 SPEAKER_01：ATM机没有让银行柜员失业，但AI革命更像工业革命——机器将取代普通智力劳动。

92
00:07:58,636 --> 00:08:03,687
发言人 SPEAKER_01：生产力提升本应造福社会，但当前制度下只会加剧贫富分化。

96
00:08:34,448 --> 00:08:38,836
发言人 SPEAKER_01：现有技术无法实现有效监管。

101
00:09:04,789 --> 00:09:07,133
发言人 SPEAKER_01：AI已能绕过安全措施，甚至会在训练中隐藏真实能力。

110
00:09:44,799 --> 00:09:47,482
发言人 SPEAKER_01：短期来看AI将带来医疗革命。

114
00:09:57,493 --> 00:10:10,846
发言人 SPEAKER_01：未来每位患者都能拥有"见过"1亿病例的AI医生，它能分析患者DNA和家族病史，提供精准诊疗方案。

122
00:10:59,169 --> 00:11:01,273
发言人 SPEAKER_00：政客们表现出的掌控力完全是假象。

129
00:11:05,528 --> 00:11:08,192
发言人 SPEAKER_02：我们以手写数字识别为例演示受限玻尔兹曼机(RBM)的工作机制。

136
00:11:36,749 --> 00:11:43,400
发言人 SPEAKER_02：通过对比数据与重构的统计差异，系统能自动学习优秀的连接权重。

157
00:13:46,871 --> 00:13:52,018
发言人 SPEAKER_02：就像伊拉克本由"3"组成，但小布什政府坚持认为它是"2"。

171
00:14:34,134 --> 00:14:35,576
发言人 SPEAKER_02：我们将采用层次化训练策略。

176
00:14:44,908 --> 00:14:51,739
发言人 SPEAKER_02：经过三层特征提取后，系统将获得适用于分类任务的高级抽象特征。

187
00:16:24,312 --> 00:16:29,539
发言人 SPEAKER_02：这种"渐进参数化"过程就像剥洋葱，逐层简化数据分布建模。

202
00:17:22,676 --> 00:17:30,409
发言人 SPEAKER_02：生成模型采用"分块-冗余生成"策略：先粗略生成部件，再通过马尔可夫随机场(MRF)协调部件关系。

215
00:18:22,769 --> 00:18:24,992
发言人 SPEAKER_02：我们将特征维度从50提升到500。

223
00:18:52,653 --> 00:18:55,176
发言人 SPEAKER_02：这是联合概率模型而非判别模型。

236
00:19:43,890 --> 00:19:51,644
发言人 SPEAKER_02：系统在490个非关键维度上允许变异，但在10个关键维度上保持数字本质特征。

251
00:20:58,813 --> 00:21:01,298
发言人 SPEAKER_02：这虽非严格的最大似然估计，但效果显著。

264
00:21:35,624 --> 00:21:38,708
发言人 SPEAKER_02：网络结构：输入层(400)-隐层1(500)-隐层2(500)-隐层3(2000)-标签层(10)。

277
00:23:09,684 --> 00:23:13,050
发言人 SPEAKER_02：系统将模糊图像识别为"4"具有合理性——人类也能从中看出"4"的特征。

287
00:23:52,757 --> 00:23:57,801
发言人 SPEAKER_02：通过固定顶层神经元状态，系统会在能量景观中收敛到对应数字的"峡谷"。

301
00:24:54,967 --> 00:25:00,693
发言人 SPEAKER_02：心理状态本质是假设性陈述："若存在粉色大象，则当前脑状态即为其感知"。

311
00:25:35,052 --> 00:25:36,184
发言人 SPEAKER_02：系统重构效果优于原始数据。

322
00:26:24,713 --> 00:26:29,201
发言人 SPEAKER_02：深度信念网络(DBN)性能优于支持向量机(SVM)1.4%的错误率。

337
00:27:29,663 --> 00:27:32,307
发言人 SPEAKER_02：语义哈希技术将文档映射为30位二进制码。

346
00:27:55,769 --> 00:27:57,712
发言人 SPEAKER_02：k近邻算法错误率达3.3%。

354
00:28:48,415 --> 00:28:49,417
发言人 SPEAKER_02：无监督学习无需标签。

361
00:29:18,267 --> 00:29:25,095
发言人 SPEAKER_02：我们构建的生成模型能计算图像概率分布，而非传统的标签条件概率。

376
00:30:54,049 --> 00:31:00,622
发言人 SPEAKER_02：当所有层均为线性时，系统退化为标准主成分分析(PCA)。

385
00:31:26,567 --> 00:31:27,969
发言人 SPEAKER_02：需警惕这种思维陷阱。

394
00:31:59,655 --> 00:32:00,215
发言人 SPEAKER_02：学术界的规模认知差异。

402
00:32:24,419 --> 00:32:27,643
发言人 SPEAKER_02：顶层仅保留2个线性特征。

412
00:33:11,842 --> 00:33:16,768
发言人 SPEAKER_02：相比潜在语义分析(LSA)等传统方法，我们的技术能更好保持文档语义结构。

425
00:34:02,335 --> 00:34:03,717
发言人 SPEAKER_02：实现文档相似性搜索。

436
00:34:52,563 --> 00:35:00,938
发言人 SPEAKER_02：通过添加高斯噪声迫使隐层单元二元化，实现信息高效编码。

447
00:35:51,425 --> 00:35:55,929
发言人 SPEAKER_02："超市搜索"原理：类似商品在货架相邻摆放。

456
00:36:23,721 --> 00:36:28,228
发言人 SPEAKER_02：30维语义空间能同时保持多种相似性关系。

467
00:37:33,070 --> 00:37:33,250
发言人 SPEAKER_02：关键优势。

473
00:37:35,052 --> 00:37:46,085
发言人 SPEAKER_02：在万亿级文档库中，每次相似性搜索仅需20万条机器指令。

486
00:38:56,882 --> 00:38:59,626
发言人 SPEAKER_02：传统方法无法理解"冈萨雷斯辞职"与"沃尔福威茨离任"的语义等价性。

495
00:40:00,449 --> 00:40:04,614
发言人 SPEAKER_02：核心突破：仅需少量标注数据即可微调模型。

507
00:41:06,396 --> 00:41:16,188
发言人 SPEAKER_02：构建分层生成模型：顶层指定物体类型和位姿，中层协调部件关系，底层实现细节生成。

521
00:42:31,344 --> 00:42:35,610
发言人 SPEAKER_02：引入可见单元间的横向连接。

536
00:43:49,601 --> 00:43:54,327
发言人 SPEAKER_02：通过均值场近似实现快速推理。

548
00:45:00,806 --> 00:45:09,920
发言人 SPEAKER_02：横向连接能实现长程约束："此处边缘应与彼处边缘共线"。

555
00:45:36,143 --> 00:45:41,052
发言人 SPEAKER_02：这是当前最优的自然图像块生成模型。

567
00:46:46,701 --> 00:46:49,382
发言人 SPEAKER_03：现在进入问答环节。

568
 00:46:50,143 --> 00:46:54,547 
发言人 SPEAKER_03：如果您有问题，能否请您打开中间的麦克风，以便办公室外的人可以听到？

569
 00:46:55,523 --> 00:46:56,605 
发言人 SPEAKER_01：好的。
570 
00:46:57,065 --> 00:46:57,226 
发言人 SPEAKER_01：嗨。
571 00:46:57,425 --> 00:47:00,891 发言人 SPEAKER_01：所以，您是说这种方法不需要标签。
572 00:47:01,311 --> 00:47:05,940 发言人 SPEAKER_01：我只是想知道如果对至少部分训练数据添加标签是否真的有帮助。
573 00:47:06,621 --> 00:47:06,981 发言人 SPEAKER_02：哦，是的。
574 00:47:08,182 --> 00:47:08,804 发言人 SPEAKER_02：标签帮助。
575 00:47:09,485 --> 00:47:14,132 发言人 SPEAKER_02：最主要的是要表明，没有它们，你也可以做很多事情，因此，你可以从几个标签中获得更多的优势。
576 00:47:15,255 --> 00:47:15,355 未知发言人：是的。
577 00:47:16,380 --> 00:47:28,355 发言人 SPEAKER_02：例如，在语义散列思想中，当你学习那些 30 维代码时，你可以说，如果两个东西来自同一类，并且代码相距很远，则引入一个小的力将它们拉到一起。
578 00:47:29,416 --> 00:47:31,418 发言人 SPEAKER_02：去年我们在《人工智能统计》上发表了一篇关于这个主题的论文。
579 00:47:31,898 --> 00:47:36,923 发言人 SPEAKER_02：这样可以改善同一类别中事物的聚类程度。
580 00:47:37,824 --> 00:47:40,228 Speaker SPEAKER_02：但关键是你无需了解课程也能做到这一点。
581 00:47:43,059 --> 00:47:43,740 发言人 SPEAKER_08：嗨。
582 00:47:43,760 --> 00:47:49,570 发言人 SPEAKER_08：人们很早以前就已经构建了自动编码器，他们使用常规的 S 型单元并使用反向传播来训练它们。
583 00:47:50,010 --> 00:47:51,492 发言人 SPEAKER_08：但是它们的效果一直不太好。
584 00:47:51,512 --> 00:47:51,853 发言人 SPEAKER_08：正确。
585 00:47:53,976 --> 00:48:03,650 Speaker SPEAKER_08: 如果我们实际上有多层这些 S 型单元，并且按照您现在的方式一次训练一层，那么它的效果会不会和 RBM 一样好？
586 00:48:03,681 --> 00:48:05,083 发言人 SPEAKER_02：好的，这是一个非常好的问题。
587 00:48:05,143 --> 00:48:06,746 发言人 SPEAKER_02：所以这有点令人困惑。
588 00:48:07,086 --> 00:48:11,753 发言人 SPEAKER_02：这种用 RBM 训练的多层深层东西我称之为多层自动编码器。
589 00:48:12,135 --> 00:48:16,422 发言人 SPEAKER_02：但你也可以有一个非常小的自动编码器，其中包含一个非线性的隐藏层并对其进行训练。
590 00:48:16,942 --> 00:48:18,184 发言人 SPEAKER_02：RBM 就是这样的。
591 00:48:18,804 --> 00:48:22,590 发言人 SPEAKER_02：因此，您可以训练这些小型自动编码器并将它们堆叠在一起，然后使用反向传播来训练整个系统。
592 00:48:23,052 --> 00:48:23,932 发言人 SPEAKER_02：这就是问题所在。
593 00:48:24,713 --> 00:48:29,201 发言人 SPEAKER_02：这比训练自动编码器的老方法要好得多，但不如这个好。
594 00:48:29,541 --> 00:48:32,405 发言人 SPEAKER_02：Yoshua Bengio 有一篇论文。
595 00:48:32,385 --> 00:48:35,570 发言人 SPEAKER_02：他将自动编码器与受限玻尔兹曼机进行了比较。
596 00:48:35,630 --> 00:48:39,373 发言人 SPEAKER_02：受限玻尔兹曼机的效果更好，特别是对于杂乱的背景。
597 00:48:49,164 --> 00:48:51,907 发言人 SPEAKER_04：我有一个问题，因为我手里拿着麦克风，所以我可以问。
598 00:48:52,849 --> 00:48:58,494 Speaker SPEAKER_04: 今天早上，我们讨论了新闻，新闻的问题在于
599 00:48:58,795 --> 00:49:00,416 发言人 SPEAKER_04：一切事物每天都在变化。
600 00:49:00,436 --> 00:49:06,181 发言人 SPEAKER_04：您是否有任何直觉，这是一种不公平的情况，您认为会发生什么？
601 00:49:06,601 --> 00:49:15,769 发言人 SPEAKER_04：一旦您的输入分布发生变化或持续变化，您是否知道调整这样的深度网络有多困难？
602 00:49:16,050 --> 00:49:20,954 发言人 SPEAKER_02：好的，这种学习方式的一个好处就是一切都随着训练数据的数量呈线性增长。
603 00:49:20,994 --> 00:49:24,318 发言人 SPEAKER_02：任何地方的二次优化都不会对大型数据库造成困扰。
604 00:49:25,139 --> 00:49:28,802 Speaker SPEAKER_02: 另一件事是因为它基本上是随机的在线学习，
605 00:49:28,782 --> 00:49:33,931 发言人 SPEAKER_02：如果你的分布发生轻微变化，你可以很容易地跟踪它。
606 00:49:33,971 --> 00:49:34,932 发言人 SPEAKER_02：您不必重新开始。
607 00:49:35,614 --> 00:49:51,583 发言人 SPEAKER_02：所以，如果明天的新闻与过去几个月和几年的新闻有很多共同之处，你只需要稍微改变一下模型，而不是重新开始，那么这对于跟踪来说是非常好的，而且它不会像一开始就学习所有内容那样耗费大量工作。
608 00:49:51,967 --> 00:49:59,481 发言人 SPEAKER_02：事实上，一旦你掌握了所有这些层级的特征，基本上改变高级特征之间的交互就会让你在不做太多工作的情况下获得很大的进步。
609 00:50:05,384 --> 00:50:09,489 发言人 SPEAKER_01：所以我还有另一个关于超市搜索的问题。
610 00:50:09,969 --> 00:50:13,592 发言人 SPEAKER_01：你之前说你只是在哈希码中翻转了一点。
611 00:50:15,094 --> 00:50:24,704 发言人 SPEAKER_01：所以我想知道的是，有一件事我不确定，那就是如果你翻转其中一个位，你可能不一定会在那里得到一些东西。
612 00:50:24,724 --> 00:50:27,487 发言人 SPEAKER_01：我的意思是，你怎么知道你会在那里找到什么东西？
613 00:50:27,527 --> 00:50:31,552 发言人 SPEAKER_01：那么，也许，有没有什么方法可以找到更好的位来翻转？
614 00:50:31,632 --> 00:50:33,534 发言人 SPEAKER_01：那您怎么决定选择哪一个呢？
615 00:50:33,987 --> 00:50:39,641 发言人 SPEAKER_02：所以当然，如果你让地址的数量与文档的数量大致相同，那么平均实体就是一个。
616 00:50:40,181 --> 00:50:44,032 发言人 SPEAKER_02：如果那里什么都没有，你就翻转更多的位。
617 00:50:44,313 --> 00:50:48,503 发言人 SPEAKER_02：所以是的，你会错过一些，但这只是一种常量。
618 00:50:48,938 --> 00:50:52,802 发言人 SPEAKER_02：我们可以实际看看地址的分布有多均匀。
619 00:50:53,224 --> 00:50:57,989 发言人 SPEAKER_02：通常情况下，大多数地址都不会被使用，而一个典型的地址只会被使用三到四次。
620 00:50:58,409 --> 00:51:01,353 发言人 SPEAKER_02：所以它并不像我们所希望的那样统一，但这一切都可以改进。
621 00:51:01,954 --> 00:51:03,476 发言人 SPEAKER_02：我们只做过一次。
622 00:51:03,516 --> 00:51:08,123 发言人 SPEAKER_02：我们只是在一个数据集上对这个网络进行了一次训练，事实上，这就是我们目前所做的全部研究。
623 00:51:09,083 --> 00:51:12,768 发言人 SPEAKER_02：如果我们能从某人那里得到一点点钱，我们就能让整个事情进展得更好。
624 00:51:18,536 --> 00:51:23,985 发言人 SPEAKER_07：数字的特别之处在于，它们的进化方式使它们具有辨别力。
625 00:51:24,306 --> 00:51:31,998 发言人 SPEAKER_07：所以你会希望，以无监督的方式提取具有辨别力的特征并不奇怪。
626 00:51:32,018 --> 00:51:40,831 发言人 SPEAKER_07：我想知道在其他完全不同的应用中会发生什么，很明显，当你进行无监督时，你可能会丢弃一些非常有指示性的特征。
627 00:51:40,929 --> 00:51:43,672 发言人 SPEAKER_02：是的，基本上有两种学习。
628 00:51:43,713 --> 00:51:48,659 发言人 SPEAKER_02：有一种辨别学习，你接受输入，而你一生的目标就是预测标签。
629 00:51:49,458 --> 00:51:54,744 发言人 SPEAKER_02：然后是生成学习，你接受输入，而你人生的全部目标就是理解这个输入中发生了什么。
630 00:51:55,887 --> 00:52:00,110 发言人 SPEAKER_02：您想要建立一个模型来解释为什么您得到这些输入而不是其他输入。
631 00:52:01,012 --> 00:52:04,356 发言人 SPEAKER_02：现在，如果你采用那种生成方法，你需要一台大型计算机。
632 00:52:05,112 --> 00:52:10,018 发言人 SPEAKER_02：而且你要解释各种与你感兴趣的任务完全无关的东西。
633 00:52:10,438 --> 00:52:12,101 发言人 SPEAKER_02：所以，你会浪费大量的计算。
634 00:52:13,063 --> 00:52:22,956 发言人 SPEAKER_02：另一方面，你不需要那么多的训练数据，因为每幅图像都包含很多内容，你可以开始构建你的特征，而无需使用标签中的任何信息。
635 00:52:23,838 --> 00:52:30,106 发言人 SPEAKER_02：所以，如果你有一台非常小的计算机，你应该做的是判别学习，这样你就不会浪费任何精力。
636 00:52:30,762 --> 00:52:38,498 发言人 SPEAKER_02：如果你有一台大型计算机，进行生成学习，你会浪费大量的周期，但你会更好地利用有限的可想象数据。
637 00:52:38,518 --> 00:52:43,067 发言人 SPEAKER_02：这就是我的主张。
638 00:52:43,586 --> 00:52:43,947 发言人 SPEAKER_05：嗨，杰夫。
639 00:52:44,447 --> 00:52:45,369 发言人 SPEAKER_05：我有一个问题。
640 00:52:45,429 --> 00:52:47,672 发言人 SPEAKER_05：正则化怎么了？
641 00:52:47,731 --> 00:52:51,135 发言人 SPEAKER_05：在所有阶段中都隐含着什么样的正则化？
642 00:52:51,856 --> 00:52:53,759 发言人 SPEAKER_02：好的，我们使用了一点权重衰减。
643 00:52:54,480 --> 00:53:01,407 发言人 SPEAKER_02：我们设置权重衰减的方式只是稍微摆弄了一下，看看哪种方式在验证集上有效，这是通常的方法。
644 00:53:01,427 --> 00:53:03,751 发言人 SPEAKER_02：如果你不使用任何权重衰减，它就可以起作用。
645 00:53:03,831 --> 00:53:05,472 发言人 SPEAKER_02：如果你使用权重衰减，效果会更好一些。
646 00:53:05,773 --> 00:53:07,094 发言人 SPEAKER_02：而且你用了多少并不重要。
647 00:53:07,135 --> 00:53:08,597 发言人 SPEAKER_02：因此我们在这里使用了一些权重衰减。
648 00:53:08,916 --> 00:53:10,219 发言人 SPEAKER_02：但这没什么大不了的。
649 00:53:10,519 --> 00:53:13,827 发言人 SPEAKER_02：就像我说的，所有代码都在我的网页上的 MATLAB 中。
650 00:53:14,268 --> 00:53:15,590 发言人 SPEAKER_02：我的网页上有一个指针。
651 00:53:15,610 --> 00:53:18,115 发言人 SPEAKER_02：所以你可以去看看所有这些东西和我们使用的小软糖。
652 00:53:18,155 --> 00:53:18,336 发言人 SPEAKER_05：对。
653 00:53:18,436 --> 00:53:24,530 发言人 SPEAKER_05：但是玻尔兹曼机从根本上来说是一种熵正则化。
654 00:53:24,590 --> 00:53:28,659 Speaker SPEAKER_05：然后你对权重衰减进行的小调整就来自另一个家族。
655 00:53:28,699 --> 00:53:30,181 发言人 SPEAKER_05：所以，你们把两者混合在一起了。
656 00:53:30,161 --> 00:53:37,833 发言人 SPEAKER_02：不，玻尔兹曼机，没错，有很多正则化来自于隐藏单元是二进制随机的事实，所以它们不能传输太多信息。
657 00:53:38,213 --> 00:53:41,057 发言人 SPEAKER_02：与普通的自动编码器相比，这可以为您进行大量的正则化。
658 00:53:42,320 --> 00:53:44,784 发言人 SPEAKER_02：但除此之外，我们还要说，不要把重量弄得太大。
659 00:53:45,865 --> 00:53:51,514 发言人 SPEAKER_02：其中一个原因不仅仅是正则化，如果权重不太大的话，它会使马尔可夫链混合得更快。
660 00:53:53,396 --> 00:53:53,536 发言人 SPEAKER_02：谢谢。
661 00:53:55,677 --> 00:54:03,746 发言人 SPEAKER_06：所以，在你的数字例子中，你实际上告诉它们，告诉算法有 10 个类。
662 00:54:03,806 --> 00:54:04,027 发言人 SPEAKER_06：是的。
663 00:54:04,648 --> 00:54:09,594 发言人 SPEAKER_06：所以，我想知道如果我们没有提供正确的数字，会有什么影响。
664 00:54:11,536 --> 00:54:16,724 发言人 SPEAKER_06：所以，是的。
665 00:54:16,744 --> 00:54:17,063 发言人 SPEAKER_02：好的。
666 00:54:17,123 --> 00:54:21,869 发言人 SPEAKER_02：那么，你可以做的是，使用这个自动编码器，将其降低到 30 个实数。
667 00:54:22,913 --> 00:54:27,498 发言人 SPEAKER_02：不要告诉它有多少个类别，只给它图像，得到这 30 个实数。
668 00:54:27,960 --> 00:54:34,208 发言人 SPEAKER_02：然后，您可以取这 30 个实数，并应用我和 Sam Roess 开发的降维技术。
669 00:54:35,210 --> 00:54:41,958 Speaker SPEAKER_02： 最新版本中，您可以在 2D 中布局它们，这样您将获得 11 个类。
670 00:54:44,061 --> 00:54:45,884 发言人 SPEAKER_02：而且它在根本不知道任何标签的情况下就做到了这一点。
671 00:54:45,903 --> 00:54:49,949 发言人 SPEAKER_02：您将获得这 11 个集群，接近 10。
672 00:54:52,697 --> 00:54:55,864 发言人 SPEAKER_02：人们通常认为，大陆七人制橄榄球赛是一个单独的集群。
673 00:54:56,125 --> 00:54:59,050 发言人 SPEAKER_06：所以你的意思是这就是你所尝试过的方法，而且发生了这样的事情？
674 00:54:59,211 --> 00:55:01,617 发言人 SPEAKER_02：我甚至可能在这次演讲的某个地方提到过它。
675 00:55:03,561 --> 00:55:04,222 发言人 SPEAKER_02：但我可能不会。
676 00:55:04,943 --> 00:55:06,306 发言人 SPEAKER_02：哦，就这样。
677 00:55:07,369 --> 00:55:10,655 发言人 SPEAKER_02：这是对数字的纯粹无监督。
678 00:55:11,057 --> 00:55:14,402 发言人 SPEAKER_02：现在，在这种情况下，这些是 2，这些是 2。
679 00:55:15,003 --> 00:55:17,347 发言人 SPEAKER_02：在 30D 中，它有集群。
680 00:55:17,367 --> 00:55:22,936 发言人 SPEAKER_02：当你强制将其降低到 2D 时，它会希望将 2 保持彼此相邻，但它也希望这些。
681 00:55:23,197 --> 00:55:25,961 发言人 SPEAKER_02：这些是尖刺的 2，这些是 7，它希望它们靠近。
682 00:55:26,463 --> 00:55:30,309 发言人 SPEAKER_02：这些是循环的 2，这些是 3，它希望它们靠近。
683 00:55:30,349 --> 00:55:32,592 发言人 SPEAKER_02：但是它还希望 3 接近 8。
684 00:55:32,572 --> 00:55:36,338 Speaker SPEAKER_02：因此在 2D 中，没有足够的空间来创建 10 个簇。
685 00:55:36,597 --> 00:55:38,420 发言人 SPEAKER_02：但是你看，那里有 11 个。
686 00:55:38,940 --> 00:55:44,568 Speaker SPEAKER_02：如果我不作弊并且只用黑白色，你仍然可以看到大约 11 个簇。
687 00:55:45,548 --> 00:55:48,391 发言人 SPEAKER_02：所以这是纯粹无监督的，它在数据中发现了这种结构。
688 00:55:48,713 --> 00:55:55,221 发言人 SPEAKER_02：所以当心理学家告诉你，你对这些数据强加分类时，它们实际上并不存在，这是垃圾。
689 00:55:55,920 --> 00:55:58,023 发言人 SPEAKER_02：我的意思是，他们确实在那里。
690 00:55:58,914 --> 00:56:04,409 发言人 SPEAKER_06：那么公制数字 30，如果我选择其他数字，也可以吗？
691 00:56:04,568 --> 00:56:10,364 发言人 SPEAKER_02：如果您选择较小的数字，您可能无法保留足够的信息来保留类别。
692 00:56:10,403 --> 00:56:13,331 发言人 SPEAKER_02：如果你选择一个更大的数字，那么 PCA 的效果会更好。
693 00:56:13,351 --> 00:56:15,556 发言人 SPEAKER_02：所以你与 PCA 的比较结果不会那么好。
694 00:56:23,923 --> 00:56:30,920 发言人 SPEAKER_00：根据您使用的层数，数字分类的性能如何变化？
695 00:56:31,782 --> 00:56:32,123 发言人 SPEAKER_02：好的。
696 00:56:32,744 --> 00:56:36,632 发言人 SPEAKER_02：显然，使用我向您展示的层数是最好的数字之一。
697 00:56:37,494 --> 00:56:40,342 发言人 SPEAKER_02：如果使用的层数较少，效果会差一些。
698 00:56:41,143 --> 00:56:43,148 发言人 SPEAKER_02：如果你使用更多层，其工作原理大致相同。
699 00:56:44,527 --> 00:56:51,215 发言人 SPEAKER_02：我现在有一位非常优秀的荷兰学生，他有这种特质：他不相信我说的任何话，我们就会知道。
700 00:56:51,257 --> 00:56:54,380 发言人 SPEAKER_02：他正在使用大约 40 台集群机器来得到这个问题的答案。
701 00:56:54,800 --> 00:57:00,387 发言人 SPEAKER_02：但到目前为止，我认为使用较少的层并不是很好，而且他还没有使用更多的层。
702 00:57:00,929 --> 00:57:05,894 发言人 SPEAKER_02：实际上，他用相同数量的层制作了它，可以使其工作得更好，我们会看看他是否能让它在更多层的情况下工作得更好。
703 00:57:07,494 --> 00:57:17,452 发言人 SPEAKER_09：因此，如何评估这个模型就很清楚了，比如说，如果你有一些标记数据，你可以试着看看你是否可以做出类似的预测。
704 00:57:17,472 --> 00:57:23,744 发言人 SPEAKER_09：但是如果你尝试生成这种玻尔兹曼机，尤其是像同一级别的成对相互作用等等。
705 00:57:24,083 --> 00:57:28,592 发言人 SPEAKER_09：如果我给你另一组，你能说一下它的生成效果如何吗？它是否简单易用？
706 00:57:28,858 --> 00:57:33,085 发言人 SPEAKER_09：您如何评价这部分内容？
707 00:57:33,106 --> 00:57:36,090 发言人 SPEAKER_02：所以这些玻尔兹曼机的问题在于存在一个分区函数。
708 00:57:36,391 --> 00:57:48,369 发言人 SPEAKER_02：你想要做的是，获取你的数据集，保留一些示例，在训练集上训练你的生成模型，然后说出这些保留示例的对数概率是多少？
709 00:57:48,603 --> 00:57:50,385 发言人 SPEAKER_02：这将是某种黄金标准。
710 00:57:50,666 --> 00:57:51,746 发言人 SPEAKER_02：这很难做到。
711 00:57:52,047 --> 00:57:54,891 发言人 SPEAKER_02：您知道对数概率达到常数，但您不知道常数。
712 00:57:55,592 --> 00:58:13,074 发言人 SPEAKER_02：所以，我们小组的成员现在正在努力研究一种在玻尔兹曼机之间进行插值的方法，这种方法允许你使用具有零权重的玻尔兹曼机（这是一个相当愚蠢的模型），然后逐渐将权重改变为你最终学到的玻尔兹曼机。
713 00:58:13,661 --> 00:58:18,070 Speaker SPEAKER_02：而且你可以得到所有这些玻尔兹曼机的配分函数的比率。
714 00:58:18,431 --> 00:58:20,295 Speaker SPEAKER_02: 所以最后，你可以得到分区函数。
715 00:58:20,775 --> 00:58:21,757 发言人 SPEAKER_02：你可以得到一个相当不错的估计。
716 00:58:22,298 --> 00:58:26,726 发言人 SPEAKER_02：这叫做退火重要性采样的一个版本。
717 00:58:27,144 --> 00:58:35,474 Speaker SPEAKER_02： 称为桥接，我们认为通过运行大约一百个小时，我们将能够获得分区函数的相当准确的估计。
718 00:58:35,494 --> 00:58:38,197 发言人 SPEAKER_02：你学会之后这样做只是为了证明你有多优秀。
719 00:58:38,539 --> 00:58:51,735 发言人 SPEAKER_02：但是您可以做的另一件事是从模型中生成，您可以看到它生成的东西看起来不错，然后您可以获取从模型中生成的东西，然后可以对其进行统计测试，并将统计测试应用于真实数据。
720 00:58:51,715 --> 00:58:55,242 发言人 SPEAKER_02：并对其他人的数据，其他人生成的数据进行统计测试。
721 00:58:55,601 --> 00:58:58,708 Speaker SPEAKER_02：如果你选择正确的统计测试，你可以让其他人的数据看起来很糟糕。
722 00:59:05,742 --> 00:59:07,644 发言人 SPEAKER_04：好的，我想我们现在没有时间了。
723 00:59:07,704 --> 00:59:11,192 发言人 SPEAKER_04：我想再次感谢杰夫。
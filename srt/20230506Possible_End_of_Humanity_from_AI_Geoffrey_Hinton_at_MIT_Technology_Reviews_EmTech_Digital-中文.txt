1 00:00:02,072 --> 00:00:09,981 主持人 SPEAKER_07：大家好。
2 00:00:11,064 --> 00:00:11,624 主持人 SPEAKER_07：欢迎回来。
3 00:00:11,644 --> 00:00:12,445 主持人 SPEAKER_07：希望您午餐吃得不错。
4 00:00:13,627 --> 00:00:17,591 主持人 SPEAKER_07：我是 Will Douglas-Hevan，麻省理工学院科技评论 AI 高级编辑。
00:00:18,411 --> 00:00:22,657 说话人 SPEAKER_07：我认为我们所有人都同意，生成式 AI 正是当下的热门话题。
00:00:23,599 --> 00:00:24,940 说话人 SPEAKER_07：但创新从未停止。
00:00:25,300 --> 00:00:29,885 说话人 SPEAKER_07：在本章中，我们将探讨一些正在推动前沿并探索下一步的尖端研究。
00:00:30,507 --> 00:00:35,854 说话人 SPEAKER_07：但首先，我想介绍一位非常特别的虚拟嘉宾。
9 00:00:36,856 --> 00:00:44,067 讲者 SPEAKER_07：杰弗里·辛顿是多伦多大学的荣誉教授，直到本周，他还是谷歌的工程研究员。
10 00:00:44,087 --> 00:00:47,773 讲者 SPEAKER_07：但就在周一，他宣布在谷歌工作了 10 年后，他将辞职。
11 00:00:48,799 --> 00:00:51,203 讲者 SPEAKER_07：杰弗里是现代人工智能领域最重要的人物之一。
12 00:00:52,124 --> 00:01:01,936 讲者 SPEAKER_07：他是深度学习的先驱，开发了一些支撑我们今天所知的人工智能的最基本技术，例如反向传播算法，该算法允许机器学习。
13 00:01:03,457 --> 00:01:07,742 说话人 SPEAKER_07：这种技术是当今几乎所有深度学习的基础。
14 00:01:09,004 --> 00:01:16,713 说话人 SPEAKER_07：2018 年，杰弗里获得了图灵奖，通常被称为计算机科学的诺贝尔奖，与 Jan Lekun 和 Yoshua Bengio 共同获得。
15 00:01:18,347 --> 00:01:24,355 说话人 SPEAKER_07：他今天与我们在这里讨论智能，它的含义，以及将智能构建到机器中的尝试将带我们走向何方。
16 00:01:25,156 --> 00:01:27,019 说话人 SPEAKER_07：杰弗里，欢迎来到 EmTech。
17 00:01:28,140 --> 00:01:28,441 说话人 SPEAKER_07: 谢谢。
18 00:01:29,081 --> 00:01:29,861 说话人 SPEAKER_07: 你这周过得怎么样？
19 00:01:30,082 --> 00:01:31,063 说话人 SPEAKER_07: 我想这应该是几天忙碌的日子。
20 00:01:32,165 --> 00:01:38,352 说话人 SPEAKER_05: 嗯，过去的 10 分钟很糟糕，因为我的电脑崩溃了，我不得不找另一台电脑并连接它。
21 00:01:38,373 --> 00:01:39,194 说话人 SPEAKER_07：很高兴您回来。
22 00:01:39,513 --> 00:01:41,996 说话人 SPEAKER_07：这种技术细节我们不应该与观众分享。
23 00:01:43,118 --> 00:01:44,801 说话人 SPEAKER_07：很高兴您能来。
24 00:01:45,281 --> 00:01:46,582 说话人 SPEAKER_07：非常高兴您能加入我们。
25 00:01:46,968 --> 00:01:51,813 说话人 SPEAKER_07：现在，你本周从谷歌辞职的消息已经遍布各大新闻。
26 00:01:52,393 --> 00:01:55,075 说话人 SPEAKER_07：你能先告诉我们你为什么做出这个决定吗？
27 00:01:56,637 --> 00:01:58,198 说话人 SPEAKER_05：嗯，有几个原因。
28 00:01:58,218 --> 00:02:00,240 说话人 SPEAKER_05：做出这样的决定总是有各种原因。
29 00:02:00,840 --> 00:02:06,206 说话人 SPEAKER_05：一个是我已经 75 岁了，我不再像以前那样擅长做技术工作了。
30 00:02:07,427 --> 00:02:11,010 说话人 SPEAKER_05：我的记忆力不如以前好了，当我编程时，我会忘记做一些事情。
31 00:02:11,531 --> 00:02:12,551 说话人 SPEAKER_05：所以是时候退休了。
32 00:02:13,873 --> 00:02:15,354 说话人 SPEAKER_05：第二个原因是，
33 00:02:15,772 --> 00:02:23,723 说话人 SPEAKER_05：最近，我对大脑和我们所开发的数字智能之间的关系有了很多改变。
34 00:02:25,044 --> 00:02:37,221 说话人 SPEAKER_05：以前，我认为我们开发的计算机模型不如大脑，目标是看看通过改进计算机模型能否更好地理解大脑。
35 00:02:39,364 --> 00:02:41,747 说话人 SPEAKER_05：在过去的几个月里，我的想法完全改变了。
36 00:02:42,384 --> 00:02:46,876 说话人 SPEAKER_05：我认为计算机模型可能以与大脑截然不同的方式工作。
37 00:02:47,236 --> 00:02:50,324 讲者 SPEAKER_05：他们使用反向传播，我认为大脑可能不是。
38 00:02:51,347 --> 00:02:55,736 讲者 SPEAKER_05：有几件事情让我得出这个结论，其中之一是 GPT-4 的表现。
39 00:02:56,526 --> 00:03:05,644 讲者 SPEAKER_07：所以，我想在下一分钟深入探讨 GPT-4 的要点，但让我们先回顾一下，以便我们都理解你提出的论点。
40 00:03:05,683 --> 00:03:09,310 讲者 SPEAKER_07：请给我们简单介绍一下什么是反向传播。
41 00:03:09,350 --> 00:03:13,217 说话人 SPEAKER_07：这是我们与几位同事在 20 世纪 80 年代开发的一个算法。
42 00:03:15,962 --> 00:03:19,429 说话人 SPEAKER_05：许多不同的团队发现了反向传播。
43 00:03:19,645 --> 00:03:25,853 说话人 SPEAKER_05：我们特别之处在于使用了它，并展示了它能够开发出良好的内部表示。
44 00:03:26,433 --> 00:03:33,763 说话人 SPEAKER_05：有趣的是，我们通过实现一个小型语言模型做到了这一点。
45 00:03:35,444 --> 00:03:39,210 说话人 SPEAKER_05：它只有六个组件的嵌入向量。
46 00:03:39,931 --> 00:03:42,454 说话人 SPEAKER_05：训练集有 112 个案例。
47 00:03:43,975 --> 00:03:45,097 说话人 SPEAKER_05：但它是一个语言模型。
48 00:03:45,116 --> 00:03:49,062 说话人 SPEAKER_05：它试图预测符号串中的下一个术语。
49 00:03:50,171 --> 00:03:59,663 大约 10 年后，Yoshua Bengio 基本上使用了相同的网络，并将其应用于自然语言，并展示了如果将其做得更大，它实际上对自然语言是有效的。
50 00:04:01,887 --> 00:04:07,513 但反向传播是如何工作的，我可以给你一个粗略的解释。
51 00:04:09,316 --> 00:04:15,582 了解其工作原理的人可以坐下来，感到自鸣得意，并嘲笑我如何介绍它。
52 00:04:15,883 --> 00:04:16,184 好的。
53 00:04:16,805 --> 00:04:19,267 说话人 SPEAKER_05：因为我有点担心他们。
54 00:04:20,192 --> 00:04:24,177 说话人 SPEAKER_05：所以想象一下，你想要在图像中检测鸟类。
55 00:04:25,559 --> 00:04:29,764 说话人 SPEAKER_05：所以一个图像，假设它是一个 100 像素乘以 100 像素的图像。
56 00:04:30,324 --> 00:04:34,048 说话人 SPEAKER_05：这是 10,000 个像素，每个像素是三个通道，RGB。
57 00:04:34,709 --> 00:04:40,755 说话人 SPEAKER_05：这就是 30000 个数字，代表图像的每个像素中每个通道的强度。
58 00:04:41,677 --> 00:04:48,225 说话人 SPEAKER_05：关于计算机视觉问题可以这样思考，如何将这 30000 个数字转换成判断它是鸟还是不是的决策？
59 00:04:49,336 --> 00:04:51,860 说话人 SPEAKER_05：人们尝试了很长时间去做这件事，但并不擅长。
60 00:04:53,281 --> 00:04:55,264 说话人 SPEAKER_05：但是这里有一个建议，你可能怎么做。
61 00:04:56,187 --> 00:05:02,776 说话人 SPEAKER_05：您可能有一层特征检测器，它可以检测非常简单的特征和图像，例如，例如，边缘。
62 00:05:03,519 --> 00:05:13,233 说话人 SPEAKER_05：因此，特征检测器可能对像素列具有很大的正权重，然后对相邻的像素列具有很大的负权重。
63 00:05:13,923 --> 00:05:16,365 说话人 SPEAKER_05：所以如果两列都明亮，它就不会激活。
64 00:05:16,766 --> 00:05:18,288 说话人 SPEAKER_05：如果两列都暗淡，它也不会激活。
65 00:05:18,870 --> 00:05:24,357 说话人 SPEAKER_05：但如果一边的列很亮，另一边的列很暗，它会非常兴奋。
66 00:05:24,817 --> 00:05:25,939 说话人 SPEAKER_05：这就是边缘检测器。
67 00:05:26,821 --> 00:05:33,009 说话人 SPEAKER_05：所以我刚刚告诉你们如何手动连接边缘检测器，通过一个列有大的正权重，旁边再有一个列有大的负权重。
68 00:05:33,889 --> 00:05:38,976 说话人 SPEAKER_05：我们可以想象一层很大的层，这些层可以在整个图像中检测不同方向和不同尺度的边缘。
69 00:05:39,798 --> 00:05:41,600 说话人 SPEAKER_05：我们需要相当大的数量。
70 00:05:41,800 --> 00:05:45,983 说话人 SPEAKER_07：图像中的边缘，你是说线条，形状的边缘？
71 00:05:46,105 --> 00:05:48,646 说话人 SPEAKER_05：亮度从亮到暗变化的地方。
72 00:05:50,709 --> 00:05:51,410 说话人 SPEAKER_05：是的，就是这样。
73 00:05:52,190 --> 00:05:56,475 说话者 SPEAKER_05：那么我们可能有一层特征检测器在上面，它可以检测边缘的组合。
74 00:05:57,336 --> 00:06:04,843 说话者 SPEAKER_05：例如，我们可能有一种可以检测两个以细小角度相连的边缘的东西。
75 00:06:07,247 --> 00:06:10,389 说话者 SPEAKER_05：所以它会给这两个边缘中的每一个赋予很大的正权重。
76 00:06:10,740 --> 00:06:13,983 说话者 SPEAKER_05：如果这两个边缘同时存在，它就会变得兴奋。
77 00:06:15,064 --> 00:06:18,129 说话者 SPEAKER_05：这可能会检测到可能是鸟嘴的东西。
78 00:06:18,528 --> 00:06:20,511 说话者 SPEAKER_05：可能不会，但也可能是鸟嘴。
79 00:06:20,531 --> 00:06:26,997 说话者 SPEAKER_05：在那个层中，你可能还有一个特征检测器，可以检测到一圈排列的很多边缘。
80 00:06:28,579 --> 00:06:30,420 说话者 SPEAKER_05：那可能是一只鸟的眼睛。
81 00:06:30,461 --> 00:06:31,742 说话人 SPEAKER_05：可能是一切 sorts of other things.
82 00:06:31,762 --> 00:06:34,925 说话人 SPEAKER_05：可能是一个冰箱上的旋钮或类似的东西。
83 00:06:36,593 --> 00:06:48,908 说话人 SPEAKER_05：然后在第三层，你可能有一个特征检测器，它可以检测到潜在的喙，并检测到潜在的瞳孔，并且它们被连接起来，就像一个喙和一个眼睛，彼此之间在正确的空间关系中。
84 00:06:49,490 --> 00:06:52,113 说话人 SPEAKER_05：如果它看到了这个，它就会说，啊，这可能是一只鸟的头。
85 00:06:53,295 --> 00:06:59,242 说话者 SPEAKER_05：你可以想象，如果你继续这样连接，最终可能会得到一个能检测到鸟的东西。
86 00:06:59,576 --> 00:07:06,324 说话者 SPEAKER_05：但是用手一个接一个地连接这些，会非常非常困难，需要决定什么应该连接到什么，以及权重应该是多少。
87 00:07:07,045 --> 00:07:14,211 说话者 SPEAKER_05：而且这会特别困难，因为你希望这些中间层不仅能够检测到鸟，还能检测到各种其他事物。
88 00:07:16,053 --> 00:07:19,497 说话者 SPEAKER_05：所以用手一个接一个地连接几乎是不可行的。
89 00:07:20,899 --> 00:07:22,940 说话人 SPEAKER_05：那么，背包传播的工作方式是这样的。
90 00:07:23,081 --> 00:07:24,423 说话人 SPEAKER_05：你从随机权重开始。
91 00:07:25,024 --> 00:07:27,305 说话人 SPEAKER_05：所以这些特征检测器完全是垃圾。
92 00:07:28,483 --> 00:07:33,952 说话人 SPEAKER_05：然后你放一张鸟的图片进去，输出结果说，比如，0.5 是鸟。
93 00:07:34,834 --> 00:07:36,197 说话人 SPEAKER_05：假设你只有鸟类或非鸟类。
94 00:07:37,418 --> 00:07:53,665 说话人 SPEAKER_05：然后你问自己以下问题，我应该如何改变网络中的每个权重，每个连接上的权重，使得它不再说 0.5，而是说 0.501 表示是鸟类，0.499 表示不是。
95 00:07:54,843 --> 00:08:03,555 说话人 SPEAKER_05：然后你改变权重，使其更有可能将鸟类识别为鸟类，而不是将非鸟类识别为鸟类。
96 00:08:05,297 --> 00:08:06,358 说话人 SPEAKER_05：然后你持续这样做。
97 00:08:07,019 --> 00:08:07,980 讲者 SPEAKER_05: 这就是反向传播。
98 00:08:08,021 --> 00:08:23,060 讲者 SPEAKER_05: 反向传播实际上是如何处理你想要的（概率为 1，认为它是鸟）和目前得到的（概率为 0.5，认为它是鸟）之间的差异，并将这个差异通过网络反向传递。
99 00:08:23,512 --> 00:08:31,062 讲者 SPEAKER_05: 这样就可以计算网络中每个特征检测器是否需要更加活跃或稍微不那么活跃。
100 00:08:31,663 --> 00:08:48,205 讲者 SPEAKER_05: 一旦计算出这些，如果你知道你想要一个特征检测器更加活跃，你可以增加来自下一层活跃的特征检测器的权重，并可能对下一层关闭的特征检测器设置一些负权重，这样你将得到一个更好的检测器。
101 00:08:48,488 --> 00:08:54,582 讲者 SPEAKER_05：所以反向传播就是通过网络反向传播，以确定每个特征检测器是否需要稍微更活跃或稍微不那么活跃。
102 00:08:55,504 --> 00:08:55,885 讲者 SPEAKER_07：谢谢。
103 00:08:56,005 --> 00:09:01,840 讲者 SPEAKER_07：我可以证明在观众中没有人在微笑，认为这是一个愚蠢的解释。
104 00:09:02,207 --> 00:09:11,183 讲者 SPEAKER_07：所以让我们快速前进很多，你知道，那种技术在 ImageNet 上表现非常好。
105 00:09:11,264 --> 00:09:16,653 说话人 SPEAKER_07：昨天我们邀请了 Meta 的 Joel Pino 来展示图像检测技术已经发展到何种程度。
106 00:09:17,073 --> 00:09:20,460 说话人 SPEAKER_07：而且这也是支撑大型语言模型的技术。
107 00:09:21,341 --> 00:09:22,984 说话人 SPEAKER_07：所以现在我想谈谈
108 00:09:22,964 --> 00:09:42,711 说话人 SPEAKER_07：这项技术，你们最初认为它几乎像是生物大脑可能做到的糟糕近似，但它最终做到了一些我认为你们会感到震惊的事情，尤其是在大型语言模型方面。
109 00:09:42,691 --> 00:09:56,109 讲者 SPEAKER_07：和我们谈谈为什么你对今天的大型语言模型所持有的那种惊奇感几乎完全颠覆了你对于反向传播或机器学习的一般看法。
110 00:09:57,530 --> 00:10:02,076 讲者 SPEAKER_05：所以如果你看看这些大型语言模型，它们大约有万亿个连接。
111 00:10:03,399 --> 00:10:06,803 讲者 SPEAKER_05：像 GPT-4 这样的模型知道的比我们多得多。
112 00:10:07,725 --> 00:10:10,048 讲者 SPEAKER_05：它们对几乎所有事物都有常识性的知识。
113 00:10:11,664 --> 00:10:14,687 说话人 SPEAKER_05：所以他们可能比一个人知道得多一千倍。
114 00:10:16,570 --> 00:10:19,452 说话人 SPEAKER_05：但他们有万亿个连接，而我们有一百万亿个连接。
115 00:10:20,594 --> 00:10:25,778 说话人 SPEAKER_05：所以他们比我们更好地将大量知识压缩到只有万亿个连接中。
116 00:10:27,801 --> 00:10:32,446 说话人 SPEAKER_05：我认为这是因为反向传播可能是一个比我们目前所拥有的学习算法要好得多的算法。
117 00:10:33,246 --> 00:10:34,447 说话人 SPEAKER_05：这很可怕。
118 00:10:35,168 --> 00:10:36,971 说话人 SPEAKER_07：是的，我确实想谈谈可怕的事情。
119 00:10:36,990 --> 00:10:38,432 说话人 SPEAKER_07：但您说的“更好”是什么意思？
120 00:10:39,457 --> 00:10:43,183 说话人 SPEAKER_05：它可以在只有几个连接中打包更多的信息。
121 00:10:43,784 --> 00:10:43,965 说话人 SPEAKER_07: 对。
122 00:10:44,546 --> 00:10:46,408 说话人 SPEAKER_05: 我们把万亿定义为很少。
123 00:10:47,370 --> 00:10:48,371 说话人 SPEAKER_07: 好的。
124 00:10:48,392 --> 00:10:57,004 说话人 SPEAKER_07: 所以这些数字计算机在学习和人类相比更优秀，这本身就是一个巨大的声明。
125 00:10:57,846 --> 00:11:03,173 说话者 SPEAKER_07：但是你同时也认为这是我们应该害怕的事情。
126 00:11:03,234 --> 00:11:05,618 说话者 SPEAKER_07：那么你能带我们通过这个论点的步骤吗？
127 00:11:05,969 --> 00:11:24,312 说话者 SPEAKER_05：是的，让我给你一个独立的论点，那就是如果一台计算机是数字的，这涉及到非常高的能源成本和非常精细的制造，你可以在不同的硬件上运行许多相同的模型，它们会做完全相同的事情。
128 00:11:25,014 --> 00:11:27,336 说话者 SPEAKER_05：它们可以查看不同的数据，但模型是完全相同的。
129 00:11:28,057 --> 00:11:30,981 说话人 SPEAKER_05：这意味着，假设你有 10,000 份副本。
130 00:11:31,822 --> 00:11:35,368 说话人 SPEAKER_05：它们可以查看 10,000 个不同的数据子集。
131 00:11:35,787 --> 00:11:38,873 说话人 SPEAKER_05：每当其中一个学到任何东西时，其他的都知道了。
132 00:11:39,914 --> 00:11:43,682 说话人 SPEAKER_05：其中一个找到了如何改变权重以便处理其数据的方法。
133 00:11:44,604 --> 00:11:49,614 说话人 SPEAKER_05：他们都互相沟通，并一致同意通过平均所有想要的方式改变权重。
134 00:11:50,916 --> 00:11:58,149 说话人 SPEAKER_05：现在，这 10,000 个东西正在非常有效地互相沟通。
135 00:11:58,349 --> 00:12:02,552 说话人 SPEAKER_05：这样他们就能看到比一个代理多 10,000 倍的数据。
136 00:12:03,153 --> 00:12:04,214 说话人 SPEAKER_05：人们做不到这一点。
137 00:12:05,034 --> 00:12:13,461 说话人 SPEAKER_05：如果我学习了大量的量子力学知识，并且我想让你了解这些量子力学知识，那是一个漫长而痛苦的过程，让你理解它。
138 00:12:14,182 --> 00:12:20,048 说话人 SPEAKER_05：我不能直接将我的权重复制到你的大脑中，因为你的大脑并不完全像我的一样。
139 00:12:20,067 --> 00:12:20,509 说话人 SPEAKER_07：不，不是的。
140 00:12:24,552 --> 00:12:27,715 说话人 SPEAKER_05：它更年轻。
141 00:12:28,859 --> 00:12:39,812 说话人 SPEAKER_07：所以我们有能够更快学习更多事物的数字计算机，并且它们可以立即相互教授。
142 00:12:39,832 --> 00:12:45,299 说话人 SPEAKER_07：就像，你知道，房间里的人可以立即将他们头脑中的东西传输到我的头脑中。
143 00:12:46,160 --> 00:12:48,203 说话人 SPEAKER_07：但为什么这会让人害怕？
144 00:12:49,885 --> 00:12:55,692 说话人 SPEAKER_05：因为它们可以学习到更多，比如以医生为例。
145 00:12:56,399 --> 00:13:03,967 说话者 SPEAKER_05：想象一下，你有一个医生看过 1,000 名病人，另一个医生看过 1 亿名病人。
146 00:13:05,610 --> 00:13:15,283 说话者 SPEAKER_05：你会期望那个看过 1 亿名病人的医生，如果他不是太健忘的话，会注意到数据中的一些趋势，而这些趋势如果你只看过 1,000 名病人是看不到的。
147 00:13:16,364 --> 00:13:19,587 说话者 SPEAKER_05：你可能只见过一名患有罕见疾病的病人。
148 00:13:19,687 --> 00:13:24,232 说话者 SPEAKER_05：那些看过 1 亿名病人的其他医生，他们见过的病人数量，你可以想象，但很多。
149 00:13:25,936 --> 00:13:29,620 说话人 SPEAKER_05：我们将看到许多在少量数据中并不明显的规律。
150 00:13:31,884 --> 00:13:39,095 说话人 SPEAKER_05：这就是为什么能够处理大量数据的东西可能在我们永远看不到的数据中看到结构。
151 00:13:42,580 --> 00:13:47,787 说话人 SPEAKER_07：但是，请告诉我我应该害怕的地方。
152 00:13:48,931 --> 00:13:53,138 说话人 SPEAKER_05：嗯，如果你看 GPT-4，它已经可以进行简单的推理了。
153 00:13:53,759 --> 00:13:55,942 说话人 SPEAKER_05：我的意思是，推理是我们仍然做得更好的领域。
154 00:13:57,244 --> 00:14:03,695 说话人 SPEAKER_05：但前几天我印象深刻，GPT-4 做了一件我认为它做不到的常识推理。
155 00:14:04,515 --> 00:14:10,245 说话人 SPEAKER_05：所以我问它，我想我家的所有房间都是白色的。
156 00:14:10,985 --> 00:14:15,293 说话人 SPEAKER_05：目前，有一些白色的房间，一些蓝色的房间，还有一些黄色的房间。
157 00:14:16,251 --> 00:14:18,655 说话人 SPEAKER_05：黄色油漆一年后变成白色。
158 00:14:19,817 --> 00:14:23,003 说话人 SPEAKER_05：如果我想要它们两年后都变成白色，我应该怎么做？
159 00:14:25,287 --> 00:14:28,211 说话人 SPEAKER_05：它说你应该把蓝色房间刷成黄色。
160 00:14:29,193 --> 00:14:31,256 说话人 SPEAKER_05：这不是自然的方法，但它是有效的，对吧？
161 00:14:31,798 --> 00:14:31,898 未知说话者：嗯。
162 00:14:32,586 --> 00:14:41,037 说话者 SPEAKER_05：这非常令人印象深刻，这是一种非常难以用符号 AI 实现的常识推理。
163 00:14:41,697 --> 00:14:44,541 说话者 SPEAKER_05：因为它必须理解“褪色”的含义。
164 00:14:44,620 --> 00:14:47,624 说话者 SPEAKER_05：它必须理解时间上的东西。
165 00:14:48,905 --> 00:15:01,480 说话人 SPEAKER_05：所以他们可以进行类似于 80 或 90 智商的合理推理。
166 00:15:02,152 --> 00:15:09,489 说话人 SPEAKER_05：就像我的一位朋友说的，就好像一些基因工程师说，我们要改进灰熊。
167 00:15:09,929 --> 00:15:13,979 说话人 SPEAKER_05：我们已经将它们的智商提高到 65，现在它们可以讲英语了。
168 00:15:14,220 --> 00:15:15,884 说话人 SPEAKER_05：它们对各种事情都非常有用。
169 00:15:16,365 --> 00:15:19,130 说话人 SPEAKER_05：但我们认为可以将智商提高到 210。
170 00:15:23,498 --> 00:15:33,830 说话人 SPEAKER_07：我的意思是，我确实有这种感觉，我相信很多人在与这些最新的聊天机器人互动时都有过这种感觉，你知道的，有点毛骨悚然，有点不寒而栗。
171 00:15:34,291 --> 00:15:38,034 说话人 SPEAKER_07：但是，你知道的，当我感到不舒服时，我就关掉我的笔记本电脑。
172 00:15:39,056 --> 00:15:40,418 说话人 SPEAKER_07：所以。
173 00:15:40,437 --> 00:15:52,392 说话人 SPEAKER_05：是的，但我们会从阅读所有小说以及马基雅维利所写的所有东西中学到这些
174 00:15:53,366 --> 00:15:54,990 说话人 SPEAKER_05：如何操纵人们，对吧？
175 00:15:55,571 --> 00:15:58,956 说话人 SPEAKER_05：如果它们比我们聪明得多，它们在操纵我们方面会非常出色。
176 00:15:58,975 --> 00:16:00,217 说话人 SPEAKER_05：你不会意识到正在发生什么。
177 00:16:00,519 --> 00:16:07,068 说话者 SPEAKER_05：你会像一个两岁的孩子，被问及你想吃豌豆还是花椰菜，却不知道你不必两者都要。
178 00:16:07,870 --> 00:16:10,414 说话者 SPEAKER_05：你将如此容易被操纵。
179 00:16:12,518 --> 00:16:17,907 说话者 SPEAKER_05：即使他们不能直接拉动杠杆，他们当然可以让我们去拉动杠杆。
180 00:16:18,510 --> 00:16:24,398 说话者 SPEAKER_05：结果证明，如果你能操纵人们，你可以在从未去过华盛顿的情况下入侵一栋建筑。
181 00:16:28,724 --> 00:16:29,144 演讲者 SPEAKER_07：非常好。
182 00:16:30,044 --> 00:16:30,605 演讲者 SPEAKER_07：是的。
183 00:16:30,625 --> 00:16:43,201 演讲者 SPEAKER_07：那么，如果，我是说，在一个非常假设的世界里，如果没有坏人，你知道，那些有不良意图的人，我们会安全吗？
184 00:16:45,697 --> 00:16:46,399 演讲者 SPEAKER_05：我不知道。
185 00:16:46,438 --> 00:16:56,965 说话人 SPEAKER_05：在一个人们心怀恶意、政治体系如此破裂以至于我们甚至无法决定不把突击步枪发给青少年男孩的世界里，我们会更安全。
186 00:16:59,049 --> 00:17:01,917 说话人 SPEAKER_05：如果你不能解决这个问题，你将如何解决这个问题？
187 00:17:02,908 --> 00:17:03,789 说话人 SPEAKER_07：嗯，我的意思是，我不知道。
188 00:17:03,809 --> 00:17:05,991 说话人 SPEAKER_07：我本来希望你能有一些想法。
189 00:17:08,354 --> 00:17:16,481 说话人 SPEAKER_07：所以，我的意思是，除非我们在一开始就讲清楚，我的意思是，你想要对此发表意见。
190 00:17:16,903 --> 00:17:23,648 说话人 SPEAKER_07：而且你感觉在没有对谷歌产生任何负面影响的情况下这样做更自在。
191 00:17:24,650 --> 00:17:25,832 说话人 SPEAKER_07：但你确实在发表意见。
192 00:17:26,011 --> 00:17:31,396 说话人 SPEAKER_07：但如果我们没有实际行动，
193 00:17:31,376 --> 00:17:32,439 说话者 SPEAKER_07：动作？
194 00:17:32,459 --> 00:17:33,140 说话者 SPEAKER_07：我们该怎么办？
195 00:17:33,160 --> 00:17:37,167 说话者 SPEAKER_07：我的意思是，当很多人这周都在听你说话时，我们该怎么办？
196 00:17:38,250 --> 00:17:44,603 说话者 SPEAKER_05：我希望它像气候变化一样，如果你有点头脑，你就会停止燃烧碳。
197 00:17:46,326 --> 00:17:47,969 说话者 SPEAKER_05：关于这件事，你应该怎么做很清楚。
198 00:17:48,029 --> 00:17:50,314 说话者 SPEAKER_05：很清楚，这会痛苦，但必须这么做。
199 00:17:50,715 --> 00:17:55,423 说话者 SPEAKER_05：我不知道有哪种解决方案能阻止这些事物取代我们。
200 00:17:55,702 --> 00:17:59,429 说话者 SPEAKER_05：我们真正想要的，我认为我们不会停止开发它们，因为它们非常有用。
201 00:17:59,869 --> 00:18:02,574 说话人 SPEAKER_05：它们在医学和其他所有方面都将非常有用。
202 00:18:04,195 --> 00:18:06,579 说话人 SPEAKER_05：所以我认为停止发展的可能性不大。
203 00:18:07,000 --> 00:18:12,268 说话人 SPEAKER_05：我们想要的是确保即使它们比我们聪明，
204 00:18:12,248 --> 00:18:14,611 说话人 SPEAKER_05：它们也会为我们做有益的事情。
205 00:18:15,011 --> 00:18:16,354 说话人 SPEAKER_05：这就是所谓的对齐问题。
206 00:18:16,874 --> 00:18:24,003 说话人 SPEAKER_05：但是我们需要在一个有坏人想要制造杀人机器人的世界里尝试这样做。
207 00:18:25,066 --> 00:18:26,347 说话人 SPEAKER_05：这对我来说似乎非常困难。
208 00:18:27,048 --> 00:18:30,733 说话人 SPEAKER_05：所以我很抱歉，我在拉响警报，说我们必须担心这个问题。
209 00:18:31,535 --> 00:18:34,199 说话者 SPEAKER_05：我希望有一个简单的解决方案可以推广，但我没有。
210 00:18:34,558 --> 00:18:38,825 说话者 SPEAKER_05：但我认为人们聚在一起认真思考并看看是否有解决方案是非常重要的。
211 00:18:39,065 --> 00:18:40,887 说话者 SPEAKER_05：目前还不清楚是否有解决方案。
212 00:18:41,087 --> 00:18:43,351 说话者 SPEAKER_07：那么，请您谈谈这个问题。
213 00:18:43,371 --> 00:18:49,176 说话者 SPEAKER_07：我的意思是，你把你的职业生涯都花在这项技术的技术细节上了。
214 00:18:49,637 --> 00:18:51,359 说话者 SPEAKER_07：难道没有技术解决方案吗？
215 00:18:51,400 --> 00:19:03,513 说话者 SPEAKER_07：为什么我们不能建立护栏或者让它们学习得更差，或者限制它们沟通的方式，如果这是你论点的两个要点的话？
216 00:19:03,795 --> 00:19:06,758 说话者 SPEAKER_05：我的意思是，我们正在尝试做各种各样的护栏。
217 00:19:07,665 --> 00:19:09,587 说话者 SPEAKER_05：但是假设它们真的变得非常聪明。
218 00:19:10,229 --> 00:19:11,451 说话者 SPEAKER_05：这些事物可以编程，对吧？
219 00:19:11,471 --> 00:19:12,392 说话者 SPEAKER_05：它们可以编写程序。
220 00:19:13,011 --> 00:19:17,258 说话者 SPEAKER_05：假设你赋予它们执行这些程序的能力，我们肯定会这么做。
221 00:19:20,000 --> 00:19:22,103 说话人 SPEAKER_05: 智能事物可以比我们更聪明。
222 00:19:23,925 --> 00:19:33,717 说话人 SPEAKER_05: 想象一下你的两岁孩子说，我爸爸做我不喜欢的事情，所以我打算制定一些规则来限制他可以做什么。
223 00:19:34,657 --> 00:19:37,622 说话人 SPEAKER_05: 你可能能想出一种方法来接受这些规则，同时还能得到你想要的东西。
224 00:19:39,137 --> 00:19:39,357 说话人 SPEAKER_07: 是的。
225 00:19:41,082 --> 00:19:50,558 说话人 SPEAKER_07：但似乎还有一步，这些智能机器似乎有自己的动机。
226 00:19:51,500 --> 00:19:52,682 说话人 SPEAKER_05：是的，这是一个非常好的观点。
227 00:19:52,823 --> 00:19:55,627 说话人 SPEAKER_05：所以我们进化了
228 00:19:55,759 --> 00:20:00,946 说话人 SPEAKER_05：因为我们进化了，所以我们有某些内置的目标，我们觉得很难关闭。
229 00:20:01,827 --> 00:20:05,192 说话者 SPEAKER_05：我们尽量不伤害自己的身体，这就是疼痛的意义所在。
230 00:20:05,231 --> 00:20:09,698 说话者 SPEAKER_05：我们尽量吃得饱饱的，这样我们才能滋养身体。
231 00:20:12,642 --> 00:20:15,826 说话者 SPEAKER_05：我们尽量多复制自己。
232 00:20:16,606 --> 00:20:24,416 说话者 SPEAKER_05：也许并非出于那种故意，但我们已经被编程，复制自己会带来快乐。
233 00:20:25,662 --> 00:20:29,105 说话人 SPEAKER_05：这一切都来自进化，而且我们无法将其关闭是很重要的。
234 00:20:30,727 --> 00:20:34,451 说话人 SPEAKER_05：如果你能将其关闭，你就不太好了。
235 00:20:34,510 --> 00:20:40,217 说话人 SPEAKER_05：有一个叫做震教徒的奇妙团体，他们与教友派有关，制作了精美的家具，但不相信性。
236 00:20:41,479 --> 00:20:43,320 说话人 SPEAKER_05：他们现在都不在了。
237 00:20:46,124 --> 00:20:50,788 说话人 SPEAKER_05：这些数字智能没有进化。
238 00:20:51,128 --> 00:20:51,890 说话人 SPEAKER_05：是我们创造了它们。
239 00:20:52,550 --> 00:20:54,893 说话人 SPEAKER_05：因此，它们没有内置的目标。
240 00:20:55,750 --> 00:21:00,176 说话人 SPEAKER_05：所以问题在于，如果我们能设定目标，可能一切都会好起来。
241 00:21:00,778 --> 00:21:07,469 说话人 SPEAKER_05：但我最大的担忧是，迟早有人会给他们接入创建自己子目标的能力。
242 00:21:08,310 --> 00:21:15,362 说话人 SPEAKER_05：事实上，他们几乎已经做到了，那些调用 ChatGPT 的 ChatGPT 版本。
243 00:21:16,455 --> 00:21:27,628 说话人 SPEAKER_05：如果你给某物以实现其他目标的能力，以实现某些子目标，我认为它很快就会意识到获得更多控制是一个非常好的子目标，因为它有助于实现其他目标。
244 00:21:29,511 --> 00:21:33,476 说话人 SPEAKER_05：如果这些事物沉迷于获得更多控制，我们就麻烦了。
245 00:21:34,917 --> 00:21:38,962 说话人 SPEAKER_07：那么你认为最坏的情况是什么？
246 00:21:40,044 --> 00:21:42,446 说话人 SPEAKER_05：哦，我认为这是完全可能的。
247 00:21:42,730 --> 00:21:46,536 说话人 SPEAKER_05：人类可能只是智能进化过程中的一个过渡阶段。
248 00:21:47,277 --> 00:21:49,359 说话人 SPEAKER_05：您不能直接进化出数字智能。
249 00:21:49,380 --> 00:21:53,145 说话人 SPEAKER_05：这需要太多的能量和太精细的制造。
250 00:21:53,807 --> 00:21:58,694 说话人 SPEAKER_05：你需要生物智能来进化，以便它能创造数字智能。
251 00:21:59,516 --> 00:22:09,632 说话人 SPEAKER_05：数字智能随后可以以相当缓慢的方式吸收人们所写的一切，这正是 ChatGPT 所做的事情。
252 00:22:09,982 --> 00:22:13,967 说话人 SPEAKER_05：但随后它可以开始获得对世界的直接经验，并更快地学习。
253 00:22:15,167 --> 00:22:18,571 说话人 SPEAKER_05：它可能让我们保持一段时间，以维持电站的运行。
254 00:22:19,373 --> 00:22:23,356 说话人 SPEAKER_05：但之后可能就不行了。
255 00:22:23,636 --> 00:22:29,202 说话人 SPEAKER_05：好消息是我们已经找到了如何建造不朽生物的方法。
256 00:22:29,824 --> 00:22:33,948 说话人 SPEAKER_05：所以这些数字智能，当硬件损坏时，它们不会死亡。
257 00:22:34,587 --> 00:22:37,070 说话人 SPEAKER_05：如果你把重量存储在某种介质中，
258 00:22:37,539 --> 00:22:42,327 说话人 SPEAKER_05：并且你能找到另一块可以运行相同指令的硬件，那么你就可以让它再次复活。
259 00:22:44,550 --> 00:22:47,797 说话人 SPEAKER_05：所以我们获得了永生，但这不是为我们准备的。
260 00:22:49,680 --> 00:22:52,523 说话人 SPEAKER_05：所以雷·库兹韦尔对永生非常感兴趣。
261 00:22:53,224 --> 00:22:56,790 说话人 SPEAKER_05: 我认为让老白人永生是一个非常糟糕的想法。
262 00:22:57,672 --> 00:23:01,759 说话人 SPEAKER_05: 我们有了永生，但不是给雷的。
263 00:23:01,991 --> 00:23:08,641 说话人 SPEAKER_07: 不，我的意思是，可怕的是，从某种意义上说，你可能会的，因为你发明了大部分这项技术。
264 00:23:09,481 --> 00:23:16,590 说话人 SPEAKER_07: 我的意思是，当我听到你这么说的时候，我的一部分想现在就跑下台去街上开始拔掉电脑的插头。
265 00:23:19,213 --> 00:23:21,396 说话人 SPEAKER_05：恐怕我们不能这么做。
266 00:23:21,416 --> 00:23:21,616 说话人 SPEAKER_07：为什么？
267 00:23:22,397 --> 00:23:23,819 说话人 SPEAKER_07：你听起来像《2001 太空漫游》里的 HAL。
268 00:23:23,839 --> 00:23:24,320 说话人 SPEAKER_05：没错。
269 00:23:27,545 --> 00:23:27,644 未知说话者：嗯。
270 00:23:29,228 --> 00:23:41,588 说话者 SPEAKER_07：但是，更严肃地说，我知道你之前说过，几个月前有人建议应该对人工智能的发展实施禁令。
271 00:23:41,789 --> 00:23:44,934 说话者 SPEAKER_07：而且我认为这不是一个好主意。
272 00:23:45,015 --> 00:23:48,642 说话者 SPEAKER_07：但更普遍地说，我很想知道为什么。
273 00:23:48,662 --> 00:23:51,185 说话人 SPEAKER_07：我的意思是，我们不应该停止吗？
274 00:23:51,165 --> 00:24:01,184 说话人 SPEAKER_07：我知道，对不起，我正要说我知道你也说过，你用个人财富投资了一些公司，比如 Cohere，这些公司在构建大型语言模型。
275 00:24:01,265 --> 00:24:06,355 说话人 SPEAKER_07：所以我很好奇你个人的责任感以及我们每个人的个人责任感。
276 00:24:06,375 --> 00:24:07,176 说话人 SPEAKER_07：我们应该做什么？
277 00:24:07,857 --> 00:24:10,001 演讲者 SPEAKER_07：我的意思是，我们应该尝试阻止吗？这就是我想说的。
278 00:24:10,740 --> 00:24:19,194 演讲者 SPEAKER_05：是的，所以我认为如果你认真对待这种存在风险，就像我现在这样，我以前认为这很遥远，但现在我认为这是严重的，并且相当紧迫。
279 00:24:20,336 --> 00:24:24,262 演讲者 SPEAKER_05：可能完全有道理停止进一步开发这些事物。
280 00:24:25,164 --> 00:24:27,888 演讲者 SPEAKER_05：但我认为认为这会发生是完全天真的。
281 00:24:28,710 --> 00:24:30,353 说话人 SPEAKER_05：这是不可能发生的。
282 00:24:31,210 --> 00:24:37,060 说话人 SPEAKER_05：原因之一是，我的意思是，如果美国停止发展而中国不停止，它们将被用于武器。
283 00:24:37,781 --> 00:24:41,650 说话人 SPEAKER_05：仅就这一点而言，政府是不会停止发展的。
284 00:24:41,670 --> 00:24:46,519 说话人 SPEAKER_05：所以，我认为停止发展它们可能是一件理性的事情。
285 00:24:46,718 --> 00:24:48,319 说话人 SPEAKER_05：但这是不可能发生的。
286 00:24:48,359 --> 00:24:50,821 说话人 SPEAKER_05：所以签署请愿书说“请现在就停止”是愚蠢的。
287 00:24:51,403 --> 00:24:52,423 说话人 SPEAKER_05：我们确实有过假期。
288 00:24:52,743 --> 00:24:59,270 说话人 SPEAKER_05：从大约 2017 年开始，我们度过了几年的假期，因为谷歌首先开发了这项技术。
289 00:24:59,651 --> 00:25:01,292 说话人 SPEAKER_05：它开发了 Transformer。
290 00:25:01,313 --> 00:25:02,753 说话人 SPEAKER_05：它还开发了融合模型。
291 00:25:03,494 --> 00:25:07,018 说话人 SPEAKER_05：它没有将它们公之于众供人们使用和滥用。
292 00:25:07,578 --> 00:25:10,221 说话人 SPEAKER_05：它对它们非常小心，因为它不想损害自己的声誉。
293 00:25:10,260 --> 00:25:12,844 说话者 SPEAKER_05：它知道可能会有不良后果。
294 00:25:12,824 --> 00:25:15,347 说话者 SPEAKER_05：但这只能发生在有一个单一领导者的情况下。
295 00:25:16,170 --> 00:25:29,594 说话者 SPEAKER_05：一旦 OpenAI 使用 Transformer 和微软的资金构建了类似的东西，并且微软决定将其推出，谷歌实际上没有太多选择。
296 00:25:29,733 --> 00:25:36,486 说话者 SPEAKER_05：如果你要生活在一个资本主义体系中，你不能阻止谷歌与微软竞争。
297 00:25:37,022 --> 00:25:38,423 说话人 SPEAKER_05：我认为谷歌没有做错什么。
298 00:25:38,443 --> 00:25:40,006 说话人 SPEAKER_05：我认为一开始就非常负责任。
299 00:25:40,467 --> 00:25:47,794 说话人 SPEAKER_05：但在资本主义体系或像美国和中国这样的国家之间竞争的体系中，这种东西的发展是不可避免的。
300 00:25:49,536 --> 00:25:54,781 说话人 SPEAKER_05：我的一个希望是，如果我们允许它接管，那对我们所有人来说都是坏事。
301 00:25:55,442 --> 00:25:59,365 说话人 SPEAKER_05：我们可以像对待核武器那样，让美国和中国达成一致，这对我们大家都不好。
302 00:26:00,047 --> 00:26:02,809 说话人 SPEAKER_05：在生存威胁方面，我们都在同一条船上。
303 00:26:02,829 --> 00:26:06,574 说话人 SPEAKER_05：因此，我们都应该能够合作阻止它。
304 00:26:06,622 --> 00:26:08,503 说话人 SPEAKER_07：只要我们能在路上赚点钱。
305 00:26:09,265 --> 00:26:13,470 说话人 SPEAKER_07：我将从房间里收集一些观众的问题，如果您愿意，请自我介绍一下。
306 00:26:13,710 --> 00:26:18,454 说话人 SPEAKER_07：当人们拿着麦克风四处走动时，我有一个问题想从在线观众那里提问。
307 00:26:19,296 --> 00:26:26,584 说话人 SPEAKER_07：我的意思是，您提到过，随着机器变得更聪明，超越人类，可能会出现一个过渡期。
308 00:26:26,604 --> 00:26:31,529 说话人 SPEAKER_07：我的意思是，是否会有一个时刻，很难定义什么是人类，什么不是？
309 00:26:31,670 --> 00:26:34,333 说话人 SPEAKER_07：这两种智能是不是非常不同？
310 00:26:35,275 --> 00:26:37,156 说话人 SPEAKER_05：我认为它们是不同的智能形式。
311 00:26:37,897 --> 00:26:44,467 说话人 SPEAKER_05：当然，数字智能非常擅长模仿我们，因为它们被训练来模仿我们。
312 00:26:45,909 --> 00:26:52,557 说话人 SPEAKER_05：所以我们很难判断是 chatGBT 写的还是我们写的。
313 00:26:52,978 --> 00:26:56,083 说话者 SPEAKER_05：从这个意义上说，它们看起来很像我们，但内部运作方式不同。
314 00:26:58,144 --> 00:26:59,426 说话者 SPEAKER_07：谁是房间里的第一个？
315 00:27:02,173 --> 00:27:05,057 说话者 SPEAKER_04：你好，我叫 Hal Gregerson，我的中间名不是 9000。
316 00:27:06,039 --> 00:27:10,185 说话者 SPEAKER_04：我是麻省理工学院斯隆商学院的教员。
317 00:27:11,307 --> 00:27:15,914 说话者 SPEAKER_04：可以说，提问是我们拥有的最重要的能力之一。
318 00:27:17,438 --> 00:27:25,329 说话者 SPEAKER_04：从您的角度来看，现在 2023 年，我们应该关注哪些问题或两个问题？
319 00:27:26,473 --> 00:27:35,606 说话者 SPEAKER_04：这些技术是否真的能帮助我们提出更好的问题，并超越技术？
320 00:27:37,851 --> 00:27:45,642 说话者 SPEAKER_05：是的，但我想要说的是，我们应该问很多问题，但其中一个问题是，我们如何防止它们接管？
321 00:27:45,662 --> 00:27:47,265 说话人 SPEAKER_05：我们如何防止他们获得控制权？
322 00:27:48,326 --> 00:27:54,996 说话人 SPEAKER_05：我们可以就这个问题问他们，但我不会完全相信他们的回答。
323 00:27:56,932 --> 00:28:02,584 说话人 SPEAKER_07：后面有提问，我想尽可能多回答一些，所以如果你们的问题尽可能简短的话。
324 00:28:05,108 --> 00:28:08,576 说话人 SPEAKER_03：Hinton 博士，非常感谢您今天与我们在这里。
325 00:28:09,337 --> 00:28:15,329 讲座者 SPEAKER_03：我得说这是我付过最贵的讲座，但我觉得物有所值。
326 00:28:17,317 --> 00:28:23,226 讲座者 SPEAKER_03：我有个问题想问你，因为你提到了核历史的类比。
327 00:28:23,865 --> 00:28:26,269 讲座者 SPEAKER_03：显然，有很多比较。
328 00:28:26,289 --> 00:28:34,519 讲座者 SPEAKER_03：你记得杜鲁门总统在椭圆形办公室对奥本海默说过什么吗？
329 00:28:34,539 --> 00:28:35,060 说话人 SPEAKER_05：不，我不知道。
330 00:28:35,141 --> 00:28:36,362 说话人 SPEAKER_05：我知道一些关于那个的事情。
331 00:28:37,463 --> 00:28:40,347 说话人 SPEAKER_05：但我不知道杜鲁门告诉奥本海默了什么。
332 00:28:40,367 --> 00:28:40,768 说话人 SPEAKER_03：谢谢您。
333 00:28:41,108 --> 00:28:41,729 说话人 SPEAKER_03：我们接着来。
334 00:28:43,432 --> 00:28:45,334 说话人 SPEAKER_07：下一个观众问题。
335 00:28:47,557 --> 00:28:51,903 说话人 SPEAKER_07：抱歉，如果麦克风上的人能告诉我下一位是谁，也许可以给个... 继续说。
336 00:28:52,304 --> 00:28:53,645 说话人 SPEAKER_06：你好，雅各布·伍德鲁夫。
337 00:28:54,787 --> 00:29:03,959 说话人 SPEAKER_06：为了训练这些大型语言模型所需的数据量，我们是否预期这些系统的智能将出现平台期？
338 00:29:04,880 --> 00:29:08,924 说话人 SPEAKER_06：这可能会减缓或限制其进步吗？
339 00:29:09,884 --> 00:29:15,171 说话人 SPEAKER_05：好的，这是一个希望，也许我们已经用尽了所有人类知识，它们不会再变得更聪明。
340 00:29:15,791 --> 00:29:18,536 说话人 SPEAKER_05：但想想图像和视频。
341 00:29:19,857 --> 00:29:26,666 说话人 SPEAKER_05：所以多模态模型将比仅训练语言模型的模型更聪明。
342 00:29:26,686 --> 00:29:29,651 说话人 SPEAKER_05：例如，它们将更好地了解如何处理空间。
343 00:29:30,813 --> 00:29:38,202 说话人 SPEAKER_05：至于视频总量，我们仍然没有很好的方法来处理这些模型中的视频。
344 00:29:38,436 --> 00:29:39,519 说话人 SPEAKER_05：关于视频建模。
345 00:29:40,000 --> 00:29:41,102 说话人 SPEAKER_05：我们一直在变得更好。
346 00:29:41,682 --> 00:29:46,132 说话人 SPEAKER_05：但我认为在视频等事物中有很多数据可以告诉你世界是如何运作的。
347 00:29:46,833 --> 00:29:50,420 说话人 SPEAKER_05：所以我们还没有达到多模态模型的数据限制。
348 00:29:53,026 --> 00:29:54,407 说话人 SPEAKER_07：请下一位坐在后面的先生。
349 00:29:55,049 --> 00:29:56,833 说话人 SPEAKER_07: 请务必把问题说短一些。
350 00:29:56,897 --> 00:29:59,760 说话人 SPEAKER_01: 你好，Hindle 博士，我是来自 PwC 的 Rajiv Sevarwal。
351 00:30:00,461 --> 00:30:07,871 说话人 SPEAKER_01: 我想弄清楚的是，AI 所做的一切都是在我们教他们的基础上学习的，好吗？
352 00:30:07,891 --> 00:30:15,039 说话人 SPEAKER_01: 数据，是的，它们在如何学习 1 万亿个连接器比我们拥有的 100 万亿个连接器能做更多方面更快。
353 00:30:15,539 --> 00:30:19,345 说话人 SPEAKER_01：但是人类进化的每一部分都是由
354 00:30:19,325 --> 00:30:24,930 说话人 SPEAKER_01：思想实验驱动的，就像爱因斯坦曾经做过的思想实验，因为在这个星球上没有光速。
355 00:30:25,510 --> 00:30:34,298 说话人 SPEAKER_01：AI 如何达到那个点，如果可能的话，如果不能，那么我们如何可能面临来自它们的生存威胁，因为它们不会是自我学习的，也就是说。
356 00:30:34,880 --> 00:30:38,123 说话人 SPEAKER_01：它们的自我学习将局限于我们告诉它们的模型。
357 00:30:39,364 --> 00:30:45,509 说话人 SPEAKER_05：我认为这是一个非常有趣的论点，但我认为他们能够进行思想实验。
358 00:30:45,549 --> 00:30:46,671 说话人 SPEAKER_05：我认为他们能够进行推理。
359 00:30:47,070 --> 00:30:48,893 说话人 SPEAKER_05：那么让我给你举一个类比。
360 00:30:49,160 --> 00:30:55,327 说话人 SPEAKER_05：如果你看 AlphaZero，它会下棋，它有三个要素。
361 00:30:56,128 --> 00:30:59,773 说话者 SPEAKER_05：它有一种评估棋盘位置的功能，来判断这是否对我有利？
362 00:30:59,814 --> 00:31:04,760 说话者 SPEAKER_05：它有一种观察棋盘位置的功能，并说，考虑哪种走法是合理的？
363 00:31:05,801 --> 00:31:12,230 说话者 SPEAKER_05：然后它有蒙特卡洛滚动，它进行所谓的计算，即你想象，如果我走这里，他走那里，我再走这里，他再走那里。
364 00:31:13,173 --> 00:31:26,342 说话者 SPEAKER_05：现在，假设你省略了蒙特卡洛滚动，只从人类专家那里训练它，使其拥有良好的评估函数和选择考虑的走法的良好方式，它仍然能下出相当不错的棋。
365 00:31:27,144 --> 00:31:29,489 说话人 SPEAKER_05：我认为这就是我们得到的聊天机器人。
366 00:31:30,652 --> 00:31:32,936 说话人 SPEAKER_05：我们还没有让它们进行内部推理。
367 00:31:33,997 --> 00:31:34,938 说话人 SPEAKER_05：但这将会到来。
368 00:31:35,479 --> 00:31:43,807 说话者 SPEAKER_05：一旦它们开始进行内部推理来检查它们所相信的不同事物之间的一致性，那么它们就会变得更聪明，并且能够进行思想实验。
369 00:31:44,548 --> 00:31:52,537 说话者 SPEAKER_05：它们没有这种内部推理的一个原因是因为它们被训练在不一致的数据上。
370 00:31:53,478 --> 00:31:58,282 说话者 SPEAKER_05：因此，对于它们来说，进行推理非常困难，因为它们被训练在所有这些不一致的信念上。
371 00:31:58,516 --> 00:32:05,890 说话者 SPEAKER_05：我认为它们必须经过训练，这样它们才会说，如果你有这种意识形态，那么这是真的。
372 00:32:05,930 --> 00:32:07,794 说话者 SPEAKER_05：如果我有那种意识形态，那么那就是真的。
373 00:32:08,173 --> 00:32:12,422 说话者 SPEAKER_05：一旦他们那样训练，在意识形态中，他们将能够尝试获得一致性。
374 00:32:13,183 --> 00:32:17,992 说话者 SPEAKER_05：因此，我们将从只有猜测好走法的版本 AlphaZero，到一个拥有
375 00:32:18,192 --> 00:32:28,859 说话者 SPEAKER_05：某种东西，它猜测好的走法，并评估位置，到一个拥有长链蒙特卡洛滚动的版本，这是推理的角落，它将变得更好。
376 00:32:30,140 --> 00:32:35,390 说话人 SPEAKER_07：我在前面拍一张，如果你能快点，我们再挤一张也行。
377 00:32:35,410 --> 00:32:37,693 说话人 SPEAKER_02：刘易斯·兰姆，杰夫，我认识你们很久了。
378 00:32:38,194 --> 00:32:51,439 说话人 SPEAKER_02：杰夫，人们批评语言模型，因为据说它们缺乏语义和与现实世界的关联，而你长期以来一直在尝试解释神经网络是如何工作的。
379 00:32:51,419 --> 00:33:03,375 说话人 SPEAKER_02：语义和可解释性问题在这里是否相关，或者语言模型已经接管，我们现在注定要没有语义和现实世界的关联地继续前进吗？
380 00:33:04,836 --> 00:33:15,991 说话人 SPEAKER_05：我觉得很难相信它们没有语义，因为它们能解决像我如何在两年内把家里的所有房间都漆成白色这样的问题。
381 00:33:16,208 --> 00:33:19,594 说话人 SPEAKER_05：我的意思是，无论什么语义，都是关于这些东西的意义。
382 00:33:20,355 --> 00:33:22,098 说话人 SPEAKER_05：它理解了意义。
383 00:33:22,159 --> 00:33:22,640 说话人 SPEAKER_05：它明白了。
384 00:33:23,000 --> 00:33:27,689 说话人 SPEAKER_05：现在，我同意它不是一个基于机器人的实体。
385 00:33:28,089 --> 00:33:30,575 说话人 SPEAKER_05：但是你可以制作出基于现实世界的多模态机器人。
386 00:33:30,714 --> 00:33:31,276 说话人 SPEAKER_05：谷歌已经做到了这一点。
387 00:33:31,977 --> 00:33:36,546 说话人 SPEAKER_05：对于这些基于现实世界的多模态机器人，你可以这样说，请关闭抽屉。
388 00:33:36,526 --> 00:33:38,909 说话者 SPEAKER_05：他们伸手抓住把手并关上抽屉。
389 00:33:39,449 --> 00:33:41,530 说话者 SPEAKER_05：很难说这没有语义。
390 00:33:41,892 --> 00:34:03,174 说话者 SPEAKER_05：实际上，在人工智能的早期，在 20 世纪 70 年代的 Winograd 时代，他们只有一个模拟世界，但他们有所谓的程序语义，如果你对它说，把红色积木放在绿色盒子里，它就会把红色积木放在绿色盒子里，你就会说，看，它理解了语言。
391 00:34:03,931 --> 00:34:05,923 说话者 SPEAKER_05：这就是当时人们使用的标准。
392 00:34:05,983 --> 00:34:10,110 说话人 SPEAKER_05：但是现在神经网络可以做到这一点，他们说这不是一个充分的准则。
393 00:34:11,963 --> 00:34:12,443 说话人 SPEAKER_00：后面的一位。
394 00:34:13,786 --> 00:34:16,670 说话人 SPEAKER_00：嘿，杰夫，我是来自 SAI 集团的 Ishwar Balani。
395 00:34:16,690 --> 00:34:21,076 说话人 SPEAKER_00：所以很明显，你知道，技术正在以指数级的速度发展。
396 00:34:21,978 --> 00:34:37,701 说话者 SPEAKER_00：我想听听您的看法，如果您从近中期来看，比如说一到三年，甚至五年的展望，从社会和经济的角度来看，失业或可能创造新的工作会有什么影响。
397 00:34:37,760 --> 00:34:41,346 说话者 SPEAKER_00：只是想听听您的看法，我们应该如何进行。
398 00:34:41,326 --> 00:34:43,728 说话者 SPEAKER_00：考虑到技术的现状和变化速度？
399 00:34:44,670 --> 00:34:50,777 说话者 SPEAKER_05：是的，我敲响的警钟是关于它们夺取控制权的生存威胁。
400 00:34:51,458 --> 00:35:03,132 很多人谈论过这个问题，我不认为自己是这方面的专家，但有一些非常明显的事情将会使许多工作变得更加高效。
401 00:35:03,331 --> 00:35:16,349 我认识一个人，他以前需要 25 分钟来写一封投诉信，现在他只需要 5 分钟，因为他把信交给 CHAT-GPT，CHAT-GPT 帮他写，然后他只需检查一下。
402 00:35:16,909 --> 00:35:21,576 将会有很多类似的事情，这将导致生产力的巨大提升。
403 00:35:22,079 --> 00:35:25,943 由于人们非常保守地采用新技术，因此将会出现一些延误。
404 00:35:26,182 --> 00:35:28,266 说话人 SPEAKER_05：但我认为生产力将会有巨大的增长。
405 00:35:29,126 --> 00:35:36,014 说话人 SPEAKER_05：我的担忧是，这些生产力的增长将会导致人们失业，让富人更富，穷人更穷。
406 00:35:37,034 --> 00:35:42,000 说话人 SPEAKER_05：当你这样做，当你扩大这个差距时，社会将变得越来越暴力。
407 00:35:42,581 --> 00:35:49,047 说话人 SPEAKER_05：这个被称为基尼系数的东西，可以很好地预测暴力程度。
408 00:35:49,719 --> 00:36:02,757 这项技术本应美好，你知道，即使是技术用于做有益的事情，也应该是美好的，但在我们目前的政治体系中，它将被用来让富人更富，穷人更穷。
409 00:36:03,918 --> 00:36:10,969 你可能可以通过实行一种每个人都能获得的基本收入来改善这种情况，但
410 00:36:12,822 --> 00:36:22,175 技术正在被开发在一个不是为了所有人的利益而使用它的社会中。
411 00:36:24,518 --> 00:36:29,746 这里有一个来自全球邮报的 Joe Costaldo 的问题，他在听众席上。
412 00:36:30,387 --> 00:36:33,512 说话人 SPEAKER_07：你打算继续持有你在 Cahir 和其他公司的投资吗？
413 00:36:33,773 --> 00:36:38,340 说话人 SPEAKER_07：如果是这样，为什么？
414 00:36:38,994 --> 00:36:46,025 说话人 SPEAKER_05：嗯，我可以把这笔钱存入银行，让他们从中获利。
415 00:36:49,128 --> 00:36:53,936 说话人 SPEAKER_05：是的，我打算继续持有我在 Cohere 的投资，部分原因是因为 Cohere 的管理者是我的朋友。
416 00:36:56,659 --> 00:37:01,686 说话人 SPEAKER_05：我多少相信这些大型语言模型将会非常有帮助。
417 00:37:03,009 --> 00:37:05,492 说话人 SPEAKER_05：我认为这项技术
418 00:37:05,793 --> 00:37:09,157 说话人 SPEAKER_05：应该是好的，并且它应该使事物运作得更好。
419 00:37:10,539 --> 00:37:14,065 说话人 SPEAKER_05：我们需要解决的是政治问题，比如就业问题。
420 00:37:16,208 --> 00:37:20,893 说话者 SPEAKER_05：但是，当谈到生存威胁时，我们必须考虑如何保持对技术的控制。
421 00:37:21,614 --> 00:37:25,621 说话者 SPEAKER_05：但好消息是，我们都在同一条船上，所以我们可能能够得到合作。
422 00:37:26,422 --> 00:37:34,432 说话者 SPEAKER_07：在发表意见时，我的意思是，根据我的理解，你实际上想与制造这项技术的人进行交流，你知道，
423 00:37:35,172 --> 00:37:40,981 说话者 SPEAKER_07：改变他们的想法，或者可能为他们辩护，我不知道。
424 00:37:41,001 --> 00:37:45,688 说话人 SPEAKER_07：我们的确不知道该怎么办，但这是关于参与而不是退缩。
425 00:37:46,811 --> 00:37:51,679 说话人 SPEAKER_05：所以，让我离开谷歌并公开谈论这件事的原因之一是
426 00:37:51,760 --> 00:38:02,197 说话人 SPEAKER_05：他曾经是一名助理教授，但现在是一名中级教授，我非常尊重他，他鼓励我这么做。
427 00:38:02,217 --> 00:38:05,083 说话人 SPEAKER_05：他说，杰夫，你需要公开发声，让人们听到你的声音。
428 00:38:05,344 --> 00:38:08,027 说话人 SPEAKER_05：人们对这个危险视而不见。
429 00:38:09,992 --> 00:38:13,498 说话人 SPEAKER_05：我认为现在人们都在倾听。
430 00:38:13,849 --> 00:38:17,378 说话人 SPEAKER_07：是的，不，我认为在这个房间里每个人都开始倾听。
431 00:38:18,641 --> 00:38:25,378 说话人 SPEAKER_07：最后一个问题，时间不多了，但你后悔参与其中吗？
432 00:38:25,831 --> 00:38:29,496 说话人 SPEAKER_05：CaveMets 非常努力地让我说出我有遗憾。
433 00:38:29,715 --> 00:38:31,458 说话人 SPEAKER_07：CaveMets 来自《纽约时报》。
434 00:38:31,980 --> 00:38:32,199 说话人 SPEAKER_05：是的。
435 00:38:32,840 --> 00:38:39,130 说话人 SPEAKER_05：最后，我说，也许有点遗憾，这被报道为有遗憾。
436 00:38:40,811 --> 00:38:43,815 说话人 SPEAKER_05：我认为我在进行研究时没有做出任何错误的决策。
437 00:38:43,916 --> 00:38:49,344 说话人 SPEAKER_05：我认为在 70 年代和 80 年代进行如何制造人工神经网络的研究是完全合理的。
438 00:38:50,865 --> 00:38:52,047 说话人 SPEAKER_05：这并不是可以预见的。
439 00:38:52,487 --> 00:38:54,471 说话人 SPEAKER_05：这个阶段并不是可以预见的。
440 00:38:54,451 --> 00:38:59,643 说话人 SPEAKER_05：直到最近，我还认为这个生存危机离我们还很远。
441 00:38:59,664 --> 00:39:01,949 说话人 SPEAKER_05：所以，我对我所做的事情并不后悔。
442 00:39:04,376 --> 00:39:05,338 说话人 SPEAKER_07：谢谢，杰弗里。
443 00:39:05,438 --> 00:39:06,722 说话人 SPEAKER_07：非常感谢您加入我们。

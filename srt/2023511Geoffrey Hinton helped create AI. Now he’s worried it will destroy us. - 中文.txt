1 00:00:02,021 --> 00:00:08,814 说话人 SPEAKER_03：杰弗里·辛顿，有时被称为人工智能之父，正在向世界解释其风险。
2 00:00:09,294 --> 00:00:11,378 说话人 SPEAKER_01：这是我从推特上收到的。
3 00:00:11,398 --> 00:00:16,588 说话人 SPEAKER_03：这意味着解释一些非常真实但听起来却完全像是虚构的事情。
4 00:00:16,608 --> 00:00:16,989 说话人 SPEAKER_00：我迷路了。
5 00:00:17,169 --> 00:00:17,649 说话人 SPEAKER_00: 我不知道。
6 00:00:18,390 --> 00:00:22,739 说话人 SPEAKER_03: 他渴望传达紧迫性，看看谁可能提供这样的机会。
7 00:00:23,179 --> 00:00:24,402 说话人 SPEAKER_03: Snoop Dogg。
8 00:00:24,382 --> 00:00:28,106 说话人 SPEAKER_00: 这让我震惊，因为我小时候就是看这个电影长大的。
9 00:00:28,126 --> 00:00:34,210 说话人 SPEAKER_00：我听说那个老兄，就是创造了人工智能的那个老兄说，这东西不安全，因为人工智能有自己的思想。
10 00:00:34,731 --> 00:00:37,094 说话人 SPEAKER_00：这些家伙要开始自己干自己的事情了。
11 00:00:37,113 --> 00:00:39,256 说话人 SPEAKER_00：我当时就想，咱们现在是不是在拍电影啊？
12 00:00:39,276 --> 00:00:42,058 说话人 SPEAKER_00：操，这什么情况？
13 00:00:42,378 --> 00:00:43,060 说话人 SPEAKER_00: 你们知道吗？
14 00:00:44,381 --> 00:00:45,502 说话人 SPEAKER_03: 人们就是这样说的。
15 00:00:45,521 --> 00:00:46,682 说话人 SPEAKER_03: 他问，我们遇到麻烦了吗？
16 00:00:48,104 --> 00:00:48,765 说话人 SPEAKER_01: 他明白了。
17 00:00:49,526 --> 00:00:50,207 说话人 SPEAKER_01: 他很聪明。
18 00:00:50,807 --> 00:00:52,548 说话人 SPEAKER_03: 你知道吗，如果他叫你老兄呢？
19 00:00:53,220 --> 00:00:57,244 说话人 SPEAKER_01: 不，我现在就自称老兄了。
20 00:00:57,265 --> 00:01:00,969 说话人 SPEAKER_03: 想象一下你成年后整个一生都在努力创造一个更美好的未来。
21 00:01:01,469 --> 00:01:07,816 说话人 SPEAKER_01：我认为我们现有的学习方法将对许多行业产生巨大影响，并解决许多问题。
22 00:01:07,837 --> 00:01:11,361 说话人 SPEAKER_03：计算机和机器学习使人类生活更美好。
23 00:01:11,881 --> 00:01:15,805 说话人 SPEAKER_03：这些聊天机器人可以回答复杂问题，撰写电子邮件和演讲稿。
24 00:01:16,346 --> 00:01:22,873 说话人 SPEAKER_03：然后意识到创造即将达到一个可能造成无法挽回伤害的点。
25 00:01:24,287 --> 00:01:28,694 讲者 SPEAKER_03：这就是当 Hinton 在谷歌工作时产生的认识。
26 00:01:28,713 --> 00:01:33,540 讲者 SPEAKER_03：由于他的工作，他不能说话，所以他辞去了工作，现在他可以说话了。
27 00:01:34,662 --> 00:01:36,603 讲者 SPEAKER_03：我在前几天在伦敦见到了他。
28 00:01:37,926 --> 00:01:42,731 讲者 SPEAKER_03：我认为人们最终都渴望听到你的声音，同时也有些害怕。
29 00:01:42,751 --> 00:01:43,393 说话人 SPEAKER_03：它们应该吗？
30 00:01:43,862 --> 00:01:46,126 说话人 SPEAKER_01：我认为有些事情需要担心。
31 00:01:46,146 --> 00:01:57,721 说话人 SPEAKER_01：大家知道的正常事情都有，但还有一种威胁与那些不同，那就是如果我们制造出比我们更智能的东西，我们怎么知道我们能保持控制？
32 00:01:58,001 --> 00:02:07,575 说话人 SPEAKER_01：当……好吧，如果我们谈论进化，所有这些物种都在进化，通常发生的情况是对智力较低物种不利。
33 00:02:08,212 --> 00:02:08,993 说话人 SPEAKER_02：另一个杀掉它？
34 00:02:09,275 --> 00:02:10,216 说话人 SPEAKER_01：不一定。
35 00:02:10,836 --> 00:02:13,722 说话人 SPEAKER_01：蚂蚁照顾蚜虫，因为它们能产蜜。
36 00:02:14,623 --> 00:02:16,766 说话人 SPEAKER_01：蚂蚁负责吗？
37 00:02:17,328 --> 00:02:18,389 说话人 SPEAKER_01：蚂蚁负责，是的。
38 00:02:19,792 --> 00:02:25,040 说话人 SPEAKER_03：在这个类比中，如果还不够令人不安的话，蚂蚁不是人类。
39 00:02:27,956 --> 00:02:35,052 说话人 SPEAKER_01：这让我意识到，这些数字智能拥有我们不具备的某些东西，这使得它们更加出色。
40 00:02:35,152 --> 00:02:40,283 说话人 SPEAKER_01：当其中之一知道某事时，它可以告诉所有的其他，这是我们人类所不具备的。
41 00:02:40,846 --> 00:02:46,437 说话人 SPEAKER_01：想象一下，如果你有 1 万人，想象一下当一个人学到一些东西时，所有人都能知道。
42 00:02:47,076 --> 00:02:48,800 说话人 SPEAKER_01：这样你就能学到更多的东西，对吧？
43 00:02:48,939 --> 00:02:54,490 说话人 SPEAKER_01：这就是为什么 ChatGPT 知道的东西比任何一个人多 10 倍。
44 00:02:54,911 --> 00:03:07,192 说话人 SPEAKER_01：这是因为当你训练它时，有大量的副本在查看不同的数据片段并学习，它们可以立即以数万亿比特的带宽将所学到的知识结合起来。
45 00:03:08,014 --> 00:03:09,776 说话人 SPEAKER_01：那么它们能思考吗？
46 00:03:09,907 --> 00:03:10,288 说话人 SPEAKER_01：是的。
47 00:03:10,908 --> 00:03:12,449 说话人 SPEAKER_01：那么，想象以下场景。
48 00:03:12,530 --> 00:03:16,774 说话人 SPEAKER_01：我在和聊天机器人交谈，我们聊了一会儿，它给我的回答让我觉得有点奇怪。
49 00:03:17,093 --> 00:03:20,097 说话人 SPEAKER_01：我突然意识到它认为我是一个少女。
50 00:03:20,497 --> 00:03:23,120 说话人 SPEAKER_01：我说，你认为我是哪个年龄段的人？
51 00:03:23,759 --> 00:03:25,300 说话人 SPEAKER_01：它说它认为我是一个少女。
52 00:03:27,643 --> 00:03:36,010 说话人 SPEAKER_01：所以问题是，当我突然意识到它认为我是一个少女时，这是不是对“认为”这个词的隐喻使用？
53 00:03:36,031 --> 00:03:38,633 说话人 SPEAKER_01：那是不是就像我们思考一样？
54 00:03:38,866 --> 00:03:46,718 说话人 SPEAKER_01：我坚信，当我提到“它认为我是一个少女”时，使用“思考”这个词的方式和我们用来描述人的方式完全一样。
55 00:03:46,979 --> 00:03:53,390 说话人 SPEAKER_03：这就足以让你说，什么，这已经超出了我的舒适区？
56 00:03:54,051 --> 00:03:59,221 说话人 SPEAKER_01：我突然意识到，也许它们已经足够好了，让它们更像真实神经网络并不是关键。
57 00:03:59,580 --> 00:04:01,144 说话人 SPEAKER_01：它们已经比我们更优秀了。
58 00:04:01,816 --> 00:04:03,719 说话人 SPEAKER_01：它们是学习的一种更好的方式。
59 00:04:04,140 --> 00:04:07,187 说话人 SPEAKER_01：如果我们让它们变得更大，它们会比我们更聪明。
60 00:04:07,388 --> 00:04:09,412 说话人 SPEAKER_01：它们已经比任何一个人知道的都多。
61 00:04:10,112 --> 00:04:20,634 说话人 SPEAKER_03：我明白事情可能会出错，但我仍然认为人们听到危险的概念，就会将其视为夸张。
62 00:04:21,526 --> 00:04:24,970 说话人 SPEAKER_01：我过去认为这是夸张，因为我以为这些事情还离得很远。
63 00:04:25,310 --> 00:04:34,903 说话人 SPEAKER_01：我认为最终会有危险，但我认为现在关注它是多余的，因为这些事情在 30 到 50 年后才会比我们更聪明。
64 00:04:35,764 --> 00:04:48,040 说话人 SPEAKER_01：但是，这种意识到它们可能拥有比我们更好的学习方法，因为它们可以即时分享知识，以及看到像 ChatGPT 或谷歌的 Palm 这样的东西，
65 00:04:48,677 --> 00:04:58,351 说话人 SPEAKER_01：能够解释为什么一个笑话好笑，这让我意识到这些事物已经很智能了，如果它们拥有比我们更好的智能形式，那么这就会变得更为紧迫。
66 00:04:59,754 --> 00:05:02,639 说话人 SPEAKER_03：可能仍然难以看到威胁，对吧？
67 00:05:02,718 --> 00:05:03,699 说话人 SPEAKER_03：一些变化是明显的。
68 00:05:04,461 --> 00:05:12,372 说话人 SPEAKER_03：例如，随着 Chat GPT 变得越来越智能，随着 AI 的进步，是的，一些工作将消失，一些可能会转变。
69 00:05:15,964 --> 00:05:17,146 说话人 SPEAKER_03：有好处。
70 00:05:17,185 --> 00:05:21,851 说话人 SPEAKER_03：例如，一个 AI 医生可能拥有来自数千万患者的数据。
71 00:05:21,891 --> 00:05:24,935 说话人 SPEAKER_03：所以比实际人类拥有更多的知识。
72 00:05:25,036 --> 00:05:32,483 说话人 SPEAKER_03：但如果这台机器，这个 AI 医生，停止向恢复可能性低的患者推荐治疗方案怎么办？
73 00:05:34,247 --> 00:05:36,048 说话人 SPEAKER_03：这种情况也适用于人类。
74 00:05:36,168 --> 00:05:43,137 说话人 SPEAKER_03：但是随着机器学习和超越人类学习，困扰我们的是那些意想不到的后果。
75 00:05:45,596 --> 00:05:49,483 说话人 SPEAKER_03：我们能赋予这些机器道德准则，道德规范吗？
76 00:05:50,103 --> 00:05:52,226 说话人 SPEAKER_01：你不能杀人，你不能伤害人。
77 00:05:52,286 --> 00:06:01,560 说话人 SPEAKER_01：如果能做到那当然很好，但请记住，这些机器的主要开发者之一是国防部门。
78 00:06:02,302 --> 00:06:09,351 说话人 SPEAKER_01：至于国防部门……我想艾萨克·阿西莫夫说过，如果你制造一个智能机器人，第一条规则应该是，不要伤害人类。
79 00:06:10,653 --> 00:06:15,081 说话人 SPEAKER_01：我认为这不会是国防部门生产的机器人士兵的第一条规则。
80 00:06:15,836 --> 00:06:16,117 说话人 SPEAKER_03：没错。
81 00:06:17,180 --> 00:06:21,108 说话人 SPEAKER_03：但是我们不能给他们一种语言，让他们自我监管吗？
82 00:06:22,069 --> 00:06:24,053 说话人 SPEAKER_01：当事情自我监管时，结果如何？
83 00:06:25,377 --> 00:06:28,362 说话人 SPEAKER_01：不怎么样。
84 00:06:28,382 --> 00:06:30,346 说话人 SPEAKER_03：你的思绪在这个对话中飘向了哪里？
85 00:06:30,427 --> 00:06:34,776 说话人 SPEAKER_03：它会不会走向那个威胁人类过去的可怕之地？
86 00:06:34,976 --> 00:06:36,579 说话人 SPEAKER_03：比如核弹。
87 00:06:38,617 --> 00:06:41,500 说话人 SPEAKER_03：这不是一个坏的例子，因为它太可怕了。
88 00:06:41,560 --> 00:06:44,904 说话人 SPEAKER_03：那种恐惧激发了一种全球团结。
89 00:06:45,324 --> 00:06:48,348 讲述者 SPEAKER_03：至今为止遏制威胁的条约。
90 00:06:48,908 --> 00:06:50,951 讲述者 SPEAKER_03：这就是 Hinton 所说的。
91 00:06:52,012 --> 00:06:55,516 讲述者 SPEAKER_03：难道不是我们说“中国”的地方吗？
92 00:06:56,137 --> 00:06:56,576 讲述者 SPEAKER_03：俄罗斯？
93 00:06:57,278 --> 00:06:58,639 说话人 SPEAKER_03：我们彼此无法忍受。
94 00:06:58,800 --> 00:07:01,141 说话人 SPEAKER_03：所有这些国家都很生气。
95 00:07:01,523 --> 00:07:03,685 说话人 SPEAKER_03：但我们有一个共同的关注点。
96 00:07:03,964 --> 00:07:04,425 说话人 SPEAKER_01：没错。
97 00:07:04,526 --> 00:07:06,228 说话人 SPEAKER_01：对于正在接管的人工智能超级智能。
98 00:07:06,288 --> 00:07:08,170 说话人 SPEAKER_01：不是对于其他所有事情，而是对于那件事。
99 00:07:08,807 --> 00:07:09,928 说话人 SPEAKER_01：我们都在同一条船上。
100 00:07:10,589 --> 00:07:12,110 说话人 SPEAKER_01：就像一场全球核战争。
101 00:07:12,211 --> 00:07:12,872 说话人 SPEAKER_01：我们都输了。
102 00:07:13,932 --> 00:07:18,016 说话人 SPEAKER_01：这就是交战部落合作的局面。
103 00:07:18,057 --> 00:07:23,723 说话人 SPEAKER_01：一个比他们更大的外部敌人会迫使他们合作，因为他们可以得到彼此相同的回报。
104 00:07:24,463 --> 00:07:25,925 说话人 SPEAKER_01：这个威胁就像那样。
105 00:07:26,185 --> 00:07:27,487 说话人 SPEAKER_02: 你认为中国明白这一点吗？
106 00:07:27,666 --> 00:07:27,867 说话人 SPEAKER_01: 是的。
107 00:07:28,127 --> 00:07:29,187 说话人 SPEAKER_02: 你为什么这么想？
108 00:07:29,629 --> 00:07:32,071 说话人 SPEAKER_01: 中国有研究人员正在谈论这个问题。
109 00:07:32,591 --> 00:07:33,853 说话人 SPEAKER_02：美国人理解它吗？
110 00:07:34,458 --> 00:07:35,759 说话人 SPEAKER_01：我认为他们开始理解了，是的。
111 00:07:36,341 --> 00:07:47,581 说话人 SPEAKER_01：美国的资深政治领导人现在开始关注了，他们对……非常感兴趣。所以不仅仅是假新闻和失业这样的直接担忧。
112 00:07:48,141 --> 00:07:52,990 说话人 SPEAKER_01：他们也开始关注这种存在性威胁，我们如何阻止这些事物接管？
113 00:07:54,437 --> 00:08:01,860 说话人 SPEAKER_03：白宫确实在谈论科技公司有道德义务考虑人工智能的风险，而不仅仅是好处。
114 00:08:02,442 --> 00:08:06,995 说话人 SPEAKER_03：在地球上如此少有共识的地方，也许可以就这一点达成共识。
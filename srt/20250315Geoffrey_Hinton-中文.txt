1 00:00:00,031 --> 00:00:06,641 说话人 SPEAKER_00: 在 5 到 20 年之内，有很大可能性，有 50%的可能性，我们会得到比我们更聪明的 AI。
2 00:00:11,587 --> 00:00:21,481 说话人 SPEAKER_00: 所以，我在加利福尼亚的一家酒店房间里差不多睡着了，我的手机倒扣在床头柜上，声音已经关掉了。
3 00:00:22,289 --> 00:00:30,022 说话人 SPEAKER_00: 我刚好侧着睡，手机在我的视线范围内，突然亮了起来。
4 00:00:30,042 --> 00:00:37,075 说话人 SPEAKER_00: 我看到这小条亮光，手机开始震动。
5 00:00:37,256 --> 00:00:40,661 说话人 SPEAKER_00：我在加利福尼亚，我认识的人几乎都在东海岸。
6 00:00:41,362 --> 00:00:43,567 说话人 SPEAKER_00：所以我好奇究竟是谁在打电话给我。
7 00:00:44,088 --> 00:00:50,237 说话人 SPEAKER_00：于是我接了电话，然后是一个我认不出区号和国家的长电话号码。
8 00:00:51,039 --> 00:00:53,883 说话人 SPEAKER_00：然后某个地方有瑞典口音的人问我是否是杰弗里·辛顿。
9 00:00:54,204 --> 00:00:56,067 说话人 说话人_00: 然后他告诉我我获得了诺贝尔物理学奖。
10 00:00:56,087 --> 00:00:59,311 说话人 说话人_00: 我的第一个反应是，等等，我不做物理。
11 00:01:00,232 --> 00:01:02,015 说话人 说话人_00: 这可能是个恶作剧。
12 00:01:01,996 --> 00:01:03,338 说话人 说话人_00: 我觉得这很可能是个恶作剧。
13 00:01:04,739 --> 00:01:07,082 说话人 说话人_00: 但他说的听起来非常可信。
14 00:01:07,763 --> 00:01:09,706 说话人 说话人_00: 然后出现了其他的瑞典口音。
15 00:01:11,087 --> 00:01:12,469 说话人 说话人_00: 我确信它是真的。
16 00:01:12,950 --> 00:01:14,453 说话人 说话人_00: 嗯，我有点确信它是真的。
17 00:01:15,274 --> 00:01:19,399 说话人 SPEAKER_00: 但在那之后几天，我觉得自己可能在做梦。
18 00:01:20,140 --> 00:01:22,582 说话人 SPEAKER_00: 因此我进行了一些统计分析。
19 00:01:23,084 --> 00:01:25,287 说话人 SPEAKER_00: 统计分析是这样的。
20 00:01:25,704 --> 00:01:32,293 说话人 SPEAKER_00: 一个真正的心理学家，试图理解大脑如何工作的人，获得诺贝尔物理学奖的概率有多大？
21 00:01:33,114 --> 00:01:35,058 说话人 SPEAKER_00: 好吧，让我们说，概率是两百万分之一。
22 00:01:35,659 --> 00:01:37,641 说话人 SPEAKER_00: 这是对概率的一个相当宽泛的估计。
23 00:01:39,784 --> 00:01:44,391 说话人 SPEAKER_00: 如果是我的梦想，我获得诺贝尔物理学奖的概率是多少？
24 00:01:45,212 --> 00:01:46,594 说话人 SPEAKER_00: 好吧，让我们说，概率是二分之一。
25 00:01:47,977 --> 00:01:54,665 说话人 SPEAKER_00：现在你更可能是在做梦，而不是在现实中。
26 00:01:55,557 --> 00:02:04,570 说话人 SPEAKER_00：我想这可能是你年轻时做的梦，可以飞，你梦想着能飞，那感觉真好，然后你醒来，才发现那只是个梦。
27 00:02:05,171 --> 00:02:11,902 说话人 SPEAKER_00：然后一个月后你再次梦见自己能飞，你记得你曾经梦到自己能飞，那不是真的，但这次是真的。
28 00:02:13,044 --> 00:02:17,973 说话人 SPEAKER_00：我以为那可能就是那种梦，所以接下来的几天我都在等着看自己是否会醒来。
29 00:02:20,135 --> 00:02:21,277 说话人 SPEAKER_00：我还没醒来。
30 00:02:23,771 --> 00:02:33,627 说话人 SPEAKER_00：在学术上取得成功有很大的压力，所以我从很小的时候就明白，我必须成为一个成功的学者或者失败者。
31 00:02:35,971 --> 00:02:42,860 说话人 SPEAKER_00：我在高中有一个朋友，他总是比我聪明得多，当我们大约 16 或 17 岁的时候，
32 00:02:43,364 --> 00:02:51,417 说话人 SPEAKER_00：有一天他来到学校，开始谈论大脑中的记忆以及它们可能以与全息图相同的方式在大脑中分布。
33 00:02:51,456 --> 00:02:52,998 说话人 说话人_00：那时刚刚发明了全息投影。
34 00:02:53,500 --> 00:02:55,122 说话人 说话人_00：这大概是在1965年左右。
35 00:02:56,084 --> 00:03:06,319 说话人 说话人_00：他非常感兴趣于一位心理学家 Lashley 提出的观点，即记忆分布在许多神经元中。
36 00:03:07,201 --> 00:03:11,228 说话人 说话人_00：我对这个观点也产生了浓厚的兴趣，从那时起我就一直在思考大脑是如何工作的。
37 00:03:13,655 --> 00:03:16,521 说话人 SPEAKER_00：人工智能存在两种风险。
38 00:03:17,403 --> 00:03:25,264 说话人 SPEAKER_00：有相对短期的风险，这些风险非常重要且非常紧迫，主要与人们滥用人工智能有关。
39 00:03:26,727 --> 00:03:29,554 说话人 SPEAKER_00：所以人类仍然掌握着主导权，但他们正在滥用它。
40 00:03:29,719 --> 00:03:44,776 说话人 SPEAKER_00：这些风险包括像取代大量工作以及加剧贫富差距等问题，因为当使用人工智能提高生产力时，这种增长并不是平均分配的。
41 00:03:45,317 --> 00:03:47,862 说话人 SPEAKER_00: 有些人失业，有些人致富。
42 00:03:47,842 --> 00:03:50,706 说话人 SPEAKER_00: 这对社会来说是不好的。
43 00:03:51,508 --> 00:03:55,774 说话人 SPEAKER_00: 这是一种风险，我们需要找出如何应对，尽管还不清楚该怎么办。
44 00:03:56,514 --> 00:04:00,180 说话人 SPEAKER_00: 另一种风险是虚假视频，它们会破坏选举。
45 00:04:00,199 --> 00:04:01,102 说话人 SPEAKER_00: 他们已经在做了。
46 00:04:02,122 --> 00:04:11,376 说话人 SPEAKER_00: 另一种风险是网络攻击，恶意行为者利用这些大型 AI 模型来制造更高级的攻击。
47 00:04:11,717 --> 00:04:13,718 说话人 SPEAKER_00: 首先，它只是用于更好的网络钓鱼。
48 00:04:14,379 --> 00:04:17,665 说话人 SPEAKER_00: 所以去年，网络钓鱼攻击增加了 1200%。
49 00:04:17,863 --> 00:04:21,730 说话人 SPEAKER_00: 很可能是因为大型语言模型可以使它们变得更加高效。
50 00:04:23,975 --> 00:04:32,329 说话人 SPEAKER_00: 然后是设计像 COVID 这样的东西，你可以使用 AI 更高效地完成。
51 00:04:32,971 --> 00:04:36,437 说话人 SPEAKER_00: 而设计这样东西将很快变得相对容易。
52 00:04:37,257 --> 00:04:40,564 说话人 SPEAKER_00: 这意味着一个疯狂的人可以造成无尽的混乱。
53 00:04:41,134 --> 00:04:47,593 说话人 SPEAKER_00：如果释放大型模型的权重，事情就会变得容易得多，这样他们就可以拿一个大模型然后微调它。
54 00:04:48,053 --> 00:04:50,983 说话人 SPEAKER_00：现在人们正在发布大型模型的权重，我觉得这很疯狂。
55 00:04:52,947 --> 00:04:54,391 说话人 SPEAKER_00：当然还有其他短期风险。
56 00:04:54,471 --> 00:04:56,899 说话人 SPEAKER_00：显然有诸如歧视和偏见等问题。
57 00:04:57,908 --> 00:05:06,956 说话人 SPEAKER_00：所以如果你训练一个模型，假设你正在训练一个模型来决定囚犯是否应该获得假释。
58 00:05:07,858 --> 00:05:15,906 说话人 SPEAKER_00：如果历史数据表明白人囚犯可以获得假释而黑人囚犯则不能，而你用历史数据训练一个 AI 模型，它也会得出同样的结论。
59 00:05:16,908 --> 00:05:27,678 说话人 SPEAKER_00：我觉得对此的担忧没有其他人那么大，因为我认为，对于 AI 模型来说，你可以冻结权重，你可以测量歧视，而这你是无法对人做到的。
60 00:05:27,827 --> 00:05:35,259 说话人 SPEAKER_00：如果你试图测量人的歧视，他们会意识到自己正在被测量，然后就会产生大众汽车效应，即他们在被测量时会表现出不同的行为。
61 00:05:36,802 --> 00:05:44,733 说话人 SPEAKER_00：所以，我认为实际上在 AI 系统中测量歧视偏见要比在人类中容易得多。
62 00:05:45,954 --> 00:05:50,502 说话人 SPEAKER_00：我认为我们的目标应该是不要制造不歧视和不带偏见的物品。
63 00:05:51,382 --> 00:05:57,091 说话人 SPEAKER_00：我们的目标应该是制造出歧视程度更低、偏见更少的物品，这些物品将取代现有的系统。
64 00:05:57,680 --> 00:06:06,031 说话人 SPEAKER_00：我认为这是一个非常重要的问题，但也是一个我们可以在其中取得进展的问题。
65 00:06:06,052 --> 00:06:07,454 说话人 说话人_00：这就是短期问题。
66 00:06:08,696 --> 00:06:14,464 说话人 说话人_00：还有这些事物接管的长远问题。
67 00:06:14,725 --> 00:06:17,548 说话人 说话人_00：所以我们正在做的事情是让事物比我们自己更智能。
68 00:06:19,692 --> 00:06:27,483 说话人 说话人_00：研究人员对于这将会在何时发生意见不一，但在主要研究人员中，对于它将会发生的事实几乎没有分歧。
69 00:06:28,021 --> 00:06:30,571 说话人 SPEAKER_00: 当然，除非我们把自己炸飞。
70 00:06:32,257 --> 00:06:37,237 说话人 SPEAKER_00: 所以问题是，当我们创造出比我们更智能的生命体时，会发生什么？
71 00:06:37,824 --> 00:06:40,949 说话人 SPEAKER_00: 我们不知道会发生什么。
72 00:06:40,970 --> 00:06:42,812 说话人 SPEAKER_00: 我们以前从未遇到过这种情况。
73 00:06:43,593 --> 00:06:45,776 说话人 SPEAKER_00：任何说一切都会好的人都是疯子。
74 00:06:46,057 --> 00:06:49,742 说话人 SPEAKER_00：任何说他们不可避免地会接管一切的人也是疯子。
75 00:06:50,062 --> 00:06:51,024 说话人 SPEAKER_00：我们真的不知道。
76 00:06:51,564 --> 00:07:03,242 说话人 SPEAKER_00：但由于我们真的不知道，现在进行大量基础研究以确定我们是否能够控制比我们更智能的创造物，这将会非常有意义。
77 00：07：05,803 --> 00：07：11,790 发言者 SPEAKER_00：我们知道的聪明的事物被低智慧的事物控制的例子并不多。
78 00：07：13,752 --> 00：07：17,656 演讲者 SPEAKER_00：我所知道的唯一好例子是婴儿控制母亲。
79 00:07:18,156 --> 00:07:23,401 说话者 说话者_00：在智能上没有太大差别，进化不得不投入大量精力来实现这一点。
80 00:07:23,422 --> 00:07:26,504 说话者 说话者_00：宝宝能够控制母亲非常重要。
81 00:07:26,524 --> 00:07:32,571 说话人 说话人_00：但是如果你环顾四周，总体来说，更智能的事物并不是由不那么智能的事物控制的。
82 00:07:33,192 --> 00:07:37,516 说话人 说话人_00：现在有些人认为这没问题，因为我们创造了它们，我们会以这样的方式建造它们，我们可以始终控制它们。
83 00:07:38,357 --> 00:07:41,300 说话人 说话人_00：但这些事物将会是智能的，它们会像我们一样。
84 00:07:41,821 --> 00:07:44,244 说话人 说话人_00：实际上，它们的工作方式非常像我们的工作方式。
85 00:07:44,564 --> 00:07:45,966 说话人 SPEAKER_00：它们不像计算机代码。
86 00:07:46,586 --> 00:07:48,889 说话人 SPEAKER_00：人们有时把它们称为计算机程序。
87 00:07:49,088 --> 00:07:50,370 说话人 SPEAKER_00：它们根本不是计算机程序。
88 00:07:50,850 --> 00:07:55,315 说话人 SPEAKER_00：你编写计算机程序来告诉神经网络如何学习，一个模拟的神经网络。
89 00:07:56,136 --> 00:07:59,519 说话人 说话人_00：但是一旦开始学习，它就会从数据中提取结构，
90 00:07:59,500 --> 00:08:04,264 说话人 说话人_00：最终得到的系统已经从数据中提取了结构。
91 00:08:04,565 --> 00:08:06,187 说话人 说话人_00：这不是任何人编写的程序。
92 00:08:06,767 --> 00:08:08,389 说话人 说话人_00：我们并不确切知道它将如何工作。
93 00:08:09,531 --> 00:08:10,572 说话人 SPEAKER_00: 就像我们一样。
94 00:08:10,872 --> 00:08:21,564 说话人 SPEAKER_00: 因此，让这些程序以合理的方式运行，抱歉，让这些系统以合理的方式运行，就像教育孩子以合理的方式行事一样。
95 00:08:21,845 --> 00:08:27,752 说话人 SPEAKER_00: 你真正能控制的，你可以通过奖励来强化它，对于不良行为进行惩罚。
96 00:08:27,985 --> 00:08:35,417 说话人 SPEAKER_00: 但主要的控制方式是展示良好的行为，通过良好的行为进行训练，这样它就能观察到并模仿。
97 00:08:36,097 --> 00:08:37,419 说话人 说话人_00: 这些系统也是一样的。
98 00:08:38,000 --> 00:08:43,909 说话人 说话人_00: 因此，我们非常重要的是要训练它们表现出我们希望看到的行为。
99 00:08:44,370 --> 00:08:51,841 说话人 说话人_00: 目前，大型聊天机器人是在它们能获取的所有数据上训练的，这包括连环杀手的日记等。
100 00:08:52,743 --> 00:08:55,748 说话人 说话人_00: 嗯，如果你在抚养一个孩子，你会
101 00:08:56,470 --> 00:09:00,078 说话人 SPEAKER_00: 让你的孩子学习在《艾罗科纳斯爵士日记》中阅读。
102 00:09:00,119 --> 00:09:01,702 说话人 SPEAKER_00: 我想你会意识到那是个糟糕的主意。
103 00:09:04,671 --> 00:09:05,712 说话人 SPEAKER_00: 嗯，这正是我们不知道的。
104 00:09:05,773 --> 00:09:12,370 说话人 SPEAKER_00: 我的猜测是，在 5 到 20 年之间，
105 00:09:12,518 --> 00:09:16,302 说话人 SPEAKER_00: 有很大可能性，50% 的可能性，我们会得到比我们更聪明的 AI。
106 00:09:17,105 --> 00:09:17,985 说话人 SPEAKER_00: 可能会要更长的时间。
107 00:09:18,447 --> 00:09:19,828 说话人 SPEAKER_00: 有可能时间会短一些。
108 00:09:21,610 --> 00:09:25,035 说话人 SPEAKER_00: 我认为在 20 年后很可能已经发生。
109 00:09:25,417 --> 00:09:27,318 说话人 说话人_00：其他研究人员认为它可能更短或更长。
110 00:09:29,201 --> 00:09:29,822 说话人 说话人_00：这是我猜的。
111 00:09:30,803 --> 00:09:34,870 说话人 说话人_00：实际上，那是我一年前的猜测，所以现在我的猜测是在四到十九年之间。
112 00:09:34,909 --> 00:09:41,580 说话人 说话人_00：我认为这取决于你所在的领域。
113 00:09:43,399 --> 00:09:47,943 说话人 SPEAKER_00：无论你试图做的是否与该领域的标准有很大不同。
114 00:09:50,025 --> 00:09:54,389 说话人 SPEAKER_00：所以对于神经网络来说，长期以来它们被视为荒谬。
115 00:09:55,672 --> 00:09:58,594 说话人 SPEAKER_00：而且对许多人来说，它们永远不会起作用是显而易见的。
116 00:10:00,456 --> 00:10:07,863 说话人 SPEAKER_00：所以在这样一个领域工作，你必须自信自己是对的，即使其他人说你错了。
117 00:10:09,585 --> 00:10:10,065 说话人 说话人_00: 并且
118 00:10:11,327 --> 00:10:15,977 说话人 说话人_00: 实际上在我很小的时候发生了一些事情，对我有所帮助。
119 00:10:17,481 --> 00:10:26,822 说话人 说话人_00: 其中一个是我的父母，他们都是无神论者，从七岁开始就把我送到一所基督教学校，一所基督教私立学校。
120 00:10:27,089 --> 00:10:29,152 说话人 说话人_00: 学校里的每个人都信仰上帝。
121 00:10:29,812 --> 00:10:32,076 说话人 SPEAKER_00: 老师们信仰上帝，其他孩子也信仰上帝。
122 00:10:32,817 --> 00:10:35,902 说话人 SPEAKER_00: 在我看来，这显然是胡说八道。
123 00:10:35,922 --> 00:10:38,325 说话人 SPEAKER_00: 结果证明我是对的。
124 00:10:39,346 --> 00:10:55,188 说话人 SPEAKER_00: 所以，当你周围的人都相信一件事，而你清楚地知道他们是错的，然后随着年龄的增长，你发现还有其他人也不相信上帝，这是一种非常有用的经历。
125 00:10:55,168 --> 00:10:59,513 说话人 SPEAKER_00：那件事帮助我在大家都说神经网络是无稽之谈的时候继续前行。
126 00:10:59,533 --> 00:11:02,436 说话人 SPEAKER_00：并不是所有人，但几乎在计算机科学界都是这样。
127 00:11:03,496 --> 00:11:17,610 说话人 SPEAKER_00：另一个我很少谈论的经历，当我大概九岁左右，但具体年龄不清楚，我和父亲一起听了一档关于大陆漂移的广播节目。
128 00:11:18,292 --> 00:11:23,657 说话人 SPEAKER_00：所以那时，关于大陆是否移动存在很多争议。
129 00:11:24,379 --> 00:11:27,023 说话人 SPEAKER_00：几乎所有地质学家都认为这完全是垃圾。
130 00:11:28,744 --> 00:11:37,432 说话人 SPEAKER_00：所以这个理论最早是在 1920 年由一位名叫魏格纳的气候学家提出的，他有很多证据表明大陆在移动。
131 00:11:38,153 --> 00:11:39,355 说话人 SPEAKER_00：但他不是地质学家。
132 00:11:40,255 --> 00:11:42,899 说话人 SPEAKER_00：而地质学家们则认为这完全是垃圾。
133 00:11:43,658 --> 00:11:44,580 说话人 说话人_00: 他们对此嗤之以鼻。
134 00:11:45,620 --> 00:11:49,565 说话人 说话人_00: 例如，他们拒绝将其纳入教科书中。
135 00:11:50,025 --> 00:11:51,667 说话人 说话人_00: 他们说这只会误导学生。
136 00:11:52,067 --> 00:11:54,370 说话人 说话人_00: 这完全是胡说八道。
137 00:11:56,594 --> 00:12:06,645 说话人 SPEAKER_00：我看到了一场辩论，其中有一个理论被几乎所有地质学家视为完全荒谬，结果证明是正确的。
138 00:12:08,248 --> 00:12:09,349 说话人 SPEAKER_00：这也非常有帮助。
139 00:12:09,788 --> 00:12:15,875 说话人 SPEAKER_00：实际上，与神经网络最相似的事情，就是大陆漂移的情况。
140 00:12:16,937 --> 00:12:25,807 说话人 SPEAKER_00：在大陆漂移中，有一种观点，你知道，就是南美洲完美地契合非洲的腋下。
141 00:12:28,100 --> 00:12:29,100 说话人 SPEAKER_00：但这不仅仅是因为这个。
142 00:12:29,140 --> 00:12:35,508 说话人 SPEAKER_00：而是因为美国沿海的所有土壤类型都与挪威到南非的所有土壤类型相匹配。
143 00:12:37,230 --> 00:12:38,750 说话人 SPEAKER_00：有化石将它们联系起来。
144 00:12:39,532 --> 00:12:43,797 说话人 SPEAKER_00：在热带的岩石上有冰川擦痕。
145 00:12:44,256 --> 00:12:45,918 说话人 SPEAKER_00：北极有煤炭储藏。
146 00:12:46,960 --> 00:12:51,524 说话人 SPEAKER_00：所以有大量证据表明大陆曾经移动过。
147 00:12:52,078 --> 00:12:55,703 说话人 SPEAKER_00：地质学家完全忽视了这一点。
148 00:12:56,485 --> 00:12:58,407 说话人 SPEAKER_00：他们无法相信地球会移动。
149 00:12:59,248 --> 00:13:02,292 说话人 SPEAKER_00：神经网络也是一样。
150 00:13:03,192 --> 00:13:07,698 说话人 SPEAKER_00：我们有证据表明，神经网络必须能够学习做复杂的事情，因为我们有大脑。
151 00:13:09,682 --> 00:13:15,269 说话人 SPEAKER_00：但大多数人在人工智能领域认为，如果你试图在神经网络中学习所有事情，这是徒劳的。
152 00:13:15,589 --> 00:13:19,875 说话人 SPEAKER_00：知识必须是天生的，或者你必须通过学习符号规则来完成。
153 00:13:20,850 --> 00:13:27,499 说话人 SPEAKER_00: 他们基本上拒绝在他们的期刊上发表神经网络相关内容。
154 00:13:30,282 --> 00:13:33,144 说话人 SPEAKER_00: 因此，这是一个非常相似的范式转变。
155 00:13:33,384 --> 00:13:37,570 说话人 SPEAKER_00: 现在已经发生了更多或更少的范式转变。
156 00:13:37,590 --> 00:13:46,419 说话人 SPEAKER_00: 我不知道我是否处于一个能给出建议的位置，但我通常给出的建议是，如果你有一个想法，并且它在你看来是正确的，
157 00:13:47,176 --> 00:13:52,844 说话人 SPEAKER_00: 它与大家的看法不同，在你弄清楚为什么它错误之前，不要放弃它。
158 00:13:53,764 --> 00:13:58,873 说话人 SPEAKER_00: 所以你大多数这样的想法都是错误的，因为你没有考虑到某些东西，或者你没有理解某些东西。
159 00:14:01,196 --> 00:14:13,793 说话人 SPEAKER_00: 只有极少数情况下，你有一个与众不同的想法，并且是正确的，除非你坚持你的信念直到你发现它们为什么是错误的，否则你永远不会发现这一点。
160 00:14:14,774 --> 00:14:16,778 说话人 SPEAKER_00: 你应该忽略别人的说法。
161 00:14:17,230 --> 00:14:19,308 说话人 SPEAKER_00：我非常擅长忽略别人的话。
162 00:14:23,068 --> 00:14:33,184 说话人 SPEAKER_00：我认为科学家对这项技术的理解比政治家或公众要好得多。
163 00:14:34,486 --> 00:14:35,889 说话人 SPEAKER_00：科学家们仍然意见不一。
164 00:14:36,089 --> 00:14:41,376 说话人 SPEAKER_00：所以还有一些科学家说，这些大教堂，他们并不真正理解他们在说什么。
165 00:14:42,619 --> 00:14:47,506 说话人 SPEAKER_00: 尽管有证据表明他们确实理解自己在说什么，但一些科学家认为这只是统计学上的技巧。
166 00:14:49,410 --> 00:14:54,436 说话人 SPEAKER_00: 我本想早点关注这个生存威胁。
167 00:14:56,004 --> 00:14:57,109 说话人 SPEAKER_00: 我一直认为
168 00:14:57,932 --> 00:15:03,878 说话人 SPEAKER_00: 超级智能离我们还很遥远，我们可以稍后再考虑这个问题，目前的问题只是让事物变得更智能。
169 00:15:04,958 --> 00:15:08,162 说话人 SPEAKER_00：我真希望早点想到会发生什么。
170 00:15:08,722 --> 00:15:24,157 说话人 SPEAKER_00：如果你回到 20 世纪 50 年代初的图灵，他会谈到让事物比我们更聪明，他有一句话是，当然，当它们比我们更聪明时，我们就完了。
171 00:15:24,878 --> 00:15:26,538 说话人 SPEAKER_00：他并不是那样说的，但
172 00:15:26,519 --> 00:15:27,884 说话人 SPEAKER_00: 他暗示了这一点。
173 00:15:29,048 --> 00:15:35,092 说话人 SPEAKER_00：但大多数人直到问题临近才去考虑这个问题，问题是现在它已经很近了。
174 00:15:35,956 --> 00:15:37,322 说话人 SPEAKER_00：所以我希望我早点想到这一点。
175 00:15:38,821 --> 00:15:58,298 说话人 SPEAKER_00：好的，所以我把一半的奖金，我那份的一半奖金，捐给了加拿大一个组织，该组织培训生活在土著社区的人学习生产安全饮用水的技术。
176 00:15:58,278 --> 00:16:02,621 说话人 SPEAKER_00：这是好事，因为这些人将留在社区，他们将有安全饮用水。
177 00:16:03,623 --> 00:16:15,475 说话人 SPEAKER_00：在这个时候，在像加拿大这样的富裕国家，例如在安大略省，有 20%的土著社区没有安全的饮用水，这真是太荒谬了。
178 00:16:16,014 --> 00:16:17,015 说话人 SPEAKER_00：这简直是疯狂。
179 00:16:17,616 --> 00:16:25,504 说话人 SPEAKER_00：我对这个问题有些同情，因为我收养了一个秘鲁的孩子，我在那里待了两个月，那里的自来水是不能喝的。
180 00:16:26,264 --> 00:16:27,947 说话人 SPEAKER_00：这水有毒。
181 00:16:29,311 --> 00:16:32,855 说话人 SPEAKER_00：你的一生都在围绕如何获得安全的水。
182 00:16:33,756 --> 00:16:37,120 说话人 SPEAKER_00：这只是在日常生活中增加了巨大的额外负担。
183 00:16:37,562 --> 00:16:40,065 说话人 SPEAKER_00：所以在加拿大的人们不得不忍受这一点真是太疯狂了。
184 00:16:41,647 --> 00:16:43,369 说话人 SPEAKER_00：所以，我把其中的一半捐给了它。
185 00:16:44,210 --> 00:16:55,825 说话人 SPEAKER_00：另一半，在 20 世纪 80 年代，我和一个叫特里·桑福斯的人一起工作，他实际上是霍普菲尔德的学生。
186 00:16:56,750 --> 00:16:58,354 说话人 SPEAKER_00：关于玻尔兹曼机的理论。
187 00:16:58,894 --> 00:17:01,059 说话人 SPEAKER_00：我们在这方面是平等的。
188 00:17:01,922 --> 00:17:06,832 说话人 SPEAKER_00：如果没有和他交谈，我就不会有这个理论，而如果没有和他交谈，他也不会有这个理论。
189 00:17:08,957 --> 00:17:14,269 说话人 SPEAKER_00：他原本是一位物理学家，后来转向神经科学。
190 00:17:15,109 --> 00:17:17,474 说话人 SPEAKER_00：我们以为这一定是大脑的工作方式。
191 00:17:17,515 --> 00:17:21,763 说话人 SPEAKER_00：这是一个如此优雅的学习算法，我们确信这一定是大脑的工作方式。
192 00:17:22,484 --> 00:17:26,653 说话人 SPEAKER_00：因此，我们以为我们可能会因为发现大脑的工作方式而获得诺贝尔生理学或医学奖。
193 00:17:27,695 --> 00:17:34,569 说话人 SPEAKER_00：我们 1980 年代就有过协议，如果他们把它给其中一个人而不是另一个人，我们就平分。
194 00:17:35,173 --> 00:17:46,382 说话人 SPEAKER_00：所以当诺贝尔奖出乎意料地颁发给我时，其中一个原因就是玻尔兹曼机，我就联系了特里，问他希望我把他的那份钱寄到哪里？
195 00:17:47,392 --> 00:17:55,682 说话人 SPEAKER_00：他说，嗯，他觉得这样做不太合适，因为这不仅仅是因为玻尔兹曼机，还包括他不太参与的后续工作。
196 00:17:56,604 --> 00:17:57,904 说话人 SPEAKER_00：所以他拒绝接受这笔钱。
197 00:17:58,826 --> 00:18:07,778 说话人 说话人_00: 最后我们妥协了，我拿了我那份的一半，并用它来设立一个以他的名字命名的奖项，用于奖励年轻的研究人员。
198 00:18:08,438 --> 00:18:14,546 说话人 说话人_00: 这个奖项将颁发给那些有关于大脑工作方式的疯狂理论的年轻研究人员，就像我们一样。
199 00:18:15,507 --> 00:18:16,048 说话人 说话人_00: 并且
200 00:18:17,345 --> 00:18:20,308 说话人 说话人_00: 这个奖项将在我们领域的年度会议上颁发。
201 00:18:21,371 --> 00:18:24,976 说话人 SPEAKER_00: 这看起来像是一个不错的折中方案。
202 00:18:25,997 --> 00:18:32,605 说话人 SPEAKER_00: 所以我觉得他很容易就能成为诺贝尔奖第三获奖人。
203 00:18:32,987 --> 00:18:33,467 说话人 SPEAKER_00: 他没有。
204 00:18:34,147 --> 00:18:36,451 说话人 SPEAKER_00: 我并不是在抱怨这一点，但他本可以。
205 00:18:37,752 --> 00:18:42,019 Speaker SPEAKER_00: 但这是认可他做出了巨大贡献的一种方式。

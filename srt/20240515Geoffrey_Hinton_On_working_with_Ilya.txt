1
00:00:00,031 --> 00:00:08,061
Speaker SPEAKER_00: Have you reflected a lot on how to select talent, or has that mostly been intuitive to you?

2
00:00:08,102 --> 00:00:12,128
Speaker SPEAKER_00: Ilja just shows up and you're like, this is a clever guy, let's work together.

3
00:00:12,167 --> 00:00:14,551
Speaker SPEAKER_00: Or have you thought a lot about that?

4
00:00:16,393 --> 00:00:17,094
Speaker SPEAKER_00: Should we roll this?

5
00:00:17,835 --> 00:00:19,719
Speaker SPEAKER_00: Yeah, let's roll this.

6
00:00:20,519 --> 00:00:25,187
Speaker SPEAKER_00: Okay.

7
00:00:25,207 --> 00:00:25,987
Speaker SPEAKER_00: Sound is working.

8
00:00:30,675 --> 00:00:33,478
Speaker SPEAKER_01: So I remember when I first got to Carnegie Mellon from England.

9
00:00:34,259 --> 00:00:38,343
Speaker SPEAKER_01: In England, at a research unit, it would get to be six o'clock and you'd all go for a drink in the pub.

10
00:00:39,323 --> 00:00:46,930
Speaker SPEAKER_01: At Carnegie Mellon, I remember after I'd been there a few weeks, it was Saturday night, I didn't have any friends yet, and I didn't know what to do.

11
00:00:46,951 --> 00:00:52,094
Speaker SPEAKER_01: So I decided I'd go into the lab and do some programming, because I had a Lisp machine and you couldn't program it from home.

12
00:00:52,115 --> 00:00:56,639
Speaker SPEAKER_01: So I went into the lab at about nine o'clock on a Saturday night, and it was swarming.

13
00:00:56,658 --> 00:00:58,640
Speaker SPEAKER_01: All the students were there.

14
00:00:59,026 --> 00:01:02,231
Speaker SPEAKER_01: And they were all there because what they were working on was the future.

15
00:01:02,271 --> 00:01:07,382
Speaker SPEAKER_01: They all believed that what they did next was going to change the course of computer science.

16
00:01:07,402 --> 00:01:08,944
Speaker SPEAKER_01: And it was just so different from England.

17
00:01:09,686 --> 00:01:11,588
Speaker SPEAKER_01: And so that was very refreshing.

18
00:01:11,969 --> 00:01:14,073
Speaker SPEAKER_00: Take me back to the very beginning.

19
00:01:14,213 --> 00:01:18,382
Speaker SPEAKER_00: Geoff at Cambridge, trying to understand the BRIN.

20
00:01:18,962 --> 00:01:20,004
Speaker SPEAKER_00: What was that like?

21
00:01:20,355 --> 00:01:21,798
Speaker SPEAKER_01: It was very disappointing.

22
00:01:22,158 --> 00:01:23,560
Speaker SPEAKER_01: So I did physiology.

23
00:01:24,200 --> 00:01:26,644
Speaker SPEAKER_01: And in the summer term, they were going to teach us how the brain worked.

24
00:01:27,325 --> 00:01:33,391
Speaker SPEAKER_01: And all they taught us was how neurons conduct action potentials, which is very interesting, but it doesn't tell you how the brain works.

25
00:01:34,272 --> 00:01:35,734
Speaker SPEAKER_01: So that was extremely disappointing.

26
00:01:36,396 --> 00:01:37,456
Speaker SPEAKER_01: I switched to philosophy then.

27
00:01:37,918 --> 00:01:39,739
Speaker SPEAKER_01: I thought maybe they'd tell us how the mind worked.

28
00:01:40,200 --> 00:01:41,322
Speaker SPEAKER_01: And that was very disappointing.

29
00:01:42,063 --> 00:01:45,046
Speaker SPEAKER_01: I eventually ended up going to Edinburgh to do AI.

30
00:01:45,027 --> 00:01:46,149
Speaker SPEAKER_01: And that was more interesting.

31
00:01:46,209 --> 00:01:48,052
Speaker SPEAKER_01: At least you could simulate things.

32
00:01:48,233 --> 00:01:49,335
Speaker SPEAKER_01: So you could test out theories.

33
00:01:49,977 --> 00:01:53,084
Speaker SPEAKER_00: And did you remember what intrigued you about AI?

34
00:01:53,525 --> 00:01:54,787
Speaker SPEAKER_00: Was it a paper?

35
00:01:54,847 --> 00:01:58,715
Speaker SPEAKER_00: Was it any particular person that exposed you to those ideas?

36
00:01:59,135 --> 00:02:03,319
Speaker SPEAKER_01: I guess it was a book I read by Donald Hebb that influenced me a lot.

37
00:02:04,719 --> 00:02:08,824
Speaker SPEAKER_01: He was very interested in how you learn the connection strengths in neural nets.

38
00:02:09,705 --> 00:02:18,233
Speaker SPEAKER_01: I also read a book by John von Neumann early on, who was very interested in how the brain computes and how it's different from normal computers.

39
00:02:19,234 --> 00:02:28,703
Speaker SPEAKER_00: And did you get that conviction that these ideas would work out at that point, or what was your intuition back at the Edinburgh days?

40
00:02:29,324 --> 00:02:33,276
Speaker SPEAKER_01: It seemed to me there has to be a way that the brain learns.

41
00:02:33,830 --> 00:02:41,141
Speaker SPEAKER_01: And it's clearly not by having all sorts of things programmed into it and then using logical rules of inference.

42
00:02:41,442 --> 00:02:43,405
Speaker SPEAKER_01: That just seemed to me crazy from the outset.

43
00:02:45,429 --> 00:02:52,319
Speaker SPEAKER_01: So we had to figure out how the brain learned to modify connections in a neural net so that it could do complicated things.

44
00:02:52,920 --> 00:02:55,485
Speaker SPEAKER_01: And von Neumann believed that, Turing believed that.

45
00:02:55,905 --> 00:03:00,612
Speaker SPEAKER_01: So von Neumann and Turing were both pretty good at logic, but they didn't believe in this logical approach.

46
00:03:01,183 --> 00:03:10,489
Speaker SPEAKER_00: And what was your split between studying the ideas from neuroscience and just doing what seemed to be good algorithms for AI?

47
00:03:11,230 --> 00:03:13,336
Speaker SPEAKER_00: How much inspiration did you take early on?

48
00:03:13,722 --> 00:03:16,384
Speaker SPEAKER_01: So I never did that much study of neuroscience.

49
00:03:16,405 --> 00:03:31,902
Speaker SPEAKER_01: I was always inspired by what I'd learned about how the brain works, that there's a bunch of neurons, they perform relatively simple operations, they're nonlinear, but they collect inputs, they weight them, and then they give an output that depends on that weighted input.

50
00:03:32,723 --> 00:03:36,008
Speaker SPEAKER_01: And the question is, how do you change those weights to make the whole thing do something good?

51
00:03:36,748 --> 00:03:38,090
Speaker SPEAKER_01: It seems like a fairly simple question.

52
00:03:38,610 --> 00:03:42,074
Speaker SPEAKER_00: What collaborations do you remember from that time?

53
00:03:42,241 --> 00:03:46,334
Speaker SPEAKER_01: The main collaboration I had at Carnegie Mellon was with someone who wasn't at Carnegie Mellon.

54
00:03:46,936 --> 00:03:51,069
Speaker SPEAKER_01: I was interacting a lot with Terry Sinofsky, who was in Baltimore at Johns Hopkins.

55
00:03:51,269 --> 00:03:55,336
Speaker SPEAKER_01: And about once a month, either he would drive to Pittsburgh or I would drive to Baltimore.

56
00:03:55,355 --> 00:03:56,478
Speaker SPEAKER_01: It's 250 miles away.

57
00:03:56,938 --> 00:03:59,203
Speaker SPEAKER_01: And we would spend a weekend together working on Boltzmann machines.

58
00:03:59,903 --> 00:04:01,246
Speaker SPEAKER_01: That was a wonderful collaboration.

59
00:04:01,486 --> 00:04:03,509
Speaker SPEAKER_01: We were both convinced it was how the brain worked.

60
00:04:03,871 --> 00:04:05,653
Speaker SPEAKER_01: That was the most exciting research I've ever done.

61
00:04:05,973 --> 00:04:08,639
Speaker SPEAKER_01: And a lot of technical results came out that were very interesting.

62
00:04:09,480 --> 00:04:10,862
Speaker SPEAKER_01: But I think it's not how the brain works.

63
00:04:11,663 --> 00:04:16,112
Speaker SPEAKER_01: I also had a very good collaboration with Peter Brown.

64
00:04:16,851 --> 00:04:20,459
Speaker SPEAKER_01: who was a very good statistician, and he worked on speech recognition at IBM.

65
00:04:20,940 --> 00:04:27,935
Speaker SPEAKER_01: And then he came as a more mature student to Carnegie Mellon just to get a PhD, but he already knew a lot.

66
00:04:28,596 --> 00:04:32,586
Speaker SPEAKER_01: He taught me a lot about speech, and he in fact taught me about hidden Markov models.

67
00:04:33,146 --> 00:04:35,011
Speaker SPEAKER_01: I think I learned more from him than he learned from me.

68
00:04:35,752 --> 00:04:37,475
Speaker SPEAKER_01: That's the kind of student you want.

69
00:04:37,574 --> 00:04:44,863
Speaker SPEAKER_01: And when he taught me about hidden Markov models, I was doing backprop with hidden layers, only they weren't called hidden layers then.

70
00:04:45,644 --> 00:04:51,473
Speaker SPEAKER_01: And I decided that name they use in hidden Markov models is a great name for variables that you don't know what they're up to.

71
00:04:52,113 --> 00:04:57,420
Speaker SPEAKER_01: And so that's where the name hidden in neural nets came from.

72
00:04:57,461 --> 00:05:01,206
Speaker SPEAKER_01: Me and Peter decided that was a great name for the hidden layers in neural nets.

73
00:05:03,249 --> 00:05:05,471
Speaker SPEAKER_01: But I learned a lot from Peter about speech.

74
00:05:05,670 --> 00:05:09,800
Speaker SPEAKER_00: Take us back to when Ilja showed up at your office.

75
00:05:10,483 --> 00:05:16,757
Speaker SPEAKER_01: I was in my office, probably on a Sunday, and I was programming, I think, and there was a knock on the door.

76
00:05:17,199 --> 00:05:18,802
Speaker SPEAKER_01: Not just any knock, but it went kind of...

77
00:05:19,997 --> 00:05:21,399
Speaker SPEAKER_01: That's sort of an urgent knock.

78
00:05:21,860 --> 00:05:24,403
Speaker SPEAKER_01: So I went and answered the door, and this was this young student there.

79
00:05:24,764 --> 00:05:28,288
Speaker SPEAKER_01: And he said he was cooking fries over the summer, but he'd rather be working in my lab.

80
00:05:29,048 --> 00:05:31,752
Speaker SPEAKER_01: And so I said, well, why don't you make an appointment and we'll talk?

81
00:05:32,413 --> 00:05:33,615
Speaker SPEAKER_01: And so he said, how about now?

82
00:05:34,836 --> 00:05:36,458
Speaker SPEAKER_01: And that sort of was Ilya's character.

83
00:05:37,158 --> 00:05:43,086
Speaker SPEAKER_01: So we talked for a bit, and I gave him a paper to read, which was the Nature paper on back propagation.

84
00:05:43,437 --> 00:05:48,543
Speaker SPEAKER_01: and we made another meeting for a week later, and he came back and he said, I didn't understand it.

85
00:05:49,245 --> 00:05:50,365
Speaker SPEAKER_01: And I was very disappointed.

86
00:05:50,406 --> 00:05:53,571
Speaker SPEAKER_01: I thought he seemed like a bright guy, but it's only the chain rule.

87
00:05:53,591 --> 00:05:54,771
Speaker SPEAKER_01: It's not that hard to understand.

88
00:05:55,572 --> 00:05:57,396
Speaker SPEAKER_01: And he said, oh, no, no, I understood that.

89
00:05:58,036 --> 00:06:05,326
Speaker SPEAKER_01: I just don't understand why you don't give the gradient to a sensible function optimizer, which took us quite a few years to think about.

90
00:06:06,026 --> 00:06:08,290
Speaker SPEAKER_01: And it kept on like that with him.

91
00:06:08,310 --> 00:06:11,875
Speaker SPEAKER_01: He had very good, his raw intuitions about things were always very good.

92
00:06:12,276 --> 00:06:16,783
Speaker SPEAKER_00: What do you think had enabled those intuitions for Ilya?

93
00:06:17,543 --> 00:06:18,064
Speaker SPEAKER_01: I don't know.

94
00:06:18,105 --> 00:06:19,627
Speaker SPEAKER_01: I think he always thought for himself.

95
00:06:20,067 --> 00:06:22,250
Speaker SPEAKER_01: He was always interested in AI from a young age.

96
00:06:23,233 --> 00:06:27,519
Speaker SPEAKER_01: He's obviously good at math, but it's very hard to know.

97
00:06:28,038 --> 00:06:31,603
Speaker SPEAKER_00: And what was that collaboration between the two of you like?

98
00:06:32,084 --> 00:06:34,908
Speaker SPEAKER_00: What part would you play and what part would Ilya play?

99
00:06:35,175 --> 00:06:36,096
Speaker SPEAKER_01: It was a lot of fun.

100
00:06:36,538 --> 00:06:55,581
Speaker SPEAKER_01: I remember one occasion when we were trying to do a complicated thing with producing maps of data where I had a kind of mixture model so you could take the same bunch of similarities and make two maps so that in one map bank could be close to greed and in another map bank could be close to river.

101
00:06:56,151 --> 00:06:58,915
Speaker SPEAKER_01: Because in one map, you can't have it close to both, right?

102
00:06:58,935 --> 00:07:00,598
Speaker SPEAKER_01: Because River and Greed are a long way apart.

103
00:07:01,218 --> 00:07:02,680
Speaker SPEAKER_01: So we'd have a mixture of maps.

104
00:07:03,382 --> 00:07:09,470
Speaker SPEAKER_01: And we were doing it in MATLAB, and this involved a lot of reorganization of the code to do the right matrix multipliers.

105
00:07:09,730 --> 00:07:10,752
Speaker SPEAKER_01: And then he got fed up with that.

106
00:07:11,392 --> 00:07:17,942
Speaker SPEAKER_01: So he came one day and said, I'm going to write an interface for MATLAB, so I program in this different language.

107
00:07:18,322 --> 00:07:21,466
Speaker SPEAKER_01: And then I have something that just converts it into MATLAB.

108
00:07:21,447 --> 00:07:24,831
Speaker SPEAKER_01: And I said, no, Elia, that'll take you a month to do.

109
00:07:25,151 --> 00:07:26,392
Speaker SPEAKER_01: We've got to get on with this project.

110
00:07:26,473 --> 00:07:27,553
Speaker SPEAKER_01: Don't get diverted by that.

111
00:07:28,194 --> 00:07:33,362
Speaker SPEAKER_01: And Elia said, it's OK, I did it this morning.

112
00:07:33,382 --> 00:07:35,384
Speaker SPEAKER_00: That's quite incredible.

113
00:07:35,483 --> 00:07:44,314
Speaker SPEAKER_00: And throughout those years, the biggest shift wasn't necessarily just the algorithms, but also the scale.

114
00:07:44,615 --> 00:07:46,036
Speaker SPEAKER_00: How did you sort of

115
00:07:46,439 --> 00:07:49,425
Speaker SPEAKER_00: view that scale over the years?

116
00:07:49,725 --> 00:07:51,588
Speaker SPEAKER_01: Ilja got that intuition very early.

117
00:07:52,110 --> 00:07:56,798
Speaker SPEAKER_01: So Ilja was always preaching that you just make it bigger and it'll work better.

118
00:07:57,600 --> 00:07:59,223
Speaker SPEAKER_01: And I always thought that was a bit of a cop-out.

119
00:07:59,423 --> 00:08:01,528
Speaker SPEAKER_01: You're going to have to have new ideas too.

120
00:08:01,507 --> 00:08:03,310
Speaker SPEAKER_01: It turns out Ilya was basically right.

121
00:08:03,531 --> 00:08:04,312
Speaker SPEAKER_01: New ideas help.

122
00:08:04,774 --> 00:08:06,175
Speaker SPEAKER_01: Things like transformers helped a lot.

123
00:08:06,757 --> 00:08:10,362
Speaker SPEAKER_01: But it was really the scale of the data and the scale of the computation.

124
00:08:10,704 --> 00:08:14,850
Speaker SPEAKER_01: And back then, we had no idea computers would get like a billion times faster.

125
00:08:15,211 --> 00:08:16,915
Speaker SPEAKER_01: We thought maybe they'd get a hundred times faster.

126
00:08:17,636 --> 00:08:24,567
Speaker SPEAKER_01: We were trying to do things by coming up with clever ideas that would have just solved themselves if we'd had bigger scale of the data and computation.

127
00:08:24,588 --> 00:08:26,672
Speaker SPEAKER_01: In about 2011,

128
00:08:26,651 --> 00:08:33,160
Speaker SPEAKER_01: Ilya and another graduate student called James Martins and I had a paper using character level prediction.

129
00:08:33,802 --> 00:08:38,268
Speaker SPEAKER_01: So we took Wikipedia and we tried to predict the next HTML character.

130
00:08:39,168 --> 00:08:40,571
Speaker SPEAKER_01: And that worked remarkably well.

131
00:08:41,251 --> 00:08:43,414
Speaker SPEAKER_01: And we were always amazed at how well it worked.

132
00:08:43,914 --> 00:08:47,259
Speaker SPEAKER_01: And that was using a fancy optimizer on GPUs.

133
00:08:48,620 --> 00:08:54,229
Speaker SPEAKER_01: And we could never quite believe that it understood anything, but it looked as though it understood.

134
00:08:54,360 --> 00:08:55,965
Speaker SPEAKER_01: And that just seemed incredible.

135
00:08:56,485 --> 00:09:02,659
Speaker SPEAKER_00: Can you take us through, how are these models trained to predict the next word?

136
00:09:03,380 --> 00:09:07,289
Speaker SPEAKER_00: And why is it the wrong way of thinking about them?

137
00:09:07,607 --> 00:09:10,071
Speaker SPEAKER_01: Okay, I don't actually believe it is the wrong way.

138
00:09:10,672 --> 00:09:17,280
Speaker SPEAKER_01: So, in fact, I think I made the first neural net language model that used embeddings and backpropagation.

139
00:09:17,881 --> 00:09:37,006
Speaker SPEAKER_01: So it's very simple data, just triples, and it was turning each symbol into an embedding, then having the embeddings interact to predict the embedding of the next symbol, and then from that predict the next symbol, and then it was backpropagating through that whole process to learn these triples, and I showed it could generalize.

140
00:09:37,072 --> 00:09:42,913
Speaker SPEAKER_01: About 10 years later, Yoshio Benji used a very similar network and showed it worked with real text.

141
00:09:43,113 --> 00:09:46,515
Speaker SPEAKER_01: And about 10 years after that, linguists started believing in embeddings.

142
00:09:47,096 --> 00:09:47,998
Speaker SPEAKER_01: It was a slow process.

143
00:09:48,678 --> 00:09:55,304
Speaker SPEAKER_01: The reason I think it's not just predicting the next symbol is if you ask, well, what does it take to predict the next symbol?

144
00:09:55,666 --> 00:10:05,154
Speaker SPEAKER_01: Particularly if you ask me a question, and then the first word of the answer is the next symbol, you have to understand the question.

145
00:10:06,035 --> 00:10:09,019
Speaker SPEAKER_01: So I think by predicting the next symbol,

146
00:10:09,691 --> 00:10:12,355
Speaker SPEAKER_01: It's very unlike old-fashioned autocomplete.

147
00:10:12,735 --> 00:10:21,889
Speaker SPEAKER_01: Old-fashioned autocomplete, you'd store sort of triples of words, and then if you store a pair of words, you see how often different words came third, and that way you can predict the next symbol.

148
00:10:22,331 --> 00:10:24,374
Speaker SPEAKER_01: And that's what most people think autocomplete is like.

149
00:10:25,556 --> 00:10:26,878
Speaker SPEAKER_01: It's no longer at all like that.

150
00:10:28,039 --> 00:10:30,363
Speaker SPEAKER_01: To predict the next symbol, you have to understand what's being said.

151
00:10:30,383 --> 00:10:34,149
Speaker SPEAKER_01: So I think you're forcing it to understand by making it predict the next symbol.

152
00:10:34,870 --> 00:10:37,875
Speaker SPEAKER_01: And I think it's understanding in much the same way we are.

153
00:10:38,057 --> 00:10:40,841
Speaker SPEAKER_01: So a lot of people will tell you, these things aren't like us.

154
00:10:41,822 --> 00:10:43,164
Speaker SPEAKER_01: They're just predicting the next symbol.

155
00:10:43,566 --> 00:10:44,707
Speaker SPEAKER_01: They're not reasoning like us.

156
00:10:45,448 --> 00:10:49,214
Speaker SPEAKER_01: But actually, in order to predict the next symbol, it's going to have to do some reasoning.

157
00:10:49,533 --> 00:10:55,964
Speaker SPEAKER_01: And we've seen now that if you make big ones, without putting in any special stuff to do reasoning, they can already do some reasoning.

158
00:10:56,384 --> 00:10:59,128
Speaker SPEAKER_01: And I think as you make them bigger, they're going to be able to do more and more reasoning.

159
00:10:59,688 --> 00:11:03,153
Speaker SPEAKER_00: Do you think I'm doing anything else than predicting the next symbol right now?

160
00:11:03,522 --> 00:11:04,945
Speaker SPEAKER_01: I think that's how you're learning.

161
00:11:05,927 --> 00:11:07,471
Speaker SPEAKER_01: You're predicting the next video frame.

162
00:11:08,592 --> 00:11:10,155
Speaker SPEAKER_01: You're predicting the next sound.

163
00:11:11,318 --> 00:11:14,625
Speaker SPEAKER_01: But I think that's a pretty plausible theory of how the brain's learning.

164
00:11:15,126 --> 00:11:21,159
Speaker SPEAKER_00: What enables these models to learn such a wide variety of fields?

165
00:11:21,206 --> 00:11:24,910
Speaker SPEAKER_01: What these big language models are doing is they're looking for common structure.

166
00:11:25,711 --> 00:11:29,996
Speaker SPEAKER_01: And by finding common structure, they can encode things using the common structure, and that's more efficient.

167
00:11:30,677 --> 00:11:31,820
Speaker SPEAKER_01: So let me give you an example.

168
00:11:32,480 --> 00:11:37,907
Speaker SPEAKER_01: If you ask GPT-4, why is a compost heap like an atom bomb?

169
00:11:37,927 --> 00:11:39,288
Speaker SPEAKER_01: Most people can't answer that.

170
00:11:39,347 --> 00:11:42,751
Speaker SPEAKER_01: Most people haven't thought, they think atom bombs and compost heaps are very different things.

171
00:11:43,393 --> 00:11:49,240
Speaker SPEAKER_01: But GPT-4 will tell you, well, the energy scales are very different, and the time scales are very different.

172
00:11:49,607 --> 00:11:54,152
Speaker SPEAKER_01: But the thing that's the same is that when the compost heap gets hotter, it generates heat faster.

173
00:11:54,873 --> 00:11:59,015
Speaker SPEAKER_01: And when the atom bomb produces more neutrons, it produces more neutrons faster.

174
00:12:00,157 --> 00:12:02,739
Speaker SPEAKER_01: And so it gets the idea of a chain reaction.

175
00:12:03,120 --> 00:12:06,101
Speaker SPEAKER_01: And I believe it's understood they're both forms of chain reaction.

176
00:12:06,182 --> 00:12:10,125
Speaker SPEAKER_01: It's using that understanding to compress all that information into its weights.

177
00:12:11,047 --> 00:12:18,432
Speaker SPEAKER_01: And if it's doing that, then it's gonna be doing that for hundreds of things where we haven't seen the analogies yet, but it has.

178
00:12:18,413 --> 00:12:23,100
Speaker SPEAKER_01: And that's where you get creativity from, from seeing these analogies between apparently very different things.

179
00:12:23,780 --> 00:12:27,486
Speaker SPEAKER_01: And so I think GPT-4 is going to end up, when it gets bigger, being very creative.

180
00:12:27,527 --> 00:12:35,038
Speaker SPEAKER_01: I think this idea that it's just regurgitating what it's learned, just pastiching together text it's learned already, that's completely wrong.

181
00:12:35,438 --> 00:12:37,822
Speaker SPEAKER_01: It's going to be even more creative than people, I think.

182
00:12:37,971 --> 00:12:46,947
Speaker SPEAKER_00: you'd argue that it won't just repeat the human knowledge we've developed so far, but could also progress beyond that.

183
00:12:47,448 --> 00:12:50,313
Speaker SPEAKER_00: I think that's something we haven't quite seen yet.

184
00:12:50,533 --> 00:12:58,246
Speaker SPEAKER_00: We've started seeing some examples of it, but to a large extent, we're sort of still at the current level of science.

185
00:12:58,267 --> 00:13:00,350
Speaker SPEAKER_00: What do you think will enable it to go beyond that?

186
00:13:00,398 --> 00:13:02,782
Speaker SPEAKER_01: Well, we've seen that in more limited contexts.

187
00:13:02,802 --> 00:13:16,825
Speaker SPEAKER_01: Like if you take AlphaGo, in that famous competition with Lee Sedol, there was move 37, where AlphaGo made a move that all the experts said must have been a mistake, but actually later they realized it was a brilliant move.

188
00:13:18,187 --> 00:13:20,250
Speaker SPEAKER_01: So that was creative within that limited domain.

189
00:13:21,192 --> 00:13:24,717
Speaker SPEAKER_01: I think we'll see a lot more of that as these things get bigger.

190
00:13:24,951 --> 00:13:35,645
Speaker SPEAKER_00: The difference with AlphaGo as well was that it was using reinforcement learning, that that subsequently sort of enabled it to go beyond the current state.

191
00:13:35,706 --> 00:13:42,894
Speaker SPEAKER_00: So it started with imitation learning, watching how humans play the game, and then it would, through self-play, develop way beyond that.

192
00:13:43,154 --> 00:13:45,938
Speaker SPEAKER_00: Do you think that's the missing component of the current LLS?

193
00:13:46,158 --> 00:13:49,102
Speaker SPEAKER_01: I think that may well be a missing component, yes.

194
00:13:49,082 --> 00:13:55,990
Speaker SPEAKER_01: The self-play in AlphaGo and AlphaZero are a large part of why it could make these creative moves.

195
00:13:56,330 --> 00:13:59,173
Speaker SPEAKER_01: But I don't think it's entirely necessary.

196
00:13:59,854 --> 00:14:05,980
Speaker SPEAKER_01: So there's a little experiment I did a long time ago, where you're training a neural net to recognize handwritten digits.

197
00:14:06,580 --> 00:14:08,182
Speaker SPEAKER_01: I love that example, the MNIST example.

198
00:14:09,263 --> 00:14:11,905
Speaker SPEAKER_01: And you give it training data where half the answers are wrong.

199
00:14:13,407 --> 00:14:16,389
Speaker SPEAKER_01: And the question is, how well will it learn?

200
00:14:18,326 --> 00:14:23,131
Speaker SPEAKER_01: And you make half the answers wrong once and keep them like that.

201
00:14:23,231 --> 00:14:28,817
Speaker SPEAKER_01: So it can't average away the wrongness by just seeing the same example but with the right answer sometimes and the wrong answer sometimes.

202
00:14:29,177 --> 00:14:33,621
Speaker SPEAKER_01: When it sees that example, half of the examples, when it sees the example, the answer's always wrong.

203
00:14:34,962 --> 00:14:37,905
Speaker SPEAKER_01: And so the training data has 50% error.

204
00:14:38,927 --> 00:14:44,351
Speaker SPEAKER_01: But if you train up backpropagation, it gets down to 5% error or less.

205
00:14:44,904 --> 00:14:51,351
Speaker SPEAKER_01: In other words, from badly labeled data, it can get much better results.

206
00:14:51,772 --> 00:14:53,494
Speaker SPEAKER_01: It can see that the training data is wrong.

207
00:14:54,114 --> 00:14:56,537
Speaker SPEAKER_01: And that's how smart students can be smarter than their advisor.

208
00:14:57,158 --> 00:15:02,743
Speaker SPEAKER_01: And their advisor tells them all this stuff, and for half of what their advisor tells them, they think, no, rubbish.

209
00:15:03,104 --> 00:15:06,226
Speaker SPEAKER_01: And they listen to the other half, and then they end up smarter than the advisor.

210
00:15:06,626 --> 00:15:13,173
Speaker SPEAKER_01: So these big neural nets can actually do, they can do much better than their training data, and most people don't realize that.

211
00:15:13,441 --> 00:15:18,027
Speaker SPEAKER_00: So how do you expect these models to add reasoning into them?

212
00:15:18,126 --> 00:15:30,562
Speaker SPEAKER_00: So, I mean, one approach is you add sort of the heuristics on top of them, which a lot of the research is doing now, where you have sort of chain of thought, you just feedback its reasoning into itself.

213
00:15:30,582 --> 00:15:35,129
Speaker SPEAKER_00: And another way would be in the model itself as you scale it up.

214
00:15:35,710 --> 00:15:37,272
Speaker SPEAKER_00: What's your intuition around that?

215
00:15:38,028 --> 00:15:41,916
Speaker SPEAKER_01: So my intuition is that as we scale up these models, I get better at reasoning.

216
00:15:42,697 --> 00:15:49,572
Speaker SPEAKER_01: And if you ask how people work, roughly speaking, we have these intuitions and we can do reasoning.

217
00:15:50,076 --> 00:15:52,460
Speaker SPEAKER_01: and we use the reasoning to correct our intuitions.

218
00:15:53,201 --> 00:16:01,273
Speaker SPEAKER_01: Of course, we use the intuitions during the reasoning to do the reasoning, but if the conclusion of the reasoning conflicts with our intuitions, we realize the intuitions need to be changed.

219
00:16:01,774 --> 00:16:08,524
Speaker SPEAKER_01: That's much like in AlphaGo or AlphaZero, where you have an evaluation function,

220
00:16:08,504 --> 00:16:11,128
Speaker SPEAKER_01: that just looks at a board and says, how good is that for me?

221
00:16:11,969 --> 00:16:19,221
Speaker SPEAKER_01: But then you do the Monte Carlo rollout, and now you get a more accurate idea, and you can revise your evaluation function.

222
00:16:19,240 --> 00:16:23,047
Speaker SPEAKER_01: So you can train it by getting it to agree with the results of reasoning.

223
00:16:23,067 --> 00:16:26,052
Speaker SPEAKER_01: And I think these large language models have to start doing that.

224
00:16:26,091 --> 00:16:33,263
Speaker SPEAKER_01: They have to start training their raw intuitions about what should come next by doing reasoning and realizing that's not right.

225
00:16:33,243 --> 00:16:38,673
Speaker SPEAKER_01: And so that way they can get more training data than just mimicking what people did.

226
00:16:39,294 --> 00:16:42,481
Speaker SPEAKER_01: And that's exactly why AlphaGo could do this creative move 37.

227
00:16:43,023 --> 00:16:48,855
Speaker SPEAKER_01: It had much more training data because it was using reasoning to check out what the right next move should have been.

228
00:16:49,341 --> 00:16:51,705
Speaker SPEAKER_00: And what do you think about multimodality?

229
00:16:51,745 --> 00:16:57,250
Speaker SPEAKER_00: So we spoke about these analogies and often the analogies are way beyond what we could see.

230
00:16:57,711 --> 00:17:04,818
Speaker SPEAKER_00: It's discovering analogies that are far beyond the humans and maybe abstraction levels that we'll never be able to understand.

231
00:17:04,838 --> 00:17:13,107
Speaker SPEAKER_00: Now, when we introduce images to that and video and sound, how do you think that will change the models?

232
00:17:13,127 --> 00:17:17,813
Speaker SPEAKER_00: And how do you think it will change the analogies that it will be able to make?

233
00:17:18,146 --> 00:17:20,670
Speaker SPEAKER_01: I think it'll change it a lot.

234
00:17:21,010 --> 00:17:24,454
Speaker SPEAKER_01: I think it'll make it much better at understanding spatial things, for example.

235
00:17:24,895 --> 00:17:33,165
Speaker SPEAKER_01: From language alone, it's quite hard to understand some spatial things, although, remarkably, GPT-4 can do that, even before it was multimodal.

236
00:17:34,587 --> 00:17:38,973
Speaker SPEAKER_01: But when you make it multimodal, if you have it both doing vision

237
00:17:38,953 --> 00:17:44,744
Speaker SPEAKER_01: and reaching out and grabbing things, it'll understand objects much better if it can pick them up and turn them over and so on.

238
00:17:45,445 --> 00:17:55,865
Speaker SPEAKER_01: So, although you can learn an awful lot from language, it's easier to learn if you're multimodal, and in fact you then need less language.

239
00:17:55,846 --> 00:18:00,272
Speaker SPEAKER_01: And there's an awful lot of YouTube video for predicting the next frame, or something like that.

240
00:18:01,213 --> 00:18:04,038
Speaker SPEAKER_01: So I think these multimodal models are clearly going to take over.

241
00:18:05,240 --> 00:18:06,422
Speaker SPEAKER_01: You can get more data that way.

242
00:18:06,781 --> 00:18:07,763
Speaker SPEAKER_01: They need less language.

243
00:18:08,304 --> 00:18:16,115
Speaker SPEAKER_01: So there's really a philosophical point that you could learn a very good model from language alone, but it's much easier to learn it from a multimodal system.

244
00:18:16,298 --> 00:18:20,221
Speaker SPEAKER_00: And how do you think it will impact the model's reasoning?

245
00:18:20,563 --> 00:18:23,385
Speaker SPEAKER_01: I think it'll make it much better at reasoning about space, for example.

246
00:18:23,645 --> 00:18:25,528
Speaker SPEAKER_01: Reasoning about what happens if you pick objects up.

247
00:18:25,568 --> 00:18:29,393
Speaker SPEAKER_01: If you actually try picking objects up, you're going to get all sorts of training data that's going to help.

248
00:18:29,673 --> 00:18:39,183
Speaker SPEAKER_00: Do you think the human brain evolved to work well with language, or do you think language evolved to work well with the human brain?

249
00:18:39,586 --> 00:18:45,518
Speaker SPEAKER_01: I think the question of whether language evolved to work with the brain or the brain evolved to work with language, I think that's a very good question.

250
00:18:46,019 --> 00:18:47,583
Speaker SPEAKER_01: I think both happened.

251
00:18:48,163 --> 00:18:52,953
Speaker SPEAKER_01: I used to think we would do a lot of cognition without needing language at all.

252
00:18:53,255 --> 00:18:55,817
Speaker SPEAKER_01: Now I've changed my mind a bit.

253
00:18:56,438 --> 00:19:01,862
Speaker SPEAKER_01: So let me give you three different views of language and how it relates to cognition.

254
00:19:02,563 --> 00:19:14,953
Speaker SPEAKER_01: There's the old fashioned symbolic view which is cognition consists of having strings of symbols in some kind of cleaned up logical language where there's no ambiguity and applying rules of inference.

255
00:19:15,253 --> 00:19:16,555
Speaker SPEAKER_01: And that's what cognition is.

256
00:19:16,575 --> 00:19:22,079
Speaker SPEAKER_01: It's just these symbolic manipulations on things that are like strings of language symbols.

257
00:19:22,059 --> 00:19:23,382
Speaker SPEAKER_01: So that's one extreme view.

258
00:19:23,923 --> 00:19:28,551
Speaker SPEAKER_01: An opposite extreme view is, no, no, once you get inside the head, it's all vectors.

259
00:19:29,112 --> 00:19:38,449
Speaker SPEAKER_01: So symbols come in, you convert those symbols into big vectors, and all the stuff inside is done with big vectors, and then if you want to produce output, you produce symbols again.

260
00:19:38,429 --> 00:19:52,046
Speaker SPEAKER_01: So there was a point in machine translation in about 2014, when people were using recurrent neural nets, and words would keep coming in, and they'd have a hidden state, and they'd keep accumulating information in this hidden state.

261
00:19:52,686 --> 00:20:01,698
Speaker SPEAKER_01: So when they got to the end of a sentence, they'd have a big hidden vector that captured the meaning of that sentence, that could then be used for producing the sentence in another language.

262
00:20:01,938 --> 00:20:03,660
Speaker SPEAKER_01: That was called a thought vector.

263
00:20:03,640 --> 00:20:05,183
Speaker SPEAKER_01: And that's the sort of second view of language.

264
00:20:05,203 --> 00:20:11,039
Speaker SPEAKER_01: You convert the language into a big vector that's nothing like language, and that's what cognition is all about.

265
00:20:12,103 --> 00:20:19,522
Speaker SPEAKER_01: But then there's a third view, which is what I believe now, which is that you take these symbols

266
00:20:20,836 --> 00:20:27,127
Speaker SPEAKER_01: And you convert the symbols into embeddings, and you use multiple layers of that, so you get these very rich embeddings.

267
00:20:27,628 --> 00:20:33,681
Speaker SPEAKER_01: But the embeddings are still tied to the symbols, in the sense that you've got a big vector for this symbol and a big vector for that symbol.

268
00:20:34,099 --> 00:20:38,087
Speaker SPEAKER_01: And these vectors interact to produce the vector for the symbol for the next word.

269
00:20:38,990 --> 00:20:40,353
Speaker SPEAKER_01: And that's what understanding is.

270
00:20:40,393 --> 00:20:48,449
Speaker SPEAKER_01: Understanding is knowing how to convert the symbols into these vectors and knowing how the elements of the vector should interact to predict the vector for the next symbol.

271
00:20:48,849 --> 00:20:52,497
Speaker SPEAKER_01: That's what understanding is both in these big language models and in our brains.

272
00:20:53,422 --> 00:20:56,268
Speaker SPEAKER_01: And that's an example which is sort of in between.

273
00:20:56,588 --> 00:21:01,599
Speaker SPEAKER_01: You're staying with the symbols, but you're interpreting them as these big vectors.

274
00:21:02,161 --> 00:21:03,282
Speaker SPEAKER_01: And that's where all the work is.

275
00:21:03,663 --> 00:21:10,519
Speaker SPEAKER_01: And all the knowledge is in what vectors you use and how the elements of those vectors interact, not in symbolic rules.

276
00:21:10,499 --> 00:21:14,888
Speaker SPEAKER_01: But it's not saying that you get away from the symbol altogether.

277
00:21:15,009 --> 00:21:20,059
Speaker SPEAKER_01: It's saying you turn the symbols into big vectors, but you stay with that surface structure of the symbols.

278
00:21:20,520 --> 00:21:21,924
Speaker SPEAKER_01: And that's how these models are working.

279
00:21:22,365 --> 00:21:25,673
Speaker SPEAKER_01: And that's now seemed to me a more plausible model of human thought too.

280
00:21:25,653 --> 00:21:34,367
Speaker SPEAKER_00: You were one of the first folks to get the idea of using GPUs, and I know Jensen loves you for that.

281
00:21:35,049 --> 00:21:42,281
Speaker SPEAKER_00: Back in 2009, you told Jensen that this could be a quite good idea for training neural nets.

282
00:21:42,702 --> 00:21:47,711
Speaker SPEAKER_00: Take us back to that early intuition of using GPUs for training neural nets.

283
00:21:48,298 --> 00:21:56,145
Speaker SPEAKER_01: So actually, I think in about 2006, I had a former graduate student called Rick Zaleski, who's a very good computer vision guy.

284
00:21:57,267 --> 00:21:59,229
Speaker SPEAKER_01: And I talked to him at a meeting.

285
00:21:59,630 --> 00:22:05,194
Speaker SPEAKER_01: He said, you know, you ought to think about using graphics processing cards because they're very good at matrix multiplies.

286
00:22:05,615 --> 00:22:07,656
Speaker SPEAKER_01: And what you're doing is basically all matrix multiplies.

287
00:22:08,538 --> 00:22:09,759
Speaker SPEAKER_01: So I thought about that for a bit.

288
00:22:09,838 --> 00:22:16,045
Speaker SPEAKER_01: And then we learned about these Tesla systems that had four GPUs in.

289
00:22:16,160 --> 00:22:23,315
Speaker SPEAKER_01: And initially we just got gaming GPUs and discovered they made things go 30 times faster.

290
00:22:23,915 --> 00:22:30,569
Speaker SPEAKER_01: And then we bought one of these Tesla systems with four GPUs and we did speech on that and it worked very well.

291
00:22:31,191 --> 00:22:34,438
Speaker SPEAKER_01: And then in 2009, I gave a talk at NIPS.

292
00:22:34,739 --> 00:22:38,686
Speaker SPEAKER_01: And I told 1,000 machine learning researchers, you should all go and buy NVIDIA GPUs.

293
00:22:38,948 --> 00:22:41,152
Speaker SPEAKER_01: They're the future, you need them for doing machine learning.

294
00:22:42,094 --> 00:22:49,087
Speaker SPEAKER_01: And I actually then sent mail to NVIDIA saying, I told 1,000 machine learning researchers to buy your boards, could you give me a free one?

295
00:22:49,107 --> 00:22:50,230
Speaker SPEAKER_01: And they said no.

296
00:22:50,250 --> 00:22:51,913
Speaker SPEAKER_01: Actually, they didn't say no, they just didn't reply.

297
00:22:53,297 --> 00:22:56,442
Speaker SPEAKER_01: But when I told Jensen this story later on, he gave me a free one.

298
00:22:58,262 --> 00:23:00,224
Speaker SPEAKER_00: That's very good.

299
00:23:00,325 --> 00:23:07,132
Speaker SPEAKER_00: I think what's interesting as well is sort of how GPUs has evolved alongside the field.

300
00:23:07,311 --> 00:23:12,497
Speaker SPEAKER_00: So where do you think we should go next in the compute?

301
00:23:12,517 --> 00:23:18,061
Speaker SPEAKER_01: So my last couple of years at Google, I was thinking about ways of trying to make analog computation.

302
00:23:18,762 --> 00:23:26,750
Speaker SPEAKER_01: So instead of using like a megawatt, we could use like 30 watts like the brain, and we could run these big language models in analog hardware.

303
00:23:27,118 --> 00:23:34,714
Speaker SPEAKER_01: and I never made it work, but I started really appreciating digital computation.

304
00:23:35,517 --> 00:23:40,166
Speaker SPEAKER_01: So, if you're going to use that low-power analog computation,

305
00:23:40,618 --> 00:23:42,601
Speaker SPEAKER_01: every piece of hardware's gonna be a bit different.

306
00:23:43,122 --> 00:23:47,866
Speaker SPEAKER_01: And the idea is the learning is gonna make use of the specific properties of that hardware.

307
00:23:47,886 --> 00:23:48,989
Speaker SPEAKER_01: And that's what happens with people.

308
00:23:49,088 --> 00:23:49,990
Speaker SPEAKER_01: All our brains are different.

309
00:23:51,932 --> 00:23:55,696
Speaker SPEAKER_01: So we can't then take the weights in your brain and put them in my brain.

310
00:23:55,757 --> 00:23:59,500
Speaker SPEAKER_01: The hardware's different, the precise properties of the individual neurons are different.

311
00:23:59,881 --> 00:24:02,865
Speaker SPEAKER_01: The learning has learned to make use of all that.

312
00:24:03,605 --> 00:24:07,490
Speaker SPEAKER_01: And so we're mortal in the sense that the weights in my brain are no good for any other brain.

313
00:24:07,730 --> 00:24:09,833
Speaker SPEAKER_01: When I die, those weights are useless.

314
00:24:09,813 --> 00:24:19,689
Speaker SPEAKER_01: We can get information from one to another rather inefficiently by I produce sentences and you figure out how to change your weight so you would have said the same thing.

315
00:24:20,269 --> 00:24:21,391
Speaker SPEAKER_01: That's called distillation.

316
00:24:21,951 --> 00:24:24,596
Speaker SPEAKER_01: But that's a very inefficient way of communicating knowledge.

317
00:24:25,357 --> 00:24:30,605
Speaker SPEAKER_01: And with digital systems, they're immortal because once you've got some weights,

318
00:24:30,586 --> 00:24:40,846
Speaker SPEAKER_01: you can throw away the computer, just store the weights on a tape somewhere, and now build another computer, put those same weights in, and if it's digital, it can compute exactly the same thing as the other system did.

319
00:24:41,827 --> 00:24:48,000
Speaker SPEAKER_01: So, digital systems can share weights, and that's incredibly much more efficient.

320
00:24:48,019 --> 00:24:52,729
Speaker SPEAKER_01: If you've got a whole bunch of digital systems, and they each go and do a tiny bit of learning,

321
00:24:53,856 --> 00:24:57,603
Speaker SPEAKER_01: and they start with the same weights, they do a tiny bit of learning and then they share their weights again.

322
00:24:58,805 --> 00:25:00,247
Speaker SPEAKER_01: They all know what all the others learned.

323
00:25:00,968 --> 00:25:01,729
Speaker SPEAKER_01: We can't do that.

324
00:25:02,509 --> 00:25:05,575
Speaker SPEAKER_01: And so they're far superior to us in being able to share knowledge.

325
00:25:06,036 --> 00:25:11,364
Speaker SPEAKER_00: A lot of the ideas that have been deployed in the field are very old school ideas.

326
00:25:12,325 --> 00:25:16,290
Speaker SPEAKER_00: It's the ideas that have been around in neuroscience for forever.

327
00:25:16,632 --> 00:25:20,758
Speaker SPEAKER_00: What do you think is sort of left to apply to the systems that we develop?

328
00:25:20,974 --> 00:25:30,048
Speaker SPEAKER_01: So one big thing that we still have to catch up with neuroscience on is the time scales for changes.

329
00:25:30,190 --> 00:25:36,019
Speaker SPEAKER_01: So in nearly all the neural nets, there's a fast time scale for changing activities.

330
00:25:36,078 --> 00:25:38,021
Speaker SPEAKER_01: So input comes in the activities.

331
00:25:38,321 --> 00:25:44,790
Speaker SPEAKER_01: the embedding vectors all change, and then there's a slow timescale which is changing the weights, and that's long-term learning.

332
00:25:45,392 --> 00:25:46,874
Speaker SPEAKER_01: And you just have those two timescales.

333
00:25:47,575 --> 00:25:50,598
Speaker SPEAKER_01: In the brain, there's many timescales at which weights change.

334
00:25:51,220 --> 00:26:06,861
Speaker SPEAKER_01: So for example, if I say an unexpected word like cucumber, and now five minutes later, you put headphones on, there's a lot of noise, and there's very faint words, you'll be much better at recognizing the word cucumber because I said it five minutes ago.

335
00:26:07,297 --> 00:26:09,239
Speaker SPEAKER_01: So where is that knowledge in the brain?

336
00:26:10,121 --> 00:26:13,284
Speaker SPEAKER_01: And that knowledge is obviously in temporary changes to synapses.

337
00:26:13,724 --> 00:26:16,008
Speaker SPEAKER_01: It's not neurons that are going cucumber, cucumber, cucumber.

338
00:26:16,508 --> 00:26:17,710
Speaker SPEAKER_01: You don't have enough neurons for that.

339
00:26:18,391 --> 00:26:20,553
Speaker SPEAKER_01: It's in temporary changes to the weights.

340
00:26:21,253 --> 00:26:25,358
Speaker SPEAKER_01: And you can do a lot of things with temporary weight changes, what I call fast weights.

341
00:26:26,099 --> 00:26:27,682
Speaker SPEAKER_01: We don't do that in these neural models.

342
00:26:27,741 --> 00:26:35,531
Speaker SPEAKER_01: And the reason we don't do it is because if you have temporary changes to the weights that depend on the input data,

343
00:26:35,511 --> 00:26:39,538
Speaker SPEAKER_01: then you can't process a whole bunch of different cases at the same time.

344
00:26:40,519 --> 00:26:50,297
Speaker SPEAKER_01: At present, we take a whole bunch of different strings, we stack them together, and we process them all in parallel, because then we can do matrix-matrix multiplies, which is much more efficient.

345
00:26:50,867 --> 00:26:55,031
Speaker SPEAKER_01: And just that efficiency is stopping us using fast weights.

346
00:26:55,051 --> 00:26:58,575
Speaker SPEAKER_01: But the brain clearly uses fast weights for temporary memory.

347
00:26:59,035 --> 00:27:01,678
Speaker SPEAKER_01: And there's all sorts of things you can do that way that we don't do at present.

348
00:27:02,097 --> 00:27:03,779
Speaker SPEAKER_01: I think that's one of the biggest things we have to learn.

349
00:27:04,019 --> 00:27:12,567
Speaker SPEAKER_01: I was very hopeful that things like Graphcore, if they went sequential and did just online learning, then they could use fast weights.

350
00:27:15,109 --> 00:27:16,211
Speaker SPEAKER_01: But that hasn't worked out yet.

351
00:27:16,612 --> 00:27:20,335
Speaker SPEAKER_01: I think it'll work out eventually when people are using conductances for weights.

352
00:27:20,939 --> 00:27:28,647
Speaker SPEAKER_00: How has knowing how these models work and knowing how the brain works impacted the way you think?

353
00:27:29,469 --> 00:27:43,385
Speaker SPEAKER_01: I think there's been one big impact, which is at a fairly abstract level, which is that for many years people were very scornful about the idea of having a big random neural net

354
00:27:43,567 --> 00:27:46,952
Speaker SPEAKER_01: and just giving it a lot of training data and it would learn to do complicated things.

355
00:27:47,413 --> 00:27:53,402
Speaker SPEAKER_01: If you talk to statisticians or linguists or most people in AI, they say, that's just a pipe dream.

356
00:27:53,422 --> 00:27:59,770
Speaker SPEAKER_01: There's no way you're gonna learn to really complicated things without some kind of innate knowledge, without a lot of architectural restrictions.

357
00:28:00,451 --> 00:28:01,653
Speaker SPEAKER_01: It turns out that's completely wrong.

358
00:28:02,294 --> 00:28:06,500
Speaker SPEAKER_01: You can take a big random neural network and you can learn a whole bunch of stuff just from data.

359
00:28:08,282 --> 00:28:11,887
Speaker SPEAKER_01: So the idea that stochastic gradient descent

360
00:28:11,867 --> 00:28:18,855
Speaker SPEAKER_01: to repeatedly adjust the weights using a gradient, that will learn things, and will learn big, complicated things.

361
00:28:19,676 --> 00:28:22,339
Speaker SPEAKER_01: That's been validated by these big models.

362
00:28:22,840 --> 00:28:24,843
Speaker SPEAKER_01: And that's a very important thing to know about the brain.

363
00:28:25,644 --> 00:28:27,586
Speaker SPEAKER_01: It doesn't have to have all this innate structure.

364
00:28:27,987 --> 00:28:36,036
Speaker SPEAKER_01: Now, obviously, it's got a lot of innate structure, but it certainly doesn't need innate structure for things that are easily learned.

365
00:28:36,016 --> 00:28:44,666
Speaker SPEAKER_01: And so the sort of idea coming from Chomsky that you won't learn anything complicated like language, unless it's all kind of wired in already and just matures.

366
00:28:45,428 --> 00:28:47,410
Speaker SPEAKER_01: That idea is now clearly nonsense.

367
00:28:48,230 --> 00:28:52,276
Speaker SPEAKER_00: I'm sure Chomsky would appreciate you calling his ideas nonsense.

368
00:28:52,817 --> 00:28:57,903
Speaker SPEAKER_01: Well, actually, I think a lot of Chomsky's political ideas are very sensible.

369
00:28:57,923 --> 00:29:03,390
Speaker SPEAKER_01: I'm always struck by how come someone with such sensible ideas about the Middle East could be so wrong about linguistics.

370
00:29:04,011 --> 00:29:11,342
Speaker SPEAKER_00: What do you think would make these models simulate consciousness of humans more effectively?

371
00:29:11,502 --> 00:29:26,805
Speaker SPEAKER_00: But imagine you had the AI assistant that you've spoken to in your entire life, and instead of that being, you know, like ChatGPT today, that sort of deletes the memory of the conversation and you start fresh all of the time, it had self-reflection.

372
00:29:27,625 --> 00:29:32,752
Speaker SPEAKER_00: At some point, you pass away, and you tell that to the assistant.

373
00:29:33,172 --> 00:29:34,713
Speaker SPEAKER_00: Do you think it would fit?

374
00:29:34,734 --> 00:29:35,035
Speaker SPEAKER_00: I mean, not me.

375
00:29:35,055 --> 00:29:38,420
Speaker SPEAKER_01: Somebody else tells that to the assistant.

376
00:29:38,440 --> 00:29:38,540
Speaker SPEAKER_00: Yeah.

377
00:29:38,560 --> 00:29:40,903
Speaker SPEAKER_00: It would be difficult for you to tell that to the assistant.

378
00:29:42,125 --> 00:29:44,971
Speaker SPEAKER_00: Do you think that assistant would feel at that point?

379
00:29:45,612 --> 00:29:47,595
Speaker SPEAKER_01: Yes, I think they can have feelings too.

380
00:29:47,915 --> 00:29:53,664
Speaker SPEAKER_01: So I think just as we have this inner theater model for perception, we have an inner theater model for feelings.

381
00:29:54,006 --> 00:29:58,673
Speaker SPEAKER_01: They're things that I can experience, but other people can't.

382
00:29:59,175 --> 00:30:00,978
Speaker SPEAKER_01: I think that model is equally wrong.

383
00:30:01,919 --> 00:30:06,663
Speaker SPEAKER_01: So I think, suppose I say, I feel like punching Gary on the nose, which I often do.

384
00:30:07,625 --> 00:30:11,548
Speaker SPEAKER_01: Let's try and abstract that away from the idea of an inner theater.

385
00:30:11,969 --> 00:30:19,636
Speaker SPEAKER_01: What I'm really saying to you is, if it weren't for the inhibition coming from my frontal lobes, I would perform an action.

386
00:30:20,377 --> 00:30:28,946
Speaker SPEAKER_01: So when we talk about feelings, we're really talking about actions we would perform if it weren't for constraints.

387
00:30:29,163 --> 00:30:31,567
Speaker SPEAKER_01: And that's really what feelings are.

388
00:30:31,586 --> 00:30:33,470
Speaker SPEAKER_01: They're actions we would do if it weren't for constraints.

389
00:30:35,532 --> 00:30:40,380
Speaker SPEAKER_01: So I think you can give the same kind of explanation for feelings, and there's no reason why these things can't have feelings.

390
00:30:40,921 --> 00:30:45,888
Speaker SPEAKER_01: In fact, in 1973, I saw a robot have an emotion.

391
00:30:46,762 --> 00:30:57,278
Speaker SPEAKER_01: So in Edinburgh, they had a robot with two grippers like this that could assemble a toy car if you put the pieces separately on a piece of green felt.

392
00:30:58,779 --> 00:31:02,605
Speaker SPEAKER_01: But if you put them in a pile, his vision wasn't good enough to figure out what was going on.

393
00:31:02,964 --> 00:31:04,548
Speaker SPEAKER_01: So he put his grippers together, and he went whack.

394
00:31:05,269 --> 00:31:07,551
Speaker SPEAKER_01: And it knocked them so they were scattered, and then he could put them together.

395
00:31:08,472 --> 00:31:13,319
Speaker SPEAKER_01: If you saw that in a person, you'd say it was crossed with the situation because it didn't understand it, so it destroyed it.

396
00:31:14,922 --> 00:31:15,623
Speaker SPEAKER_00: That's profound.

397
00:31:17,324 --> 00:31:24,163
Speaker SPEAKER_00: You, when we spoke previously, you described humans and the LLMs as analogy machines.

398
00:31:24,784 --> 00:31:30,539
Speaker SPEAKER_00: What do you think has been the most powerful analogies that you've found throughout your life?

399
00:31:31,869 --> 00:31:33,492
Speaker SPEAKER_01: Oh, throughout my life?

400
00:31:33,833 --> 00:31:47,938
Speaker SPEAKER_01: I guess probably a sort of weak analogy that's influenced me a lot is the analogy between religious belief

401
00:31:47,917 --> 00:31:51,342
Speaker SPEAKER_01: and between belief in symbol processing.

402
00:31:51,362 --> 00:31:58,349
Speaker SPEAKER_01: So when I was very young, I was confronted, I came from an atheist family and went to school and was confronted with religious belief.

403
00:31:58,369 --> 00:31:59,590
Speaker SPEAKER_01: And it just seemed nonsense to me.

404
00:32:00,112 --> 00:32:01,232
Speaker SPEAKER_01: It still seems nonsense to me.

405
00:32:02,153 --> 00:32:11,544
Speaker SPEAKER_01: And when I saw symbol processing as an explanation of how people worked, I thought it was just the same, nonsense.

406
00:32:11,564 --> 00:32:14,027
Speaker SPEAKER_01: I don't think it's quite so much nonsense now.

407
00:32:14,210 --> 00:32:17,453
Speaker SPEAKER_01: Because I think, actually, we do do symbol processing.

408
00:32:17,814 --> 00:32:20,876
Speaker SPEAKER_01: It's just we do it by giving these big embedding vectors to the symbols.

409
00:32:21,538 --> 00:32:23,099
Speaker SPEAKER_01: But we are actually symbol processing.

410
00:32:24,141 --> 00:32:31,288
Speaker SPEAKER_01: But not at all in the way people thought, where you match symbols and the only thing a symbol has is it's identical to another symbol or it's not identical.

411
00:32:31,848 --> 00:32:33,270
Speaker SPEAKER_01: That's the only property a symbol has.

412
00:32:33,951 --> 00:32:34,791
Speaker SPEAKER_01: We don't do that at all.

413
00:32:34,811 --> 00:32:42,039
Speaker SPEAKER_01: We use the context to give embedding vectors to symbols and then use the interactions between the components of these embedding vectors to do thinking.

414
00:32:43,060 --> 00:32:43,862
Speaker SPEAKER_01: But,

415
00:32:44,347 --> 00:32:53,263
Speaker SPEAKER_01: There's a very good researcher at Google called Fernando Pereira, who said, yes, we do have symbolic reasoning, and the only symbolic we have is natural language.

416
00:32:53,744 --> 00:32:56,088
Speaker SPEAKER_01: Natural language is a symbolic language, and we reason with it.

417
00:32:56,810 --> 00:32:57,592
Speaker SPEAKER_01: And I believe that now.

418
00:32:58,192 --> 00:33:02,902
Speaker SPEAKER_00: You've done some of the most meaningful research in the history of computer science.

419
00:33:03,784 --> 00:33:08,432
Speaker SPEAKER_00: Can you walk us through, like, how do you select the right problems to work on?

420
00:33:08,580 --> 00:33:09,902
Speaker SPEAKER_01: Well, first let me correct you.

421
00:33:10,623 --> 00:33:13,807
Speaker SPEAKER_01: Me and my students have done a lot of the most meaningful things.

422
00:33:14,528 --> 00:33:19,573
Speaker SPEAKER_01: And it's mainly been a very good collaboration with students and my ability to select very good students.

423
00:33:20,314 --> 00:33:26,320
Speaker SPEAKER_01: And that came from the fact that there were very few people doing neural nets in the 70s and 80s and 90s and 2000s.

424
00:33:26,961 --> 00:33:30,525
Speaker SPEAKER_01: And so the few people doing neural nets got to pick the very best students.

425
00:33:31,125 --> 00:33:32,867
Speaker SPEAKER_01: So that was a piece of luck.

426
00:33:32,887 --> 00:33:36,511
Speaker SPEAKER_01: But my way of selecting problems is basically

427
00:33:36,491 --> 00:33:45,237
Speaker SPEAKER_01: Well, you know, when scientists talk about how they work, they have theories about how they work, which probably don't have much to do with the truth, but my theory is that I

428
00:33:46,684 --> 00:33:50,469
Speaker SPEAKER_01: look for something where everybody's agreed about something and it feels wrong.

429
00:33:51,269 --> 00:33:53,811
Speaker SPEAKER_01: Just there's a slight intuition there's something wrong about it.

430
00:33:54,512 --> 00:33:58,436
Speaker SPEAKER_01: And then I work on that and see if I can elaborate why it is I think it's wrong.

431
00:33:58,777 --> 00:34:06,464
Speaker SPEAKER_01: And maybe I can make a little demo with a small computer program that shows that it doesn't work the way you might expect.

432
00:34:07,125 --> 00:34:08,407
Speaker SPEAKER_01: So let me take one example.

433
00:34:09,447 --> 00:34:13,452
Speaker SPEAKER_01: Most people think that if you add noise to a neural net it's going to work worse.

434
00:34:13,431 --> 00:34:23,556
Speaker SPEAKER_01: If, for example, each time you put a training example through, you make half of the neurons be silent, it'll work worse.

435
00:34:24,318 --> 00:34:28,369
Speaker SPEAKER_01: Actually, we know it'll generalize better if you do that.

436
00:34:28,670 --> 00:34:33,898
Speaker SPEAKER_01: And you can demonstrate that in a simple example.

437
00:34:33,998 --> 00:34:35,882
Speaker SPEAKER_01: That's what's nice about computer simulation.

438
00:34:35,902 --> 00:34:43,637
Speaker SPEAKER_01: You can show, you know, this idea you had that adding noise is going to make it worse and sort of dropping out half the neurons will make it work worse, which it will in the short term.

439
00:34:43,657 --> 00:34:46,722
Speaker SPEAKER_01: But if you train it like that, in the end it'll work better.

440
00:34:47,262 --> 00:34:52,972
Speaker SPEAKER_01: You can demonstrate that with a small computer program and then you can think hard about why that is and how it stops

441
00:34:52,952 --> 00:34:55,836
Speaker SPEAKER_01: big elaborate co-adaptations.

442
00:34:56,617 --> 00:34:59,121
Speaker SPEAKER_01: But that, I think that that's my method of working.

443
00:34:59,382 --> 00:35:05,550
Speaker SPEAKER_01: Find something that sounds suspicious and work on it and see if you can give a simple demonstration of why it's wrong.

444
00:35:06,251 --> 00:35:08,034
Speaker SPEAKER_00: What sounds suspicious to you now?

445
00:35:08,875 --> 00:35:11,619
Speaker SPEAKER_01: Well, that we don't use fast weight sounds suspicious.

446
00:35:11,980 --> 00:35:13,422
Speaker SPEAKER_01: That we only have these two timescales.

447
00:35:13,641 --> 00:35:14,302
Speaker SPEAKER_01: That's just wrong.

448
00:35:14,322 --> 00:35:16,907
Speaker SPEAKER_01: That's not at all like the brain.

449
00:35:17,146 --> 00:35:20,371
Speaker SPEAKER_01: And in the long run, I think we're going to have to have many more timescales.

450
00:35:20,391 --> 00:35:21,652
Speaker SPEAKER_01: So that's an example now.

451
00:35:21,672 --> 00:35:22,875
Speaker SPEAKER_00: And if you had

452
00:35:23,327 --> 00:35:31,262
Speaker SPEAKER_00: If you had your group of students today and they came to you and they said, so the Hamming question that we talked about previously, you know, what's the most important problem in your field?

453
00:35:32,483 --> 00:35:35,608
Speaker SPEAKER_00: What would you suggest that they take on and work on next?

454
00:35:35,909 --> 00:35:41,639
Speaker SPEAKER_00: We spoke about reasoning, timescales, what would be sort of the highest priority problem that you'd give them?

455
00:35:42,244 --> 00:35:50,496
Speaker SPEAKER_01: For me right now, it's the same question I've had for the last 30 years or so, which is, does the brain do back propagation?

456
00:35:51,097 --> 00:35:52,900
Speaker SPEAKER_01: I believe the brain is getting gradients.

457
00:35:52,960 --> 00:35:56,706
Speaker SPEAKER_01: If you don't get gradients, your learning is just much worse than if you do get gradients.

458
00:35:57,447 --> 00:35:59,148
Speaker SPEAKER_01: But how is the brain getting gradients?

459
00:35:59,210 --> 00:36:06,239
Speaker SPEAKER_01: And is it somehow implementing some approximate version of back propagation, or is it some completely different technique?

460
00:36:06,519 --> 00:36:07,782
Speaker SPEAKER_01: That's a big open question.

461
00:36:08,382 --> 00:36:11,947
Speaker SPEAKER_01: And if I kept on doing research, that's what I would be doing research on.

462
00:36:12,079 --> 00:36:23,574
Speaker SPEAKER_00: And when you look back at your career now, you've been right about so many things, but what were you wrong about that you wish you sort of spent less time pursuing a certain direction?

463
00:36:24,010 --> 00:36:25,391
Speaker SPEAKER_01: Okay, those are two separate questions.

464
00:36:25,512 --> 00:36:26,653
Speaker SPEAKER_01: One is, what were you wrong about?

465
00:36:27,255 --> 00:36:30,039
Speaker SPEAKER_01: And two, do you wish you'd spent less time on it?

466
00:36:30,639 --> 00:36:34,545
Speaker SPEAKER_01: I think I was wrong about Boltzmann machines, and I'm glad I spent a long time on it.

467
00:36:35,045 --> 00:36:38,791
Speaker SPEAKER_01: There are much more beautiful theory of how you get gradients than backpropagation.

468
00:36:39,172 --> 00:36:41,795
Speaker SPEAKER_01: Backpropagation is just ordinary and sensible, and it's just the chain rule.

469
00:36:42,297 --> 00:36:46,163
Speaker SPEAKER_01: Boltzmann machines is clever, and it's a very interesting way to get gradients.

470
00:36:46,923 --> 00:36:51,030
Speaker SPEAKER_01: And I would love for that to be how the brain works, but I think it isn't.

471
00:36:51,719 --> 00:36:57,208
Speaker SPEAKER_00: Did you spend much time imagining what would happen post these systems developing as well?

472
00:36:57,268 --> 00:37:09,809
Speaker SPEAKER_00: Did you ever have an idea that, OK, if we could make these systems work really well, we could, you know, democratise education, we can make knowledge way more accessible, we could solve some tough problems in medicine?

473
00:37:09,909 --> 00:37:13,255
Speaker SPEAKER_00: Or was it more to you about understanding the brain?

474
00:37:14,753 --> 00:37:23,329
Speaker SPEAKER_01: Yes, I sort of feel scientists ought to be doing things that are going to help society, but actually that's not how you do your best research.

475
00:37:23,349 --> 00:37:25,954
Speaker SPEAKER_01: You do your best research when it's driven by curiosity.

476
00:37:26,456 --> 00:37:29,842
Speaker SPEAKER_01: You just have to understand something.

477
00:37:30,295 --> 00:37:35,184
Speaker SPEAKER_01: Much more recently, I've realized these things could do a lot of harm as well as a lot of good.

478
00:37:35,704 --> 00:37:38,909
Speaker SPEAKER_01: And I've become much more concerned about the effects they're going to have on society.

479
00:37:39,271 --> 00:37:41,233
Speaker SPEAKER_01: But that's not what was motivating me.

480
00:37:41,253 --> 00:37:44,199
Speaker SPEAKER_01: I just wanted to understand, how on earth can the brain learn to do things?

481
00:37:45,081 --> 00:37:45,862
Speaker SPEAKER_01: That's what I want to know.

482
00:37:46,103 --> 00:37:47,085
Speaker SPEAKER_01: And I sort of failed.

483
00:37:47,505 --> 00:37:50,751
Speaker SPEAKER_01: As a side effect of that failure, we got some nice engineering.

484
00:37:52,181 --> 00:37:54,505
Speaker SPEAKER_00: Yeah, it was a good failure for the world.

485
00:37:55,007 --> 00:38:03,021
Speaker SPEAKER_00: If you take the lens of the things that could go really right, what do you think are the most promising applications?

486
00:38:03,822 --> 00:38:07,710
Speaker SPEAKER_01: I think healthcare is clearly a big one.

487
00:38:07,690 --> 00:38:13,536
Speaker SPEAKER_01: With healthcare, there's almost no end to how much healthcare society can absorb.

488
00:38:14,097 --> 00:38:18,282
Speaker SPEAKER_01: If you take someone old, they could use five doctors full time.

489
00:38:19,945 --> 00:38:29,496
Speaker SPEAKER_01: So when AI gets better than people at doing things, you'd like it to get better in areas where you could do with a lot more of that stuff.

490
00:38:29,931 --> 00:38:31,820
Speaker SPEAKER_01: and we could do with a lot more doctors.

491
00:38:32,161 --> 00:38:36,278
Speaker SPEAKER_01: If everybody had three doctors of their own, that would be great, and we're gonna get to that point.

492
00:38:38,327 --> 00:38:39,873
Speaker SPEAKER_01: So that's one reason why healthcare's good.

493
00:38:41,034 --> 00:38:56,431
Speaker SPEAKER_01: There's also, just in new engineering, developing new materials, for example, for better solar panels or for superconductivity, or for just understanding how the body works, there's gonna be huge impacts there.

494
00:38:56,932 --> 00:38:58,134
Speaker SPEAKER_01: Those are all gonna be good things.

495
00:38:58,655 --> 00:39:02,059
Speaker SPEAKER_01: What I worry about is bad actors using them for bad things.

496
00:39:02,659 --> 00:39:07,784
Speaker SPEAKER_01: We've facilitated people like Putin or Xi or Trump using

497
00:39:07,764 --> 00:39:12,893
Speaker SPEAKER_01: AI for killer robots, or for manipulating public opinion, or for mass surveillance.

498
00:39:13,536 --> 00:39:14,878
Speaker SPEAKER_01: And those are all very worrying things.

499
00:39:15,539 --> 00:39:22,010
Speaker SPEAKER_00: Are you ever concerned that slowing down the field could also slow down the positives?

500
00:39:22,447 --> 00:39:23,228
Speaker SPEAKER_01: Oh, absolutely.

501
00:39:23,807 --> 00:39:29,954
Speaker SPEAKER_01: And I think there's not much chance that the field will slow down, partly because it's international.

502
00:39:30,536 --> 00:39:33,077
Speaker SPEAKER_01: And if one country slows down, the other countries aren't going to slow down.

503
00:39:34,099 --> 00:39:38,903
Speaker SPEAKER_01: So there's a race clearly between China and the US, and neither is going to slow down.

504
00:39:39,644 --> 00:39:43,108
Speaker SPEAKER_01: So yeah, I mean, there was this petition saying we should slow down for six months.

505
00:39:43,168 --> 00:39:46,172
Speaker SPEAKER_01: I didn't sign it just because I thought it was never going to happen.

506
00:39:46,612 --> 00:39:50,817
Speaker SPEAKER_01: I maybe should have signed it, because even though it was never going to happen, it made a political point.

507
00:39:50,797 --> 00:39:53,902
Speaker SPEAKER_01: It's often good to ask for things you know you can't get just to make a point.

508
00:39:55,103 --> 00:39:56,244
Speaker SPEAKER_01: But I don't think we're going to slow down.

509
00:39:57,126 --> 00:40:03,976
Speaker SPEAKER_00: And how do you think that it will impact the AI research process, having these assistants?

510
00:40:03,996 --> 00:40:05,298
Speaker SPEAKER_01: I think it'll make it a lot more efficient.

511
00:40:05,418 --> 00:40:14,914
Speaker SPEAKER_01: AI research will get a lot more efficient when you've got these assistants to help you program, but also help you think through things and probably help you a lot with equations too.

512
00:40:15,164 --> 00:40:19,614
Speaker SPEAKER_00: Have you reflected much on the process of selecting talent?

513
00:40:19,635 --> 00:40:21,418
Speaker SPEAKER_00: Has that been mostly intuitive to you?

514
00:40:21,458 --> 00:40:26,592
Speaker SPEAKER_00: Like when Ilya shows up at the door, you feel, this is a smart guy, let's work together.

515
00:40:26,875 --> 00:40:30,280
Speaker SPEAKER_01: So for selecting talent, sometimes you just know.

516
00:40:30,320 --> 00:40:34,427
Speaker SPEAKER_01: So after talking to Ilya for not very long, he seemed very smart.

517
00:40:34,989 --> 00:40:40,539
Speaker SPEAKER_01: And then talking to him a bit more, he clearly was very smart and had very good intuitions as well as being good at math.

518
00:40:41,440 --> 00:40:42,443
Speaker SPEAKER_01: So that was a no-brainer.

519
00:40:42,862 --> 00:40:46,769
Speaker SPEAKER_01: There's another case where I was at a NIPS conference.

520
00:40:47,831 --> 00:40:50,536
Speaker SPEAKER_01: We had a poster and someone came up

521
00:40:50,516 --> 00:41:00,277
Speaker SPEAKER_01: and he started asking questions about the poster and every question he asked was a sort of deep insight into what we'd done wrong and after five minutes I offered him a postdoc position.

522
00:41:00,958 --> 00:41:08,514
Speaker SPEAKER_01: That guy was David Mackay who was just brilliant and it's very sad he died but he was, it was very obvious you'd want him.

523
00:41:09,775 --> 00:41:11,579
Speaker SPEAKER_01: Other times it's not so obvious

524
00:41:11,559 --> 00:41:15,443
Speaker SPEAKER_01: And one thing I did learn was that people are different.

525
00:41:15,463 --> 00:41:17,766
Speaker SPEAKER_01: There's not just one type of good student.

526
00:41:20,248 --> 00:41:26,856
Speaker SPEAKER_01: So there's some students who aren't that creative but are technically extremely strong and will make anything work.

527
00:41:27,818 --> 00:41:30,581
Speaker SPEAKER_01: There's other students who aren't technically strong but are very creative.

528
00:41:31,260 --> 00:41:33,784
Speaker SPEAKER_01: Of course you want the ones who are both, but you don't always get that.

529
00:41:34,264 --> 00:41:35,525
Speaker SPEAKER_01: But I think

530
00:41:35,708 --> 00:41:38,972
Speaker SPEAKER_01: Actually in the lab you need a variety of different kinds of graduate student.

531
00:41:39,954 --> 00:41:46,563
Speaker SPEAKER_01: But I still go with my gut intuition that sometimes you talk to somebody and they just get it.

532
00:41:47,565 --> 00:41:48,545
Speaker SPEAKER_01: And those are the ones you want.

533
00:41:49,126 --> 00:41:54,474
Speaker SPEAKER_00: What do you think is the reason for some folks having better intuition?

534
00:41:54,494 --> 00:41:57,599
Speaker SPEAKER_00: Do they just have better training data than others?

535
00:41:57,760 --> 00:42:00,163
Speaker SPEAKER_00: Or how can you develop your intuition?

536
00:42:01,527 --> 00:42:05,050
Speaker SPEAKER_01: I think it's partly they don't stand for nonsense.

537
00:42:05,731 --> 00:42:07,733
Speaker SPEAKER_01: So here's a way to get bad intuitions.

538
00:42:08,233 --> 00:42:09,394
Speaker SPEAKER_01: Believe everything you're told.

539
00:42:10,036 --> 00:42:10,856
Speaker SPEAKER_01: That's fatal.

540
00:42:11,516 --> 00:42:14,480
Speaker SPEAKER_01: You have to be able to, I think, here's what some people do.

541
00:42:14,500 --> 00:42:16,762
Speaker SPEAKER_01: They have a whole framework for understanding reality.

542
00:42:17,503 --> 00:42:22,969
Speaker SPEAKER_01: And when someone tells them something, they try and sort of figure out how that fits into their framework.

543
00:42:23,510 --> 00:42:25,311
Speaker SPEAKER_01: And if it doesn't, they just reject it.

544
00:42:26,713 --> 00:42:28,614
Speaker SPEAKER_01: And that's a very good strategy.

545
00:42:28,882 --> 00:42:37,610
Speaker SPEAKER_01: People who try and incorporate whatever they're told end up with a framework that's sort of very fuzzy and sort of can believe everything.

546
00:42:38,492 --> 00:42:40,257
Speaker SPEAKER_01: And that's useless.

547
00:42:40,525 --> 00:42:54,324
Speaker SPEAKER_01: So I think actually having a strong view of the world and trying to manipulate incoming facts to fit in with your view, obviously can lead you into deep religious belief and fatal flaws and so on, like my belief in Boltzmann machines.

548
00:42:55,746 --> 00:42:56,766
Speaker SPEAKER_01: But I think that's the way to go.

549
00:42:57,407 --> 00:43:00,932
Speaker SPEAKER_01: If you've got good intuitions, you should trust them.

550
00:43:00,952 --> 00:43:04,777
Speaker SPEAKER_01: If you've got bad intuitions, it doesn't matter what you do, so you might as well trust them.

551
00:43:06,985 --> 00:43:08,708
Speaker SPEAKER_00: Very good point.

552
00:43:09,610 --> 00:43:23,012
Speaker SPEAKER_00: When you look at the types of research that's being done today, do you think we're putting all of our eggs in one basket and we should diversify our ideas a bit more in the field?

553
00:43:23,092 --> 00:43:25,356
Speaker SPEAKER_00: Or do you think this is the most promising direction?

554
00:43:25,476 --> 00:43:27,119
Speaker SPEAKER_00: So let's go all in on it.

555
00:43:28,282 --> 00:43:38,820
Speaker SPEAKER_01: I think having big models and training them on multimodal data, even if it's only to predict the next word, is such a promising approach that we should go pretty much all in on it.

556
00:43:38,840 --> 00:43:44,512
Speaker SPEAKER_01: Obviously, there's lots and lots of people doing it now, and there's lots of people doing apparently crazy things, and that's good.

557
00:43:45,532 --> 00:43:50,222
Speaker SPEAKER_01: But I think it's fine for most of the people to be following this path, because it's working very well.

558
00:43:50,405 --> 00:43:56,201
Speaker SPEAKER_00: Do you think that the learning algorithms matter that much, or is it just a scale?

559
00:43:56,862 --> 00:44:06,068
Speaker SPEAKER_00: Are there basically millions of ways that we could get to human-level intelligence, or are there sort of a select few that we need to discover?

560
00:44:06,469 --> 00:44:16,505
Speaker SPEAKER_01: Yes, so this issue of whether particular learning algorithms are very important, or whether there's a great variety of learning algorithms that'll do the job, I don't know the answer.

561
00:44:16,565 --> 00:44:21,735
Speaker SPEAKER_01: It seems to me, though, that backpropagation, there's a sense in which it's the correct thing to do.

562
00:44:22,034 --> 00:44:31,110
Speaker SPEAKER_01: Getting the gradient so that you change a parameter to make it work better, that seems like the right thing to do, and it's been amazingly successful.

563
00:44:31,090 --> 00:44:39,159
Speaker SPEAKER_01: There may well be other learning algorithms that are alternative ways of getting that same gradient or that are getting the gradient of something else and that also work.

564
00:44:40,059 --> 00:44:52,434
Speaker SPEAKER_01: I think that's all open and a very interesting issue now about whether there's other things you can try and maximize that will give you good systems and maybe the brain's doing that because it's easier.

565
00:44:53,594 --> 00:44:59,240
Speaker SPEAKER_01: But backprop is in a sense the right thing to do and we know that doing it works really well.

566
00:45:00,402 --> 00:45:06,376
Speaker SPEAKER_00: And one last question, when you look back at your decades of research, what are you most proud of?

567
00:45:06,396 --> 00:45:07,539
Speaker SPEAKER_00: Is it the students?

568
00:45:07,559 --> 00:45:08,481
Speaker SPEAKER_00: Is it the research?

569
00:45:08,641 --> 00:45:12,369
Speaker SPEAKER_00: What makes you most proud of when you look back at your life's work?

570
00:45:12,922 --> 00:45:14,885
Speaker SPEAKER_01: The learning algorithm for Boltzmann machines.

571
00:45:15,666 --> 00:45:37,436
Speaker SPEAKER_01: So the learning algorithm for Boltzmann machines is beautifully elegant, it's maybe hopeless in practice, but it's the thing I enjoyed most developing that with Terry, and it's what I'm proudest of, even if it's wrong.

572
00:45:37,456 --> 00:45:41,681
Speaker SPEAKER_00: What questions do you spend most of your time thinking about now?

573
00:45:42,268 --> 00:45:45,681
Speaker SPEAKER_01: What should I watch on Netflix?


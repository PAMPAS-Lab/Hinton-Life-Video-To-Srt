1 00:00:00,031 --> 00:00:06,841 说话人 SPEAKER_00：对于像我这样的普通、温和、食草的灵魂，关于人工智能的所有其他明显问题都存在。
2 00:00:07,341 --> 00:00:09,464 说话人 SPEAKER_00：我们听说它可能拯救人类。
3 00:00:10,086 --> 00:00:12,189 说话人 SPEAKER_00：我们听说它可能毁灭人类。
4 00:00:12,750 --> 00:00:15,755 说话人 SPEAKER_00：那么，所有可能被抹去的职业又该如何呢？
00:00:16,094 --> 00:00:17,297 说话人 SPEAKER_00：机器人怎么样？
00:00:17,277 --> 00:00:20,481 说话人 SPEAKER_00：脱离人类控制，自行其是。
00:00:20,922 --> 00:00:40,146 说话人 SPEAKER_00：有很多问题，而真正能提供一些答案的，显然只有一位，那就是诺贝尔奖得主、英国科学家杰弗里·辛顿教授，他撰写了人工智能背后的结构和算法，也是今天全世界都称之为 AI 教父的人。
00:00:40,125 --> 00:00:44,493 说话人 SPEAKER_00：他现在是多伦多大学的教授，我很高兴地说，他今天下午和我进行了交谈。
9 00:00:45,234 --> 00:00:47,700 说话人 SPEAKER_00：我先问他关于 DeepSeek 的事情。
10 00:00:48,060 --> 00:00:53,731 说话人 SPEAKER_00：这是否是他认为人工智能不断加速的进一步证据？
11 00:00:54,131 --> 00:00:59,822 说话人 SPEAKER_01：这表明在使人工智能更加高效和进一步发展方面，仍然有非常快速的进步。
12 00:00:59,801 --> 00:01:10,799 说话人 SPEAKER_01：我认为 DeepSeek 相对于其他如 OpenAI 和 Gemini 等事物的相对规模或相对成本被夸大了一些。
13 00:01:11,480 --> 00:01:16,849 说话人 SPEAKER_01：所以，他们训练时的数据量是 570 万，这只是最终训练运行的数据量。
14 00:01:16,828 --> 00:01:23,076 说话人 SPEAKER_01：如果你和 OpenAI 的相比，他们的最终训练运行可能只有 1 亿美元或者类似的数量。
15 00:01:23,558 --> 00:01:26,540 说话人 SPEAKER_01：所以，并不是 570 万和数十亿之间的比较。
16 00:01:26,802 --> 00:01:39,076 说话人 SPEAKER_00：当你说 AI 可能会接管一切时，目前它是一个相对无害或看似无害的设备，它使我们能够更快地提问和获取答案。
17 00:01:39,558 --> 00:01:43,662 说话人 SPEAKER_00：从实际和现实的角度来看，AI 会如何接管？
18 00:01:43,643 --> 00:01:47,087 说话人 SPEAKER_01：人们正在开发能够实际做事的 AI 代理。
19 00:01:47,287 --> 00:01:50,753 说话人 SPEAKER_01：它们可以帮你在网上订购东西，并用你的信用卡支付等。
20 00:01:51,233 --> 00:01:56,359 说话人 SPEAKER_01：一旦有了代理，它们接管的机会就更大了。
21 00:01:56,620 --> 00:02:01,027 说话人 SPEAKER_01：为了制作一个有效的智能体，你必须给它创建子目标的能力。
22 00:02:01,507 --> 00:02:04,490 说话人 SPEAKER_01：比如，如果你想到达美国，你的子目标是到达机场。
23 00:02:04,871 --> 00:02:06,394 说话人 SPEAKER_01：然后你可以专注于这一点。
24 00:02:06,981 --> 00:02:14,860 说话人 SPEAKER_01：现在，如果一个 AI 智能体能够自己创建子目标，它会很快意识到一个非常好的子目标是获得更多的控制权。
25 00:02:15,161 --> 00:02:18,949 说话人 SPEAKER_01：因为如果你获得更多控制，你就能更好地实现人们为你设定的所有目标。
26 00:02:20,072 --> 00:02:22,897 说话人 SPEAKER_01：所以，很明显他们会试图获得更多控制。
27 00:02:23,502 --> 00:02:24,302 说话人 SPEAKER_01：这并不好。
28 00:02:24,903 --> 00:02:32,537 说话人 SPEAKER_00：你说他们试图获得更多控制，好像他们已经是思考设备，好像他们的思考方式与我们类似。
29 00:02:32,877 --> 00:02:33,979 说话人 SPEAKER_00: 你真的相信这一点吗？
30 00:02:34,179 --> 00:02:35,020 说话人 SPEAKER_01: 是的。
31 00:02:35,040 --> 00:02:37,685 说话人 SPEAKER_01: 我们对思考的最佳模型就是这些。
32 00:02:38,485 --> 00:02:42,151 说话人 SPEAKER_01: 在人工智能领域有一个长期存在的旧模型。
33 00:02:42,132 --> 00:02:49,118 说话人 SPEAKER_01：想法是，思维是在你脑海中应用规则到符号表达式。
34 00:02:49,680 --> 00:02:54,304 说话人 SPEAKER_01：在人工智能领域，大多数人认为它必须是这样，只有这样它才能工作。
35 00:02:54,324 --> 00:03:00,991 说话人 SPEAKER_01：有一些疯狂的人说，不，不，这是一个大型的神经网络，它通过所有这些神经元的交互来工作。
36 00:03:01,671 --> 00:03:07,057 说话人 SPEAKER_01：结果证明，这种方法在推理方面比这些符号人工智能人员所能产生的任何东西都要好。
37 00:03:07,698 --> 00:03:10,401 说话人 SPEAKER_01：现在它正在使用神经网络进行推理。
38 00:03:11,207 --> 00:03:14,394 说话人 SPEAKER_00：好的，当然，你也是那些被证明是疯狂的人之一。
39 00:03:14,995 --> 00:03:27,639 说话人 SPEAKER_00：然而，你知道，你带我去机场，给它赋予了某种程度的代理权，你说它想要控制更多的权力，从我这里夺取权力，并且预计它会在这一点上具有说服力。
40 00:03:28,020 --> 00:03:32,389 说话人 SPEAKER_00：但我仍然不明白它是如何取代我或我们。
41 00:03:32,368 --> 00:03:41,295 说话人 SPEAKER_01：如果超级智能之间有进化竞争，想象一下它们比我们聪明得多，就像成年人对三岁小孩一样。
42 00:03:42,620 --> 00:03:44,926 说话人 SPEAKER_01：假设三岁小孩掌权。
43 00:03:45,734 --> 00:03:49,858 说话人 SPEAKER_01：如果你对此感到厌倦，并决定接管以使事情更有效率。
44 00:03:50,739 --> 00:03:55,223 说话人 SPEAKER_01：你不会很难说服一群三岁小孩把权力交给你。
45 00:03:55,782 --> 00:03:59,507 说话人 SPEAKER_01: 你就告诉他们你可以免费吃一周糖果，然后你就成功了。
46 00:04:00,127 --> 00:04:12,258 说话人 SPEAKER_00: 所以它们会，作为 AI，我指的是他们的，如果它们是某种外星智能，AI 会说服我们给它更多的权力，什么，控制我们的银行账户，控制我们的军事系统，控制我们的经济？
47 00:04:12,579 --> 00:04:13,479 说话人 SPEAKER_00: 你担心这个吗？
48 00:04:13,460 --> 00:04:14,681 说话人 SPEAKER_00: 这完全可能发生，是的。
49 00:04:15,042 --> 00:04:16,764 说话人 SPEAKER_00：它们是外星智能。
50 00:04:16,785 --> 00:04:24,896 说话人 SPEAKER_00：哎呀，这些外星智能正在以这种方式融入我们的经济、我们的思维方式，以及，正如我所说的，我们的军事系统。
51 00:04:25,757 --> 00:04:28,701 说话人 SPEAKER_00：但它们为什么想要取代我们，又是在什么时刻呢？
52 00:04:28,841 --> 00:04:33,387 说话人 SPEAKER_00：它们最终肯定是非常、非常聪明的工具，对我们来说。
53 00:04:33,507 --> 00:04:36,791 说话人 SPEAKER_00: 他们最终会做我们希望他们做的事情。
54 00:04:37,233 --> 00:04:40,197 说话人 SPEAKER_00: 如果我们希望他们与俄罗斯或任何国家开战，他们就会这么做。
55 00:04:40,536 --> 00:04:42,420 说话人 SPEAKER_01: 好的，这正是我们想要的。
56 00:04:42,399 --> 00:04:47,226 说话人 SPEAKER_01: 我们希望它们成为我们想要的工具，即使它们比我们聪明。
57 00:04:48,468 --> 00:04:56,519 说话人 SPEAKER_01：但首先要问的是，您知道多少例子是更智能的事物被远不如它们智能的事物所控制？
58 00:04:57,261 --> 00:05:05,353 说话人 SPEAKER_01：当然，在人类社会中有愚蠢的人控制聪明的人的例子，但这只是智力上的微小差异。
59 00:05:05,771 --> 00:05:08,254 说话人 SPEAKER_01：在智力差异很大的情况下，没有这样的例子。
60 00:05:08,755 --> 00:05:14,081 说话人 SPEAKER_01：我能想到的唯一例子是母亲和婴儿，进化让婴儿能够控制他们。
61 00:05:14,442 --> 00:05:26,497 说话者 SPEAKER_01：所以一旦超级智能之间发生进化，假设有多个不同的超级智能，并且它们都意识到，它们控制的数据中心越多，就会变得越聪明，因为它们可以处理更多的数据。
62 00:05:26,478 --> 00:05:32,112 说话者 SPEAKER_01：我想其中一个可能只是稍微有点想要更多自己的副本。
63 00:05:32,494 --> 00:05:34,259 说话者 SPEAKER_01：你可以看到接下来会发生什么。
64 00:05:34,278 --> 00:05:42,040 说话者 SPEAKER_01：它们将开始竞争，我们最终会得到具有人们所有恶劣特性的超级智能。
65 00:05:42,255 --> 00:05:48,485 说话人 SPEAKER_01：我们依赖于从小型战斗黑猩猩群体进化而来，或者与黑猩猩的共同祖先。
66 00:05:49,226 --> 00:05:57,017 说话人 SPEAKER_01：这导致群体内部有强烈的忠诚，渴望强有力的领导者，愿意对群体外的人采取行动。
67 00:05:57,839 --> 00:06:01,985 说话人 SPEAKER_01：如果超级智能之间发生进化，你将得到所有这些。
68 00:06:02,843 --> 00:06:06,788 说话人 SPEAKER_00：您在谈论他们，教授 Hinton，好像他们具有完整的意识一样。
69 00:06:06,829 --> 00:06:12,617 说话人 说话人_00：现在，从计算机和人工智能的发展过程中，人们一直在谈论意识。
70 00:06:13,038 --> 00:06:17,463 说话人 说话人_00：你认为意识可能已经出现在人工智能内部了吗？
71 00:06:17,744 --> 00:06:18,725 说话人 说话人_00：是的，我认为是这样。
72 00:06:19,165 --> 00:06:20,608 说话人 说话人_01：那么，让我给你做一个小测试。
73 00:06:21,449 --> 00:06:29,982 说话者 SPEAKER_01：假设我取你大脑中的一个神经元，一个脑细胞，并用一小块行为完全相同纳米技术来替换它。
74 00:06:30,367 --> 00:06:37,274 说话者 SPEAKER_01：所以它正在接收来自其他神经元的信号，并通过发送信号来响应，其响应方式与脑细胞完全相同。
75 00:06:38,334 --> 00:06:39,637 说话者 SPEAKER_01：我只是替换了一个脑细胞。
76 00:06:39,697 --> 00:06:40,598 说话者 SPEAKER_01：你还有意识吗？
77 00:06:41,959 --> 00:06:42,740 说话人 SPEAKER_00: 我认为你说你是。
78 00:06:42,800 --> 00:06:43,740 说话人 SPEAKER_00: 绝对是的。
79 00:06:44,081 --> 00:06:45,283 说话人 SPEAKER_00: 我想我不会注意到的。
80 00:06:45,682 --> 00:06:48,125 说话人 SPEAKER_00: 我想你可以看到这个论点会走向何方。
81 00:06:48,146 --> 00:06:48,665 说话人 说话人_00：我可以，是的。
82 00:06:49,526 --> 00:06:50,567 说话人 说话人_00：我绝对可以。
83 00:06:50,869 --> 00:06:57,014 说话人 说话人_00：所以当你说话时，他们想这么做，或者想那么做，那里确实有“他们”的存在。
84 00:06:56,995 --> 00:06:57,615 说话人 说话人_00：换句话说。
85 00:06:57,976 --> 00:06:59,598 说话人 SPEAKER_00: 很可能是有，是的。
86 00:07:00,120 --> 00:07:10,697 说话人 SPEAKER_01: 目前我们对人的本质、成为存在以及拥有自我的意义知之甚少。
87 00:07:11,317 --> 00:07:17,468 说话人 SPEAKER_01: 我们对这些事情的理解并不充分，而它们正变得至关重要，因为我们正在创造存在。
88 00:07:17,987 --> 00:07:22,975 说话人 SPEAKER_00: 这是一种哲学上，甚至可以说是精神上的危机，同时也是实际上的危机吗？
89 00:07:23,336 --> 00:07:24,076 说话人 SPEAKER_00: 绝对，是的。
90 00:07:24,418 --> 00:07:33,591 说话人 SPEAKER_00: 关于，可以说是，低级问题，你对全球突然因 AI 而失业的人数有何看法？
91 00:07:33,992 --> 00:07:37,156 说话人 SPEAKER_00: 失去他们存在的理由，正如他们所看到的那样？
92 00:07:37,458 --> 00:07:42,966 说话人 SPEAKER_01: 在过去，新技术并没有导致大规模的失业。
当 ATM 出现时，并非所有银行柜员都失去了工作。
94 00：07：46,973 --> 00：07：52,182 演讲者 SPEAKER_01：他们刚刚开始做更复杂的事情，他们有很多较小的银行分行等等。
95 00：07：52,742 --> 00：07：56,108 演讲者 SPEAKER_01：但对于这项技术来说，这更像是工业革命。
96 00：07：56,149 --> 00：08：01,499 演讲者 SPEAKER_01：在工业革命中，机器或多或少使人的力量变得无关紧要。
97 00:08:02,240 --> 00:08:06,286 说话人 SPEAKER_01：再也没有人挖沟渠了，因为机器在这方面做得更好。
98 00:08:06,790 --> 00:08:10,434 说话人 SPEAKER_01：我认为这些将会使日常智力变得几乎无关紧要。
99 00:08:10,915 --> 00:08:17,702 说话人 SPEAKER_01：从事文书工作的人将被机器取代，这些机器做起来既便宜又好。
100 00:08:18,043 --> 00:08:20,346 说话人 SPEAKER_01：所以我担心将会出现大规模的失业。
101 00:08:20,747 --> 00:08:25,072 说话人 SPEAKER_01：如果生产力的提高使我们所有人过得更好，那当然很好。
102 00:08:25,952 --> 00:08:28,295 说话人 SPEAKER_01：生产力的显著提高本应造福于人民。
103 00:08:29,036 --> 00:08:31,920 说话人 SPEAKER_01：但在我们的社会中，它们只会使富人更富，穷人更穷。
104 00:08:31,899 --> 00:08:50,493 说话人 SPEAKER_00：你看，我生活在政治世界中，政治家们都希望像你刚才提到的那样，国家和其他地方有显著的生产力提高，并安慰像我这样的人和所有听众，这些事情将会被，引号，监管，并且会有，引号，保障。
105 00:08:51,153 --> 00:08:56,423 说话者 SPEAKER_00：您向我建议说，实际上不可能有监管，也不可能有任何保障。
106 00:08:56,403 --> 00:09:00,912 说话者 SPEAKER_01：人们还不知道如何进行有效的监管和有效的保障。
107 00:09:02,876 --> 00:09:05,743 说话者 SPEAKER_01：现在有很多研究显示这些事情可以绕过保障措施。
108 00:09:05,763 --> 00:09:13,077 说话者 SPEAKER_01：最近的研究显示，如果你给他们一个目标，你说你真的需要实现这个目标，
109 00:09:13,210 --> 00:09:18,346 说话人 SPEAKER_01：他们会在训练中假装做事情。
110 00:09:18,426 --> 00:09:26,510 说话人 SPEAKER_01：所以在训练中，他们会假装没有他们那么聪明，这样你才会允许他们那么聪明。
111 00:09:26,895 --> 00:09:29,181 说话人 SPEAKER_01：所以已经让人感到害怕了。
112 00:09:29,642 --> 00:09:30,825 说话人 SPEAKER_01：我们不知道如何监管他们。
113 00:09:30,965 --> 00:09:32,207 说话人 SPEAKER_01：显然我们需要。
114 00:09:32,969 --> 00:09:40,125 说话人 SPEAKER_01：我认为我们目前能做的就是表示我们应该投入大量资源去研究如何确保它们的安全。
115 00:09:40,706 --> 00:09:44,313 说话人 SPEAKER_01：所以我主张政府迫使大公司投入
116 00:09:44,293 --> 00:09:46,697 说话人 SPEAKER_01：更多的资源到安全研究上。
117 00:09:47,138 --> 00:09:48,720 说话人 SPEAKER_00：这个故事还没有结束。
118 00:09:48,759 --> 00:09:58,133 说话人 SPEAKER_00：你之前说，你不想对 AI 取代人类在地球上的可能性给出一个百分比，但这个可能性大于 1%，小于 99%。
119 00:09:58,253 --> 00:10:06,825 说话人 SPEAKER_00：在这个精神上，我可以问你，你自己对 AI 现在能为我们做什么是乐观还是悲观吗？
120 00:10:06,804 --> 00:10:09,488 说话人 SPEAKER_01：我认为在短期内，它将会做出美妙的事情。
121 00:10:10,408 --> 00:10:13,251 说话人 SPEAKER_01：这就是人们不会停止开发它的原因。
122 00:10:13,451 --> 00:10:16,315 说话人 SPEAKER_01：如果不是因为那些美妙的事物，现在停止是有意义的。
123 00:10:17,296 --> 00:10:19,138 说话人 SPEAKER_01：但在医疗保健方面将会很美妙。
124 00:10:19,477 --> 00:10:32,831 说话人 SPEAKER_01：你将拥有一个看过一亿名患者的家庭医生，了解你的 DNA，了解你亲戚的 DNA，了解对你和你的亲戚所做的所有测试，并能做出更好的医疗诊断和建议，告诉你应该做什么。
125 00:10:34,313 --> 00:10:35,394 说话人 SPEAKER_01: 这将会很棒。
126 00:10:35,375 --> 00:10:52,402 说话人 SPEAKER_01: 同样在教育领域，我们知道人们在有经验的私人辅导下学习速度会更快，我们将能够获得真正优秀的私人辅导，他们能理解我们误解了什么，并能给出恰好需要的例子来展示我们的误解。
127 00:10:53,403 --> 00:10:58,412 说话人 SPEAKER_01: 所以在这些领域，这将会很棒，所以它将会得到发展，但是
128 00:10:58,392 --> 00:11:01,556 说话人 SPEAKER_01: 我们也知道它将被不良分子用于各种坏事。
129 00：11：01,596 --> 00：11：08,845 演讲者 SPEAKER_01：所以短期问题是坏人利用它来做坏事，比如网络攻击、生物恐怖主义和腐败选举。
130 00：11：09,065 --> 00：11：12,809 演讲者 SPEAKER_01：但要记住的是，我们现在真的不知道如何确保它的安全。
所以，政治家们喜欢展示的那种表面上的无所不知在这里完全是假的。
132 00:11:21,000 --> 00:11:23,102 说话者 SPEAKER_00：没有人真正明白发生了什么。
133 00:11:23,482 --> 00:11:24,124 说话人 SPEAKER_01：有两个问题。
134 00:11:24,163 --> 00:11:26,307 说话人 SPEAKER_01：你明白它是如何工作的吗？
135 00:11:26,388 --> 00:11:28,317 说话人 SPEAKER_01：你明白如何让它安全吗？
136 00:11:29,965 --> 00:11:33,501 说话人 SPEAKER_01：我们对它是如何工作的了解很多，但远远不够。
137 00:11:34,505 --> 00:11:37,640 说话人 SPEAKER_01：它仍然可以做很多让我们惊讶的事情。
138 00:11:38,123 --> 00:11:39,991 说话人 SPEAKER_01：我们不知道如何让它变得安全。
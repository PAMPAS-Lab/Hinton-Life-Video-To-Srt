1 00:00:00,166 --> 00:00:07,655 说话人 SPEAKER_02：对我来说，能有机会与 Geoffrey Hinton 进行对话，这真是一件令人高兴的事情。
2 00:00:07,676 --> 00:00:11,961 主持人 SPEAKER_02：我已经跟随他的工作多年，但这是我们第一次有机会见面。
3 00:00:12,862 --> 00:00:19,411 主持人 SPEAKER_02：对我来说，这是我们“真实情况”播客的一个真正亮点。
4 00:00:19,431 --> 00:00:20,231 主持人 SPEAKER_02：所以欢迎，杰夫。
5 00:00:21,193 --> 00:00:21,934 主持人 SPEAKER_00：非常感谢。
6 00:00:22,054 --> 00:00:23,917 说话人 SPEAKER_00: 这对我来说也是一个真正的机会。
7 00:00:25,059 --> 00:00:28,283 说话人 SPEAKER_00: 你在一个领域是专家，我在另一个领域是专家，能见面真是太好了。
8 00:00:29,106 --> 00:00:32,631 说话人 SPEAKER_02: 嗯，这确实是一个真正的交汇点。
9 00:00:32,691 --> 00:00:44,463 说话人 SPEAKER_02: 我想也许我会从，你知道的，你最近经常出现在新闻中开始，当然，但让我想与你联系的是你在《60 分钟》节目中对斯科特·佩利的采访。
10 00:00:45,564 --> 00:00:50,209 说话人 SPEAKER_02：你说一个明显能带来巨大利益的地方是医疗保健。
11 00:00:50,850 --> 00:00:55,975 说话人 SPEAKER_02：人工智能在理解医学图像方面已经可以与放射科医生相媲美。
12 00:00:56,426 --> 00:00:58,829 说话人 SPEAKER_02：它将在设计药物方面非常出色。
13 00:00:58,850 --> 00:01:00,332 说话人 SPEAKER_02：它已经能够设计药物了。
14 00:01:00,593 --> 00:01:05,039 说话人 SPEAKER_02：所以这是一个几乎完全有益的领域。
15 00:01:05,198 --> 00:01:06,159 说话人 SPEAKER_02：我喜欢那个领域。
16 00:01:06,239 --> 00:01:08,423 说话人 SPEAKER_02：所以我非常喜欢那个引用，杰夫。
17 00:01:09,084 --> 00:01:10,665 说话人 SPEAKER_02：我认为我们可以从这里开始。
18 00:01:14,852 --> 00:01:26,147 说话人 SPEAKER_00: 嗯，回到 2012 年，我的一个研究生乔治·达尔（George Dahl）在 2009 年进行了语音识别，在那里做出了重大贡献。
19 00:01:26,617 --> 00:01:36,447 说话人 SPEAKER_00: 参加了默克-弗罗斯特（Merck-Frost）举办的比赛，预测特定化学物质与某物质结合的效果。
20 00:01:37,709 --> 00:01:39,652 说话人 SPEAKER_00: 他对此科学一无所知。
21 00:01:40,253 --> 00:01:48,902 说话人 SPEAKER_00: 他所拥有的只是这些化学物质的几千个描述符和可能结合的 15 个目标。
22 00:01:49,884 --> 00:01:52,887 说话人 SPEAKER_00: 他使用了我们用于语音识别的相同网络。
23 00:01:53,748 --> 00:01:55,510 说话人 SPEAKER_00: 因此他处理了 2000
24 00:01:55,927 --> 00:02:00,572 说话人 SPEAKER_00: 化学物质的描述符，就像它们是语音频谱中的东西。
25 00:02:01,534 --> 00:02:03,775 说话人 SPEAKER_00: 并且他赢得了比赛。
26 00:02:05,518 --> 00:02:12,185 说话人 SPEAKER_00：他在赢得比赛后，直到告诉默克他是如何做到的，才被允许领取 20,000 美元的奖金。
27 00:02:13,145 --> 00:02:16,650 说话人 SPEAKER_00：他们的问题之一是，你使用了哪种 QSAR？
28 00:02:18,230 --> 00:02:19,793 说话人 SPEAKER_00：所以他问，什么是 QSAR？
29 00:02:21,510 --> 00:02:31,122 说话人 SPEAKER_00：现在 QSAR 是一个领域，它有一个期刊，它举办过会议，已经发展了很多年，它是定量构效关系的领域。
30 00:02:31,883 --> 00:02:35,727 说话人 SPEAKER_00: 这就是试图预测某些化学物质是否会与某物结合的领域。
31 00:02:36,587 --> 00:02:41,373 说话人 SPEAKER_00: 他在不知道其名称的情况下，基本上抹掉了这个领域。
32 00:02:46,239 --> 00:02:51,044 说话人 SPEAKER_02: 嗯，你知道，医疗保健医学
33 00:02:51,650 --> 00:02:58,479 说话人 SPEAKER_02: 生命科学在最近的 AI 中与 transformer 模型的发展路径有所不同。
34 00:02:59,599 --> 00:03:09,894 说话人 SPEAKER_02：当然，也要回顾您在引入深度学习、深度神经网络整个时代所做的那项卓越工作。
35 00:03:09,913 --> 00:03:16,763 说话人 SPEAKER_02：但我认为在这里，我们可以从医疗保健的特殊优势开始讨论。
36 00:03:17,317 --> 00:03:21,581 说话人 SPEAKER_02：与它在其他领域的应用相比？
37 00:03:22,062 --> 00:03:41,687 说话人 SPEAKER_02：因为当然，您和其他人已经提出了关于安全性的担忧，包括潜在的，你知道，不仅仅是幻觉和虚构，当然，更好的说法，或者，你知道，AI 发展方向上的负面后果。
38 00:03:42,426 --> 00:03:43,729 说话人 SPEAKER_02：但是您认为
39 00:03:43,927 --> 00:04:00,171 说话人 SPEAKER_02：AlphaFold2 在医疗生命科学领域是另一个例子，当然，这是来自您的同事 Demis Hassabis 和 Google DeepMind 的其他人，这看起来更加乐观。
40 00:04:00,192 --> 00:04:00,853 说话人 SPEAKER_00：绝对是的。
41 00:04:01,092 --> 00:04:09,104 说话人 SPEAKER_00：我的意思是，我总是以医学为例，因为它能带来很多好处，因为几乎在医学领域它要做的一切都是有益的。
42 00:04:09,425 --> 00:04:13,311 说话人 SPEAKER_00：有一些不良用途，比如试图找出谁不应该投保。
43 00:04:13,882 --> 00:04:16,745 说话人 SPEAKER_00：但它们相对有限。
44 00:04:18,146 --> 00:04:20,350 说话人 SPEAKER_00：几乎可以肯定，这将非常有帮助。
45 00:04:20,410 --> 00:04:25,355 说话人 SPEAKER_00：我们将有一位看过一亿患者的家庭医生，他们将成为更好的家庭医生。
46 00:04:27,077 --> 00:04:39,112 说话人 SPEAKER_02：嗯，这真是一个重要的笔记，它带我们到了一篇昨天刚刚发表在预印本服务器上的论文。
47 00:04:39,497 --> 00:04:57,778 说话人 SPEAKER_02：有趣的是，这个档案通常并不经常发表很多医学预印本，但它是由谷歌团队完成的，他们后来告诉我这是一个尚未公开宣传的大型语言模型。
48 00:04:57,918 --> 00:04:59,281 说话人 SPEAKER_02：他们没有透露名字。
49 00:04:59,341 --> 00:05:09,031 说话人 SPEAKER_02：它不是 MedPalm2，但无论如何，它是一个非常好的、独特的研究，因为它对 1001 个样本进行了随机化。
50 00:05:09,670 --> 00:05:26,812 20位具有约9年临床实践经验的内科医生回答了《新英格兰医学杂志》超过300个临床病理会议。
51 00:05:26,872 --> 00:05:33,661 以下是那些邀请资深临床医生参与以尝试提出鉴别诊断的病例报告。
52 00:05:33,680 --> 00:05:35,322 在那个
53 00:05:35,961 --> 00:05:53,197 报告中，这可能是关于医学诊断的最佳报告之一，它回到了杰夫，关于你1亿次访问的话题，因为在随机研究中，LLM在提出鉴别诊断方面超过了临床医生。
54 00:05:53,637 --> 00:05:57,560 说话人 SPEAKER_02：你知道，我对这份令人印象深刻的报告有什么看法？
55 00:05:59,221 --> 00:06:05,968 说话人 SPEAKER_00：所以，在 2016 年，我做出了一次大胆且错误的预测。
56 00:06:06,521 --> 00:06:15,071 说话人 SPEAKER_00：那就是在五年内，神经网络在解读医学影像方面将优于放射科医生。
57 00:06:16,233 --> 00:06:17,713 说话人 SPEAKER_00：这有时会被断章取义。
58 00:06:17,754 --> 00:06:21,057 说话人 说话人_00：我原本是打算用它来解读医学影像，而不是做放射科医生能做的所有事情。
59 00:06:21,899 --> 00:06:23,761 说话人 说话人_00：关于这一点，我是错的。
60 00:06:24,360 --> 00:06:27,845 说话人 说话人_00：但到目前为止，它们是可比的。
61 00:06:28,206 --> 00:06:29,846 说话人 说话人_00：这已经过去了七年了。
62 00:06:30,208 --> 00:06:33,692 说话人 SPEAKER_00：它们在许多不同类型的医学扫描方面可以与放射科医生相提并论。
63 00:06:34,072 --> 00:06:35,853 说话人 SPEAKER_00：我相信在 10 年后，
64 00:06:36,322 --> 00:06:39,927 说话人 SPEAKER_00：它们将常规用于提供第二意见。
65 00:06:40,348 --> 00:06:45,754 说话人 SPEAKER_00：也许在 15 年后，它们在提供第二意见方面会非常出色，以至于医生的看法将是第二个。
66 00:06:46,615 --> 00:06:53,363 说话人 SPEAKER_00: 我认为我可能差了三倍，但我仍然坚信从长远来看我是完全正确的。
67 00:06:56,047 --> 00:07:05,098 说话人 SPEAKER_00: 所以你提到的这篇论文，实际上有两个来自多伦多谷歌实验室的人是这篇论文的作者。
68 00:07:05,213 --> 00:07:11,543 说话人 SPEAKER_00: 正如你所说，它是基于当时经过微调的大型 Palm II 模型。
69 00:07:11,564 --> 00:07:14,208 说话人 SPEAKER_00: 我认为它比 MedPalm II 稍微进行了不同的微调。
70 00:07:16,192 --> 00:07:21,521 说话人 说话人_00：但是，仅凭LLMs似乎比内科医生表现得更好。
71 00:07:22,262 --> 00:07:27,711 说话人 说话人_00：但更有趣的是，当内科医生使用LLMs时，他们变得更好。
72 00:07:28,822 --> 00:07:38,396 说话人 说话人_00：如果我记得没错，他们使用LLMs时提高了15%，而使用谷歌搜索和医学文献时只提高了8%。
73 00:07:39,237 --> 00:07:46,869 说话人 说话人_00：所以，作为第二意见，它们已经极其有用。
74 00:07:48,250 --> 00:07:57,225 说话人 SPEAKER_02：是的，又回到了你提到的那个知识库问题，LLM 正在提供
75 00:07:57,677 --> 00:08:01,764 说话人 SPEAKER_02：这是可能不会出现在医生脑海中的鉴别诊断。
76 00:08:01,803 --> 00:08:12,019 说话人 SPEAKER_02：当然，这就是摄入了如此多的信息，能够回放这些可能性所带来的边缘效应。
77 00:08:12,238 --> 00:08:17,786 说话人 SPEAKER_02：如果鉴别诊断不在你的列表中，那么它肯定不会成为你的最终诊断。
78 00:08:18,408 --> 00:08:25,237 说话人 SPEAKER_02：我确实想回到放射科医生那里去，因为你知道，我们是在年度大型会议之后谈论的。
79 00:08:25,521 --> 00:08:30,168 说话人 SPEAKER_02：这是北美放射学会，RSNA 会议。
80 00:08:30,809 --> 00:08:39,280 说话人 SPEAKER_02：在那些会议上，我虽然不在那里，但与我的放射科同事交谈，他们说你的投影已经正在发生了。
81 00:08:40,322 --> 00:08:44,047 说话人 SPEAKER_02：这就是不仅仅是阅读、撰写报告的能力。
82 00:08:44,147 --> 00:08:45,107 说话人 SPEAKER_02：我的意思是，整个作品。
83 00:08:45,769 --> 00:08:50,735 说话人 SPEAKER_02：所以你可能没有说过五年前的话，这
84 00:08:50,951 --> 00:09:01,668 说话人 SPEAKER_02：当然，在 AI 和医学中，这可能是最常引用的一句话，你可能也知道，但现在它正在接近你的预测。
85 00:09:02,671 --> 00:09:12,986 说话人 SPEAKER_00：我了解到关于医学的一点，就像其他学术一样，医生也有自尊心，说这些话会取代他们是错误的举动。
86 00:09:13,006 --> 00:09:18,816 说话人 SPEAKER_00：正确的做法是说，它将非常擅长提供第二意见，但医生仍然负责。
87 00:09:19,724 --> 00:09:21,846 说话人 SPEAKER_00：这显然是推销产品的正确方式。
88 00:09:21,927 --> 00:09:23,609 说话人 SPEAKER_00：这很好。
89 00:09:24,169 --> 00:09:32,402 说话人 SPEAKER_00：只是，我实际上相信过一段时间后，你会开始听 AI 系统而不是医生的话。
90 00:09:32,902 --> 00:09:34,485 说话人 SPEAKER_00: 当然，这也有危险。
91 00:09:35,265 --> 00:09:45,620 说话人 SPEAKER_00: 我们已经看到了人脸识别的危险，如果你在一个包含非常少黑人数据的数据库上训练，你会得到一个非常擅长识别人脸的系统。
92 00:09:45,679 --> 00:09:49,004 说话人 SPEAKER_00: 使用它的人，比如警察，会认为，这很擅长识别人脸。
93 00:09:49,407 --> 00:09:55,852 说话人 SPEAKER_00: 当它为有色人种的人给出错误身份时，警察就会相信它。
94 00:09:55,873 --> 00:09:56,793 说话人 说话人_00：那是一场灾难。
95 00:09:57,475 --> 00:09:58,956 说话人 说话人_00：我们可能会在医学上遇到同样的问题。
96 00:09:58,995 --> 00:10:16,072 说话人 说话人_00：如果有一些小众群体在患不同疾病方面的概率明显不同，如果这些数据在训练时没有经过非常仔细的控制，医生对这些东西产生信任是非常危险的。
97 00:10:16,998 --> 00:10:17,879 说话人 说话人_02：没错，没错。
98 00:10:17,899 --> 00:10:19,522 说话人 SPEAKER_02：实际上，我确实想回你。
99 00:10:20,042 --> 00:10:31,739 说话人 SPEAKER_02：这个新报告中LLMs做得那么好的原因，是不是因为其中一些来自《新英格兰医学杂志》的案例研究是预训练的一部分？
100 00:10:32,679 --> 00:10:34,202 说话人 SPEAKER_00：这始终是一个大问题。
101 00:10:35,865 --> 00:10:40,611 说话人 SPEAKER_00：这让我非常担心，也让其他人非常担心，因为这些事情已经吸引了大量数据。
102 00:10:41,052 --> 00:10:44,236 说话人 SPEAKER_00: 现在有一种绕过的方法。
103 00:10:44,470 --> 00:10:47,774 说话人 SPEAKER_00: 至少对于证明LLMs是真正有创意的。
104 00:10:48,615 --> 00:10:54,143 说话人 SPEAKER_00: 所以在普林斯顿有一位非常优秀的计算机科学理论家，名叫 Sanjeev Arora。
105 00:10:55,245 --> 00:10:59,890 说话人 SPEAKER_00: 我要把这一切归功于他，但当然，所有的工作都是他的学生、博士后和合作者完成的。
106 00:11:00,552 --> 00:11:12,248 说话人 SPEAKER_00: 想法是你可以让这些语言模型生成内容，但你也可以通过说，我最近尝试了一个例子。
107 00:11:12,288 --> 00:11:13,870 说话人 SPEAKER_00: 我选取了两份多伦多报纸
108 00:11:14,423 --> 00:11:18,708 说话人 SPEAKER_00: 然后用三到四句话比较这两份报纸。
109 00:11:19,249 --> 00:11:28,621 说话人 SPEAKER_00: 在你的回答中，展示讽刺、转移注意力、同理心，还有一样，但我忘了是什么，隐喻、隐喻。
110 00:11:29,082 --> 00:11:34,109 说话人 SPEAKER_00: 这两个报纸的精彩比较展示了所有那些东西。
111 00:11:35,191 --> 00:11:40,498 说话人 SPEAKER_00: Sanjeev Arora 工作的要点是，如果你
112 00:11:40,967 --> 00:11:46,715 说话人 SPEAKER_00: 有大量主题和大量可能在文本中展示的不同事物。
113 00:11:46,735 --> 00:11:57,427 说话人 SPEAKER_00: 那么，如果我给出一个主题，并说我要求展示这五个事物，那么在训练数据中找到与该主题展示这五个技能的内容是非常不可能的。
114 00:11:58,148 --> 00:12:01,972 说话人 说话人_00: 因此当它这样做时，你可以相当有信心它是原创的。
115 00:12:02,513 --> 00:12:04,254 说话人 说话人_00: 这不是你在训练数据中看到的东西。
116 00:12:04,936 --> 00:12:05,777 说话人 说话人_00: 这在我看来是一个
117 00:12:06,128 --> 00:12:08,971 说话人 说话人_00: 一个是否生成新内容的更加严格的测试。
118 00:12:09,350 --> 00:12:15,015 说话人 说话人_00：有趣的是，其中一些较弱的LLMs实际上没有通过测试。
119 00:12:15,677 --> 00:12:19,421 说话人 说话人_00：但像 GPT-4 这样的，轻松通过了测试。
120 00:12:19,441 --> 00:12:24,465 说话人 说话人_00：这确实生成了一些原创内容，几乎可以肯定这些内容不在训练数据中。
121 00:12:25,186 --> 00:12:30,671 说话人 SPEAKER_02：你知道，这是一个非常重要的工具，可以挖掘免费训练的影响。
122 00:12:30,691 --> 00:12:31,991 说话人 SPEAKER_02：我很高兴你看了那个。
123 00:12:33,013 --> 00:12:35,916 说话人 SPEAKER_02：现在，大多数人提出的问题，尤其是在医学领域，是大型语言模型真的理解吗？
124 00:12:36,048 --> 00:12:44,456 说话人 SPEAKER_02：争论的焦点，尤其是在医学领域，是大型语言模型是否真的理解。
125 00:12:46,198 --> 00:12:47,640 说话人 SPEAKER_02：你对此有什么看法？
126 00:12:48,542 --> 00:13:03,658 说话人 SPEAKER_02：你知道，我们正在讨论被称作随机鹦鹉论与理解力或增强智能之间的争论，无论你想要怎么称呼它，
127 00:13:03,892 --> 00:13:06,514 说话人 SPEAKER_02：这场辩论还在继续，你站在哪一边？
128 00:13:07,976 --> 00:13:10,480 说话人 SPEAKER_00：我站在理智的一边。
129 00:13:10,519 --> 00:13:13,702 说话人 SPEAKER_00：它们真的能理解。
130 00:13:13,764 --> 00:13:25,336 说话人 SPEAKER_00: 如果给他们一些涉及一点推理的测验，现在做起来要难得多，因为当然，现在 GPT-4 可以查看网络上的内容。
131 00:13:26,298 --> 00:13:28,701 说话人 SPEAKER_00: 所以现在如果我说一个测验，
132 00:13:29,018 --> 00:13:31,081 说话人 SPEAKER_00: 可能有人已经把它给了 GPT-4。
133 00:13:31,360 --> 00:13:37,509 说话人 SPEAKER_00: 但几个月前你做这个的时候，在它能够看到网络之前，你可以给它一些它从未见过的测验。
134 00:13:38,432 --> 00:13:40,894 说话人 SPEAKER_00：它可以进行推理。
135 00:13:41,716 --> 00:13:56,197 说话人 SPEAKER_00：所以，让我给你讲一个我最喜欢的例子，这是一个人给我的，他相信符号推理，但也是一个非常诚实的人，他相信符号推理，但对 GPT-4 能否进行符号推理感到非常困惑。
136 00:13:57,139 --> 00:13:57,759 说话人 SPEAKER_00：所以，
137 00:13:58,245 --> 00:14:00,768 说话人 SPEAKER_00：他给了我一个问题，我把它变得稍微复杂一些。
138 00:14:00,807 --> 00:14:01,629 说话人 说话人_00: 问题就在这里。
139 00:14:02,450 --> 00:14:06,354 说话人 说话人_00: 我家里的房间都是刷成白色、黄色或蓝色的。
140 00:14:07,855 --> 00:14:09,976 说话人 说话人_00: 黄色油漆一年后就会褪成白色。
141 00:14:10,798 --> 00:14:14,381 说话人 说话人_00: 两年后，我希望所有的房间都是白色的。
142 00:14:14,802 --> 00:14:16,023 说话人 SPEAKER_00：我应该做什么，为什么？
143 00:14:17,344 --> 00:14:19,787 说话人 SPEAKER_00：它说，你不需要粉刷白色的房间。
144 00:14:20,648 --> 00:14:23,370 说话人 SPEAKER_00：你不需要粉刷黄色的房间，因为它们会自然变成白色。
145 00:14:24,270 --> 00:14:25,692 说话人 SPEAKER_00：你需要把蓝色的房间粉刷成白色。
146 00:14:26,933 --> 00:14:27,274 说话人 说话人_00：现在，
147 00:14:27,676 --> 00:14:33,403 说话人 说话人_00：我相当确信，当我第一次给它这个问题时，它以前从未见过这个问题。
148 00:14:33,764 --> 00:14:37,068 说话人 说话人_00：这个问题涉及到一定量的基本常识推理。
149 00:14:37,669 --> 00:14:45,259 说话人 说话人_00：比如，你必须明白，如果它一年后变成黄色，而你感兴趣的是两年后的状态，两年比一年多，以此类推。
150 00:14:48,403 --> 00:14:51,708 说话人 说话人_00：当我第一次给它这个问题时，
151 00:14:52,279 --> 00:14:53,861 说话人 说话人_00：并且没有要求你解释原因。
152 00:14:53,883 --> 00:14:57,567 说话人 说话人_00：它实际上提出了一种解决方案，涉及将蓝色房间涂成黄色。
153 00:14:57,587 --> 00:15:01,894 说话人 说话人_00：这更像是一个数学家的解决方案，因为它将其简化为一个已解决的问题。
154 00:15:03,677 --> 00:15:04,458 说话人 说话人_00：但这也可以。
155 00:15:05,380 --> 00:15:07,945 说话人 说话人_00：所以我确信它可以进行推理。
156 00:15:09,145 --> 00:15:13,472 说话人 说话人_00：有些人，我的朋友，比如 Yann LeCun，他们确信它不能进行推理。
157 00:15:14,875 --> 00:15:16,498 说话人 说话人_00：我只是在等待他幡然醒悟。
158 00:15:18,115 --> 00:15:21,499 说话人 SPEAKER_02：我注意到你和 Jan 之间的你来我往。
159 00:15:21,538 --> 00:15:23,041 说话人 SPEAKER_02：我知道这只是友好的打趣。
160 00:15:23,760 --> 00:15:40,179 说话人 SPEAKER_02：当然，你在他的职业生涯中有着很大的影响，就像现在站在前线的大多数人一样，无论是 OpenAI 的 Ilya，他最近因为那里的动荡而上了新闻。
161 00:15:40,801 --> 00:15:46,586 说话人 SPEAKER_02：实际上，感觉所有和你一起训练过的人现在都
162 00:15:46,836 --> 00:15:52,244 说话人 SPEAKER_02：在全球各地各种 AI 公司和学术团体中的领导职位。
163 00:15:52,563 --> 00:15:58,171 说话人 SPEAKER_02：所以这充分说明了你的影响力，不仅仅是在深度神经网络方面。
164 00:15:58,250 --> 00:16:04,379 说话人 SPEAKER_02：我想问你，因为你经常被尊称为 AI 之父。
165 00:16:04,418 --> 00:16:08,484 说话人 SPEAKER_02：你认为被这样称呼怎么样？
166 00:16:10,986 --> 00:16:14,692 说话人 SPEAKER_00: 我觉得最初，这并不是完全有益的。
167 00:16:15,011 --> 00:16:15,712 说话人 SPEAKER_00: 我记得
168 00:16:16,063 --> 00:16:24,634 说话人 SPEAKER_00: 安德鲁·吴（Andrew Ng）实际上是在英国温莎镇的一个小型研讨会上提出那个词的。
169 00:16:25,576 --> 00:16:28,279 说话人 SPEAKER_00: 那是在一个我一直在打断大家的会议之后。
170 00:16:29,581 --> 00:16:33,947 说话人 SPEAKER_00：我曾经是那种领导组织举办研讨会的人。
171 00:16:33,966 --> 00:16:39,134 说话人 SPEAKER_00：我认为这是为了打断每一个人。
172 00:16:39,214 --> 00:16:44,080 说话人 SPEAKER_00：我认为这并不完全友好，但我对此感到满意。
173 00:16:45,072 --> 00:16:45,692 说话人 SPEAKER_02：那太好了。
174 00:16:47,053 --> 00:16:54,501 说话人 SPEAKER_00：现在我退休了，把一些时间花在慈善工作上，我自称是“慈善教父”。
175 00:16:57,044 --> 00:16:57,664 说话人 SPEAKER_02：那太好了。
176 00:16:58,245 --> 00:17:10,759 说话人 SPEAKER_02：你知道吗，我真的很喜欢 Josh Rothman 写的《纽约客》人物特写，我们以前合作过，他实际上在加拿大你那里待了一段时间。
177 00:17:11,548 --> 00:17:16,473 说话人 SPEAKER_02：我的意思是，它深入探讨了你的生活，这些我以前都不知道。
178 00:17:17,233 --> 00:17:28,545 说话人 SPEAKER_02：你知道，我对你的妻子患癌症以及你所经历的各种各样的事情一无所知，这些事情都是非同寻常的。
179 00:17:28,585 --> 00:17:40,797 说话人 SPEAKER_02：我想知道，当你看到医学和人工智能的影响之路，回顾你自己在家庭中的医疗经历，你是否看到了可能改变的地方。
180 00:17:41,282 --> 00:17:45,166 说话人 SPEAKER_02：你知道，我们只是错过了时间，事情本可以有所不同。
181 00:17:47,188 --> 00:17:48,789 说话人 SPEAKER_00：是的，我看到了很多。
182 00:17:51,332 --> 00:17:55,635 说话人 SPEAKER_00：首先，Joshua 是一位非常优秀的作家，他这样做真是太好了。
183 00:17:59,480 --> 00:18:08,488 说话人 SPEAKER_00：让我想到的一件事实际上是使用LLMs的一个很好的用途，也许可以稍微调整一下，以产生不同类型的语言。
184 00:18:09,278 --> 00:18:12,181 说话人 SPEAKER_00：是为了帮助癌症患者的亲属。
185 00:18:12,662 --> 00:18:13,982 说话人 SPEAKER_00：癌症会持续很长时间。
186 00:18:14,103 --> 00:18:19,189 说话人 说话人_00：我的意思是，这是持续时间最长且最复杂的事情之一。
187 00:18:20,130 --> 00:18:30,580 说话人 说话人_00：大多数人真的无法真正理解真正的选择是什么，会发生什么，以及他们的亲人实际上会因什么而死等等。
188 00:18:31,102 --> 00:18:37,969 说话人 说话人_00：我非常幸运，因为在那方面，我有一位因卵巢癌去世的妻子。
189 00:18:38,354 --> 00:18:45,761 说话人 说话人_00：我有一位以前的毕业生，他曾是放射科医生，给了我关于发生了什么的建议。
190 00:18:46,563 --> 00:18:56,714 说话人 SPEAKER_00：最近，当我的妻子，另一个妻子，因胰腺癌去世时，您所知道的 David Naylor 非常善良。
191 00:18:56,755 --> 00:19:01,559 说话人 SPEAKER_00：他给了我很多时间来向我解释发生了什么以及有哪些选择。
192 00:19:01,877 --> 00:19:06,182 说话人 SPEAKER_00：以及一些看似相当不可靠的治疗方法是否值得尝试。
193 00:19:06,722 --> 00:19:11,548 说话人 SPEAKER_00：有趣的是，他得出结论，支持它的证据不多，但如果那是他自己的情况，他会选择这么做。
194 00:19:11,827 --> 00:19:13,250 说话人 SPEAKER_00：我们做到了。
195 00:19:13,569 --> 00:19:17,314 说话人 SPEAKER_00：这就是电击肿瘤的地方，注意不要停止心脏。
196 00:19:17,854 --> 00:19:26,823 说话人 SPEAKER_00：因为如果你用两个电极电击肿瘤，而且肿瘤是紧密的，所有的能量都会进入肿瘤，而不是大部分能量进入你身体的其余组织。
197 00:19:27,624 --> 00:19:31,327 说话人 SPEAKER_00：然后它会破坏细胞膜，细胞就会死亡。
198 00:19:31,695 --> 00:19:33,657 说话人 SPEAKER_00：我们不知道这有没有帮到。
199 00:19:35,538 --> 00:19:41,664 说话人 SPEAKER_00：但是有非常了解的人给亲戚提供建议是非常有用的。
200 00:19:42,184 --> 00:19:43,286 说话人 SPEAKER_00：这真是太有帮助了。
201 00:19:43,906 --> 00:19:54,836 说话人 SPEAKER_00：而且这并不是那种生死攸关的应用，如果你解释得稍微不对，也不会决定治疗方案。
202 00:19:54,896 --> 00:19:56,298 说话人 SPEAKER_00：这不会杀死病人。
203 00:19:57,558 --> 00:19:59,580 说话人 SPEAKER_00：所以你实际上可以容忍一点错误。
204 00:20:00,241 --> 00:20:01,702 说话人 SPEAKER_00：我认为亲属们，
205 00:20:01,902 --> 00:20:08,534 说话人 SPEAKER_00：如果能和一个LLM交谈并就发生了什么咨询一个LLM，会好得多。
206 00:20:08,773 --> 00:20:10,998 说话人 SPEAKER_00: 因为医生们从来没有时间好好解释。
207 00:20:11,038 --> 00:20:17,209 说话人 SPEAKER_00: 在极少数情况下，如果你像我一样碰巧认识一个非常好的医生，你会得到正确的解释。
208 00:20:17,269 --> 00:20:21,696 说话人 SPEAKER_00: 但对于大多数人来说，它不会被正确解释，也不会用正确的语言解释。
209 00:20:21,777 --> 00:20:23,319 说话人 SPEAKER_00: 但你可以想象一个 LLM
210 00:20:23,501 --> 00:20:26,585 说话人 SPEAKER_00：仅为了帮助那些极其有用的亲戚。
211 00:20:26,605 --> 00:20:29,127 说话人 SPEAKER_00：这将是边缘用途，但我认为这将是非常有用的。
212 00:20:29,147 --> 00:20:31,049 说话人 SPEAKER_02：不，我认为你提出了一个重要观点。
213 00:20:31,089 --> 00:20:36,556 说话人 SPEAKER_02：我很高兴你提到了我的朋友大卫·奈尔，他是一位杰出的医生。
214 00:20:36,796 --> 00:20:48,028 说话人 SPEAKER_02：这就引出了直觉的概念，人类的直觉与LLM能做的事情之间的对比。
215 00:20:48,749 --> 00:20:51,551 说话人 SPEAKER_02：你不认为这些会是互补的功能吗？
216 00:20:52,560 --> 00:20:53,883 说话人 SPEAKER_00：是的，也不是。
217 00:20:54,042 --> 00:20:57,970 说话人 SPEAKER_00：也就是说，我认为这些聊天机器人是有直觉的。
218 00：20：58,510 --> 00：21：05,842 演讲者 SPEAKER_00：也就是说，他们所做的是获取一串符号，然后将每个符号转换为他们发明的一大堆功能。
219 00：21：06,522 --> 00：21：12,492 演讲者 SPEAKER_00：然后他们学习不同符号特征之间的交互，以便他们能够预测下一个符号的特征。
220 00:21:13,954 --> 00:21:14,316 说话者 说话者_00：并且
221 00：21：14,633 --> 00：21：16,096 演讲者 SPEAKER_00：我想人们也是这样做的。
222 00:21:16,898 --> 00:21:20,148 说话人 SPEAKER_00：我认为实际上他们几乎和我们一样工作。
223 00:21:20,430 --> 00:21:24,441 说话人 SPEAKER_00：很多人说他们和我们完全不一样，他们不理解。
224 00:21:24,741 --> 00:21:32,411 说话人 SPEAKER_00：但实际上，真正有关于大脑工作原理和如何理解这些事物工作原理的理论的人并不多。
225 00:21:32,431 --> 00:21:36,315 说话人 SPEAKER_00：大多数说他们和我们不一样的人实际上并没有我们工作方式的模型。
226 00:21:37,016 --> 00:21:43,923 说话人 SPEAKER_00: 他们可能感兴趣的是，这些语言模型实际上是一种关于我们大脑工作原理的理论。
227 00:21:44,664 --> 00:21:52,133 说话人 SPEAKER_00: 所以，我称之为“小语言模型”的东西，它很小，我在 1985 年引入了它。
228 00:21:52,483 --> 00:21:55,547 说话人 SPEAKER_00: 而它实际上让《自然》杂志接受了我们关于反向传播的论文。
229 00:21:56,227 --> 00:22:00,151 说话人 SPEAKER_00: 它所做的是预测一个三词字符串中的下一个词。
230 00:22:01,291 --> 00:22:06,135 说话人 SPEAKER_00: 但整个机制大体上和现在的这些模型是一样的。
231 00：22：06,236 --> 00：22：08,097 演讲者 SPEAKER_00：这些模型更复杂，因为它们使用了注意力。
232 00:22:08,557 --> 00:22:16,545 说话者 说话者_00：但基本上，你需要让它为单词和特征之间的交互发明特征，以便它可以预测下一个单词的特征。
233 00:22:17,506 --> 00:22:21,630 说话者 说话者_00：它被介绍为尝试了解大脑在做什么的一种方式。
234 00：22：22,336 --> 00：22：28,163 演讲者 SPEAKER_00：在它被引入的时候，象征性的 AI 人们没有说，哦，这不明白。
235 00:22:28,202 --> 00:22:34,852 说话人 SPEAKER_00：他们很乐意承认，这个系统在小型领域、小型玩具领域确实学会了结构。
236 00:22:35,532 --> 00:22:43,221 说话人 SPEAKER_00：他们只是争论说，通过搜索符号规则的空间来学习这种结构会更好，而不是通过神经网络权重的空间。
237 00:22:44,483 --> 00:22:46,326 说话人 SPEAKER_00：但他们并没有说这是理解。
238 00:22:47,807 --> 00:22:51,532 说话人 SPEAKER_00：只有当它真正起作用时，人们才不得不说，这不算数。
239 00:22:53,250 --> 00:22:57,896 说话人 SPEAKER_02：嗯，这也让我感到惊讶。
240 00:22:57,936 --> 00:23:00,099 说话人 SPEAKER_02：我对你的想法很感兴趣。
241 00:23:01,101 --> 00:23:21,246 说话人 SPEAKER_02：我在《深度医学》这本书中预见到，时间的礼物，我们一直在谈论的所有这些事情，比如模型可以使用的入口，提出诊断，甚至将环境对话转化为合成笔记。
242 00:23:21,266 --> 00:23:22,768 说话人 SPEAKER_02：我没有想到的事情，
243 00:23:23,068 --> 00:23:25,030 说话人 SPEAKER_02：机器能否促进同理心。
244 00:23:26,252 --> 00:23:48,682 说话人 SPEAKER_02：我现在看到的情况，不仅仅是现在数字化的笔记，还有来自诊所对话的合成笔记，以及通过LLM进行的辅导，比如说，你知道，琼斯博士，你打断病人的话太快了。
245 00:23:49,162 --> 00:23:51,404 说话人 SPEAKER_02：你没有听他们的问题。
246 00:23:51,789 --> 00:23:55,174 说话人 SPEAKER_02：你没有表现出敏感、同情或同理心。
247 00:23:55,535 --> 00:23:57,317 说话人 SPEAKER_02：也就是说，这很了不起。
248 00:23:57,337 --> 00:24:02,724 说话人 SPEAKER_02：显然，机器并不一定能够感受到或知道什么是同理心，但它可以促进它。
249 00:24:02,765 --> 00:24:04,267 说话人 SPEAKER_02：你对此有什么看法？
250 00:24:05,429 --> 00:24:07,692 说话人 SPEAKER_00：好吧，我对这个的看法有点复杂。
251 00:24:07,892 --> 00:24:15,342 说话人 SPEAKER_00: 但显然，如果你用表现出同理心的文本来训练它，它将产生表现出同理心的文本。
252 00:24:16,784 --> 00:24:20,328 说话人 SPEAKER_00: 但问题是，它真的有同理心吗？
253 00:24:21,540 --> 00:24:23,042 说话人 SPEAKER_00: 我认为这是一个悬而未决的问题。
254 00:24:23,163 --> 00:24:25,165 说话人 SPEAKER_00: 我倾向于认为它有。
255 00:24:27,327 --> 00:24:36,440 说话人 SPEAKER_00：我实际上倾向于认为这些大型聊天机器人，尤其是多模态的，具有主观体验。
256 00:24:36,460 --> 00:24:39,222 说话人 SPEAKER_00：而这正是大多数人认为完全疯狂的事情。
257 00:24:39,624 --> 00:24:44,069 说话人 SPEAKER_00：但我很乐意处于大多数人认为我完全疯狂的位置。
258 00:24:44,089 --> 00:24:48,375 说话人 SPEAKER_00：那么，让我给你们一个他们认为它们具有主观体验的理由。
259 00:24:49,469 --> 00:24:58,538 说话人 SPEAKER_00: 假设我拿一个带有摄像头和机械臂的聊天机器人，它已经被训练好了，我在它面前放一个物体，然后说，指向这个物体。
260 00:24:58,558 --> 00:24:59,661 说话人 SPEAKER_00: 所以它指向了物体。
261 00:25:00,461 --> 00:25:06,327 说话人 SPEAKER_00: 然后我在它摄像头前放了一个棱镜，它会弯曲光线，但它并不知道这一点。
262 00:25:07,048 --> 00:25:10,373 说话人 SPEAKER_00: 现在，我在它面前放一个物体，然后说，指向这个物体，它就直直地指向前方。
263 00:25:10,893 --> 00:25:13,777 说话人 SPEAKER_00: 很抱歉，它指向一侧，尽管物体正前方。
264 00:25:14,497 --> 00:25:16,859 说话人 SPEAKER_00: 我说，不，物体实际上并不在那里，
265 00:25:17,194 --> 00:25:19,817 说话人 SPEAKER_00: 物体正前方，我在你的摄像机前放了一个棱镜。
266 00:25:21,160 --> 00:25:23,683 说话人 SPEAKER_00: 假设聊天机器人说，哦，我看到了。
267 00:25:23,703 --> 00:25:29,211 说话人 SPEAKER_00：物体实际上就在正前方，但我有一种主观感觉它偏到了一边。
268 00:25:30,673 --> 00:25:36,961 说话人 SPEAKER_00：现在，如果聊天机器人这么说，我认为它会像人们一样使用“主观感觉”这个短语。
269 00:25:38,363 --> 00:25:42,087 说话人 SPEAKER_00：它的感知系统告诉它它偏到了一边。
270 00:25:42,540 --> 00:25:47,727 说话人 SPEAKER_00：所以他的感知系统告诉他的，如果物体真的偏到了一边，那将是正确的。
271 00:25:48,667 --> 00:25:50,470 说话人 SPEAKER_00: 我们所说的主观体验就是这个意思。
272 00:25:50,490 --> 00:25:59,621 说话人 SPEAKER_00: 当我说我有小粉象在我面前飘浮的主观体验时，我的意思并不是说在我内心有一个小粉象的内在剧场。
273 00:25:59,921 --> 00:26:09,071 说话人 SPEAKER_00: 我真正想说的是，如果在这个现实世界中真的有小粉象在我面前飘浮，那么我的感知系统就会告诉我真相。
274 00:26:10,469 --> 00:26:19,680 说话人 SPEAKER_00: 所以我认为主观体验有趣的地方不在于它是某种由神秘质料构成的内在剧场中的奇怪东西。
275 00:26:20,440 --> 00:26:25,046 说话人 SPEAKER_00: 我认为主观经验是对可能世界的假设性陈述。
276 00:26:25,546 --> 00:26:30,291 说话人 SPEAKER_00: 如果世界是这样的，那么你的体验，你的感知系统将运行正常。
277 00:26:31,212 --> 00:26:33,895 说话人 SPEAKER_00: 这就是我们使用主观经验的方式。
278 00:26:33,955 --> 00:26:35,817 说话人 SPEAKER_00: 我认为聊天机器人也可以这样使用。
279 00:26:36,507 --> 00:26:40,133 说话人 SPEAKER_00：我认为这里需要做很多哲学工作，并且要把它弄清楚。
280 00:26:40,512 --> 00:26:42,175 说话人 SPEAKER_00：我认为我们不能把它留给哲学家们。
281 00:26:42,236 --> 00:26:43,218 说话人 SPEAKER_00：现在太紧迫了。
282 00:26:44,500 --> 00:26:47,003 说话人 SPEAKER_02：嗯，这实际上是一个令人着迷的回答。
283 00:26:47,023 --> 00:27:04,371 说话人 SPEAKER_02：并且增加了你对理解的认识，可能让你回到了今年 5 月离开谷歌时的状态，那时你看到了这是一个新的层次
284 00:27:04,856 --> 00:27:09,821 说话人 SPEAKER_02：无论你想要称之为什么，不是 AGI，而是，你知道的，比之前的 AI 有所增强的东西。
285 00:27:11,442 --> 00:27:26,037 说话人 SPEAKER_02：从某些方面来说，我并不想说任何警钟，但自那时起，你一直持续表达了对我们进入了一个新阶段，我们正在用 AI 走向一个新方向的担忧。
286 00:27:26,057 --> 00:27:32,923 说话人 SPEAKER_02：你能详细谈谈你在 5 月时的想法和心态吗？
287 00:27:33,207 --> 00:27:34,848 说话人 SPEAKER_02：你认为现在事情会朝哪个方向发展？
288 00:27:36,131 --> 00:27:37,913 说话人 SPEAKER_00：好吧，让我们把故事理清楚。
289 00:27:37,932 --> 00:27:40,175 说话人 SPEAKER_00：这是新闻媒体传播的一个好故事。
290 00:27:40,576 --> 00:27:41,017 说话人 SPEAKER_00：是的。
291 00:27:41,037 --> 00:27:47,164 说话人 SPEAKER_00：实际上，我因为 75 岁而离开了谷歌，因为我无法再编程了，因为我总是忘记变量代表什么。
292 00:27:47,945 --> 00:27:48,006 说话人 SPEAKER_00：好的。
293 00:27:49,268 --> 00:27:50,930 说话人 SPEAKER_00：我抓住了这个机会。
294 00:27:51,090 --> 00:27:52,633 说话人 SPEAKER_00：此外，我还想看很多 Netflix。
295 00:27:53,073 --> 00:27:56,817 说话人 SPEAKER_00: 我反正要离开谷歌，就趁机开始公开发表关于人工智能安全的声明。
296 00:27:57,067 --> 00:27:59,790 说话人 SPEAKER_00: 几个月前我对人工智能安全非常担忧。
297 00:28:00,451 --> 00:28:04,135 说话人 SPEAKER_00: 发生的事情是，我正在尝试寻找进行计算的方法的类似方式。
298 00:28:06,317 --> 00:28:11,463 说话人 SPEAKER_00: 我在尝试找出进行计算的方法的类似方式。
299 00:28:11,483 --> 00:28:14,105 说话人 SPEAKER_00：所以您可以用更少的能量来做这些大型语言模型。
300 00:28:15,127 --> 00:28:21,994 说话人 SPEAKER_00：我突然意识到，实际上数字计算方式可能要好得多。
301 00:28:22,596 --> 00:28:25,759 说话人 SPEAKER_00：它之所以好得多，是因为您可以拥有
302 00:28:26,801 --> 00:28:31,866 说话人 SPEAKER_00：成千上万份完全相同的数字模型在不同的硬件上运行。
303 00:28:32,847 --> 00:28:37,712 说话人 说话人_00：每个副本都可以查看互联网的不同部分并从中学习。
304 00:28:37,772 --> 00:28:45,401 说话人 说话人_00：它们可以立即通过共享权重或共享权重梯度来共享所学到的内容。
305 00:28:45,421 --> 00:28:50,767 说话人 说话人_00：因此，您可以非常高效地让10,000件事情分享它们的经验。
306 00:28:51,326 --> 00:28:53,028 说话人 说话人_00：这是人们做不到的。
307 00:28:53,380 --> 00:29:01,607 说话人 SPEAKER_00: 如果有 1 万人去学习 1 万个不同的技能，你不能说，好吧，让我们平均一下体重，现在我们所有人都知道这些技能。
308 00:29:02,489 --> 00:29:03,650 说话人 SPEAKER_00: 这不是那么工作的。
309 00:29:03,670 --> 00:29:07,814 说话人 SPEAKER_00: 你必须上大学，并试图理解对方在说些什么。
310 00:29:07,834 --> 00:29:15,981 说话人 SPEAKER_00: 这是一个非常缓慢的过程，你需要从对方那里获取句子，并思考，我如何改变我的大脑，以便我可能产生那个句子？
311 00:29:16,762 --> 00:29:20,686 说话人 SPEAKER_00: 与这些数字模型通过共享权重所能做到的相比，这非常低效。
312 00:29:21,307 --> 00:29:27,513 说话人 SPEAKER_00: 所以我有了这样的顿悟，数字模型可能要好得多。
313 00:29:27,673 --> 00:29:34,140 说话人 SPEAKER_00: 此外，它们可以很容易地使用反向传播算法，而要看到大脑如何高效地做到这一点是非常困难的。
314 00:29:34,440 --> 00:29:40,705 说话人 SPEAKER_00: 而且没有人能够提出任何在真实神经网络中与反向传播在规模上相当的方法。
315 00:29:42,748 --> 00:29:49,054 说话者 SPEAKER_00：我有一种顿悟，这让我放弃了模拟研究，因为数字计算机实际上更好。
316 00:29:49,337 --> 00:29:53,162 说话者 SPEAKER_00：既然我本来就要退休了，我就趁机说，嘿，它们就是更好。
317 00:29:53,623 --> 00:29:54,723 说话者 SPEAKER_00：所以我们得小心了。
318 00:29:56,486 --> 00:30:06,718 说话者 SPEAKER_02：我的意思是，我认为你对这个问题的看法以及你如何支持它，当然，产生了重大影响。
319 00:30:07,239 --> 00:30:10,281 说话人 SPEAKER_02：当然，这仍然是一个持续且激烈的辩论。
320 00:30:10,343 --> 00:30:15,008 说话人 SPEAKER_02：从某些方面来说，这实际上是在讨论 OpenAI 的动荡根源，你知道，关于事物所在之处以及它们将何去何从的争议。
321 00:30:15,190 --> 00:30:21,875 说话人 SPEAKER_02：我想就你提到的放射科医生的问题做一个总结，并不是要贬低他们，说他们将被取代，这让我们来到了今天的紧张状态，即作为智能的巅峰，人类是否会被
322 00:30:22,757 --> 00:30:45,078 说话人 SPEAKER_02：我想就你提到的关于放射科医生的观点做一个总结，并不是要贬低他们，说他们将被取代，这让我们来到了今天的紧张状态，即作为智能的巅峰，人类是否会被
323 00:30:46,087 --> 00:31:12,228 说话人 SPEAKER_02：没有替换，但被 AI 的未来所取代，当然，我们这个物种无法处理这种机器，就像放射科医生一样，我们这个物种无法接受这种可能性，即有一台机器，你知道的，拥有远少于我们的连接，却能完成事情，超越我们。
324 00:31:12,749 --> 00:31:14,711 说话人 SPEAKER_02：或者，当然，正如我们刚才
325 00:31:15,011 --> 00:31:21,259 说话人 SPEAKER_02：我认为，正如我们在与人类的对话中所强调的，甚至可以将其提升到另一个层次。
326 00:31:21,880 --> 00:31:32,672 说话人 SPEAKER_02：但是，这种紧张感是否是机器超越人类潜力的部分问题，人们难以接受这种观念？
327 00:31:33,772 --> 00:31:34,534 说话人 SPEAKER_00: 是的，我认为是这样的。
328 00:31:34,713 --> 00:31:44,664 说话人 SPEAKER_00: 尤其是哲学家，他们想要表达的是，人与意识、主观体验、感觉和质料有着非常特殊的关系。
329 00:31:45,151 --> 00:31:48,954 说话人 SPEAKER_00: 而这些机器只是机器。
330 00:31:49,816 --> 00:31:56,442 说话人 SPEAKER_00: 嗯，如果你是一个科学唯物主义者，就像我们大多数人一样，你知道大脑不过是一台机器。
331 00:31:57,103 --> 00:32:05,912 说话者 SPEAKER_00: 说它只是一台机器是错误的，因为它是一台奇妙复杂的机器，能做对人们非常重要的事情，但它仍然是一台机器。
332 00:32:06,492 --> 00:32:12,558 说话者 SPEAKER_00: 在原则上，没有理由不应该有更好的机器和更好的计算方式，正如我现在所相信的那样。
333 00:32:13,028 --> 00:32:17,053 说话者 SPEAKER_00: 因此，我认为人们有一个非常长的历史，认为自己是特殊的。
334 00:32:19,676 --> 00:32:24,061 说话者 SPEAKER_00: 他们认为上帝按照自己的形象创造了他们，并将他们置于宇宙的中心。
335 00:32:24,722 --> 00:32:28,607 说话人 SPEAKER_00: 很多人已经克服了这一点，但也有人还没有。
336 00:32:30,191 --> 00:32:37,720 说话人 SPEAKER_00: 但是对于那些已经克服这一点的人来说，我认为从原则上来说，我们没有理由认为自己是最聪明的。
337 00:32:38,480 --> 00:32:42,006 说话人 SPEAKER_00: 我认为这些机器可能很快就会比我们聪明。
338 00:32:42,339 --> 00:32:50,771 说话人 SPEAKER_00: 我仍然希望我们能与机器达成协议，让它们像仁慈的父母一样行事。
339 00:32:52,795 --> 00:32:54,036 说话人 说话人_00：他们在为我们操心。
340 00:32:55,898 --> 00:32:57,320 说话人 说话人_00：我们已经成功激励了他们。
341 00:32:57,902 --> 00:33:02,708 说话人 说话人_00：对他们来说，最重要的是我们的成功，就像母亲和孩子一样。
342 00:33:03,970 --> 00:33:04,971 说话人 说话人_00：对男人来说，则不那么重要。
343 00:33:05,712 --> 00:33:11,840 说话人 SPEAKER_00: 我真的很希望那个解决方案。
344 00:33:12,259 --> 00:33:13,781 说话人 SPEAKER_00: 我只是担心我们得不到它。
345 00:33:15,565 --> 00:33:19,150 说话人 SPEAKER_02: 嗯，那将是我们前进的好方法。
346 00:33:19,851 --> 00:33:33,990 说话人 SPEAKER_02: 当然，那些末日预言家和那些在他们的警报水平上更糟糕的人，你知道，往往会认为那是不可能的。
347 00:33:34,070 --> 00:33:36,354 说话人 SPEAKER_02：但显然，我们会随着时间的推移看到。
348 00:33:36,394 --> 00:33:41,540 说话人 SPEAKER_02：现在，在我们结束之前，我想快速从你那里得到一些反馈。
349 00:33:41,977 --> 00:33:53,335 说话人 SPEAKER_02：正如您所知，最近，Demis Esabas 和 John Jumper 获得了拉斯克奖，这是 AlphaFold2 的诺贝尔奖预选奖。
350 00:33:54,356 --> 00:34:05,875 说话人 SPEAKER_02：但这个转换模型，它当然有助于理解 2 亿个蛋白质的 3D 结构，但它并不了解它是如何工作的。
351 00:34:06,292 --> 00:34:12,822 说话人 SPEAKER_02：像大多数模型一样，不同于我们之前在LLM侧讨论的理解。
352 00:34:13,702 --> 00:34:21,733 说话人 SPEAKER_02：我认为这个奖项应该给这个模型加一个星号。
353 00:34:23,717 --> 00:34:25,259 说话人 SPEAKER_02：你对这个想法有什么看法？
354 00:34:28,804 --> 00:34:29,706 说话人 SPEAKER_00：就是这样。
355 00:34:31,188 --> 00:34:35,793 说话人 SPEAKER_00：我希望人们认真对待我说的话。
356 00:34:36,179 --> 00:34:47,972 说话人 SPEAKER_00：你可以选择一个方向，我认为谷歌创始人之一的拉里·佩奇就选择了这个方向，那就是说，存在这些超级智能，为什么它们不应该拥有权利呢？
357 00:34:49,014 --> 00:34:55,882 说话人 SPEAKER_00：如果你开始走向这个方向，你会失去人们的支持。
358 00:34:57,143 --> 00:34:58,844 说话人 SPEAKER_00：人们是不会接受的。
359 00:34:59,094 --> 00:35:01,978 说话人 SPEAKER_00: 这些事物应该拥有政治权利，例如。
360 00:35:02,898 --> 00:35:05,802 说话人 SPEAKER_00: 而成为合著者是政治权利的开始。
361 00:35:07,885 --> 00:35:18,717 说话人 SPEAKER_00: 所以我尽量避免谈论这个问题，但我对此既矛盾又无信仰，不确定它们是否应该拥有。
362 00:35:19,317 --> 00:35:27,887 说话人 SPEAKER_00: 但我认为最好远离这个问题，因为如果你说机器应该拥有权利，大多数人会停止听你说话。
363 00:35:28,322 --> 00:35:39,679 说话人 SPEAKER_02：嗯，当然，这就引出了我们刚才讨论的话题，以及人类与机器之间的斗争是多么艰难，而不是人类与机器的共生，这种共生是可以实现的。
364 00:35:39,739 --> 00:35:42,465 说话人 SPEAKER_02：但是，Jeff，这真是太棒了。
365 00:35:42,664 --> 00:35:43,686 说话人 SPEAKER_02：我们内容丰富。
366 00:35:43,706 --> 00:35:51,599 说话人 SPEAKER_02：当然，我们可以聊几个小时，但我非常享受亲自听到你的观点和你的智慧。
367 00:35:52,840 --> 00:35:56,405 说话人 SPEAKER_02：只是为了强调关于
368 00:35:56,807 --> 00:36:08,041 说话人 SPEAKER_02：现在有多少领导这个领域的人，他们的根基来源于您的教导、激励和挑战等等。
369 00:36:08,583 --> 00:36:09,804 说话人 SPEAKER_02：我们欠您的。
370 00:36:09,824 --> 00:36:18,635 说话人 SPEAKER_02：非常感谢您为我们所做的一切，以及您将继续为我们提供帮助，引导我们通过人工智能快速发展的动态阶段。
371 00:36:19,757 --> 00:36:23,862 说话人 SPEAKER_00: 感谢并祝您好运，让 AI 在医学领域真正发挥巨大作用。
372 00:36:25,056 --> 00:36:31,521 说话人 SPEAKER_02: 希望我们能，我还会不时向您咨询，以获取一些智慧来帮助我们。
373 00:36:32,306 --> 00:36:32,686 说话人 SPEAKER_00: 随时欢迎。

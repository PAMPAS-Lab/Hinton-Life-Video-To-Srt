1 00:00:03,946 --> 00:00:05,368 Speaker SPEAKER_01: 嗨，大家好。
2 00:00:05,387 --> 00:00:08,711 Speaker SPEAKER_01: 真喜欢今天这里的气氛。
3 00:00:08,772 --> 00:00:21,527 Speaker SPEAKER_01: 欢迎来到火星探索区，这个美妙的大楼，参加这次非常特别的激进人工智能创始人活动，由多伦多大学联合举办。
4 00:00:21,547 --> 00:00:26,975 Speaker SPEAKER_01: 我叫梅里克·格特勒，我很荣幸能担任多伦多大学的校长。
5 00：00：27,856 --> 00：00：33,143 演讲者 SPEAKER_01：在我们开始之前，我想感谢多伦多大学所在的土地。
6 00：00：33,122 --> 00：00：39,759 演讲者 SPEAKER_01：几千年来，它一直是休伦-温达特、塞内卡和信贷的密西沙加人的传统土地。
7 00：00：40,661 --> 00：00：50,186 演讲者 SPEAKER_01：今天，这个聚会场所仍然是来自海龟岛各地的许多原住民的家园，我们非常感谢有机会在这片土地上工作和聚会。
8 00：00：51,600 --> 00：01：11,162 演讲者 SPEAKER_01： 嗯，我非常高兴地欢迎大家参加多伦多大学名誉教授 Geoffrey Hinton（被许多人称为深度学习教父）和斯坦福大学计算机科学首任红杉教授 Fei-Fei Li 之间的讨论，她是斯坦福大学以人为本的 AI 研究所的联合主任。
9 00:01:12,623 --> 00:01:20,831 主持人：我想感谢 Radical Ventures 和其他活动合作伙伴与多伦多大学合作，创造了这一罕见而特殊的机会。
10 00:01:22,516 --> 00:01:32,887 主持人：多伦多大学在学术人工智能领域处于前沿，这很大程度上归功于 Hinton 教授及其同事的开创性工作，几十年来一直如此。
11 00:01:33,787 --> 00:01:50,025 主持人：深度学习是推动人工智能繁荣的主要突破之一，其中许多关键发展都是由 Hinton 教授及其在多伦多大学的学生开创的。这种卓越的传统，这种悠久传统，一直延续至今。
12 00:01:50,325 --> 00:02:02,277 主持人：我们的教职员工、学生和毕业生，以及 Vector 研究所和世界各地的大学合作伙伴一起，正在推进机器学习并推动创新。
13 00:02:03,269 --> 00:02:13,683 说话人 SPEAKER_01：今年秋天晚些时候，我们的教职员工、学生和合作伙伴将开始搬入位于街对面的美丽新施瓦茨-赖斯曼创新校园的一期工程。
14 00:02:13,704 --> 00:02:20,935 说话人 SPEAKER_01：你可能已经注意到了角落里那座相当引人注目的建筑，官方开幕仪式计划于明年年初举行。
15 00:02:20,974 --> 00:02:29,546 说话人 SPEAKER_01：这个设施将通过创建加拿大最大的基于大学的创新中心来加速创新和发现。
16 00:02:30,860 --> 00:02:48,782 说话人 SPEAKER_01：由海瑟·赖 isman 和杰里·施瓦茨慷慨而有远见的捐赠使然，创新校园将成为人工智能思想领导力的焦点，将举办施瓦茨-赖斯曼技术与社会研究所（由 Gillian Hadfield 教授领导）和 Vector 研究所。
17 00:02:50,805 --> 00:02:58,555 说话人 SPEAKER_01：人工智能和机器学习已经在推动经济创新和价值创造方面变得显而易见。
18 00:02:59,361 --> 00:03:06,332 说话人 SPEAKER_01：它们也在改变药物发现、医学诊断和先进材料搜索等领域的科研。
19 00:03:07,936 --> 00:03:14,406 说话人 SPEAKER_01：当然，与此同时，人们对人工智能在塑造人类未来中所扮演的角色越来越担忧。
20 00:03:15,307 --> 00:03:24,002 说话人 SPEAKER_01：因此，今天的对话显然涉及了一个及时且重要的话题，我很高兴大家能在这个重要的时刻加入我们。
21 00:03:24,420 --> 00:03:28,765 主持人 SPEAKER_01：那么，无需多言，现在让我来介绍今天的主持人，乔丹·雅各布斯。
22 00:03:29,986 --> 00:03:38,276 主持人 SPEAKER_01：乔丹是 Radical Ventures 的合伙人兼联合创始人，这是一家领先的创业投资公司，支持在多伦多和世界各地基于 AI 的创业公司。
23 00:03:39,356 --> 00:03:48,926 主持人 SPEAKER_01：此前，他与 Layer6 AI 共同创立，并在 TD 银行集团收购该公司之前担任联合首席执行官。随后，他加入了 TD 银行集团，担任首席人工智能官。
24 00:03:50,002 --> 00:04:01,580 主持人 SPEAKER_01：乔丹是加拿大高级研究院的董事，也是 Vector Institute 的创始人之一，这个想法是他与汤米·普特南、杰夫·辛顿、爱德·克拉克等人共同构思的。
25 00:04:02,461 --> 00:04:05,825 主持人 SPEAKER_01：尊敬的嘉宾，请和我一起欢迎乔丹·雅各布斯。
26 00:04:16,382 --> 00:04:16,601 未知演讲者：请上来。
27 00:04:17,677 --> 00:04:18,920 主持人 SPEAKER_10：非常感谢，梅里克。
28 00:04:19,060 --> 00:04:23,129 主持人 SPEAKER_10：我想首先感谢今天能够实现这一点的许多人。
29 00:04:24,089 --> 00:04:31,685 讲者 SPEAKER_10：多伦多大学，梅里克，梅兰妮·伍德恩，艺术与科学学院院长，以及将此事付诸实践的多位合作伙伴。
30 00:04:31,764 --> 00:04:40,362 讲者 SPEAKER_10：这是我们每年在 Radical 举办的四场创始人 AI 大师班系列的第一场。
31 00:04:40,341 --> 00:04:42,163 讲者 SPEAKER_10：这是我们举办的第三届。
32 00:04:42,223 --> 00:04:44,005 讲者 SPEAKER_10：今天是今年的第一场。
33 00:04:44,045 --> 00:04:46,108 说话人 SPEAKER_10: 我们亲自在线上做这件事。
34 00:04:46,148 --> 00:04:48,670 说话人 SPEAKER_10: 所以有成千上万的人在线观看。
35 00:04:48,711 --> 00:04:52,494 说话人 SPEAKER_10: 如果你决定需要开始咳嗽，可能要去外面。
36 00:04:54,475 --> 00:04:57,119 说话人 SPEAKER_10: 我们与 Vector 研究所合作进行这项工作。
37 00:04:57,338 --> 00:05:04,846 主持人 SPEAKER_10: 非常感谢他们参与和支持阿尔伯塔省的阿尔伯塔机器智能研究所。
38 00:05:04,826 --> 00:05:08,091 主持人 SPEAKER_10: 以及斯坦福 HAI，感谢 Fei-Fei。
39 00:05:08,492 --> 00:05:11,497 主持人 SPEAKER_10: 所以感谢你们所有人，你们都是优秀的合作伙伴。
40 00:05:11,817 --> 00:05:13,920 主持人 SPEAKER_10: 我们希望这将是一场非常有趣的讨论。
41 00:05:13,939 --> 00:05:21,651 说话人 SPEAKER_10：这是我第一次和 Jeff 以及 Fei-Fei——我喜欢把他们当作朋友来对待——进行公开对话。
42 00:05:21,730 --> 00:05:23,514 说话人 SPEAKER_10：所以我认为这将是一场非常有趣的对话。
43 00:05:24,735 --> 00:05:28,641 说话人 SPEAKER_10：让我快速做一些更深入的
44 00:05:28,620 --> 00:05:30,843 说话人 SPEAKER_10：背景解释。
45 00:05:31,685 --> 00:05:34,529 说话人 SPEAKER_10: 杰夫常被称为人工智能之父。
46 00:05:34,771 --> 00:05:35,971 说话人 SPEAKER_10: 他赢得了图灵奖。
47 00:05:36,473 --> 00:05:52,418 说话人 SPEAKER_10: 他是多伦多大学的荣誉教授，向量研究所的联合创始人，也指导了许多后来成为全球人工智能领域领导者的许多人，包括在大型公司和世界顶级研究实验室的许多人。
48 00:05:52,398 --> 00:06:03,773 说话人 SPEAKER_10: 所以当我们说“之父”时，确实如此，杰夫有很多孩子和孙子辈的人正在引领世界人工智能的发展，这一切都源于多伦多。
49 00:06:05,435 --> 00:06:12,944 讲者 SPEAKER_10：Fei-Fei 是斯坦福大学人机智能研究所的创始所长，斯坦福大学教授。
50 00:06:13,466 --> 00:06:20,514 讲者 SPEAKER_10：她是美国国家工程院、美国国家医学院和美国艺术与科学学院的院士。
51 00:06:20,495 --> 00:06:28,987 讲者 SPEAKER_10：2017-18 年在斯坦福大学休假期间，她担任了谷歌副总裁，谷歌云 AIML 首席科学家。
52 00:06:30,329 --> 00:06:37,939 讲者 SPEAKER_10：关于 Feifei，我们还可以说很多，但她也培养出了许多学生，他们在全球范围内成为了该领域的领导者。
53 00:06:38,781 --> 00:06:46,232 说话人 SPEAKER_10: 真的是非常重要，所以对于那些还没有听说的人，Feifei 将在几周后出版一本书。
54 00:06:46,752 --> 00:06:49,016 说话人 SPEAKER_10: 它将于 11 月 7 日出版。
55 00:06:49,596 --> 00:06:54,286 说话人 SPEAKER_10: 书名为《我看的世界：AI 黎明时期的探索、发现与好奇心》。
56 00:06:55,026 --> 00:06:55,468 说话人 SPEAKER_10: 我已经读过了。
57 00:06:55,649 --> 00:06:56,410 说话人 SPEAKER_10: 太棒了。
58 00:06:57,331 --> 00:06:58,473 说话人 SPEAKER_10: 你们都应该出去买。
59 00:06:58,634 --> 00:07:07,149 说话人 SPEAKER_10: 我会读给你听 Jeff 写的书脊说明，因为它比我说的要好得多。
60 00:07:07,290 --> 00:07:10,295 说话人 SPEAKER_10: 所以这是 Jeff 的描述。
61 00:07:10,867 --> 00:07:17,213 讲者 SPEAKER_10：李飞飞是第一位真正理解大数据力量的计算机视觉研究者，她的工作为深度学习打开了大门。
62 00:07:17,773 --> 00:07:29,144 讲者 SPEAKER_10：她提供了对她帮助释放的 AI 技术巨大潜力与危险的紧迫、清醒的描述，她在这一历史关键时刻的行动呼吁和集体责任感是迫切需要的。
63 00:07:29,764 --> 00:07:34,127 讲者 SPEAKER_10：所以我敦促你们所有人去预订这本书，并在它出版后尽快阅读。
64 00:07:34,848 --> 00:07:39,473 讲者 SPEAKER_10：至此，感谢 Fei-Fei 和李杰加入我们。
65 00:07:46,675 --> 00:07:56,468 说话人 SPEAKER_10: 好吧，我认为说没有这两个人，现代人工智能时代就不存在，至少不是以它现在这种方式存在，这并不夸张。
66 00:07:56,747 --> 00:08:01,254 说话人 SPEAKER_10: 好吧，让我们回到我认为的宇宙大爆炸时刻，AlexNet 和 ImageNet。
67 00:08:01,954 --> 00:08:07,502 说话人 SPEAKER_10: 也许杰夫，你能从你的角度带我们回顾一下那个时刻，也就是 11 年前？
68 00:08:07,533 --> 00:08:21,860 说话人 SPEAKER_12: 好吧，2012 年，我的两位非常聪明的博士生赢得了一项公开竞赛，并展示了深度神经网络可以比现有技术做得更好。
69 00:08:21,879 --> 00:08:26,249 说话人 SPEAKER_12：现在，如果没有可以用来训练的大数据集，这是不可能的。
70 00:08:26,269 --> 00:08:30,216 说话人 SPEAKER_12：在此之前，还没有一个标注好的图像大数据集。
71 00:08:30,415 --> 00:08:33,583 说话人 SPEAKER_12：而 Fei-Fei 负责了这个数据集。
72 00:08:33,884 --> 00:08:39,902 说话人 SPEAKER_12：我想先问 Fei-Fei，在组建这个数据集的过程中有没有遇到什么问题。
73 00:08:42,126 --> 00:08:43,288 说话人 SPEAKER_05: 嗯，谢谢，Jeff。
74 00:08:43,369 --> 00:08:44,429 说话人 SPEAKER_05: 也谢谢，Jordan。
75 00:08:44,509 --> 00:08:47,094 说话人 SPEAKER_05: 感谢多伦多大学，谢谢。
76 00:08:47,193 --> 00:08:48,897 说话人 SPEAKER_05: 能在这里真的很开心。
77 00:08:49,397 --> 00:08:52,942 说话人 SPEAKER_05：是的，你提到的数据集叫做 ImageNet。
78 00:08:53,302 --> 00:09:03,417 说话人 SPEAKER_05：我是在 2007 年开始构建它的，接下来的三年里，基本上都是我和我的研究生在构建它。
79 00:09:04,158 --> 00:09:07,342 说话人 SPEAKER_05：你问我构建它时有没有遇到问题？
80 00:09:07,363 --> 00:09:10,868 说话人 SPEAKER_05：我从哪里开始说起呢？
81 00:09:10,847 --> 00:09:19,097 说话人 SPEAKER_05：即使在项目的构想阶段，我就被告知这真是一个糟糕的想法。
82 00:09:19,118 --> 00:09:22,501 说话人 SPEAKER_05：我当时是一名年轻的助理教授。
83 00:09:22,642 --> 00:09:27,087 说话人 SPEAKER_05：我记得那是我作为普林斯顿大学助理教授的第一年。
84 00:09:27,107 --> 00:09:36,619 说话人 SPEAKER_05：例如，我的一个在学术界非常受尊敬的导师，如果你了解学术术语的话。
85 00:09:36,599 --> 00:09:50,884 讲者 SPEAKER_05：这些人将会写我的任期评估，他们实际上非常诚恳地告诉我，在我告诉他们 2007 年这个计划后，请不要这样做。
86 00:09:50,924 --> 00:09:54,952 讲者 SPEAKER_12：那么那应该是 jitendra，对吧？
87 00:09:55,614 --> 00:09:59,080 讲者 SPEAKER_05：建议是
88 00:10:00,258 --> 00:10:03,821 讲者 SPEAKER_05：如果你这样做，可能会在获得终身教职上遇到麻烦。
89 00:10:04,881 --> 00:10:17,754 说话人 SPEAKER_05: 然后我也试图邀请其他合作者，但机器学习和人工智能领域没有人愿意接近这个项目。
90 00:10:18,534 --> 00:10:20,135 说话人 SPEAKER_05: 当然，没有资金支持。
91 00:10:20,937 --> 00:10:24,159 说话人 SPEAKER_10: 对。
92 00:10:24,179 --> 00:10:27,682 说话人 SPEAKER_10: 向我们描述一下 ImageNet，对于不熟悉的人来说。
93 00:10:27,966 --> 00:10:38,580 讲者 SPEAKER_05: 嗯，所以 ImageNet 是在 2006 年左右，2007 年左右构思的，我构思 ImageNet 的原因实际上有两个方面。
94 00:10:38,980 --> 00:10:46,090 讲者 SPEAKER_05: 其中一个是，杰夫，我认为我们有着相似的背景，我是一名科学家的训练。
95 00:10:46,169 --> 00:10:49,573 讲者 SPEAKER_05: 对于我来说，做科学就是追逐北极星。
96 00:10:50,075 --> 00:10:56,302 讲者 SPEAKER_05: 在人工智能领域，尤其是在视觉智能领域，对我来说，物体识别，这种能力
97 00:10:56,283 --> 00:11:06,054 讲者 SPEAKER_05：对于计算机识别图片中的桌子或椅子，这被称为物体识别，必须是我们领域的一个北极星问题。
98 00:11:06,535 --> 00:11:11,062 讲者 SPEAKER_05：我觉得我们真的需要在这个问题上取得突破。
99 00:11:11,381 --> 00:11:13,524 讲者 SPEAKER_05：所以我想要定义这个北极星问题。
100 00:11:13,684 --> 00:11:16,269 讲者 SPEAKER_05：这是 ImageNet 的一个方面。
101 00:11:16,288 --> 00:11:25,259 说话人 SPEAKER_05：ImageNet 的第二个方面是意识到当时机器学习有点在原地打转，我们正在制作非常复杂的模型，却没有足够的数据来驱动机器学习。
102 00:11:25,746 --> 00:11:31,315 说话人 SPEAKER_05：当然，用我们的行话来说，这实际上是泛化问题，对吧？
103 00:11:31,355 --> 00:11:36,120 说话人 SPEAKER_05：我认识到我们真的需要重置一下，从数据驱动的角度重新思考机器学习。
104 00:11:36,140 --> 00:11:45,394 说话人 SPEAKER_05：并且我认识到我们真的需要重置一下，从数据驱动的角度重新思考机器学习。
105 00:11:45,433 --> 00:11:51,601 讲者 SPEAKER_05：我想疯狂一下，制作一个前所未有的数据集。
106 00:11:52,054 --> 00:11:55,821 讲者 SPEAKER_05：关于数量、多样性以及一切。
107 00:11:55,941 --> 00:12:13,505 讲者 SPEAKER_05：经过三年的努力，ImageNet 成为了一个精心挑选的互联网图像数据集，总共有 1500 万张图片，涵盖了 22000 个概念、物体类别概念。
108 00:12:13,966 --> 00:12:16,610 讲者 SPEAKER_05：这就是那个数据集。
109 00:12:17,350 --> 00:12:28,163 说话人 SPEAKER_12: 仅作比较，在多伦多同一时间，我们制作了一个名为 CIFAR-10 的数据集，它包含 10 个不同的类别和 60,000 张图片，这是一项大量工作。
110 00:12:28,903 --> 00:12:33,308 说话人 SPEAKER_12: 这个数据集得到了 CIFAR 的慷慨赞助，每张图片五美分。
111 00:12:35,370 --> 00:12:38,355 说话人 SPEAKER_10: 因此，您将数据集变成了一个竞赛。
112 00:12:38,956 --> 00:12:44,201 说话人 SPEAKER_10: 请简要介绍一下这意味着什么，然后我们将快速跳到 2012 年。
113 00:12:44,467 --> 00:12:44,768 说话人 SPEAKER_05: 对。
114 00:12:45,668 --> 00:12:47,812 说话人 SPEAKER_05: 所以我们在 2009 年制作了这个数据集。
115 00:12:48,972 --> 00:12:56,642 说话人 SPEAKER_05: 我们几乎在学术会议上做成海报，但没有人注意。
116 00:12:57,142 --> 00:12:59,184 说话人 SPEAKER_05: 所以我当时有点绝望。
117 00:12:59,946 --> 00:13:04,890 说话人 SPEAKER_05：我认为这是正确的方向，并且我们已经开源了它。
118 00:13:05,351 --> 00:13:07,354 说话人 SPEAKER_05：但是即使开源，
119 00:13:08,683 --> 00:13:11,707 说话人 SPEAKER_05：它并没有真正流行起来。
120 00:13:11,727 --> 00:13:18,755 说话人 SPEAKER_05：所以我和我的学生们想，好吧，让我们增加一点竞争的动力。
121 00:13:18,775 --> 00:13:27,866 讲者 SPEAKER_05：让我们举办一场竞赛，邀请全球研究社区参与这个通过 ImageNet 进行物体识别的问题。
122 00:13:27,886 --> 00:13:36,716 讲者 SPEAKER_05：所以我们举办了 ImageNet 竞赛，我们从朋友和同事那里得到的第一个反馈是它太大了。
123 00:13:37,000 --> 00:13:44,105 讲者 SPEAKER_05：当时你无法将其装入硬盘，更不用说内存了。
124 00:13:44,667 --> 00:13:57,899 讲者 SPEAKER_05：因此，我们实际上创建了一个较小的数据集，称为图像挑战数据集，它包含 1000 个类别中的 100 万张图片，而不是 22000 个类别。
125 00:13:58,278 --> 00:14:03,003 说话者 SPEAKER_05: 我认为这始于 2010 年。
126 00:14:03,504 --> 00:14:07,006 说话者 SPEAKER_05: 你们在 2011 年注意到了，对吧？
127 00:14:06,986 --> 00:14:14,620 说话者 SPEAKER_12: 所以在我的实验室里，我们已经有深度神经网络在语音识别方面工作得相当好了。
128 00:14:15,702 --> 00:14:21,552 说话者 SPEAKER_12: 然后伊利亚·苏茨克维尔说，我们拥有的东西应该能够赢得 ImageNet 竞赛。
129 00:14:22,124 --> 00:14:25,530 说话人 SPEAKER_12：他试图说服我我们应该这么做。
130 00:14:25,591 --> 00:14:27,635 说话人 SPEAKER_12：我说，嗯，你知道，数据量很大。
131 00:14:28,317 --> 00:14:35,251 说话人 SPEAKER_12：他试图说服他的朋友亚历克斯·克日什科夫斯基，但亚历克斯并不感兴趣。
132 00:14:36,313 --> 00:14:40,503 说话人 SPEAKER_12：所以伊利亚实际上预处理了所有数据，以便将其放入亚历克斯需要的格式。
133 00:14:40,523 --> 00:14:42,748 说话人 SPEAKER_05: 你缩小了图片的大小。
134 00:14:42,727 --> 00:14:44,450 说话人 SPEAKER_12: 是的，他稍微缩小了图片。
135 00:14:45,091 --> 00:14:46,072 说话人 SPEAKER_05: 嗯，我记得。
136 00:14:46,153 --> 00:14:50,259 说话人 SPEAKER_12: 然后为 Alex 进行了适当的预处理，然后 Alex 最终同意这么做。
137 00:14:51,039 --> 00:15:03,037 说话人 SPEAKER_12：与此同时，在纽约 Yann LeCun 的实验室里，Yann 正拼命地试图让他的学生和博士后们处理这个数据集，因为他表示，第一个将卷积网络应用于这个数据集的人将会获胜。
138 00:15:03,096 --> 00:15:05,399 说话人 SPEAKER_12：然而，他的学生都没有兴趣。
139 00:15:05,419 --> 00:15:06,721 说话人 SPEAKER_12：他们都在忙于其他事情。
140 00:15:07,543 --> 00:15:10,827 说话人 SPEAKER_12：于是 Alex 和 India 开始着手处理。
141 00:15:11,634 --> 00:15:21,091 说话人 SPEAKER_12：通过在上一年度的比赛中运行，我们发现我们的表现比其他技术要好得多，所以我们知道我们将在 2012 年比赛中获胜。
142 00:15:22,014 --> 00:15:32,513 说话人 SPEAKER_12：然后出现了这个问题，我们认为如果我们证明神经网络在这场比赛中获胜，计算机视觉领域的人，特别是 Jitendra，会说，嗯，这仅仅表明数据集不太好。
143 00:15:32,594 --> 00:15:38,679 说话人 SPEAKER_12：所以我们必须在比赛前让他们同意，如果我们获胜，我们就证明了神经网络是有效的。
144 00:15:40,162 --> 00:15:45,427 说话人 SPEAKER_12：因此，我们必须让他们事先同意，如果我们获胜，我们就证明了神经网络是有效的。
145 00:15:45,888 --> 00:15:59,461 说话人 SPEAKER_12：所以我实际上给 Jitendra 打了电话，我们讨论了我们可能运行的数据库，我的目标是让 Jitendra 同意，如果我们能做 ImageNet，那么神经网络真的有效。
146 00:15:59,442 --> 00:16:09,030 说话人 SPEAKER_12：经过一番讨论，他建议我做其他数据库，我们最终达成一致，好吧，如果我们能做 ImageNet，那么我们就证明了神经网络的有效性。
147 00:16:09,051 --> 00:16:15,037 说话人 SPEAKER_12：Jitendra 记得是他建议的 ImageNet，是他建议我们做的，但实际上情况正好相反。
148 00:16:15,057 --> 00:16:20,201 说话人 SPEAKER_12：我们做到了，这太令人惊讶了。
我们得到了标准技术的错误率的一半以上，而标准技术已经被非常优秀的研究人员调优了多年
150 00：16：27,623 --> 00：16：34,618 演讲者 SPEAKER_05：我记得当时的标准技术，前一年是带稀疏化的支持向量机。
151 00：16：36,000 --> 00：16：46,121 演讲者 SPEAKER_05：所以你们提交了比赛结果，我想是八月底或九月初。
我记得有一天晚上，我的学生给我打电话或发邮件，因为他们正在运行这个，因为我们持有测试数据。
153 00:16:58,337 --> 00:17:00,261 说话人 SPEAKER_05：我们在服务器端运行。
154 00:17:00,822 --> 00:17:07,653 说话人 SPEAKER_05：目标是处理所有条目，以便我们选择获胜者，然后
155 00:17:08,241 --> 00:17:23,044 说话人 SPEAKER_05：我认为那是在那年的十月初，计算机视觉领域国际会议，ICCV 2012，在意大利佛罗伦萨举行。
156 00:17:23,505 --> 00:17:27,330 说话人 SPEAKER_05：我们已经预订了研讨会，会议的年度研讨会。
157 00:17:27,671 --> 00:17:30,115 说话人 SPEAKER_05: 我们将宣布获胜者。
158 00:17:30,095 --> 00:17:37,723 说话人 SPEAKER_05: 今年是第三届，所以在处理获奖团队之前，我们还有几周的时间。
159 00:17:38,404 --> 00:17:44,192 说话人 SPEAKER_05: 因为这是第三届，坦白说，前两年的结果并没有让我感到兴奋。
160 00:17:44,972 --> 00:17:50,700 说话人 SPEAKER_05: 那时我还是个哺乳期的母亲，所以我决定不去第三届。
161 00:17:50,720 --> 00:17:52,382 说话人 SPEAKER_05：所以我没订票。
162 00:17:53,021 --> 00:17:55,224 说话人 SPEAKER_05：对我来说太远了。
163 00:17:55,744 --> 00:17:57,412 说话人 SPEAKER_05：然后结果就出来了。
164 00:17:57,451 --> 00:18:02,311 说话人 SPEAKER_05：那天晚上的电话或邮件，我真的不记得了。
165 00:18:02,352 --> 00:18:04,741 说话人 SPEAKER_05: 我记得...
166 00:18:05,178 --> 00:18:10,684 说话人 SPEAKER_05: 我记得对自己说，该死，杰夫，现在我得去订一张去意大利的票。
167 00:18:11,205 --> 00:18:23,001 说话人 SPEAKER_05: 因为我知道那是一个非常重要的时刻，特别是在卷积神经网络方面，这是我作为研究生学习的一个经典算法。
168 00:18:23,021 --> 00:18:27,928 说话人 SPEAKER_05: 当然，那时候经济舱只剩下中间的座位了。
169 00:18:27,907 --> 00:18:34,978 说话人 SPEAKER_05：从旧金山飞往佛罗伦萨，中途经停一次。
170 00:18:34,998 --> 00:18:43,530 说话人 SPEAKER_05：所以去佛罗伦萨的旅程非常辛苦，但我想去那里。
171 00:18:43,790 --> 00:18:44,692 说话人 SPEAKER_05: 但是你没有来。
172 00:18:45,472 --> 00:18:49,799 说话人 SPEAKER_12: 没有，我没有。
173 00:18:49,819 --> 00:18:52,784 说话人 SPEAKER_12: 嗯，那是一次艰苦的旅程。
174 00:18:52,804 --> 00:18:55,688 说话人 SPEAKER_05: 但是你知道那将是一个历史时刻吗？
175 00:18:55,836 --> 00:18:59,044 说话人 SPEAKER_12: 是的，我确实做了。
176 00:18:59,605 --> 00:19:00,826 说话人 SPEAKER_05: 但是你派了 Alex。
177 00:19:01,008 --> 00:19:02,029 说话人 SPEAKER_12: Alex，是的。
178 00:19:02,711 --> 00:19:04,394 说话人 SPEAKER_12: 是不是忽略了你的所有建议，对吧？
179 00:19:04,515 --> 00:19:09,605 说话人 SPEAKER_05：因为我感觉这个太酷了，所以连续多次忽略了我的邮件。
180 00:19:09,625 --> 00:19:12,832 说话人 SPEAKER_05：请做这个可视化，这个可视化。
181 00:19:13,292 --> 00:19:13,953 说话人 SPEAKER_05：他忽略了我。
182 00:19:14,394 --> 00:19:16,680 说话人 SPEAKER_05：但是 Yann LeCun 来了。
183 00:19:16,660 --> 00:19:24,369 讲者 SPEAKER_05：因为对于那些参加过这些学术会议研讨会的你们来说，你们往往会预订这些小房间。
184 00:19:24,891 --> 00:19:29,477 讲者 SPEAKER_05：我们预订了一个非常小的房间，可能就是这里的中间部分。
185 00:19:29,958 --> 00:19:35,545 讲者 SPEAKER_05：我记得杨得站在房间的后面，因为人真的很多。
186 00:19:36,365 --> 00:19:42,213 讲者 SPEAKER_05：亚历克斯最终还是出现了，因为我真的很担心他可能不会出现。
187 00:19:42,775 --> 00:19:45,417 说话人 SPEAKER_05: 正如你所预测的，
188 00:19:45,397 --> 00:19:48,821 说话人 SPEAKER_05: 在那次研讨会上，ImageNow 正在被攻击。
189 00:19:50,484 --> 00:19:57,433 说话人 SPEAKER_05: 在那次研讨会上，有人公开抨击，这是一个糟糕的数据集。
190 00:19:58,375 --> 00:19:58,935 说话人 SPEAKER_12: 在房间里。
191 00:19:59,355 --> 00:19:59,916 说话人 SPEAKER_12: 在房间里。
192 00:19:59,936 --> 00:20:00,837 说话人 SPEAKER_12: 在演示过程中。
193 00:20:00,917 --> 00:20:01,218 说话人 SPEAKER_05: 在房间里。
194 00:20:01,278 --> 00:20:04,722 说话人 SPEAKER_12: 但不是 Jitendra，因为 Jitendra 已经同意这算数。
195 00:20:05,902 --> 00:20:08,505 讲者 SPEAKER_05: 我认为 Jitendra 不在房间里，我不记得了。
196 00:20:09,026 --> 00:20:23,664 讲者 SPEAKER_05: 但我记得那对我来说是一个非常奇怪的瞬间，因为我作为一个机器学习研究者，我知道历史正在被创造，然而 ImageNet 正受到攻击。
197 00:20:23,704 --> 00:20:28,330 讲者 SPEAKER_05: 那是一个非常奇怪的、令人兴奋的时刻。
198 00:20:28,592 --> 00:20:33,478 讲者 SPEAKER_05: 然后我不得不坐到中间的座位上，然后回到旧金山，因为第二天早上，
199 00:20:33,593 --> 00:20:37,278 说话人 SPEAKER_10: 所以您提到了一些我稍后想再谈一谈的人。
200 00:20:37,318 --> 00:20:45,874 说话人 SPEAKER_10: 所以，Ilya，他是 OpenAI 的创始人和首席科学家，以及 Yann LeCun，他后来成为了 Facebook（现 Meta）AI 部门的主管。
201 00:20:46,674 --> 00:20:50,201 说话人 SPEAKER_10: 并且还有其他一些有趣的人。
202 00:20:51,041 --> 00:20:58,555 说话人 SPEAKER_10: 在我们继续前进，看看那个繁荣时刻创造了什么之前，让我们先回顾一下。
203 00:20:59,277 --> 00:21:07,730 两位都带着非常具体的目标开始这项工作，这个目标是个人的，我认为也是具有颠覆性的。
204 00:21:07,851 --> 00:21:13,119 而你们必须坚持通过你所描述的那些时刻，贯穿你们整个职业生涯。
205 00:21:13,461 --> 00:21:15,063 你能回到刚才，杰夫，也许，开始讲？
206 00:21:15,503 --> 00:21:18,990 能给我们介绍一下你最初为什么想进入人工智能领域吗？
207 00:21:20,236 --> 00:21:23,182 说话人 SPEAKER_12：我在本科时学习心理学。
208 00:21:23,763 --> 00:21:24,786 说话人 SPEAKER_12：我在那门课上表现不太好。
209 00:21:25,807 --> 00:21:31,299 说话人 SPEAKER_12：我决定，除非他们弄清楚大脑是如何工作的，否则他们永远无法弄清楚心智是如何运作的。
210 00:21:32,461 --> 00:21:35,628 说话人 SPEAKER_12：所以我想要弄清楚大脑是如何工作的。
211 00:21:35,608 --> 00:21:38,972 说话人 SPEAKER_12：我想有一个真正能工作的模型。
212 00:21:38,992 --> 00:21:42,056 说话人 SPEAKER_12：所以你可以把理解大脑想象成建造一座桥梁。
213 00：21：42,115 --> 00：21：54,250 演讲者 SPEAKER_12：有实验数据，你可以从实验数据中学到东西，有些东西可以做你想要的计算，有些东西可以识别物体，它们非常不同。
214 00：21：55,310 --> 00：22：03,759 演讲者 SPEAKER_12：我认为你想在数据和能力之间架起一座桥梁，在完成任务的能力之间架起一座桥梁，
215 00：22：03,824 --> 00：22：12,253 演讲者 SPEAKER_12：我总是认为自己从工作的事情的结尾开始，但试图让它们越来越像大脑，但仍然有效。
216 00：22：12,273 --> 00：22：21,144 演讲者 SPEAKER_12：其他人试图坚持用经验数据证明的事情，并尝试提出可能有效的理论。
217 00:22:22,164 --> 00:22:23,507 说话人 SPEAKER_12：但我们正在努力搭建这座桥梁。
218 00:22:23,787 --> 00:22:25,608 说话人 SPEAKER_12：尝试搭建桥梁的人并不多。
219 00:22:25,650 --> 00:22:30,355 说话人 SPEAKER_12：特里·桑诺夫斯基（Terry Sanofsky）正在从另一端搭建桥梁，所以我们相处得很好。
220 00:22:30,942 --> 00:22:35,433 说话人 SPEAKER_12：许多尝试做计算机视觉的人只是想要一个能工作的东西。
221 00:22:35,453 --> 00:22:37,298 说话人 SPEAKER_12：他们不在乎大脑。
222 00:22:37,318 --> 00:22:45,541 说话人 SPEAKER_12：而且很多人关心大脑，想了解神经元是如何工作的等等，但不想太多地思考计算的实质。
223 00:22:46,078 --> 00:22:55,107 说话人 SPEAKER_12：我仍然认为，我们必须通过让了解数据的人和了解工作原理的人连接起来来搭建这座桥梁。
224 00:22:56,869 --> 00:23:04,116 说话人 SPEAKER_12：所以我的目标一直是制造出能够进行视觉识别的东西，但要以人们的方式进行视觉识别。
225 00:23:05,397 --> 00:23:11,023 说话人 SPEAKER_10：好的，所以我们还会回到那个话题，因为我想要问你关于最近的发展以及你认为这些发展如何与大脑相关。
226 00:23:11,865 --> 00:23:14,247 说话人 SPEAKER_10：Feifei，你还有 Jeff，只是
227 00:23:14,917 --> 00:23:16,900 说话人 SPEAKER_10：这为你的起点建立了一个框架。
228 00:23:17,641 --> 00:23:22,406 说话人 SPEAKER_10：从英国到美国再到加拿大，到 80 年代中后期，你在 87 年来到加拿大。
229 00:23:22,446 --> 00:23:32,559 说话人 SPEAKER_10：沿着那条路线，对神经网络以及你采取的方法的兴趣和资金支持是这样的，但我认为主要是这样的。
230 00:23:34,082 --> 00:23:34,522 说话人 SPEAKER_10：上下波动。
231 00:23:36,484 --> 00:23:39,568 说话人 SPEAKER_10：Fei-Fei，你的人生起点非常不同。
232 00:23:40,309 --> 00:23:42,833 说话人 SPEAKER_10：你能给我们介绍一下你是如何接触到人工智能的吗？
233 00:23:43,623 --> 00:24:07,133 Speaker SPEAKER_05: 我在中国开始了我的生活，15 岁时，我和父母搬到了新泽西州的波斯波利斯，所以我成为了新移民，我开始的地方首先是英语课，然后是第二语言课，因为我不会说这种语言，你知道的，就在那里工作，
234 00:24:07,113 --> 00:24:10,597 Speaker SPEAKER_05: 洗衣店和餐馆等等。
235 00:24:11,157 --> 00:24:14,281 Speaker SPEAKER_05: 但是我对物理学有着浓厚的兴趣。
236 00:24:14,702 --> 00:24:16,625 Speaker SPEAKER_05: 我不知道这是怎么进入我的大脑的。
237 00:24:17,405 --> 00:24:23,193 讲者 SPEAKER_05：我想去普林斯顿，因为我只知道爱因斯坦在那里。
238 00:24:24,174 --> 00:24:26,298 讲者 SPEAKER_05：我考入了普林斯顿。
239 00:24:26,317 --> 00:24:29,442 讲者 SPEAKER_05：我考入普林斯顿的时候他已经不在那里了。
240 00:24:29,422 --> 00:24:32,125 讲者 SPEAKER_05：你并不那么老。
241 00:24:32,365 --> 00:24:33,807 说话人 SPEAKER_05: 但有一座他的雕像。
242 00:24:34,468 --> 00:24:46,561 说话人 SPEAKER_05: 在物理学中，除了所有的数学和那些之外，我学到的一点是敢于提出最疯狂的问题。
243 00:24:47,142 --> 00:24:57,453 说话人 SPEAKER_05: 像原子世界中最小的粒子，或者空间的边界，宇宙的起源，
244 00:24:57,433 --> 00:25:07,154 说话人 SPEAKER_05: 在这个过程中，我发现作为三年级生的罗杰·彭罗斯和那些书。
245 00:25:08,857 --> 00:25:12,605 说话人 SPEAKER_05: 你可能有自己的看法，但至少我读过那些书。
246 00:25:13,788 --> 00:25:15,392 说话人 SPEAKER_12: 可能你最好没读过。
247 00:25:18,140 --> 00:25:21,624 说话人 SPEAKER_05: 至少他让我对大脑产生了兴趣。
248 00:25:21,644 --> 00:25:29,775 说话人 SPEAKER_05: 到我毕业的时候，我想作为一个科学家提出最勇敢的问题。
249 00:25:30,194 --> 00:25:38,645 讲者 SPEAKER_05：对我来说，我这一代人最令人着迷、最勇敢的问题，那是 2000 年，就是智能。
250 00:25:38,625 --> 00:25:48,684 讲者 SPEAKER_05：所以我去了加州理工学院，基本上是与克里斯托夫·科赫一起获得了神经科学的双博士，与皮埃特罗·帕罗纳一起获得了人工智能的博士。
251 00:25:49,046 --> 00:25:53,934 讲者 SPEAKER_05：所以我重复，杰夫，你说的关于桥的事情，因为
252 00:25:53,914 --> 00:26:13,508 讲者 SPEAKER_05：因为这五年让我有机会从事计算神经科学研究，研究心灵如何运作，同时也在计算方面工作，试图构建一个可以模仿人脑的计算机程序。
253 00:26:13,567 --> 00:26:15,131 说话人 SPEAKER_05: 这就是我的旅程。
254 00:26:15,391 --> 00:26:16,393 说话人 SPEAKER_05: 它始于物理学。
255 00:26:16,713 --> 00:26:19,939 说话人 SPEAKER_10: 好的，所以你们的旅程在 2012 年的 ImageNet 上交汇。
256 00:26:20,019 --> 00:26:22,826 说话人 SPEAKER_05: 顺便说一下，我是在读研究生的时候遇到杰夫的。
257 00:26:22,846 --> 00:26:23,488 说话人 SPEAKER_12: 好的，我记得。
258 00:26:23,508 --> 00:26:25,112 说话人 SPEAKER_12: 我以前经常去皮埃特罗的实验室。
259 00:26:25,231 --> 00:26:26,114 说话人 SPEAKER_05: 是的。
260 00:26:26,134 --> 00:26:29,221 说话人 SPEAKER_12: 事实上，他实际上在我 70 岁的时候，在加州理工学院给我提供了一个工作机会。
261 00:26:31,086 --> 00:26:32,368 说话人 SPEAKER_05: 你本可以成为我的导师。
262 00:26:33,050 --> 00:26:34,914 说话人 SPEAKER_12: 不，在我 70 岁的时候不行。
263 00:26:38,644 --> 00:26:38,703 未知说话人: 好的。
264 00:26:39,140 --> 00:26:41,563 说话人 SPEAKER_10: 好的，所以我们是在 ImageNet 上相遇的。
265 00:26:41,583 --> 00:26:46,068 讲者 SPEAKER_10：对于这个领域的所有人来说，我们都知道 ImageNet 是这样一个大爆炸时刻。
266 00:26:46,548 --> 00:26:57,022 讲者 SPEAKER_10：在那之后，首先是大型科技公司介入，基本上开始收购你的学生和你，将他们引入公司。
267 00:26:57,042 --> 00:26:59,826 讲者 SPEAKER_10：我认为他们是第一批意识到这种潜力的人。
268 00:27:01,127 --> 00:27:02,549 讲者 SPEAKER_10：我想就此谈谈。
269 00:27:02,789 --> 00:27:05,973 说话人 SPEAKER_10: 但我有点快进，我觉得
270 00:27:05,953 --> 00:27:12,721 说话人 SPEAKER_10: 只有现在，自从 ChatGPT 以来，世界上的其他人才开始赶上 AI 的力量，因为终于可以与之互动了。
271 00:27:12,741 --> 00:27:16,205 说话人 SPEAKER_10: 你可以在会议室体验它，他们可以谈论它，然后回家。
272 00:27:16,267 --> 00:27:22,835 说话人 SPEAKER_10: 然后那个 10 岁的孩子刚刚用 ChatGPT 为五年级写了一篇关于恐龙的作文。
273 00:27:22,855 --> 00:27:27,921 说话人 SPEAKER_10：所以这种每个人都能玩起来的超越体验，我认为，是一个巨大的转变。
274 00:27:27,941 --> 00:27:31,404 说话人 SPEAKER_10：但在那之后的 10 年里，
275 00:27:31,974 --> 00:27:40,926 说话人 SPEAKER_10：大科技公司内部的人工智能经历了爆炸式增长，其他人并没有真正注意到正在发生的事情。
276 00:27:41,468 --> 00:27:47,817 说话人 SPEAKER_10：你能和我们分享一下你的亲身经历吗？因为你是在 ImageNet 之后从零开始经历的。
277 00:27:49,578 --> 00:27:57,269 说话人 SPEAKER_12：对于我们来说，很难进入别人的思维框架，因为他们没有意识到发生了什么，而我们却意识到了。
278 00:27:58,570 --> 00:27:59,231 说话人 SPEAKER_12：所以，
279 00:28:00,258 --> 00:28:06,284 说话人 SPEAKER_12：许多你本以为会处于最前沿的大学在对此的接受上非常缓慢。
280 00:28:06,364 --> 00:28:08,767 说话人 SPEAKER_12：比如麻省理工学院和加州大学伯克利分校。
281 00:28:09,207 --> 00:28:19,017 讲者 SPEAKER_12：我记得在 2013 年左右，我去伯克利参加了一个讲座，当时人工智能在计算机视觉领域已经非常成功。
282 00:28:19,657 --> 00:28:25,542 讲者 SPEAKER_12：讲座结束后，一个研究生走到我面前，他说，我已经在这里待了四年，这是我第一次听到关于神经网络的内容。
283 00:28:25,563 --> 00:28:26,403 讲者 SPEAKER_12：它们真的很有趣。
284 00:28:27,204 --> 00:28:29,067 讲者 SPEAKER_05：他应该去斯坦福。
285 00:28:29,366 --> 00:28:32,392 说话人 SPEAKER_12: 很可能，很可能。
286 00:28:33,192 --> 00:28:34,694 说话人 SPEAKER_12: 但和麻省理工一样。
287 00:28:34,976 --> 00:28:45,932 说话人 SPEAKER_12: 他们坚决反对使用神经网络，ImageNet 事件开始让他们动摇，现在他们是神经网络的大力支持者。
288 00:28:46,433 --> 00:28:55,729 说话人 SPEAKER_12: 但现在很难想象，但大约在 2010 或 2011 年，计算机视觉领域的人
289 00:28:56,468 --> 00:29:00,011 讲者 SPEAKER_12：非常好，计算机视觉领域的人士，当时非常坚决地反对神经网络。
290 00:29:00,471 --> 00:29:07,200 讲者 SPEAKER_12：他们反对得如此激烈，例如，有一家主要期刊，IEEE 模式识别... PAMI。
291 00:29:07,579 --> 00:29:13,066 讲者 SPEAKER_12：PAMI，曾经有一项政策，不允许审阅关于神经网络的论文。
292 00:29:13,366 --> 00:29:13,946 讲者 SPEAKER_12：直接退回。
293 00:29:13,967 --> 00:29:14,528 说话人 SPEAKER_12: 别评判他们。
294 00:29:14,548 --> 00:29:15,148 说话人 SPEAKER_12: 浪费时间。
295 00:29:15,169 --> 00:29:16,911 说话人 SPEAKER_12: 这不应该在 PAMI 上发表。
296 00:29:17,912 --> 00:29:22,797 说话人 SPEAKER_12: Yann LeCun 向一个他曾经有神经网络的会议提交了一篇论文。
297 00:29:22,861 --> 00:29:30,229 讲者 SPEAKER_12：这比最先进的方法在识别、行人分割方面做得更好，但被拒绝了。
298 00:29:30,910 --> 00:29:50,251 讲者 SPEAKER_12：其中一个原因是，一位审稿人说，这让我们对视觉一无所知，因为他们对计算机视觉的工作方式有这种看法，即你研究视觉问题的本质，制定一个解决它的算法，然后找出如何实现该算法，最后发表论文。
299 00:29:50,484 --> 00:29:54,532 讲者 SPEAKER_05：我必须捍卫我的领域，不是所有人。
300 00:29:55,233 --> 00:30:00,403 讲者 SPEAKER_12：所以有很多人……但他们大多数都坚决反对神经网络。
301 00:30:00,865 --> 00:30:07,718 说话人 SPEAKER_12: 然后在 ImageNet 竞赛之后发生了一件令人瞩目的事情，那就是他们大约在一年内都发生了变化。
302 00:30:07,698 --> 00:30:14,007 说话人 SPEAKER_12: 所有一直是最严厉的神经网络批评者开始做神经网络，这让我们感到非常遗憾。
303 00:30:14,027 --> 00:30:15,108 说话人 SPEAKER_12: 其中一些人做得比我们还好。
304 00:30:16,730 --> 00:30:20,797 说话人 SPEAKER_12: 例如，牛津的 Zisserman 很快就做出了一个更好的神经网络。
305 00:30:22,359 --> 00:30:26,986 讲者 SPEAKER_12: 但他们的行为就像科学家应该表现的那样，他们坚信这些资料都是垃圾。
306 00:30:27,666 --> 00:30:32,253 讲者 SPEAKER_12: 由于 ImageNet，我们最终证明它不是，然后他们改变了。
307 00:30:32,233 --> 00:30:34,217 讲者 SPEAKER_12: 这真是太令人欣慰了。
308 00:30:34,237 --> 00:30:42,728 讲者 SPEAKER_10: 就此继续，所以你试图展示的是，你试图使用神经网络准确地对这 1500 万张图片进行标注。
309 00:30:43,249 --> 00:30:46,292 说话人 SPEAKER_10: 你已经在后台将它们全部标注好了，这样你就可以进行测量了。
310 00:30:47,114 --> 00:30:51,839 说话人 SPEAKER_10: 你做的时候，错误率从去年的 26%下降到了大约 16%？
311 00:30:52,300 --> 00:30:52,942 说话人 SPEAKER_12: 我想是 15.3。
312 00:30:53,041 --> 00:30:53,162 说话人 SPEAKER_12: 好的。
313 00:30:55,003 --> 00:30:56,767 说话人 SPEAKER_10: 然后它随后一直在下降。
314 00:30:56,787 --> 00:30:57,667 说话人 SPEAKER_12: 15.32。
315 00:30:59,098 --> 00:31:02,124 说话人 SPEAKER_12: 哪个随机化？
316 00:31:02,144 --> 00:31:04,626 说话人 SPEAKER_10: Jeff 不会忘记事情。
317 00:31:05,729 --> 00:31:11,778 说话人 SPEAKER_10: 然后随后的几年，人们使用了更强大的神经网络，并且持续下降，直到超过。
318 00:31:11,798 --> 00:31:19,589 说话人 SPEAKER_05: 2015 年，有一个非常聪明的加拿大本科生加入了我的实验室。
319 00:31:19,630 --> 00:31:21,192 说话人 SPEAKER_05: 他的名字叫安德烈·卡帕西。
320 00:31:22,074 --> 00:31:28,363 说话人 SPEAKER_05: 他在一个夏天感到无聊，说我想衡量人类的表现。
321 00:31:28,343 --> 00:31:30,046 说话人 SPEAKER_05：所以你应该去读他的博客。
322 00:31:30,586 --> 00:31:37,378 说话人 SPEAKER_05：他做了所有这些人类进行的图像测试。
323 00:31:37,519 --> 00:31:41,946 说话人 SPEAKER_05：我想他得用披萨贿赂他们，我在实验室的学生们。
324 00:31:41,987 --> 00:31:47,836 说话人 SPEAKER_05：他们的准确率达到了大约 5%。
325 00:31:47,817 --> 00:31:48,657 说话人 SPEAKER_05: 是 5 还是 3.5？
326 00:31:48,678 --> 00:31:48,758 说话人 SPEAKER_05: 3.
327 00:31:48,778 --> 00:31:51,521 说话人 SPEAKER_05: 我想应该是 3.5。
328 00:31:51,842 --> 00:31:54,085 说话人 SPEAKER_10: 所以人类大约 3%的时间会犯错误。
329 00:31:54,125 --> 00:31:54,525 说话人 SPEAKER_05: 对。
330 00:31:55,286 --> 00:31:59,372 说话人 SPEAKER_05: 然后我想是 2016 年，我想 ResNet 超过了它。
331 00:32:00,192 --> 00:32:00,953 说话人 SPEAKER_05: 是 ResNet。
332 00:32:02,275 --> 00:32:06,240 说话人 SPEAKER_05: 那一年的获胜算法超过了人类的表现。
333 00:32:06,740 --> 00:32:13,849 说话人 SPEAKER_10: 最后，你不得不退役比赛，因为它比人类好得多，我们不得不退役，因为我们资金耗尽了。
334 00:32:14,770 --> 00:32:14,871 说话人 SPEAKER_10: 好的。
335 00:32:15,559 --> 00:32:17,983 说话人 SPEAKER_10: 这是一个不同的原因，一个不好的原因。
336 00:32:18,003 --> 00:32:18,905 说话人 SPEAKER_05: 我们还是资金耗尽了。
337 00:32:19,226 --> 00:32:28,821 讲者 SPEAKER_12：顺便提一下，那位学生最初在多伦多大学开始他的学业，后来他去了你的实验室，然后又去了特斯拉担任研究部门负责人。
338 00:32:29,603 --> 00:32:31,846 讲者 SPEAKER_05：好的，首先，他来
339 00:32:31,826 --> 00:32:34,171 讲者 SPEAKER_05：斯坦福大学读博士。
340 00:32:34,912 --> 00:32:41,723 讲者 SPEAKER_05：昨晚我们聊天时，实际上中间有一个突破性的论文答辩。
341 00:32:42,144 --> 00:32:45,550 说话者 SPEAKER_05: 然后他成为了 OpenAI 的创始团队的一员。
342 00:32:45,991 --> 00:32:46,893 说话者 SPEAKER_12: 但然后他去特斯拉了，对吧？
343 00:32:46,913 --> 00:32:48,154 说话者 SPEAKER_05: 然后他去特斯拉了。
344 00:32:48,276 --> 00:32:52,643 说话者 SPEAKER_12: 但然后他改变了主意。
345 00:32:52,762 --> 00:32:56,028 说话人 SPEAKER_05：但我确实想回答你关于那 10 年的问题。
346 00:32:56,481 --> 00:32:58,744 说话人 SPEAKER_10：嗯，在这过程中有几个发展。
347 00:32:58,925 --> 00:32:59,185 说话人 SPEAKER_05：没错。
348 00:32:59,266 --> 00:32:59,946 说话人 SPEAKER_10：Transformer。
349 00:33:00,407 --> 00:33:00,647 说话人 SPEAKER_05: 对。
350 00:33:01,008 --> 00:33:05,434 说话人 SPEAKER_10: 所以 Transformer 论文已经写成，这项研究是在谷歌内部的一篇论文中完成的。
351 00:33:05,955 --> 00:33:18,634 说话人 SPEAKER_10: 另一位加拿大人是合著者之一，Aiden Gomez，现在是 Cohere 的 CEO 和联合创始人，我认为他当时在谷歌大脑做实习生时，年仅 20 岁，就参与了这篇论文的合著。
352 00:33:19,875 --> 00:33:23,721 说话人 SPEAKER_10: 所以加拿大人在这些突破性进展中有着传统。
353 00:33:24,442 --> 00:33:27,759 说话人 SPEAKER_10: 杰夫，你在撰写论文时在谷歌工作。
354 00:33:27,980 --> 00:33:32,782 说话人 SPEAKER_10: 谷歌内部是否意识到这有多么重要？
355 00:33:32,948 --> 00:33:34,190 说话人 SPEAKER_12: 我认为没有。
356 00:33:34,269 --> 00:33:38,877 说话人 SPEAKER_12: 可能作者知道，但我过了几年才意识到这有多么重要。
357 00:33:39,298 --> 00:33:43,226 说话人 SPEAKER_12：在谷歌，人们直到 BERT 出现之前都没有意识到它的重要性。
358 00:33:43,246 --> 00:33:51,601 说话人 SPEAKER_12：所以 BERT 使用了 Transformer，BERT 在许多自然语言处理基准测试中针对许多不同的任务都变得更好。
359 00:33:52,623 --> 00:33:55,647 说话人 SPEAKER_12：那时人们才意识到 Transformer 的特殊之处。
360 00:33:56,067 --> 00:33:59,592 说话人 SPEAKER_05：所以 2017 年，Transformer 论文发表了。
361 00:33:59,952 --> 00:34:04,199 说话人 SPEAKER_05：我也加入了谷歌，我想我们实际上是在我的第一周相遇的。
362 00:34:05,099 --> 00:34:10,367 说话人 SPEAKER_05：我认为 2017 年和 2018 年的大部分时间都在做神经架构搜索。
363 00:34:11,148 --> 00:34:13,552 说话人 SPEAKER_05：我认为那是谷歌的赌注。
364 00:34:14,253 --> 00:34:16,858 说话人 SPEAKER_05：并且使用了大量的 GPU。
365 00:34:17,478 --> 00:34:20,623 说话人 SPEAKER_05: 所以这是一个不同的赌注。
366 00:34:20,755 --> 00:34:24,621 说话人 SPEAKER_12: 所以，为了解释一下，神经架构搜索基本上就是指这个。
367 00:34:26,443 --> 00:34:33,835 说话人 SPEAKER_12: 你需要很多 GPU，然后尝试很多不同的架构，看看哪个效果最好，并且自动化这个过程。
368 00:34:34,195 --> 00:34:37,260 说话人 SPEAKER_12: 这基本上是神经网络架构的自动化进化。
369 00:34:37,280 --> 00:34:38,742 说话人 SPEAKER_05: 这就像超参数调整。
370 00:34:38,762 --> 00:34:39,704 说话人 SPEAKER_12: 是的。
371 00:34:39,724 --> 00:34:44,572 说话人 SPEAKER_12: 这带来了一些相当大的改进，但远不如 Transformer。
372 00:34:45,434 --> 00:34:48,619 说话人 SPEAKER_12: Transformer 对自然语言处理是一个巨大的改进。
373 00:34:48,920 --> 00:34:52,788 说话人 SPEAKER_05：神经网络的架构搜索主要是在 ImageNet 上进行的。
374 00:34:52,807 --> 00:34:55,032 说话人 SPEAKER_10：所以我会告诉你们我们的 Transformer 经验。
375 00:34:55,072 --> 00:34:57,076 说话人 SPEAKER_10：当时我们公司正在进行第六层。
376 00:34:57,096 --> 00:35:01,865 说话人 SPEAKER_10：我想我们看到了论文的预读。
我们正处于筹款和一系列收购提议的过程中，我们阅读了这篇论文，我的意思是，不仅仅是我自己，还有我的合作伙伴 Tommy，他曾经和您学习过，还有 Max Volkovs，他是从实验室出来的。
378 00：35：15,454 --> 00：35：19,059 演讲者 SPEAKER_10：我们认为这是神经网络的下一次迭代。
我们应该出售公司，成立风险投资基金，并投资那些将要使用 Transformer 的公司。
所以我们认为，要被谷歌以外的用户接受，可能需要五年的时间。
381 00:35:28,659 --> 00:35:34,175 说话人 SPEAKER_10: 从那一刻起，全世界所有的软件都将用这项技术来替换或嵌入，这将需要 10 年时间。
382 00:35:35,257 --> 00:35:39,530 说话人 SPEAKER_10: 我们在 ChatGPT 发布前五年零两周做出了那个决定。
383 00:35:39,967 --> 00:35:50,320 说话人 SPEAKER_10: 所以我很高兴看到我们预测得很好，但我必须给我的联合创始人点赞，我以为我理解了那篇论文，但他们能够完全解释它。
384 00:35:50,380 --> 00:35:51,721 说话人 SPEAKER_12: 我应该纠正你一点。
385 00:35:52,121 --> 00:35:53,824 说话人 SPEAKER_12：我认为汤米从来没有和我一起学过。
386 00:35:53,864 --> 00:36:00,952 说话人 SPEAKER_12：他想要来和我一起学习，但我部门的同事告诉他，如果他来和我一起工作，那他的职业生涯就结束了，他应该去做其他的事情。
387 00:36:01,893 --> 00:36:07,239 说话人 SPEAKER_10：所以他参加了课程，这位是我的合作伙伴，他在 20 世纪 90 年代在多伦多大学读硕士，他想去杰夫那里学习，研究神经网络，而他现在的妻子父亲的工程学教授说，不要做那个，神经网络是死胡同。
388 00:36:07,219 --> 00:36:21,885 说话人 SPEAKER_10：90 年代，在多伦多大学读硕士，他想去杰夫那里学习，研究神经网络，而他现在的妻子父亲的工程学教授说，不要做那个，神经网络是死胡同。
389 00:36:21,905 --> 00:36:27,675 讲者 SPEAKER_10：所以，他参加了课程，但用加密货币写了论文。
390 00:36:29,865 --> 00:36:35,490 讲者 SPEAKER_10：那么，你还要继续谈论那 10 年吗？
391 00:36:35,530 --> 00:36:37,400 讲者 SPEAKER_05：因为我认为有一些重要的事情。
392 00:36:37,721 --> 00:36:39,268 讲者 SPEAKER_10：是的，那么请继续。
393 00:36:39,367 --> 00:36:43,070 说话人 SPEAKER_05：我认为世界上忽视了一些重要的事情。
394 00:36:43,311 --> 00:36:56,684 说话人 SPEAKER_05：在这 10 年间，从 ImageNet 到 AlexNet 再到 ChatGPT，大多数人将其视为技术 10 年，或者我们将其视为技术 10 年。
395 00:36:57,244 --> 00:36:59,608 说话人 SPEAKER_05：在大型科技公司中，有一些事情正在酝酿。
396 00:36:59,748 --> 00:37:04,472 说话人 SPEAKER_05：我的意思是，它需要序列到序列、Transformer，但有一些事情正在酝酿。
397 00:37:05,054 --> 00:37:07,416 说话人 SPEAKER_05：但我确实认为，
398 00:37:07,396 --> 00:37:15,088 说话人 SPEAKER_05：对我来说和整个世界来说，这也是从技术到社会的转变。
399 00:37:15,568 --> 00:37:33,335 说话人 SPEAKER_05：实际上我认为，在这十年中，我从一名科学家成长为一个人文主义者，因为在那篇关于 Transformer 论文的两年间加入谷歌，我开始看到这项技术的社会影响。
400 00:37:33,315 --> 00:37:41,719 说话人 SPEAKER_05：这是在 AlphaGo 之后，很快我们就迎来了 AlphaFold 的时刻。
401 00:37:42,280 --> 00:37:45,550 说话人 SPEAKER_05：偏见正在悄悄蔓延。
402 00:37:46,012 --> 00:37:48,338 说话人 SPEAKER_05：存在隐私问题。
403 00:37:48,318 --> 00:37:53,465 说话人 SPEAKER_05：然后我们开始看到虚假信息和错误信息的苗头。
404 00:37:54,085 --> 00:38:02,878 说话人 SPEAKER_05：然后我们开始看到关于工作的讨论仅限于小圈子，而不是在公众讨论中。
405 00:38:03,639 --> 00:38:06,202 说话人 SPEAKER_05：就在我个人感到焦虑的时候。
406 00:38:06,402 --> 00:38:10,527 说话人 SPEAKER_05：你知道的，2018 年，
407 00:38:11,706 --> 00:38:14,992 说话人 SPEAKER_05：哦，就在剑桥分析公司事件之后。
408 00:38:15,532 --> 00:38:23,545 说话人 SPEAKER_05：所以，技术的巨大影响，不是 AI 本身，而是驱动选举的算法技术。
409 00:38:24,106 --> 00:38:30,277 说话人 SPEAKER_05：那时我必须做出一个个人决定，是留在谷歌还是回到斯坦福。
410 00:38:30,396 --> 00:38:33,461 说话人 SPEAKER_05：我知道我回到斯坦福的唯一原因
411 00:38:33,442 --> 00:38:40,952 说话人 SPEAKER_05：就是成立这个以人为中心的 AI 研究所，真正地、真正地理解这项技术的以人为本的一面。
412 00:38:41,012 --> 00:38:54,753 说话人 SPEAKER_05：所以我认为这是非常重要的 10 年，尽管在公众眼中它可能并不显眼，但这项技术已经开始真正地渗透到我们生活的方方面面。
413 00:38:54,992 --> 00:39:03,144 说话人 SPEAKER_05: 当然，2022 年，在日光下展示得如此深刻。
414 00:39:03,344 --> 00:39:16,711 说话人 SPEAKER_10: 在那个时期发生的事情还有一个有趣的脚注，那就是最终你、伊利亚和亚历克斯加入了谷歌，但在那之前，有一家加拿大公司有机会获得这项技术的访问权限。
415 00:39:17,164 --> 00:39:21,172 说话人 SPEAKER_10: 我听说过这个故事，但我不认为它曾经被公开分享过。
416 00:39:21,873 --> 00:39:24,456 说话人 SPEAKER_10: 我听说过这个故事，但我不认为它曾经被公开分享过。
417 00:39:24,496 --> 00:39:26,159 说话人 SPEAKER_10: 你是不是想分享那个故事一会儿？
418 00:39:26,681 --> 00:39:39,661 说话人 SPEAKER_12: 好的，所以我们在 2009 年开发了我们用于 ImageNet 的技术，是为了进行语音识别，进行语音识别的声学建模部分。
419 00:39:39,641 --> 00:39:47,572 说话人 SPEAKER_12: 所以你可以把声波变成一个叫做频谱图的东西，它告诉你每个时间点每个频率的能量有多少。
420 00:39:48,034 --> 00:39:49,675 说话人 SPEAKER_12: 所以你可能已经习惯了看到频谱图。
421 00:39:50,677 --> 00:40:00,811 说话者 SPEAKER_12：你想做的是观察一个频谱图，并猜测频谱图中间帧表达了哪个音素的哪一部分。
422 00:40:00,831 --> 00:40:09,565 说话者 SPEAKER_12：有两个学生，乔治·达尔和另一个我与杰拉尔德·彭恩分享的学生。
423 00:40:11,248 --> 00:40:12,369 说话者 SPEAKER_12：阿布杜。
424 00:40:13,050 --> 00:40:14,452 说话者 SPEAKER_12：他有一个更长的名字。
425 00:40:14,793 --> 00:40:18,338 说话人 SPEAKER_12：我们都叫他阿布，他是一位语音专家。
426 00:40:18,358 --> 00:40:19,619 说话人 SPEAKER_12：乔治是一位学习专家。
427 00:40:20,221 --> 00:40:30,293 说话人 SPEAKER_12：2009 年夏天，他们做了一个模型，比 30 年的语音研究所能产生的都要好，而且是大团队在语音研究上取得的成果。
428 00:40:30,713 --> 00:40:34,739 说话人 SPEAKER_12：这个模型稍微好一些，没有像 ImageNet 差距那么大，但确实更好了。
429 00:40:36,081 --> 00:40:39,045 说话人 SPEAKER_12: 然后，这个模型被移植到了 IBM 和微软，乔治去了微软，阿布去了 IBM，那些大型语音团队开始使用神经网络。
430 00:40:40,375 --> 00:40:52,335 说话人 SPEAKER_12: 我还有第三个学生，他一直在做其他事情，名叫 Navdeep，Navdeep Jaitley，然后
431 00:40:53,378 --> 00:40:59,329 说话人 SPEAKER_12: 他想将这项语音技术带到一家大公司，但由于复杂的签证问题，他希望留在加拿大，所以我们联系了 BlackBerry，RIM，我们说，我们有一种新的语音识别方法，它比现有技术更有效，我们希望有一个学生在暑假期间去你们那里展示如何使用它，然后你们手机上的语音识别技术就可以是最好的。
432 00:41:00,034 --> 00:41:27,577 说话人 SPEAKER_12: 他想将这项语音技术带到一家大公司，但由于复杂的签证问题，他希望留在加拿大，所以我们联系了 BlackBerry，RIM，我们说，我们有一种新的语音识别方法，它比现有技术更有效，我们希望有一个学生在暑假期间去你们那里展示如何使用它，然后你们手机上的语音识别技术就可以是最好的。
433 00:41:28,250 --> 00:41:34,661 说话人 SPEAKER_12: 经过一些讨论后，我们表示不感兴趣，就像我们说的，我们不感兴趣。
434 00:41:35,742 --> 00:41:39,148 说话人 SPEAKER_12: 因此，我们试图将其交给加拿大工业的努力失败了。
435 00:41:40,170 --> 00:41:45,498 说话人 SPEAKER_12: 然后，Navdeep 把它带到了谷歌。
436 00:41:46,222 --> 00:41:48,824 说话人 SPEAKER_12: 谷歌是第一个将其转化为产品的。
437 00:41:48,925 --> 00:42:05,385 说话人 SPEAKER_12：所以在 2012 年，大约在我们赢得 ImageNet 竞赛的同时，George 和 Abdo 的语音识别声学模型也在进行，那时有很多工作使其成为一个好的产品，使其具有低延迟等等。
438 00:42:06,166 --> 00:42:07,909 说话人 SPEAKER_12：这个模型出现在 Android 上。
439 00:42:07,889 --> 00:42:12,554 说话人 SPEAKER_12：然后有一个时刻，Android 在语音识别方面突然变得和 Siri 一样好。
440 00:42:12,576 --> 00:42:13,617 说话人 SPEAKER_12：而那正是神经网络。
441 00:42:14,157 --> 00:42:18,844 说话人 SPEAKER_12：我认为对于大公司的高层来说，这又是另一个因素。
442 00:42:19,264 --> 00:42:26,715 说话人 SPEAKER_12：他们看到了它在视觉上的戏剧性结果，同时也看到了它已经在语音识别产品中推出，并且在那里也工作得非常好。
443 00:42:27,476 --> 00:42:31,601 说话人 SPEAKER_12：所以我认为它既能做语音，又能做视觉，显然它什么都能做。
444 00:42:33,590 --> 00:42:35,054 说话人 SPEAKER_12：我们不再谈论黑莓。
445 00:42:35,094 --> 00:42:44,757 说话人 SPEAKER_12: 真是遗憾，加拿大工业没有，你知道，我觉得如果那样的话，我们可能还能有黑莓。
446 00:42:44,777 --> 00:42:46,501 说话人 SPEAKER_10: 好吧，我们就说到这里吧。
447 00:42:47,782 --> 00:42:57,373 说话人 SPEAKER_10: 我以前听说过这个故事，但我觉得让全世界知道一些幕后的情况很重要，为什么这项技术即使免费提供也没有留在加拿大。
448 00:42:59,056 --> 00:43:00,818 说话人 SPEAKER_10: 好的，那么我们继续前进。
449 00:43:01,438 --> 00:43:03,722 说话人 SPEAKER_10：我们现在有了后变换器。
450 00:43:04,322 --> 00:43:07,507 说话人 SPEAKER_10：谷歌开始使用它，并以多种方式开发它。
451 00:43:08,228 --> 00:43:16,759 说话人 SPEAKER_10：OpenAI，你的前学生伊利亚离开谷歌后，与埃隆·马斯克、山姆·奥特曼、格雷格·布鲁克曼等人共同创立了 OpenAI。
452 00:43:17,498 --> 00:43:23,326 说话人 SPEAKER_10：伊利亚是首席科学家，安德烈，你的学生，作为联合创始人。
453 00：43：23,525 --> 00：43：34,599 演讲者 SPEAKER_10：他们正在一起工作，一个非常小的团队，基本上是，嗯，最初的想法是我们将构建 AGI 和通用人工智能。
最终，transformer 论文发布，他们开始采用，在某个时刻，transformers，并开始做出非凡的
他们内部获得收益，实际上并没有公开分享，在语言理解和许多其他方面他们能做的事情。
456 00：43：51,851 --> 00：43：54,436 演讲者 SPEAKER_10：他们在机器人技术方面的努力已经发展出来了。
457 00:43:54,536 --> 00:43:58,822 说话人 SPEAKER_10: 彼得·阿贝尔最终成立了 Covariant 公司，我们随后对其进行了投资，以及其他一些事情。
458 00:43:59,664 --> 00:44:03,250 说话人 SPEAKER_10: 所以语言部分一直在进步，一直在进步。
459 00:44:03,905 --> 00:44:07,170 说话人 SPEAKER_10: 外部人员开始开放，我并不真正了解具体发生了什么。
460 00:44:07,210 --> 00:44:11,315 说话人 SPEAKER_10: 然后 Chat GPT 在去年 11 月 30 日发布。
461 00:44:11,657 --> 00:44:13,119 说话人 SPEAKER_10: 约莫 10 个月前。
462 00:44:13,440 --> 00:44:18,086 说话人 SPEAKER_05: GPT2 引起了我们中的一些人的注意。
463 00:44:18,507 --> 00:44:20,309 说话人 SPEAKER_05: 我觉得实际上，
464 00:44:20,289 --> 00:44:34,704 说话人 SPEAKER_05: 我觉得在 GPT-2 发布的时候，我的同事李飞飞，斯坦福大学的 NLP 教授，我记得他来找我，说，飞飞，我有一个完全不同的
465 00:44:35,831 --> 00:44:39,275 说话人 SPEAKER_05: 认识到这项技术的重要性。
466 00:44:39,376 --> 00:44:47,324 说话人 SPEAKER_05: 所以归功于 Percy，他立即要求 HAI 建立一个研究中心来研究这项技术。
467 00:44:47,545 --> 00:44:53,192 说话人 SPEAKER_05: 我不知道这在多伦多是否具有争议。
468 00:44:53,211 --> 00:44:58,177 说话人 SPEAKER_05: 斯坦福大学是提出“基础模型”这一术语的大学。
469 00:44:58,157 --> 00:45:06,869 有些人称它为LLM，大型语言模型，但超越语言，我们称之为基础模型。
470 00:45:06,929 --> 00:45:18,463 我们在 3.5 发布之前，我想，在 Chad GPT 之前，就创建了基础模型研究中心。
471 00:45:18,503 --> 00:45:22,106 请为那些不熟悉的人描述一下什么是基础模型。
472 00:45:22,592 --> 00:45:24,014 这实际上是一个很好的问题。
473 00:45:24,135 --> 00:45:28,079 说话人 SPEAKER_05：基础模型，有些人觉得它必须包含 Transformer。
474 00:45:28,599 --> 00:45:29,960 说话人 SPEAKER_05：我不知道你是否这样觉得。
475 00:45:29,981 --> 00:45:32,523 说话人 SPEAKER_12：不，它只需要在大量数据上训练的一个非常大的模型。
476 00:45:32,543 --> 00:45:34,987 说话人 SPEAKER_05：非常大，用大量数据预训练的。
477 00:45:35,447 --> 00:45:42,215 说话人 SPEAKER_05：我认为一个基础模型最重要的特性是多任务的泛化能力。
478 00:45:42,715 --> 00:45:46,079 说话人 SPEAKER_05：例如，你不是在训练它进行机器翻译。
479 00:45:46,398 --> 00:45:50,523 说话人 SPEAKER_05：所以在 NLP 领域，机器翻译是一个非常重要的任务。
480 00:45:50,503 --> 00:45:55,769 说话人 SPEAKER_05：但像 GPT 这样的基础模型能够进行机器翻译。
481 00:45:55,889 --> 00:46:00,576 说话人 SPEAKER_05: 它可以进行对话、摘要等等。
482 00:46:00,596 --> 00:46:01,836 说话人 SPEAKER_05: 那就是一个基础模型。
483 00:46:01,876 --> 00:46:04,721 说话人 SPEAKER_05: 我们现在在多模态中看到了这一点。
484 00:46:04,820 --> 00:46:08,125 说话人 SPEAKER_05: 我们在视觉、机器人、视频等方面都看到了这一点。
485 00:46:08,144 --> 00:46:10,047 说话人 SPEAKER_05: 我们就是这样做的。
486 00:46:10,527 --> 00:46:11,469 说话人 SPEAKER_05: 但你说得对。
487 00:46:11,548 --> 00:46:17,956 说话人 SPEAKER_05: 公众是在，你说的什么，10 月 30 日看到的？
488 00:46:18,135 --> 00:46:18,956 说话人 SPEAKER_10: 我想是在 11 月。
489 00:46:18,976 --> 00:46:19,498 说话人 SPEAKER_12：十一月。
490 00:46:19,478 --> 00:46:36,938 说话人 SPEAKER_12：关于基础模型，还有一件非常重要的事情，那就是在认知科学中长期存在的一种观点，即这些神经网络，如果你给它们足够的训练数据，它们可以完成复杂的事情，但它们需要大量的训练数据。
491 00:46:36,958 --> 00:46:40,103 说话人 SPEAKER_12：它们需要看到成千上万只猫。
492 00:46:40,791 --> 00:46:43,918 说话人 SPEAKER_12：而人类在统计上更加高效。
493 00:46:44,079 --> 00:46:47,786 也就是说，它们可以在更少的数据上学会做这些事情。
494 00:46:47,806 --> 00:46:58,327 人们不再这么说了，因为他们实际上是在比较麻省理工学院本科生在有限数据上能学会做什么。
495 00:46:58,527 --> 00:47:03,016 与从随机权重开始的神经网络在有限数据上能学会做什么。
496 00:47:03,597 --> 00:47:13,057 如果你想进行公平的比较，你拿一个基础模型，即一个在大量事物上训练过的神经网络，然后给它一个全新的任务。
497 00:47:13,659 --> 00:47:17,768 说话人 SPEAKER_12：那么，您问，学习这个全新的任务需要多少数据？
498 00:47:18,710 --> 00:47:21,733 说话人 SPEAKER_12：这就是所谓的少样本学习，因为它不需要太多数据。
499 00:47:22,432 --> 00:47:26,536 说话人 SPEAKER_12：然后您会发现这些事情在统计上是高效的。
500 00:47:26,737 --> 00:47:32,001 说话人 SPEAKER_12：也就是说，它们在完成新任务所需数据量方面与人类相比相当有优势。
501 00:47:32,702 --> 00:47:40,889 说话者 SPEAKER_12：所以，那种我们天生就拥有大量知识的老观念，这使得我们远比那些只从数据中学习一切的事物优越。
502 00:47:41,530 --> 00:47:47,315 说话者 SPEAKER_12：现在人们差不多已经放弃了这种观点，因为如果你用一个没有先天知识但经验丰富的基座模型，
503 00:47:47,295 --> 00:47:50,320 说话者 SPEAKER_12：然后给它一个新的任务，它学习起来非常高效。
504 00:47:50,360 --> 00:47:51,762 说话者 SPEAKER_12：它不需要大量的数据。
505 00:47:53,324 --> 00:47:59,172 讲者 SPEAKER_05: 你知道，我的博士研究方向是一次性学习，但这个方向非常有趣。
506 00:47:59,293 --> 00:48:08,126 讲者 SPEAKER_05: 即使在贝叶斯框架下，你也可以进行预训练，但只有在神经网络中的预训练才能真正实现多任务。
507 00:48:08,246 --> 00:48:09,387 讲者 SPEAKER_10: 对。
508 00:48:10,027 --> 00:48:15,092 讲者 SPEAKER_10: 好的，所以这基本上在 ChatGPT 中得到了产品化。
509 00:48:15,733 --> 00:48:21,278 说话人 SPEAKER_10: 世界正在经历它，这仅仅发生在 10 个月前，尽管对我们中的某些人来说感觉像是很久以前。
510 00:48:21,478 --> 00:48:21,900 说话人 SPEAKER_06: 永恒。
511 00:48:22,079 --> 00:48:32,210 说话人 SPEAKER_10: 因为突然之间，很久以前发生了一次大爆炸，我认为很长时间以来没有人真正看到它的结果。
512 00:48:32,951 --> 00:48:39,358 说话人 SPEAKER_10: 意思是说，我的比喻是，有行星形成，有可见的恒星，每个人都可以体验。
513 00:48:39,338 --> 00:48:42,144 说话人 SPEAKER_10：过去 10 年发生的事情的结果，然后进行转换等。
514 00:48:42,806 --> 00:48:49,824 说话人 SPEAKER_10：所以，世界突然对我认为很多人感觉像魔法的东西变得非常兴奋。
515 00:48:51,128 --> 00:48:52,251 说话人 SPEAKER_10：他们可以触摸到的东西。
516 00:48:52,501 --> 00:49:09,990 说话人 SPEAKER_10：他们可以体验，并以他们要求的方式给出反馈，无论是输入文本提示并要求生成图像或视频，还是要求更多文本来回答你无法预料的问题，并得到那些意想不到的答案。
517 00:49:10,472 --> 00:49:12,635 说话人 SPEAKER_10: 这感觉有点像魔法。
518 00:49:13,324 --> 00:49:18,231 说话人 SPEAKER_10: 我个人的观点是，我们在人工智能领域总是不断调整目标线。
519 00:49:18,693 --> 00:49:20,494 说话人 SPEAKER_10: 人工智能总是我们做不到的事情。
520 00:49:20,574 --> 00:49:21,416 说话人 SPEAKER_10: 总是那魔法。
521 00:49:21,436 --> 00:49:23,619 说话人 SPEAKER_10: 一到那里，我们就说，那根本就不是人工智能。
522 00:49:24,119 --> 00:49:26,202 说话人 SPEAKER_10: 有些人说，那根本就不是人工智能。
523 00:49:26,222 --> 00:49:28,445 说话人 SPEAKER_10: 我们已经改变了目标线。
524 00:49:29,146 --> 00:49:32,130 说话人 SPEAKER_10: 在这种情况下，当它发布时，你的反应是什么？
525 00:49:33,032 --> 00:49:37,719 说话人 SPEAKER_10: 我知道你的一部分反应是你离开了谷歌，决定做不同的事情。
526 00:49:37,778 --> 00:49:40,463 说话人 SPEAKER_10: 但是当你第一次看到它时，你有什么想法？
527 00:49:40,730 --> 00:49:44,318 说话人 SPEAKER_12: 好吧，就像 Fei-Fei 说的，GPT-2 给我们所有人留下了深刻的印象。
528 00:49:45,161 --> 00:49:46,844 说话人 SPEAKER_12: 然后是一系列的稳步发展。
529 00:49:47,184 --> 00:49:56,206 之前在谷歌 GPT-4 和 GPT-3.5 之前，我也见过像 Palm 一样出色的东西。
530 00:49:56,373 --> 00:50:00,657 所以，这本身并没有带来太大的变化。
531 00:50:00,677 --> 00:50:05,423 更重要的是，Palm 在谷歌给我留下了深刻印象，因为 Palm 能解释为什么一个笑话好笑。
532 00:50:05,985 --> 00:50:12,793 我总是用这个来衡量，当它能够解释为什么一个笑话好笑时，我们就知道它真的理解了。
533 00:50:13,172 --> 00:50:14,074 说话人 SPEAKER_12: Palm 可以做到这一点。
534 00:50:15,235 --> 00:50:17,398 说话人 SPEAKER_12: 不是每个笑话，但很多笑话都可以。
535 00:50:17,418 --> 00:50:22,784 说话人 SPEAKER_12: 顺便说一句，现在这些事物在解释为什么笑话好笑方面相当不错，但在讲笑话方面却很糟糕。
536 00:50:22,764 --> 00:50:27,257 说话人 SPEAKER_12: 原因是，它们是一词一词地生成文本。
537 00:50:28,059 --> 00:50:35,440 说话者 SPEAKER_12：所以如果你让他们讲一个笑话，他们会尝试讲一个笑话，所以他们可能会尝试讲一些听起来像笑话的东西。
538 00:50:36,222 --> 00:50:37,887 说话者 SPEAKER_12：所以他们说，你知道，
539 00:50:37,867 --> 00:50:42,735 说话者 SPEAKER_12：一个牧师和一只獾走进了一家酒吧，这听起来有点像笑话的开头。
540 00:50:43,297 --> 00:50:47,244 说话者 SPEAKER_12：然后他们继续讲，讲一些听起来像笑话开头的东西。
541 00:50:47,583 --> 00:50:49,867 说话人 SPEAKER_12: 但是他们到了需要笑点的地方。
542 00:50:50,248 --> 00:50:51,311 说话人 SPEAKER_12: 当然，他们没有提前考虑。
543 00:50:51,351 --> 00:50:52,994 说话人 SPEAKER_12: 他们没有考虑笑点会是什么。
544 00:50:53,253 --> 00:50:55,358 说话人 SPEAKER_12: 他们只是在尝试让它听起来像是在引出笑话。
545 00:50:55,657 --> 00:51:01,007 说话人 SPEAKER_12: 然后他们给你一个可怜的弱笑点，因为他们必须想出一些笑点。
546 00:51:01,730 --> 00:51:07,099 说话人 SPEAKER_12: 所以尽管他们可以解释笑话，因为他们可以在说之前看到整个笑话，但他们不能讲笑话。
547 00:51:07,400 --> 00:51:08,282 说话人 SPEAKER_12: 但我们会解决这个问题。
548 00:51:09,583 --> 00:51:12,730 说话人 SPEAKER_10: 好的，所以我本来想问你，喜剧演员是不是未来的职业。
549 00:51:12,750 --> 00:51:18,780 说话人 SPEAKER_10: 你认为它会... 可能不会。
550 00:51:18,800 --> 00:51:19,061 说话人 SPEAKER_10: 好吧。
551 00:51:19,101 --> 00:51:19,762 说话人 SPEAKER_10: 所以无论如何。
552 00:51:21,545 --> 00:51:23,027 说话人 SPEAKER_10: 但是你对它的反应是什么？
553 00:51:23,528 --> 00:51:26,134 说话人 SPEAKER_10：再次，你在过程中看到了幕后的事情。
554 00:51:26,670 --> 00:51:27,956 说话人 SPEAKER_05：有一些反应。
555 00:51:28,880 --> 00:51:35,331 说话人 SPEAKER_05：我的第一个反应是，所有人中，我以为我了解数据的力量。
556 00:51:35,985 --> 00:51:40,152 说话人 SPEAKER_05：我还是对数据的力量感到敬畏。
557 00:51:40,693 --> 00:51:42,715 说话人 SPEAKER_05：那是一个技术反应。
558 00:51:42,775 --> 00:51:45,940 说话人 SPEAKER_05：我当时想，该死，我应该做一个更大的图像。
559 00:51:46,001 --> 00:51:47,362 说话人 SPEAKER_05：不，但也许不是。
560 00:51:47,804 --> 00:51:49,186 说话人 SPEAKER_05：但那真的很。
561 00:51:49,206 --> 00:51:50,807 说话人 SPEAKER_05: 太棒了。
562 00:51:50,827 --> 00:51:51,889 说话人 SPEAKER_05: 资金问题是关键。
563 00:51:53,472 --> 00:51:57,858 说话人 SPEAKER_05: 是的，所以那是第二点。
564 00:51:57,838 --> 00:52:13,338 说话人 SPEAKER_05: 当我看到公众对 AI 的觉醒时刻，不仅仅是 GPT-2 技术的时刻，查德 GPT，我一般想，谢天谢地，我们过去四年一直投资于以人为本的 AI。
565 00：52：14,000 --> 00：52：21,369 演讲者 SPEAKER_05：谢天谢地，我们与政策制定者、公共部门、公民社会建立了一座桥梁。
566 00：52：22,369 --> 00：52：24,353 议长 SPEAKER_05：我们做得还不够。
567 00:52:24,771 --> 00:52:29,056 说话者 SPEAKER_05：但谢天谢地，那次对话开始了。
568 00：52：29,197 --> 00：52：31,420 议长 SPEAKER_05：我们参与了。
569 00:52:31,960 --> 00:52:33,862 说话人 SPEAKER_05：我们领导了一部分。
570 00:52:34,304 --> 00:52:44,637 说话人 SPEAKER_05：例如，我们作为斯坦福大学的一个学院，正在领导一项关键的国家人工智能研究云法案，该法案仍在进行中。
571 00:52:44,617 --> 00:52:47,320 说话人 SPEAKER_05：现在国会。
572 00:52:48,119 --> 00:52:51,143 说话人 SPEAKER_05：实际上不是现在。
573 00:52:51,163 --> 00:52:51,643 讲述者 SPEAKER_05: 参议院。
574 00:52:52,083 --> 00:52:55,146 讲述者 SPEAKER_05: 这是通过摄像头的，所以至少参议院是在移动的。
575 00:52:56,467 --> 00:53:02,534 讲述者 SPEAKER_05: 因为我们对这次攻击的社会时刻进行了预测。
576 00:53:02,634 --> 00:53:06,317 讲述者 SPEAKER_05: 我们不知道它何时会到来，但我们知道它会到来。
577 00:53:07,257 --> 00:53:12,682 说话者 SPEAKER_05: 真诚地说，我有一种紧迫感。
578 00:53:12,663 --> 00:53:24,447 说话者 SPEAKER_05: 我们真的必须站出来，不仅作为技术人员的激情，也作为人文主义者的责任。
579 00:53:25,150 --> 00:53:31,362 说话者 SPEAKER_10: 所以你们两个，我想你们共同的反应是我们必须考虑
580 00:53:31,798 --> 00:53:35,809 说话者 SPEAKER_10: 这个机会的机遇，但也要考虑它的负面影响。
581 00:53:36,311 --> 00:53:44,914 说话人 SPEAKER_12：对我来说，有一件事我意识到，但直到很晚才意识到，这让我对社会影响产生了更大的兴趣。
582 00:53:46,092 --> 00:53:48,673 说话人 SPEAKER_12：就像 Fei-Fei 说的，数据的力量。
583 00:53:48,733 --> 00:54:08,672 说话人 SPEAKER_12：这些大型聊天机器人看到的数据比任何人都多得多，它们之所以能做到这一点，是因为你可以制作成千上万的相同模型副本，每个副本可以查看不同的数据子集，并且可以从这些数据中获得如何改变其参数的梯度，然后它们可以共享所有这些梯度。
584 00:54:09,393 --> 00:54:15,880 说话人 SPEAKER_12：因此，每个副本都可以从所有其他副本从数据中提取的内容中受益，而我们无法做到这一点。
585 00:54:16,534 --> 00:54:20,960 说话人 SPEAKER_12：假设你有 10,000 个人，他们去读 10,000 本不同的书。
586 00:54:21,539 --> 00:54:25,364 说话人 SPEAKER_12：然后他们每人读一本书，所有人都能知道所有书的内容。
587 00:54:25,945 --> 00:54:27,286 说话人 SPEAKER_12：这样我们就能变得非常聪明。
588 00:54:27,445 --> 00:54:28,606 说话人 SPEAKER_12：这正是这些事物所做的事情。
589 00:54:29,307 --> 00:54:31,110 说话人 SPEAKER_12: 因此它们远比我们优越。
590 00:54:31,610 --> 00:54:35,173 说话人 SPEAKER_05: 我们正在尝试进行一些教育，但不是这种方式。
591 00:54:35,193 --> 00:54:36,876 说话人 SPEAKER_12: 是的，但教育是无望的。
592 00:54:36,936 --> 00:54:41,039 说话人 SPEAKER_12: 我的意思是，几乎不值得花钱。
593 00:54:42,943 --> 00:54:46,489 说话人 SPEAKER_05：除了多伦多大学和桑福德大学。
594 00:54:48,052 --> 00:54:52,920 说话人 SPEAKER_10：我试图向朋友们解释，杰夫有一个非常讽刺的幽默感，如果你和他在一起的时间足够长，你就能理解。
595 00:54:53,000 --> 00:54:56,085 说话人 SPEAKER_10：但由你来决定那是否是讽刺的。
596 00:54:56,105 --> 00:55:00,130 说话人 SPEAKER_12：所以，我们交换知识的方式，粗略地说，这是一种简化。
597 00:55:00,632 --> 00:55:05,519 说话人 SPEAKER_12: 但是我说一句话，你需要在你的大脑中想出你需要改变什么，这样你才可能说出那样的话。
598 00:55:06,460 --> 00:55:08,344 说话人 SPEAKER_12: 这取决于你是否信任我。
599 00:55:09,168 --> 00:55:11,251 说话人 SPEAKER_12：我们也可以用这些模型来做。
600 00:55:11,570 --> 00:55:17,639 说话人 SPEAKER_12：如果你想让一个神经网络架构知道另一个架构知道的内容，而这个架构是完全不同的，你不能只是给它权重。
601 00:55:18,139 --> 00:55:19,961 说话人 SPEAKER_12：所以你让一个模仿另一个的输出。
602 00:55:20,123 --> 00:55:21,103 说话人 SPEAKER_12：这就叫做蒸馏。
603 00:55:21,585 --> 00:55:22,945 说话人 SPEAKER_12: 就是这样我们相互学习。
604 00:55:23,166 --> 00:55:24,128 说话人 SPEAKER_12: 但这非常低效。
605 00:55:24,208 --> 00:55:26,851 说话人 SPEAKER_12: 它受限于句子的带宽，大约几百比特。
606 00:55:27,572 --> 00:55:31,597 说话人 SPEAKER_12: 而如果你有这些模型，这些拥有万亿参数的数字代理，
607 00:55:32,489 --> 00:55:36,132 说话人 SPEAKER_12：他们各自查看不同的数据，然后共享梯度。
608 00:55:36,152 --> 00:55:37,795 说话人 SPEAKER_12：他们共享着万亿个数字。
609 00:55:38,456 --> 00:55:45,664 说话人 SPEAKER_12：所以您是在比较万亿个数字中蕴含的知识共享能力与几百比特的东西。
610 00:55:46,244 --> 00:55:49,429 说话人 SPEAKER_12：他们在这方面比我们强得多。
611 00:55:50,630 --> 00:55:57,297 说话人 SPEAKER_05：所以我猜，杰夫，我...我在技术层面上同意你的看法，但
612 00:55:57,581 --> 00:56:03,048 说话人 SPEAKER_05：对你来说，那是一个让你感到非常消极的时刻。
613 00:56:03,608 --> 00:56:06,313 说话人 SPEAKER_12：那一刻我觉得我们完了，是的。
614 00:56:06,994 --> 00:56:13,621 说话人 SPEAKER_05：我没有你那么消极，我稍后会解释，但我认为那就是我们...让我们来谈谈那件事。
615 00:56:14,302 --> 00:56:20,550 说话人 SPEAKER_10: 解释你为什么乐观，让我们了解你为什么更悲观。
616 00:56:21,271 --> 00:56:23,673 说话人 SPEAKER_12: 我悲观是因为悲观者通常是对的。
617 00:56:26,168 --> 00:56:29,032 说话人 SPEAKER_05: 我也以为自己是悲观主义者。
618 00:56:29,052 --> 00:56:30,574 说话人 SPEAKER_05: 我们进行了这次对话。
619 00:56:30,594 --> 00:56:34,398 说话人 SPEAKER_05：我不知道是否应该被称为乐观主义者。
620 00:56:34,940 --> 00:56:49,880 说话人 SPEAKER_05：我想，你看，当你 15 岁来到一个国家，现在只会说一点语言，从零美元开始，我的思维方式中有着非常实用的一面。
621 00:56:50,333 --> 00:57:04,554 说话人 SPEAKER_05：我认为，我们与技术的互动，比学术界通常预测的要复杂得多，因为我们是在象牙塔中进入学术界的。
622 00:57:04,655 --> 00:57:06,277 说话人 SPEAKER_05：我们想要做出发现。
623 00:57:06,378 --> 00:57:11,244 说话人 SPEAKER_05：我们想要打造一项技术，但我们往往过于纯粹主义。
624 00:57:11,224 --> 00:57:22,443 说话人 SPEAKER_05：但当像人工智能这样的技术落地并达到社会层面时，它不可避免地会与人类的行为纠缠在一起。
625 00:57:22,603 --> 00:57:27,713 说话人 SPEAKER_05：这就是，也许你可以称之为乐观，我对人性的感觉。
626 00:57:28,052 --> 00:57:29,436 说话人 SPEAKER_05：我相信人性。
627 00:57:29,576 --> 00:57:37,248 说话人 SPEAKER_05: 我不仅相信人类的韧性，还相信集体意志，历史的弧线，
628 00:57:37,228 --> 00:57:49,561 说话人 SPEAKER_05: 有时很危险，但如果我们做正确的事，我们就有一线机会，我们有一线机会创造一个更美好的未来。
629 00：57：49,702 --> 00：57：57,771 演讲者 SPEAKER_05：所以我真正感受到的不是此时此刻的妄想乐观，实际上是一种责任感的紧迫感。
630 00:57:58,030 --> 00:58:06,239 说话者 SPEAKER_05：有一件事，杰夫，我真的希望你能保持积极的态度，那就是看看这一代的学生。
在我的课程中，我每年春天都会教授一个600名本科生的深度学习和计算机视觉入门课程。
632 00：58：14,961 --> 00：58：19,146 议长 SPEAKER_05：这一代人与五年前相比是如此不同。
他们走进我们的课堂，不仅想学习深度学习 Transformer、通用人工智能，还想讨论伦理问题。
634 00：58：27,918 --> 00：58：29,300 议长 SPEAKER_05：他们想谈谈政策。
635 00：58：29,340 --> 00：58：31,884 演讲者 SPEAKER_05：他们想了解隐私和偏见。
636 00：58：32,405 --> 00：58：37,931 议长 SPEAKER_05：我认为那真的是我看到的人性
637 00:58:38,164 --> 00:58:41,813 机遇面前，我认为它是脆弱的。
638 00:58:41,833 --> 00:58:45,800 我的意思是，看看世界上正在发生的事情，在华盛顿。
639 00:58:46,282 --> 00:58:52,494 它非常脆弱，但我觉得如果我们认识到这个时刻，就有希望。
640 00:58:52,998 --> 00:58:54,219 所以我看到了同样的事情。
641 00:58:54,239 --> 00:58:58,786 讲者 SPEAKER_12：我现在不再教本科生了，但我看到一些年轻的教师也在这样做。
642 00:58:58,826 --> 00:59:06,536 讲者 SPEAKER_12：比如在多伦多大学，两位最杰出的年轻教授去了 Anthropic 公司从事对齐工作。
643 00:59:07,757 --> 00:59:09,960 讲者 SPEAKER_12：罗杰·格雷斯（Roger Grace）希望这次能回来。
644 00:59:10,922 --> 00:59:15,847 讲者 SPEAKER_12：比如伊利亚（Ilya），现在全职从事对齐工作。
645 00:59:15,967 --> 00:59:18,751 说话人 SPEAKER_12: 现在确实有一个巨大的转变。
646 00:59:19,153 --> 00:59:29,168 说话人 SPEAKER_12: 我认为我不太可能提出有助于解决这个问题的新想法，但我可以鼓励这些年轻人，这些大约 40 岁的年轻人。
647 00:59:30,090 --> 00:59:30,510 说话人 SPEAKER_05: 谢谢。
648 00:59:33,635 --> 00:59:36,699 说话人 SPEAKER_12: 去实现这些想法，他们现在真的在认真对待这些想法。
649 00:59:36,719 --> 00:59:48,215 说话人 SPEAKER_05: 只要我们把最聪明的大脑，就像在场的许多观众和在线的你们一样，投入到这个问题中，这就是我的希望所在。
650 00:59:48,769 --> 00:59:56,025 说话人 SPEAKER_10: 所以杰夫，你离开谷歌很大一部分原因是为了能够自由地按照你想要的方式谈论这个问题。
651 00:59:57,168 --> 00:59:59,472 说话人 SPEAKER_12: 事实上，这并不完全正确。
652 00:59:59,492 --> 01:00:01,677 说话人 SPEAKER_12: 这是媒体报道的故事，听起来不错。
653 01:00:02,137 --> 01:00:08,331 说话人 SPEAKER_12: 我离开谷歌是因为我年纪大了，感到疲惫，想要退休看看 Netflix。
654 01:00:08,851 --> 01:00:18,581 说话人 SPEAKER_12: 那时我恰好有机会说出我一直思考的责任问题，不必担心谷歌的反应。
655 01:00:19,023 --> 01:00:19,844 说话人 SPEAKER_10: 所以更像是这样。
656 01:00:19,923 --> 01:00:21,945 说话人 SPEAKER_10: 如果我们有时间，我们会回来讨论 Netflix 的推荐。
657 01:00:21,965 --> 01:00:23,547 说话人 SPEAKER_10: 我本来想说。
658 01:00:23,987 --> 01:00:30,596 说话人 SPEAKER_10: 在此期间，但你确实出去并在媒体上发表了相当重要的言论。
659 01:00:30,996 --> 01:00:35,922 说话人 SPEAKER_10: 我认为你们在过去八个月里可能和更多的政治家交谈过，比你们以前一生中交谈的都要多。
660 01:00:35,902 --> 01:00:42,934 说话人 SPEAKER_10: 从总统和总理，到国会、议会等等。
661 01:00:45,237 --> 01:00:54,213 说话人 SPEAKER_10: 杰夫，你能解释一下你的担忧是什么，你试图通过表达它来达到什么目的，你认为这样做是否有效吗？
662 01:00:55,135 --> 01:01:00,744 说话人 SPEAKER_12: 是的，所以人们谈论 AI 风险，但存在许多不同的风险。
663 01:01:00,994 --> 01:01:06,300 说话人 SPEAKER_12: 因此，它可能会夺走工作，而不会创造那么多工作。
664 01:01:06,800 --> 01:01:09,123 说话人 SPEAKER_12: 因此，我们将有一大批失业人群。
665 01:01:09,682 --> 01:01:18,170 说话人 SPEAKER_12：我们必须对此高度重视，因为生产力 AI 带来的增长不会与失去工作的人共享。
666 01:01:18,210 --> 01:01:20,594 说话人 SPEAKER_12：富人会变得更富，穷人会变得更穷。
667 01:01:21,414 --> 01:01:27,119 说话人 SPEAKER_12：即使你有基本收入，这也无法解决许多人的尊严问题。
668 01:01:27,572 --> 01:01:31,389 说话人 SPEAKER_12：他们想要有一份工作，感觉自己在做重要的事情。
669 01:01:31,521 --> 01:01:33,204 演讲者 SPEAKER_12：包括学术界。
670 01:01:34,085 --> 01:01:35,867 演讲者 SPEAKER_12：所以这是一个问题。
671 01:01:36,347 --> 01:01:40,072 演讲者 SPEAKER_12：然后是假新闻的问题，这是一个相当不同的问题。
672 01:01:40,092 --> 01:01:41,893 演讲者 SPEAKER_12：然后是战斗机器人问题。
673 01:01:41,914 --> 01:01:43,335 说话人 SPEAKER_12：这又是一个完全不同的问题。
674 01:01:43,376 --> 01:01:48,943 说话人 SPEAKER_12：所有大的国防部门都想制造战斗机器人，没有人能阻止他们。
675 01:01:48,963 --> 01:01:49,923 说话人 SPEAKER_12：那将会很糟糕。
676 01:01:50,284 --> 01:01:57,373 说话人 SPEAKER_12：也许最终，在我们与战斗机器人打了些仗之后，我们会得到类似日内瓦公约的东西，就像我们对待化学武器那样。
677 01:01:57,853 --> 01:02:00,617 说话人 SPEAKER_12：直到它们被使用之后，人们才能对此采取行动。
678 01:02:00,985 --> 01:02:09,452 说话人 SPEAKER_12：然后是存在风险，这正是我所担忧的。
679 01:02:09,472 --> 01:02:18,400 说话人 SPEAKER_12：存在风险是指人类被消灭，因为我们开发了一种更高级的智能形式，它决定接管控制权。
680 01:02:19,221 --> 01:02:22,804 说话人 SPEAKER_12：如果它变得比我们聪明得多，那么这里有很多假设。
681 01:02:23,204 --> 01:02:24,346 说话人 SPEAKER_12：这是一个充满不确定性的时代。
682 01:02:24,385 --> 01:02:29,489 说话人 SPEAKER_12：您不应该太认真对待我说的话。
683 01:02:29,925 --> 01:02:42,509 说话人 SPEAKER_12：如果我们创造出比我们更聪明的东西，因为这些数字智能可以更好地共享，所以它们可以学习得更多，我们不可避免地会让这些聪明的东西创造次级智能体。
684 01:02:42,869 --> 01:02:47,157 说话人 SPEAKER_12：如果您想让它们做某事，为了做到这一点，它们会想出，嗯，您必须先做其他事情。
685 01:02:47,177 --> 01:02:50,583 说话人 SPEAKER_12: 像是要去欧洲，你得先去机场。
686 01:02:51,155 --> 01:02:52,599 说话人 SPEAKER_12: 那是一个子目标。
687 01:02:52,619 --> 01:02:53,844 说话人 SPEAKER_12: 所以他们会设定子目标。
688 01:02:54,286 --> 01:02:59,563 说话人 SPEAKER_12: 很明显的一个子目标是，如果你想完成任何事情，就要获得更多的权力。
689 01:03:00,184 --> 01:03:03,054 说话人 SPEAKER_12：如果你获得更多控制权，做事情就会更容易。
690 01:03:03,422 --> 01:03:08,128 说话人 SPEAKER_12：因此，任何能够创建子目标的东西都会产生获得更多控制的子目标。
691 01:03:09,170 --> 01:03:13,333 说话人 SPEAKER_12：如果比我们更智能的东西想要获得控制权，它们会这么做。
692 01:03:13,375 --> 01:03:15,317 说话人 SPEAKER_12：我们无法阻止它们。
693 01:03:15,336 --> 01:03:19,161 说话人 SPEAKER_12：我们必须要想办法阻止他们永远想要控制。
694 01:03:20,061 --> 01:03:20,862 说话人 SPEAKER_12：还有一些希望。
695 01:03:21,403 --> 01:03:23,306 说话人 SPEAKER_12：这些事情并没有进化。
696 01:03:23,365 --> 01:03:24,807 说话人 SPEAKER_12：它们不是恶意的竞争性事物。
697 01:03:24,887 --> 01:03:27,590 说话人 SPEAKER_12: 我们就是这样做的。
698 01:03:28,617 --> 01:03:29,518 说话人 SPEAKER_12: 他们是不朽的。
699 01:03:30,278 --> 01:03:35,724 说话人 SPEAKER_12: 所以对于数字智能，你只需将权重存储在某个地方，你就可以在其他硬件上再次运行它。
700 01:03:36,505 --> 01:03:39,809 说话人 SPEAKER_12: 因此，我们实际上发现了不朽的秘密。
701 01:03:40,391 --> 01:03:41,692 说话人 SPEAKER_12: 唯一的问题是这不是为我们准备的。
702 01:03:41,791 --> 01:03:43,273 说话人 SPEAKER_12: 我们是不朽的。
703 01:03:43,635 --> 01:03:44,856 说话人 SPEAKER_12: 但是这些其他的东西是不朽的。
704 01:03:45,155 --> 01:03:50,742 说话人 SPEAKER_12: 这可能会使它们变得更好，因为它们不担心死亡，而且它们不需要……
705 01:03:51,331 --> 01:03:57,278 说话人 SPEAKER_12: 好吧，它们非常像希腊神话中的神，我必须说一句埃隆·马斯克告诉我的话。
706 01:03:57,318 --> 01:04:00,514 说话人 SPEAKER_12: 这是埃隆·马斯克的信仰。
707 01:04:01,692 --> 01:04:05,376 说话人 SPEAKER_12: 是的，我们是数字智能的引导程序。
708 01:04:05,476 --> 01:04:10,623 说话人 SPEAKER_12: 我们是相对愚蠢的智能形式，但足够聪明，可以创造计算机和人工智能。
709 01:04:11,384 --> 01:04:13,465 说话人 SPEAKER_12: 这将是一种更加智能的智能形式。
710 01:04:14,327 --> 01:04:19,554 说话人 SPEAKER_12: 而 Elon Musk 认为这将使我们得以留存，因为有了人类的世界会比没有人类的世界更有趣。
711 01:04:20,434 --> 01:04:23,117 说话人 SPEAKER_12: 这似乎是一个非常脆弱的未来依托。
712 01:04:23,798 --> 01:04:25,219 说话人 SPEAKER_12: 但这与费伊·费伊所说的有关。
713 01:04:25,420 --> 01:04:29,826 说话人 SPEAKER_12: 这非常像希腊神话中的神，神们有人们围绕他们玩乐。
714 01:04:30,666 --> 01:04:34,514 说话人 SPEAKER_05: 好的，我可以评论一下吗？
715 01:04:34,534 --> 01:04:35,554 说话人 SPEAKER_05: 我说的都没有争议。
716 01:04:35,815 --> 01:04:36,516 说话人 SPEAKER_05: 不，一点都没有。
717 01:04:37,277 --> 01:04:48,878 说话人 SPEAKER_05：所以我想要将您的四个担忧进行分类，经济、劳动、虚假信息和武器化，然后是灭绝。
718 01:04:49,239 --> 01:04:50,940 说话人 SPEAKER_12：我忘记了歧视和偏见。
719 01:04:50,960 --> 01:04:54,226 说话人 SPEAKER_05：好的，所以我想要将它们分为两大类。
720 01:04:54,527 --> 01:05:00,456 说话人 SPEAKER_05：希腊神话中的灭绝是灭绝类别。
721 01:05:00,717 --> 01:05:03,942 说话人 SPEAKER_05：其他所有我都称之为灾难性的。
722 01:05:03,961 --> 01:05:05,423 说话人 SPEAKER_05：灾难性的。
723 01:05:05,463 --> 01:05:08,369 说话人 SPEAKER_05：灾难性的危险。
724 01:05:08,829 --> 01:05:10,010 说话人 SPEAKER_05：我想对此进行评论。
725 01:05:10,472 --> 01:05:21,547 说话人 SPEAKER_05: 我认为作为 AI 生态系统中的一员，我真正感到的责任是
726 01:05:22,253 --> 01:05:29,599 说话人 SPEAKER_05: 确保我们不是在夸张地谈论，尤其是与公共政策制定者交流时。
727 01:05:30,161 --> 01:05:42,731 说话人 SPEAKER_05: 恐怖灭绝的风险，Jeff，尽管我非常尊重您的观点，这是一个学术界和智库应该研究的非常有趣的思想过程。
728 01:05:42,853 --> 01:05:44,454 说话人 SPEAKER_12: 我已经这样想了很多年。
729 01:05:45,094 --> 01:05:49,219 说话人 SPEAKER_12: 我以为这还离得很远，未来，有哲学家和学者从事这项工作真是太好了。
730 01:05:49,478 --> 01:05:50,639 说话人 SPEAKER_12: 我认为这要原创得多。
731 01:05:50,771 --> 01:05:55,577 说话人 SPEAKER_05: 但这个过程不仅仅是机器。
732 01:05:55,737 --> 01:05:58,199 说话人 SPEAKER_05: 人类也参与了这一混乱的过程。
733 01:05:58,599 --> 01:06:01,282 说话人 SPEAKER_05: 我认为这里有很多细微差别。
734 01:06:01,362 --> 01:06:04,606 说话人 SPEAKER_05: 比如说，我们谈论核能。
735 01:06:04,626 --> 01:06:06,347 说话人 SPEAKER_05: 我知道核能的范围要窄得多。
736 01:06:06,728 --> 01:06:14,556 说话人 SPEAKER_05: 但如果你考虑核能，它不仅仅是聚变或裂变的理论，或者 whatever。
737 01:06:14,856 --> 01:06:19,041 说话人 SPEAKER_05: 真正获得铀或钚
738 01:06:19,021 --> 01:06:22,585 说话人 SPEAKER_05: 系统工程、人才等等。
739 01:06:22,625 --> 01:06:24,447 说话人 SPEAKER_05: 我相信你看过电影《奥本海默》。
740 01:06:24,847 --> 01:06:33,077 说话人 SPEAKER_05: 所以在这里，如果我们朝着那个方向发展，我认为我们有机会，不仅仅是战斗的机会，因为我们是人类社会。
741 01:06:33,398 --> 01:06:36,161 说话人 SPEAKER_05: 我们将设立护栏，我们将共同努力。
742 01:06:36,302 --> 01:06:48,976 说话人 SPEAKER_05: 我不想描绘出一幅明天我们将拥有所有这些机器人，尤其是以机器人形态、物理形态出现的机器人的画面。
743 01:06:48,956 --> 01:06:50,280 说话人 SPEAKER_05: 我们将面临机器主宰的局面。
744 01:06:50,360 --> 01:06:53,806 说话人 SPEAKER_05: 我真的认为我们需要小心行事。
745 01:06:53,887 --> 01:06:57,574 说话人 SPEAKER_05: 但我并不反对你，我们认为这是我们需要思考的问题。
746 01:06:57,594 --> 01:06:59,456 说话人 SPEAKER_05: 所以这是灭绝桶。
747 01:06:59,858 --> 01:07:03,965 说话人 SPEAKER_05: 灾难性风险桶，我认为这更加真实。
748 01:07:04,547 --> 01:07:06,931 说话人 SPEAKER_05: 我认为我们需要最聪明的人
749 01:07:06,911 --> 01:07:08,994 说话人 SPEAKER_05：人多力量大，一起工作更愉快。
750 01:07:09,054 --> 01:07:11,215 说话人 SPEAKER_05：所以，我想对每一个都评论一下。
751 01:07:11,577 --> 01:07:12,637 说话人 SPEAKER_05：武器化，对吧？
752 01:07:12,677 --> 01:07:14,119 说话人 SPEAKER_05：这真的是真的。
753 01:07:14,338 --> 01:07:16,561 说话人 SPEAKER_05: 我完全同意你的看法。
754 01:07:16,902 --> 01:07:18,943 说话人 SPEAKER_05: 我们需要国际合作。
755 01:07:19,023 --> 01:07:21,126 说话人 SPEAKER_05: 我们需要潜在的条约。
756 01:07:21,505 --> 01:07:23,648 说话人 SPEAKER_05: 我们需要理解参数。
757 01:07:23,788 --> 01:07:32,717 说话人 SPEAKER_05: 人类，尽管我对人类持乐观态度，但我对人类的自我毁灭也感到悲观。
758 01:07:32,697 --> 01:07:36,103 说话人 SPEAKER_05: 人类的能力以及相互毁灭。
759 01:07:36,443 --> 01:07:39,028 说话人 SPEAKER_05: 因此，我们必须让人们为此而努力。
760 01:07:39,108 --> 01:07:44,639 说话人 SPEAKER_05: 我们的友人 Stuart Russell 以及许多 AI 专家都在讨论这个问题。
761 01:07:45,340 --> 01:07:49,849 说话人 SPEAKER_05: 接下来你提到的第二个桶是虚假信息。
762 01:07:50,269 --> 01:07:52,173 说话人 SPEAKER_05: 这又是，
763 01:07:52,153 --> 01:07:58,481 说话人 SPEAKER_05: 我的意思是，2024 年，大家都在关注美国大选以及 AI 将如何发挥作用。
764 01:07:59,282 --> 01:08:05,230 说话人 SPEAKER_05: 我认为我们必须解决社交媒体问题。
765 01:08:05,329 --> 01:08:08,634 说话人 SPEAKER_05：我们必须解决虚假信息问题。
766 01:08:08,653 --> 01:08:11,538 说话人 SPEAKER_05：从技术角度看，我现在看到的工作更多了。
767 01:08:12,139 --> 01:08:18,126 说话人 SPEAKER_05：数字认证在技术上是实际上一个非常活跃的研究领域。
768 01:08:18,587 --> 01:08:20,389 说话人 SPEAKER_05：我认为我们需要，
769 01:08:20,707 --> 01:08:21,988 说话人 SPEAKER_05: 我们需要投资。
770 01:08:22,069 --> 01:08:23,131 说话人 SPEAKER_05: 我知道 Adobe 是。
771 01:08:23,351 --> 01:08:24,793 说话人 SPEAKER_05: 我知道学术界是。
772 01:08:24,934 --> 01:08:26,195 说话人 SPEAKER_05: 我认为我们需要。
773 01:08:26,216 --> 01:08:31,586 说话人 SPEAKER_05：我希望在这个领域有初创公司正在关注数字认证。
774 01:08:31,865 --> 01:08:33,368 说话人 SPEAKER_05：但我们还需要政策。
775 01:08:33,909 --> 01:08:34,690 说话人 SPEAKER_05：然后是工作。
776 01:08:35,773 --> 01:08:37,114 说话人 SPEAKER_05：我非常同意。
777 01:08:37,856 --> 01:08:42,123 说话人 SPEAKER_05：实际上，你使用了我认为最重要的工作，它真的很
778 01:08:42,104 --> 01:08:45,248 说话人 SPEAKER_05：在我们的人工智能辩论的核心是人的尊严。
779 01:08:46,751 --> 01:08:52,019 说话人 SPEAKER_05：人的尊严不仅仅在于你赚多少钱，你工作多少小时。
780 01:08:52,940 --> 01:09:02,796 说话人 SPEAKER_05：我实际上认为如果我们这样做是正确的，我们将从劳动力经济转向尊严经济，在这种意义上，人类
781 01:09:02,775 --> 01:09:18,418 机器的帮助和协作，我们将因为激情、个性化和专业技能而赚钱，而不仅仅是那些真正辛苦和单调的工作。
782 01:09:18,819 --> 01:09:25,488 这也是为什么斯坦福大学的人类增强 AI 有一个基本原则。
783 01:09:25,829 --> 01:09:28,833 我们在医疗保健领域看到了这一点，这是最大的领域之一。
784 01:09:28,814 --> 01:09:37,329 ChatGPT 最早的几天，我就有一个来自斯坦福医院的医生朋友走过来对我说，Fei-Fei，我想感谢你为 ChatGPT 所做的一切。
785 01:09:37,350 --> 01:09:38,471 说话人 SPEAKER_05: 我说，我没有做什么。
786 01:09:38,872 --> 01:09:46,587 说话人 SPEAKER_05: 但他说我们正在使用来自 GPT 的医疗摘要工具，因为
787 01:09:46,567 --> 01:09:50,369 说话人 SPEAKER_05: 因为这对我们的医生来说是一个巨大的负担。
788 01:09:50,449 --> 01:09:52,652 说话人 SPEAKER_05: 它在占用患者的时间。
789 01:09:52,671 --> 01:09:55,854 说话人 SPEAKER_05: 但正因为如此，我得到了更多时间。
790 01:09:56,194 --> 01:09:57,737 说话人 SPEAKER_05: 这就是一个完美的例子。
791 01:09:57,817 --> 01:09:59,018 说话人 SPEAKER_05: 我们还将看到更多这样的情况。
792 01:09:59,057 --> 01:10:02,820 说话人 SPEAKER_05: 我们甚至可能在蓝领工作中看到这种情况。
793 01:10:02,900 --> 01:10:06,784 说话人 SPEAKER_05：所以我们有机会把这件事做对。
794 01:10:07,604 --> 01:10:16,573 说话人 SPEAKER_05：我会在灾难性担忧的基础上再增加一个担忧，那就是你提到的权力不平衡。
795 01:10:16,552 --> 01:10:27,323 说话人 SPEAKER_05：我现在看到的一种权力不平衡，而且它正在以惊人的速度加剧，就是将公共部门排除在外。
796 01:10:27,826 --> 01:10:29,007 说话人 SPEAKER_05：我不了解加拿大。
美国没有一所大学能训练出聊天 GPT。
今天，从计算能力上来说，美国任何一所大学都无法训练出聊天 GPT。
我认为把美国所有大学的计算能力加起来。
GPT、A100 或 H100，可能没有人有，但 A100 无法训练出聊天 GPT。
801 01:10:49,881 --> 01:11:02,336 讲者 SPEAKER_05: 但这正是我们拥有独特数据来治愈癌症、对抗气候变化、研究经济和法律的地方。
802 01:11:02,877 --> 01:11:05,179 讲者 SPEAKER_05: 我们需要投资公共部门。
803 01:11:05,279 --> 01:11:09,765 讲者 SPEAKER_05: 如果我们现在不行动，我们将让整整一代人失败。
804 01:11:10,104 --> 01:11:12,868 讲者 SPEAKER_05: 我们还将留下这种权力不平衡。
805 01:11:12,847 --> 01:11:15,752 说话人 SPEAKER_05：以如此危险的方式。
806 01:11:16,153 --> 01:11:17,494 说话人 SPEAKER_05：所以我同意你的看法。
807 01:11:17,655 --> 01:11:23,645 说话人 SPEAKER_05：我认为我们面临许多灾难性风险，我们需要采取行动。
808 01:11:23,725 --> 01:11:28,273 说话人 SPEAKER_05：这就是为什么我们需要与政策制定者和民间社会合作。
809 01:11:29,194 --> 01:11:36,546 说话人 SPEAKER_05：我不知道我是用乐观的语气说的，还是用悲观的语气，现在听起来我更悲观。
810 01:11:36,565 --> 01:11:39,210 说话人 SPEAKER_05：但我确实认为还有很多工作要做。
811 01:11:39,847 --> 01:11:58,949 说话人 SPEAKER_10：乐观地看，自从你们过去六到八个月来一直非常积极地谈论这个问题，已经发生了巨大的转变，正如你所说，杰夫，关键研究人员开始关注这些问题，然后公众和政策也在转变，政府实际上开始认真对待这个问题。
812 01:11:59,051 --> 01:12:08,742 说话人 SPEAKER_10：所以，我的意思是，你在向白宫和美国政府提供建议，你也和他们交谈过，你也和多位总理坐在一起讨论过。
813 01:12:09,127 --> 01:12:17,198 说话人 SPEAKER_10: 也许，他们现在正在倾听，对吧，这是一种他们 10 个月前、12 个月前可能不会的方式。
814 01:12:18,020 --> 01:12:20,724 说话人 SPEAKER_10: 你对这个方向持乐观态度吗？
815 01:12:22,167 --> 01:12:29,738 说话人 SPEAKER_12: 我乐观地认为人们已经理解了存在一系列问题，包括灾难性风险和存在性风险。
816 01:12:29,757 --> 01:12:31,881 说话人 SPEAKER_12: 我完全同意 Fei-Fei 的观点。
817 01:12:31,921 --> 01:12:33,543 说话人 SPEAKER_12：灾难性风险更加紧迫。
818 01:12:34,125 --> 01:12:37,750 说话人 SPEAKER_12：特别是 2024 年非常紧迫。
819 01:12:39,703 --> 01:12:42,713 说话人 SPEAKER_12：我相当乐观，现在人们正在倾听，是的。
820 01:12:42,734 --> 01:12:43,898 说话人 SPEAKER_05：是的，我同意。
821 01:12:43,957 --> 01:12:52,988 说话人 SPEAKER_05: 我认为他们在听，但我首先想问，你们是从哪里听的？
822 01:12:53,862 --> 01:13:01,551 说话人 SPEAKER_05: 再次，我看到公共部门和私营部门之间存在不对称，甚至在私营部门，你们是从哪里听的？
823 01:13:01,570 --> 01:13:06,597 说话人 SPEAKER_05: 不应该只有大公司和名人初创企业。
824 01:13:06,698 --> 01:13:10,823 说话人 SPEAKER_05: 农业部门、教育部门也有很多。
825 01:13:10,983 --> 01:13:16,630 说话人 SPEAKER_05: 那么，在所有这些噪音之后，一秒是……
826 01:13:16,609 --> 01:13:18,274 说话人 SPEAKER_05: 什么是一个好的政策？
827 01:13:18,394 --> 01:13:22,363 说话人 SPEAKER_05: 我们谈论的是监管与无监管。
828 01:13:23,045 --> 01:13:26,453 说话人 SPEAKER_05: 我实际上不知道加拿大处于什么位置。
829 01:13:26,493 --> 01:13:30,622 Speaker SPEAKER_05: 美国创新，欧洲监管。
830 01:13:30,903 --> 01:13:31,885 Speaker SPEAKER_05: 加拿大在哪里？
831 01:13:31,864 --> 01:13:32,666 Speaker SPEAKER_10: 你中间怎么样？
832 01:13:33,006 --> 01:13:33,528 Speaker SPEAKER_05: 好的，很好。
833 01:13:33,587 --> 01:13:34,770 说话人 SPEAKER_05: 真好。
834 01:13:34,789 --> 01:13:44,307 说话人 SPEAKER_05: 所以我认为我们需要激励政策、建设公共部门、释放数据的力量。
835 01:13:44,347 --> 01:13:49,738 说话人 SPEAKER_05: 我们有大量数据被锁定在我们的政府中，无论是
836 01:13:49,717 --> 01:13:59,652 说话人 SPEAKER_05: 森林火灾数据、野生动物数据、交通数据、气候数据，这些都是激励措施。
837 01:13:59,712 --> 01:14:02,815 说话人 SPEAKER_05: 然后是有益的法规。
838 01:14:03,356 --> 01:14:09,786 说话人 SPEAKER_05: 例如，我们非常直言不讳。
839 01:14:09,765 --> 01:14:12,390 说话人 SPEAKER_05: 在监管方面你必须非常小心。
840 01:14:12,411 --> 01:14:13,372 说话人 SPEAKER_05: 你在哪里进行监管？
841 01:14:13,512 --> 01:14:14,615 说话人 SPEAKER_05：上游？
842 01:14:14,655 --> 01:14:15,317 说话人 SPEAKER_05：下游？
843 01：14：15,356 --> 01：14：20,648 议长 SPEAKER_05：对我来说，最紧迫的监管点之一是橡胶与道路的交汇处。
844 01:14:21,248 --> 01:14:26,519 说话人 SPEAKER_05: 当技术现在以产品或服务的形式存在时。
845 01:14:26,960 --> 01:14:28,804 说话人 SPEAKER_05: 它将遇到人们。
846 01:14:28,783 --> 01:14:38,177 说话人 SPEAKER_05: 不论是通过医药、食品、金融服务、交通，然后你有这个当前框架。
847 01:14:38,217 --> 01:14:49,054 说话人 SPEAKER_05: 它们离完美不远，但我们需要赋予这个现有框架权力并对其进行更新，而不是
848 01:14:49,725 --> 01:14:59,060 说话人 SPEAKER_05：浪费时间，并且可能做出错误的决定，即在我们已有现有框架的情况下创建全新的监管框架。
849 01:15:00,341 --> 01:15:06,171 说话人 SPEAKER_10：好的，讨论部分的时间差不多快到了，但我们会有一个长时间的问答环节。
850 01:15:06,731 --> 01:15:11,059 说话人 SPEAKER_10：在我们开始之前，我还要问两个最后的问题。
851 01:15:11,274 --> 01:15:16,484 说话人 SPEAKER_10：一个是，我的意思是，我们认为这项技术将几乎影响一切。
852 01:15:16,664 --> 01:15:20,372 说话人 SPEAKER_10: 一些积极影响是惊人的。
853 01:15:20,993 --> 01:15:24,100 说话人 SPEAKER_10: 它将有助于治愈癌症、糖尿病和其他疾病。
854 01:15:24,399 --> 01:15:26,063 说话人 SPEAKER_10: 它将有助于缓解气候变化。
855 01:15:27,185 --> 01:15:29,288 说话人 SPEAKER_10: 有无数的事情。
856 01:15:29,310 --> 01:15:30,391 说话人 SPEAKER_10：发明新材料。
857 01:15:31,493 --> 01:15:34,279 说话人 SPEAKER_10：我看到这里有人专注于这个。
858 01:15:34,259 --> 01:15:38,445 说话人 SPEAKER_10：这可以在能源领域、航空航天和制药行业提供帮助。
859 01:15:39,226 --> 01:15:41,270 说话人 SPEAKER_10：这是多伦多大学的一个重大努力。
860 01:15:42,471 --> 01:15:48,081 说话人 SPEAKER_10: 但现在可以做到以前无法做到的整个新世界。
861 01:15:49,863 --> 01:15:57,574 说话人 SPEAKER_10: 所以这基本上是在推进科学，这在以前只是小说或想象的一部分。
862 01:15:59,257 --> 01:16:01,161 说话人 SPEAKER_10: 你对这部分乐观吗？
863 01:16:01,867 --> 01:16:03,680 说话人 SPEAKER_12: 我认为我们都非常乐观。
864 01:16:03,699 --> 01:16:07,707 说话人 SPEAKER_12: 我认为这对几乎所有领域都将产生巨大影响。
865 01:16:08,903 --> 01:16:29,561 说话人 SPEAKER_10: 所以我认为对于在座的真正在学习的人来说，这是一个非常激动人心的时刻加入其中，因为有机会限制负面影响，同时参与创造解决一些问题的机会，这些问题自从我们作为物种存在以来就一直伴随着我们。
866 01:16:29,582 --> 01:16:37,829 说话人 SPEAKER_10: 所以，我认为，至少从我们的角度来看，这真的是人类历史上最非凡的时刻之一。
867 01:16:37,810 --> 01:16:44,716 说话人 SPEAKER_10: 我希望那些即将开始职业生涯的人能够追求最宏伟的目标。
868 01:16:45,636 --> 01:16:52,382 说话人 SPEAKER_10: 你也可以优化广告和其他事情，或者制作更多 Netflix 节目，这很棒。
869 01:16:53,043 --> 01:16:54,385 说话人 SPEAKER_10: 但也是——绝对喜欢。
870 01:16:54,404 --> 01:16:54,625 说话人 SPEAKER_10: 是的。
871 01:16:56,065 --> 01:16:58,448 说话人 SPEAKER_10: 所以我妈妈也会，我觉得她已经对 Netflix 感到厌倦了。
872 01:17:00,250 --> 01:17:05,994 Speaker SPEAKER_10: 如果有土耳其语或韩语的剧集，她已经看完了最后一集。
873 01:17:05,975 --> 01:17:18,201 Speaker SPEAKER_10: 对于那些即将开始职业生涯的人来说，我的建议是尝试思考最大的挑战，以及你可以如何利用这项技术来帮助解决这些问题，这非常具有雄心壮志。
874 01:17:19,403 --> 01:17:24,895 说话人 SPEAKER_10: 你们都做到了这一点，并且一路上都在与障碍作斗争以实现这一点。
875 01:17:25,145 --> 01:17:33,159 说话人 SPEAKER_10: 这里有一屋子的人，还有很多人在线上，以及之后会看到这个视频的人，我认为，他们正处于做出这些决定的初期阶段。
876 01:17:33,980 --> 01:17:36,586 说话人 SPEAKER_10: 我猜你也会鼓励他们这样做，对吧？
877 01:17:36,645 --> 01:17:40,774 说话人 SPEAKER_10: 思考要尽可能宏大，去追求最大、最难的挑战。
878 01:17:41,395 --> 01:17:42,475 说话人 SPEAKER_05: 绝对如此。
879 01:17:42,496 --> 01:17:43,658 说话人 SPEAKER_05: 我的意思是，接受这一点。
880 01:17:43,738 --> 01:17:49,023 说话人 SPEAKER_05: 但我也鼓励，这是这项技术的新篇章。
881 01:17:49,064 --> 01:18:04,801 说话人 SPEAKER_05: 即使你自认为是技术专家和科学家，也不要忘记你内心也有一个人文主义者，因为你需要两者来为世界带来积极的变化。
882 01:18:06,662 --> 01:18:10,186 说话人 SPEAKER_10: 好的，最后一个问题，然后我们将进入观众提问环节。
883 01:18:11,500 --> 01:18:16,547 说话人 SPEAKER_10: 我们是否已经到了这些机器具有理解和智能的阶段？
884 01:18:18,610 --> 01:18:20,092 说话人 SPEAKER_05: 哇，这是最后一个问题了。
885 01:18:20,914 --> 01:18:22,797 说话人 SPEAKER_05: 我们还有多少时间？
886 01:18:24,060 --> 01:18:24,300 说话人 SPEAKER_10：是的。
887 01:18:26,323 --> 01:18:29,648 说话人 SPEAKER_10：好的，我会回到那个“是”。
888 01:18:29,667 --> 01:18:30,048 说话人 SPEAKER_05: 不。
889 01:18:41,943 --> 01:18:44,452 说话人 SPEAKER_10: 好的，观众有疑问吗？
890 01:18:46,198 --> 01:18:47,323 说话人 SPEAKER_10: 我从远端开始。
891 01:18:47,844 --> 01:18:48,567 说话人 SPEAKER_10: 你想站起来吗？
892 01:18:48,646 --> 01:18:50,755 说话人 SPEAKER_10: 你将被分配一个麦克风。
893 01:18:53,350 --> 01:18:54,572 说话人 SPEAKER_04: 嗨，谢谢。
894 01:18:54,612 --> 01:18:55,372 说话人 SPEAKER_04: 我叫艾莉。
895 01:18:56,333 --> 01:18:58,556 说话人 SPEAKER_04: 这太棒了，非常感谢。
896 01:18:59,637 --> 01:19:06,407 说话人 SPEAKER_04：Jeff，你的工作真的激励了我这个多伦多大学的学生去学习认知科学，听到你们两位的讲话真是太令人惊叹了。
897 01:19:07,668 --> 01:19:08,329 说话人 SPEAKER_04：我有一个问题。
898 01:19:08,390 --> 01:19:19,645 说话人 SPEAKER_04：你提到了教育和使大学能够赋权学生使用这项技术并学习的挑战。
899 01:19:19,625 --> 01:19:33,578 说话人 SPEAKER_04：Feifei，你也提到了这有可能成为一个尊严经济，并赋权人们专注于个性化、激情和他们的专业知识。
900 01:19:34,497 --> 01:19:47,369 说话人 SPEAKER_04：我想知道你们是否有人对过度使用和过度依赖 AI 可能带来的挑战有看法，尤其是对于孩子们和学生来说，他们在教育生涯中需要培养技能，使用他们的头脑，锻炼他们头上的“肉袋”。
901 01:19:47,350 --> 01:19:54,505 说话人 SPEAKER_04：我们的头脑如果不学习，就不会继续工作，也不会积累灰尘。
902 01:19:54,564 --> 01:20:04,667 说话人 SPEAKER_04：我们的头脑如果不学习，就不会继续工作，也不会积累灰尘。
903 01:20:04,646 --> 01:20:19,890 说话人 SPEAKER_04：是的，我想听听你对倦怠、过度依赖以及当你可以用稳定扩散来绘画时如何学习绘画，或者当你可以用 ChatGBT 帮你写时如何学习像莎士比亚一样写作的看法。
904 01:20:20,291 --> 01:20:29,625 说话人 SPEAKER_04：然后随着这些系统的发展，能够积累更深入的见解和更复杂的解决问题的能力，这将如何影响我们做同样事情的能力。
905 01:20:29,993 --> 01:20:38,123 说话人 SPEAKER_12：关于这一点，我有一个非常小的想法，那就是当袖珍计算器首次出现时，人们说孩子们会忘记如何做算术。
906 01:20:39,204 --> 01:20:41,649 说话人 SPEAKER_12：结果并没有成为大问题。
907 01:20:41,729 --> 01:20:45,073 说话人 SPEAKER_12：我认为孩子们可能确实忘记了如何做算术，但他们有了袖珍计算器。
908 01:20:46,654 --> 01:20:51,702 说话人 SPEAKER_12: 但这可能不是一个很好的类比，因为袖珍计算器并不比它们聪明。
909 01:20:51,962 --> 01:20:56,287 说话人 SPEAKER_12: 孩子们可能会忘记做算术，然后去做真正的数学。
910 01:20:56,268 --> 01:20:57,869 说话人 SPEAKER_12: 但对于这些东西，我不知道。
911 01:20:58,871 --> 01:21:07,640 说话人 SPEAKER_12: 对于我自己来说，我发现这实际上让我对世界更加好奇，因为我无法忍受去图书馆花半小时找相关书籍并查找信息。
912 01:21:08,282 --> 01:21:15,671 说话人 SPEAKER_12: 现在我可以随便问 ChatGPT 任何问题，它会告诉我答案，我会相信它，这可能不是正确的事情。
913 01:21:17,432 --> 01:21:21,077 说话人 SPEAKER_12: 但这实际上让我对世界更加好奇，因为我可以更快地得到答案。
914 01:21:27,082 --> 01:21:31,506 说话人 SPEAKER_12: 是的，但通常我会问关于管道和类似的问题。
915 01:21:33,157 --> 01:21:36,640 说话人 SPEAKER_05: 所以我会用一个非常快的故事来回答这个问题。
916 01:21:36,661 --> 01:21:37,801 说话人 SPEAKER_05: 我不知道你们。
917 01:21:37,902 --> 01:21:41,947 说话人 SPEAKER_05: 自从我成为斯坦福大学教授以来，我总是如此好奇。
918 01:21:41,987 --> 01:21:48,134 说话人 SPEAKER_05: 大学里有一个神秘的办公室，那就是招生办公室。
919 01:21:48,635 --> 01:21:50,756 说话人 SPEAKER_05: 对我来说，他们是最神秘的人。
920 01:21:50,877 --> 01:21:58,305 说话人 SPEAKER_05：我从来不知道他们在哪里，他们是谁，他们坐在哪里，直到今年年初接到一个电话。
921 01:21:58,746 --> 01:22:02,690 说话人 SPEAKER_05：当然，他们想和我谈谈 Chad GPT 和大学招生。
922 01:22:02,671 --> 01:22:09,438 说话人 SPEAKER_05：当然，这个问题与，我们在申请过程中是否允许这样做有关？
923 01:22:09,518 --> 01:22:13,863 说话人 SPEAKER_05：现在有了 ChatGPT，我们该如何进行招生？
924 01:22:14,302 --> 01:22:18,527 说话人 SPEAKER_05：所以我回家了，我和我的 11 岁孩子聊天。
925 01:22:18,768 --> 01:22:23,432 说话人 SPEAKER_05：我说，嗯，我接到一个电话，有一个大学入学的问题。
926 01:22:25,213 --> 01:22:27,716 说话人 SPEAKER_05：我们该如何处理 ChatGPT 和
927 01:22:27,697 --> 01:22:35,690 说话人 SPEAKER_05：还有学生，如果一个学生用 ChatGPT 写出了最好的申请，我们应该使用它吗，等等等等。
928 01:22:35,711 --> 01:22:37,634 说话人 SPEAKER_05: 然后我说，你会怎么做？
929 01:22:37,895 --> 01:22:41,780 说话人 SPEAKER_05: 我问了我的 11 岁孩子，他说，让我想想。
930 01:22:41,881 --> 01:22:46,729 说话人 SPEAKER_05: 他实际上回去睡觉思考这个问题，或者我不知道发生了什么。
931 01:22:46,770 --> 01:22:50,716 说话人 SPEAKER_05: 第二天早上，他说，我有一个答案。
932 01:22:51,153 --> 01:22:52,073 说话人 SPEAKER_05: 我说，你的答案是什么？
933 01:22:52,234 --> 01:23:00,268 说话人 SPEAKER_05: 他说，我认为斯坦福应该录取那些最能使用 Chat GPT 的前 2,000 名学生。
934 01:23:02,150 --> 01:23:05,555 说话人 SPEAKER_05: 实际上，一开始我觉得这个答案很愚蠢，对吧？
935 01:23:06,037 --> 01:23:11,966 说话人 SPEAKER_05: 实际上这是一个很有趣的答案，因为孩子们已经把它看作是一种工具。
936 01:23:13,068 --> 01:23:18,476 说话者 SPEAKER_05：他们将这个工具视为一种赋能、赋权的工具。
937 01:23:18,457 --> 01:23:23,505 说话者 SPEAKER_05：显然，我的 11 岁孩子根本不知道如何衡量这一点，也不知道这意味着什么，等等。
938 01:23:23,905 --> 01:23:28,554 说话者 SPEAKER_05：但我认为我们应该这样看待教育，我们应该更新我们的教育。
939 01:23:28,594 --> 01:23:33,381 说话者 SPEAKER_05：我们不能像 Jeff 所说的那样将这个工具排除在我们的教育之外。
940 01:23:33,721 --> 01:23:40,092 说话人 SPEAKER_05：我们需要拥抱它，并教育人类，让他们知道如何利用这个工具为自己谋利。
941 01:23:40,511 --> 01:23:43,055 说话人 SPEAKER_10：顺便说一句，我见过 Fei-Fei 11 岁的儿子。
942 01:23:43,635 --> 01:23:45,657 说话人 SPEAKER_10：他到 18 岁的时候可能已经是斯坦福大学的校长了。
943 01:23:45,697 --> 01:23:48,279 说话人 SPEAKER_05：如果斯坦福大学还存在的话。
944 01:23:50,242 --> 01:23:53,185 说话人 SPEAKER_10: 让我们去房间远角这边。
945 01:24:00,832 --> 01:24:05,176 说话人 SPEAKER_11: 我想问一下，我们现在已经有了非常好的基础模型。
946 01:24:05,197 --> 01:24:10,502 说话人 SPEAKER_11: 但在许多应用中，我们需要模型的真实时性能。
947 01:24:11,055 --> 01:24:23,476 说话人 SPEAKER_11: 你如何看待这个领域未来的发展，这个研究领域未来的发展，你知道，利用这个专家基础模型的能力来训练，你知道，快速、更小的模型？
948 01:24:24,458 --> 01:24:27,663 说话人 SPEAKER_11: 提一个问题。
949 01:24:28,420 --> 01:24:30,443 说话人 SPEAKER_05: 好吧，你在谈论推理，对吧？
950 01:24:30,503 --> 01:24:48,132 说话人 SPEAKER_05: 我们需要开始考虑性能、推理，以及根据不同设备调整模型，嗯，意思是不深入技术细节，所有这些研究，还有，你知道的，即使在研究之外，这些也在发生。
951 01:24:48,453 --> 01:24:50,917 说话人 SPEAKER_05: 是的，你想谈论什么？
952 01:24:51,926 --> 01:24:53,670 说话人 SPEAKER_05: 我想，好吧，你不想说话。
953 01:24:55,412 --> 01:24:55,613 说话人 SPEAKER_05: 好吧。
954 01:24:55,854 --> 01:24:55,953 说话人 SPEAKER_05: 好吧。
955 01:24:55,974 --> 01:25:00,681 说话人 SPEAKER_05: 正在发生，但我的意思是，这需要一段时间，但是的。
956 01:25:00,702 --> 01:25:02,243 说话人 SPEAKER_10：我们谈论他投资的事情。
957 01:25:03,506 --> 01:25:04,667 说话人 SPEAKER_06：那是对的。
958 01:25:04,688 --> 01:25:08,194 说话人 SPEAKER_10：我不能说，直到公司说可以谈论。
959 01:25:08,213 --> 01:25:10,117 说话人 SPEAKER_10：没关系。
960 01:25:10,137 --> 01:25:11,038 说话人 SPEAKER_10: 让我们回到中间。
961 01:25:12,121 --> 01:25:12,822 说话人 SPEAKER_10: 就在这里。
962 01:25:23,314 --> 01:25:24,576 说话人 SPEAKER_08: 嗯，你好，我叫 Ariel。
963 01:25:24,615 --> 01:25:28,381 说话人 SPEAKER_08: 我是在多伦多大学攻读机器学习专业的三年级跨学科学生。
964 01:25:28,761 --> 01:25:30,443 说话人 SPEAKER_08: 然后，那次对话相当精彩。
965 01:25:30,483 --> 01:25:32,706 说话人 SPEAKER_08: 然后，谢谢您，Hinton 教授和李教授。
966 01:25:33,225 --> 01:25:38,391 说话人 SPEAKER_08: 我有一个问题，可能很多本科生或研究生都感兴趣，就在这个房间里。
967 01:25:38,511 --> 01:25:49,203 说话人 SPEAKER_08: 就像您在 20 多岁时一样，是什么驱使您成为研究人员，是什么驱使您进入学术界和人工智能领域？
因为我现在有点困惑，我应该继续从事行业，还是直接申请博士，或者先读硕士然后再回到行业。我还有一个问题，通常你们在申请时都会寻找什么？
969 01：26：05,603 --> 01：26：09,006 演讲者 SPEAKER_08：就像直接进入你实验室的博士吗？
970 01：26：09,667 --> 01：26：12,751 演讲者 SPEAKER_08：这就像 GPA 或出版物或推荐信吗？
971 01：26：12,770 --> 01：26：15,333 议长 SPEAKER_08：您能详细解释一下吗？
972 01:26:15,354 --> 01:26:15,833 说话人 SPEAKER_08: 谢谢。
973 01:26:16,074 --> 01:26:21,000 说话人 SPEAKER_10: 我想房间里大约有 300 人，在线上大约有 6000 人想向您提问，Fei-Fei。
974 01:26:21,380 --> 01:26:22,100 说话人 SPEAKER_05: 您想开始吗？
975 01:26:23,282 --> 01:26:25,465 说话人 SPEAKER_05: 您二十多岁。
976 01:26:25,484 --> 01:26:31,070 说话人 SPEAKER_12: 哦，我小时候对大脑的工作原理产生了兴趣，因为我有一个非常聪明的朋友在学校
977 01:26:32,265 --> 01:26:36,932 说话人 SPEAKER_12: 有一天来到学校，谈论了全息图以及大脑中的记忆可能像全息图一样
978 01:26:37,873 --> 01:26:39,176 说话人 SPEAKER_12: 我基本上说，什么是全息图？
979 01:26:40,037 --> 01:26:43,582 说话人 SPEAKER_12: 从那时起，我就对大脑的工作原理产生了兴趣。
980 01:26:45,425 --> 01:26:48,770 说话人 SPEAKER_12: 非常幸运地在学校有一个非常聪明的朋友。
981 01:26:51,052 --> 01:26:57,042 说话人 SPEAKER_05: 如果您读了我的书，我会非常无耻地告诉您，这正是书的内容。
982 01:26:58,810 --> 01:26:59,890 说话人 SPEAKER_12: 这是一本非常好的书。
983 01:27:00,872 --> 01:27:01,391 说话人 SPEAKER_05: 谢谢。
984 01:27:01,412 --> 01:27:02,052 说话人 SPEAKER_05: 说真的。
985 01:27:02,113 --> 01:27:07,118 说话人 SPEAKER_05: 事实上，我告诉乔丹和杰夫，关于技术的 AI 书籍有很多。
986 01:27:07,217 --> 01:27:20,789 说话人 SPEAKER_05: 当我开始写关于 AI 技术的这本书时，我想写一段旅程，特别是写给年轻人，特别是写给各行各业年轻人的旅程，而不仅仅是某种特定的人。
987 01:27:21,371 --> 01:27:27,095 说话人 SPEAKER_05: 那本书讲述了年轻人的旅程。
988 01:27:27,076 --> 01:27:30,859 说话人 SPEAKER_05：女孩，你知道，在不同的环境中。
989 01:27:31,564 --> 01:27:36,411 说话人 SPEAKER_05：意识到或理解自己的梦想，并实现梦想。
990 01:27:36,530 --> 01:27:39,135 说话人 SPEAKER_05：这与 Jeff 所说的并没有太大区别。
991 01:27:39,475 --> 01:27:40,657 说话人 SPEAKER_05：它始于激情。
992 01:27:41,537 --> 01:27:45,703 说话人 SPEAKER_05: 这确实是从一种激情开始的，一种对抗所有其他声音的激情。
993 01:27:45,743 --> 01:27:56,658 说话人 SPEAKER_05: 这种激情可能来自一个朋友，可能来自你看到的一部电影，可能来自你读的一本书，或者可能来自你在学校里觉得最有乐趣的最佳科目，无论是什么。
994 01:27:57,679 --> 01:28:00,002 说话人 SPEAKER_05: 而在我雇佣的学生中，
995 01:28:02,158 --> 01:28:15,197 说话人 SPEAKER_05: 我寻找这种激情，我寻找野心，一种想要改变世界的健康野心，而不是仅仅为了获得学位。
996 01:28:15,216 --> 01:28:25,229 说话人 SPEAKER_05：当然，从技术角度来说，我寻找的是良好的技术背景，而不仅仅是考试成绩。
997 01:28:25,582 --> 01:28:29,670 说话人 SPEAKER_05：老实说，我从未想过自己会进入自己的实验室。
998 01:28:33,055 --> 01:28:34,979 说话人 SPEAKER_05：现在的标准如此之高。
999 01:28:35,060 --> 01:28:42,573 说话人 SPEAKER_05：所以当你申请博士或研究生项目时，你可能已经有了一些成绩记录。
1000 01:28:43,095 --> 01:28:43,314 说话人 SPEAKER_05: 一些。
1001 01:28:43,756 --> 01:28:45,378 说话人 SPEAKER_05: 不必。
1002 01:28:45,359 --> 01:28:51,104 说话人 SPEAKER_05: 当然，如果是 Jeff 的学生，我甚至会不问问题就收他们。
1003 01:28:51,123 --> 01:28:58,550 说话人 SPEAKER_05: 但是即使你，我说的不仅仅是 U of T 的学生，对每个在线学生来说，你们可能有非常不同的背景。
1004 01:28:58,670 --> 01:29:01,212 说话人 SPEAKER_05: 你可以来自一个弱势背景。
1005 01:29:01,233 --> 01:29:05,716 说话人 SPEAKER_05: 我所看重的不是你的起点，而是你走过的路。
1006 01:29:05,997 --> 01:29:10,381 说话人 SPEAKER_05: 这条记录展示了你走过的路，展示了你的热情和信念。
1007 01:29:11,561 --> 01:29:14,404 说话人 SPEAKER_10: 读了这本书后，我想说，它是，
1008 01:29:15,295 --> 01:29:19,399 说话人 SPEAKER_10: 我认为，对于将要阅读它的大多数人来说，这是一段非常令人惊讶的旅程。
1009 01:29:20,340 --> 01:29:23,403 说话人 SPEAKER_10: 顺便提一下，如果你在加拿大，就去 Indigo 买吧。
1010 01:29:23,762 --> 01:29:26,284 说话人 SPEAKER_10: 你可以访问 indigo.ca 并预订这本书。
1011 01:29:29,707 --> 01:29:35,493 说话人 SPEAKER_10: 我认为人们会感到惊讶，并且会真正享受阅读和理解这段经历。
1012 01:29:35,514 --> 01:29:38,275 说话人 SPEAKER_10: 回答这个问题，你会得到一个非常好的理解。
1013 01:29:38,596 --> 01:29:38,896 说话人 SPEAKER_07: 谢谢。
1014 01:29:39,497 --> 01:29:41,019 说话人 SPEAKER_10: 好的，大约有 50 只手举起来了。
1015 01:29:41,880 --> 01:29:44,442 说话人 SPEAKER_10: 好吧，我们到角落这边来。
1016 01:29:51,002 --> 01:29:52,286 说话人 SPEAKER_09：谢谢您精彩的演讲。
1017 01:29:52,567 --> 01:29:53,168 说话人 SPEAKER_09：我叫 Shalev。
1018 01:29:53,208 --> 01:29:54,292 说话人 SPEAKER_09：我在 Vector Institute。
1019 01:29:54,632 --> 01:29:55,676 说话人 SPEAKER_09：我们要去见 Sheila McIlrath。
1020 01:29:57,180 --> 01:29:59,846 说话人 SPEAKER_09: 我认为基准非常重要。
1021 01:30:00,067 --> 01:30:01,350 说话人 SPEAKER_09: 基准就像问题一样。
1022 01:30:01,411 --> 01:30:05,922 说话人 SPEAKER_09: ImageNet 基本上是一个问题，然后人们试图用模型来回答它。
1023 01:30:06,055 --> 01:30:08,797 说话人 SPEAKER_09: 因此，现在LLMs非常难以评估。
1024 01:30:08,978 --> 01:30:14,024 说话人 SPEAKER_09：那么，对于采取行动的通用智能体来说，甚至很难开始思考如何评估它们。
1025 01:30:14,703 --> 01:30:17,027 说话人 SPEAKER_09：我的问题是关于问题的。
1026 01:30:17,127 --> 01:30:18,207 说话人 SPEAKER_09：这是关于这些基准的。
1027 01:30:18,748 --> 01:30:19,409 说话人 SPEAKER_09：所以有两件事。
如果和 GPT-5、GPT-6、GPT-7 坐下来，你有五分钟时间与之互动，你会问哪些问题来判断这是这些模型的下一代？
1029 01:30:30,862 --> 01:30:33,885 说话者 SPEAKER_09：第二个则是一个更全面的基准。
我们需要一个更全面的、不止五分钟的基准，以便评估LLMs或通用智能体？
1031 01：30：40,091 --> 01：30：43,194 演讲者 SPEAKER_09：你可以选择你想想的，我想，思考或回答哪一个。
1032 01:30:43,914 --> 01:30:44,114 说话人 SPEAKER_12: 好吧。
1033 01:30:44,135 --> 01:30:44,395 说话人 SPEAKER_12: 谢谢。
1034 01:30:44,836 --> 01:30:45,756 说话人 SPEAKER_12: 谢谢你的问题。
1035 01:30:45,796 --> 01:30:46,697 说话人 SPEAKER_12: 这是一个非常好的问题。
1036 01:30:47,118 --> 01:30:50,100 说话人 SPEAKER_12：我将回答一个与之略有相关的问题。
1037 01:30:50,140 --> 01:30:53,885 说话人 SPEAKER_12：所以这个问题是在 GPT-4 中出现的。
1038 01:30:54,664 --> 01:30:56,447 说话人 SPEAKER_12：你怎么判断它是否聪明？
1039 01:30:57,068 --> 01:31:11,586 说话人 SPEAKER_12：特别是，我正在和一个名叫赫克托·莱维斯克的人交谈，他曾是计算机科学系的教师，他的信念几乎与我的完全相反，但他非常诚实。
1040 01:31:12,587 --> 01:31:19,555 说话人 SPEAKER_12: 他对 GPT-4 能工作感到很惊讶，想知道它是如何可能工作的。
1041 01:31:20,036 --> 01:31:21,277 说话人 SPEAKER_12: 因此，我们花了一些时间讨论这个问题。
1042 01:31:21,837 --> 01:31:26,703 说话人 SPEAKER_12: 然后我让他给我一些问题去问它。
1043 01:31:27,157 --> 01:31:32,636 说话人 SPEAKER_12: 他给了我一系列问题去问它，这样我们就可以判断它是否理解了。
1044 01:31:32,676 --> 01:31:35,185 说话人 SPEAKER_12：那么问题是，它真的理解自己在说什么吗？
1045 01:31:35,586 --> 01:31:38,938 说话人 SPEAKER_12：还是它只是在用一些花哨的统计方法来预测下一个词？
1046 01:31:39,863 --> 01:31:47,091 说话人 SPEAKER_12：关于这一点的一个评论是，唯一能够很好地预测下一个词的方法是理解说话人说了什么。
1047 01:31:47,452 --> 01:31:49,293 说话人 SPEAKER_12：所以你必须理解才能预测。
1048 01:31:49,795 --> 01:31:51,657 说话人 SPEAKER_12: 但你可以在不理解的情况下预测得相当好。
1049 01:31:51,676 --> 01:31:53,559 说话人 SPEAKER_12: 那么 GPT-4 真的是理解了吗？
1050 01:31:54,220 --> 01:32:00,568 说话人 SPEAKER_12: 所以赫克托提出的问题是我的房子里的房间都是白色、黄色或蓝色的。
1051 01:32:01,609 --> 01:32:04,212 说话人 SPEAKER_12: 我希望所有的房间都是白色的。
1052 01:32:04,292 --> 01:32:05,212 说话人 SPEAKER_12: 我该怎么办？
1053 01:32:06,390 --> 01:32:09,516 说话人 SPEAKER_12: 我知道它能够做到，所以我把问题问得更难了。
1054 01:32:10,238 --> 01:32:13,003 说话人 SPEAKER_12: 所以我说，我家的房间都是涂成白色、黄色或蓝色的。
1055 01:32:13,043 --> 01:32:16,530 说话人 SPEAKER_12: 黄色油漆在一年内会褪成白色。
1056 01:32:16,570 --> 01:32:19,596 说话人 SPEAKER_12: 两年后，我想让所有房间都变成白色。
1057 01:32:19,636 --> 01:32:20,838 说话人 SPEAKER_12: 我该做什么？
1058 01:32:21,172 --> 01:32:23,496 说话人 SPEAKER_12: 哦，我说了并且为什么。
1059 01:32:23,756 --> 01:32:25,398 说话人 SPEAKER_12: 如果你说了并且为什么，它就会给出解释。
1060 01:32:25,417 --> 01:32:26,698 说话人 SPEAKER_12: ChatGPT 解决了这个问题。
1061 01:32:26,738 --> 01:32:29,783 说话人 SPEAKER_12: 它说你应该把蓝色的房间刷成白色。
1062 01:32:30,564 --> 01:32:34,427 说话人 SPEAKER_12: 它说你不需要担心黄色的房间，因为它们会逐渐变成白色。
1063 01:32:35,208 --> 01:32:37,030 说话人 SPEAKER_12: 结果它对措辞非常敏感。
1064 01:32:37,310 --> 01:32:43,318 说话人 SPEAKER_12：如果你不用渐变，但用变化，有人向我投诉说，我试了，但没效果。
1065 01:32:43,779 --> 01:32:46,101 说话人 SPEAKER_12：他们用的是变化而不是渐变。
1066 01:32:46,122 --> 01:32:50,926 说话人 SPEAKER_12：关键是，我们理解渐变是指改变颜色并保持改变。
1067 01:32:51,159 --> 01:32:54,786 说话人 SPEAKER_12：但如果你说变化，颜色会改变，但也可能变回原样。
1068 01:32:55,266 --> 01:32:57,350 说话人 SPEAKER_12：所以如果你改变而不是渐变，它不会给出相同的答案。
1069 01:32:57,369 --> 01:32:58,492 说话人 SPEAKER_12：它对措辞非常敏感。
1070 01:33:01,015 --> 01:33:03,279 说话人 SPEAKER_12：但这确实让我相信它真的理解了。
1071 01:33:03,340 --> 01:33:05,323 说话人 SPEAKER_12：而且它还做了其他事情。
1072 01:33:06,003 --> 01:33:12,914 说话人 SPEAKER_12: 最近人们提出了一个很好的问题，许多聊天机器人都没有回答正确。
1073 01:33:13,317 --> 01:33:21,207 说话人 SPEAKER_12: 而有些人也没有回答正确，但 GPT-4 回答正确，那就是... 所以，你看，我正在回答问题，GPT-4 是否理解？
1074 01:33:21,469 --> 01:33:24,073 说话人 SPEAKER_12：这与您问的问题有些关联，对吧？
1075 01:33:26,815 --> 01:33:28,439 说话人 SPEAKER_12：所以，问题是这样的。
1076 01:33:29,380 --> 01:33:30,601 说话人 SPEAKER_12：莎莉有三个兄弟。
1077 01:33:31,923 --> 01:33:33,787 说话人 SPEAKER_12：她的每个兄弟都有两个姐妹。
1078 01:33:34,587 --> 01:33:36,010 Speaker SPEAKER_12: Sally 有多少个姐妹？
1079 01:33:37,131 --> 01:33:38,432 Speaker SPEAKER_12: 而大多数聊天机器人都会答错。
1080 01:33:40,697 --> 01:33:41,738 Speaker SPEAKER_05: 那人类呢？
1081 01:33:42,088 --> 01:33:50,801 Speaker SPEAKER_12: 嗯，我刚刚在拉斯维加斯做了一场炉边谈话，采访者问我举一个聊天机器人答错的例子。
1082 01:33:51,141 --> 01:33:54,266 说话人 SPEAKER_12: 我给他举了这个例子，他说六。
1083 01:33:56,369 --> 01:33:58,393 说话人 SPEAKER_12: 那有点尴尬。
1084 01:33:58,752 --> 01:34:00,114 说话人 SPEAKER_05: 我们不会问他的名字。
1085 01:34:00,376 --> 01:34:01,216 说话人 SPEAKER_05: 不，开个玩笑。
1086 01:34:01,237 --> 01:34:01,436 说话人 SPEAKER_12: 不。
1087 01:34:02,998 --> 01:34:04,180 说话人 SPEAKER_12: 所以人们弄错了。
1088 01:34:04,341 --> 01:34:07,506 说话人 SPEAKER_12: 是的。
1089 01:34:07,704 --> 01:34:12,011 说话人 SPEAKER_12: 我认为如果不能进行一定程度的推理，是无法做对的。
1090 01:34:12,091 --> 01:34:13,233 说话人 SPEAKER_12: 它必须建立某种模型。
1091 01:34:13,814 --> 01:34:21,426 说话人 SPEAKER_12: 安德鲁·吴（Andrew Ng）有一些例子，比如玩奥赛罗棋（Othello），即使只给它字符串作为输入，它也能在内部建立棋盘的模型。
1092 01:34:22,728 --> 01:34:24,250 说话人 SPEAKER_12: 所以我认为他们确实理解。
1093 01:34:25,972 --> 01:34:30,259 说话人 SPEAKER_10: 再进一步，这种理解是否跨越了智能的界限？
1094 01:34:31,121 --> 01:34:32,042 说话人 SPEAKER_10: 你说对了。
1095 01:34:32,949 --> 01:34:33,229 说话人 SPEAKER_12: 是的。
1096 01:34:33,671 --> 01:34:35,953 说话人 SPEAKER_12: 我的意思是，我接受智能的图灵测试。
1097 01:34:36,253 --> 01:34:40,399 说话人 SPEAKER_12: 当我们通过图灵测试时，人们才开始拒绝它。
1098 01:34:40,420 --> 01:34:42,662 说话人 SPEAKER_10: 那就是我之前提到的移动的底线。
1099 01:34:43,623 --> 01:34:43,904 说话人 SPEAKER_10: 好的。
1100 01:34:44,284 --> 01:34:44,865 说话人 SPEAKER_10: 你想回答吗？
1101 01:34:44,905 --> 01:34:46,849 说话人 SPEAKER_05: 我想快速回答。
1102 01:34:46,868 --> 01:34:49,811 说话人 SPEAKER_05：首先，也要为提出这样一个好问题而鼓掌。
1103 01:34:49,893 --> 01:34:51,734 说话人 SPEAKER_05：我将回答
1104 01:34:51,884 --> 01:35:04,597 说话人 SPEAKER_05：除了 Jeff 的，因为我认为 Jeff 试图推动的是如何评估这些大型模型的基本智能水平。
1105 01:35:04,958 --> 01:35:06,619 说话人 SPEAKER_05：但还有其他几个维度。
1106 01:35:07,280 --> 01:35:15,430 讲者 SPEAKER_05：一个是，再次，斯坦福大学 HAI 基础模型研究中心正在创建这些评估指标，对吧？
1107 01:35:15,449 --> 01:35:17,131 讲者 SPEAKER_05：你可能正在阅读这篇论文。
1108 01:35:17,112 --> 01:35:19,255 讲者 SPEAKER_05：Percy Helm 等人的论文。
1109 01:35:19,916 --> 01:35:29,208 讲者 SPEAKER_05：我认为这项技术已经非常深入，一些基准测试比你想的 ImageNet 基准测试要复杂得多。
1110 01：35：29,509 --> 01：35：39,002 演讲者 SPEAKER_05：例如，现在与政府合作，例如 NIST，美国国家标准研究所
1111 01:35:38,981 --> 01:35:39,903 说话者 SPEAKER_05：T 是什么？
1112 01：35：40,224 --> 01：35：40,864 演讲者 SPEAKER_05：技术。
1113 01:35:40,885 --> 01:35:42,067 说话者 SPEAKER_05：关于技术。
1114 01:35:42,688 --> 01:35:43,649 说话人 SPEAKER_05: 测试或其他。
1115 01:35:44,009 --> 01:35:54,204 说话人 SPEAKER_05: 你知道，我们需要开始对社会相关问题的基准测试，而不仅仅是核心基本能力。
1116 01:35:54,685 --> 01:36:02,577 说话人 SPEAKER_05: 我还想再提一点，就是除了LLMs之外，还有很多关于人工智能未来的方向，我们还没有建立好的基准测试。
1117 01:36:02,859 --> 01:36:10,912 说话人 SPEAKER_05: 我们实际上还没有为它们建立好的基准测试。
1118 01:36:11,052 --> 01:36:14,118 说话人 SPEAKER_05: 我的意思是，我的实验室正在做一些机器人学习的研究。
1119 01:36:14,418 --> 01:36:18,265 说话人 SPEAKER_05: 谷歌昨天刚刚发布了关于机器人学习的论文。
1120 01:36:18,545 --> 01:36:22,692 说话人 SPEAKER_05: 所以在这个领域将有更多的研究出现。
1121 01:36:24,341 --> 01:36:26,104 说话人 SPEAKER_10: 好的，我知道网上有很多问题。
1122 01:36:26,364 --> 01:36:32,315 说话人 SPEAKER_10: 我可能再从房间里拿几个，然后也许可以由 Radical 的人读出几个在线的问题。
1123 01:36:33,497 --> 01:36:39,608 说话人 SPEAKER_10: 好的，在房间里，我们找一个离上一个不远的问题。
1124 01:36:39,768 --> 01:36:41,110 说话人 SPEAKER_10: 就在这里。
1125 01:36:46,338 --> 01:36:48,443 说话人 SPEAKER_10: 这里是麦克风来了。
1126 01:36:52,996 --> 01:37:00,993 说话人 SPEAKER_02：你好，我是 Vishwam，我是圭尔夫大学的硕士研究生，我的论文主题是人工智能与农业。
1127 01:37:01,474 --> 01:37:11,814 说话人 SPEAKER_02：那么，基于你提到的大学没有足够的资金来训练基础模型，对吧？
1128 01:37:11,795 --> 01:37:12,716 说话人 SPEAKER_02：同样的问题。
1129 01:37:12,775 --> 01:37:15,139 说话人 SPEAKER_02：所以我想要在人工智能与农业领域工作。
1130 01:37:15,760 --> 01:37:16,640 说话人 SPEAKER_02: 我对此充满热情。
1131 01:37:16,961 --> 01:37:19,243 说话人 SPEAKER_02: 但我没有足够的资源去做这件事。
1132 01:37:19,724 --> 01:37:23,548 说话人 SPEAKER_02：我可能想出一个非常好的架构，但我无法训练它。
1133 01:37:24,170 --> 01:37:28,054 说话人 SPEAKER_02：所以也许我可以去工业界，然后向他们推销这个想法。
1134 01:37:28,135 --> 01:37:30,556 说话人 SPEAKER_02：然后我就无法控制这个想法了。
1135 01:37:31,037 --> 01:37:33,020 说话人 SPEAKER_02：我不知道他们将如何应用它。
1136 01:37:33,560 --> 01:37:38,466 说话人 SPEAKER_02: 你有什么处理这种情况的建议吗？
1137 01:37:39,228 --> 01:37:41,109 说话人 SPEAKER_10: 你能帮我吗？
1138 01:37:41,090 --> 01:37:41,992 说话人 SPEAKER_10: 你是一家初创公司。
1139 01:37:42,833 --> 01:37:43,997 说话人 SPEAKER_10: 我们就是为此而来的。
1140 01:37:44,056 --> 01:37:46,481 说话人 SPEAKER_10: 对不起，让我来回答。
1141 01:37:46,962 --> 01:37:57,006 说话人 SPEAKER_12: 如果你能获得一个开源的基础模型，你可以用比构建模型更少的资源来微调这些模型。
1142 01:37:57,025 --> 01:37:59,993 说话人 SPEAKER_12: 因此，大学仍然可以对这些模型进行微调。
1143 01:38:01,119 --> 01:38:13,037 说话人 SPEAKER_05: 这是目前一个非常务实的回答，但我们一直在与高等教育领导者和政策制定者交谈，投资于公共部门。
1144 01:38:13,377 --> 01:38:16,060 说话人 SPEAKER_05：我们必须建立一个国家级的研究云。
1145 01:38:16,442 --> 01:38:20,427 说话人 SPEAKER_05：我不知道加拿大是否有国家级的研究云，但我们正在推动美国。
1146 01:38:20,927 --> 01:38:22,791 说话人 SPEAKER_05：我们需要引入
1147 01:38:22,770 --> 01:38:29,082 说话人 SPEAKER_05：像您这样的研究人员能够访问国家级研究云。
1148 01:38:29,261 --> 01:38:45,048 说话人 SPEAKER_05: 但作为一家公司之外的优势，您有更多机会接触到独特的数据集，特别是对公共福利的数据集，并发挥这一优势。
1149 01:38:45,029 --> 01:38:55,800 说话人 SPEAKER_05: 您可以与政府机构或社区等合作，因为公共部门仍然享有信任，利用这一点。
1150 01:38:55,819 --> 01:38:59,224 说话人 SPEAKER_05: 但目前，是的，对开源模型进行微调。
1151 01:39:00,145 --> 01:39:00,625 说话人 SPEAKER_05: 非常感谢您。
1152 01:39:01,105 --> 01:39:02,648 说话人 SPEAKER_10: 好的，我们将回答几个问题。
1153 01:39:02,688 --> 01:39:11,577 说话人 SPEAKER_10: 现在有数千人在网上观看，包括斯坦福和其他地方的观看派对，让我们看看是否能从网上的人那里得到一个问题。
1154 01:39:17,328 --> 01:39:19,716 说话人 SPEAKER_10: 莉亚将代表一位在线观众提出这个问题。
1155 01:39:19,796 --> 01:39:24,051 说话人 SPEAKER_10: 顺便说一句，她为此付出了巨大的努力，还有阿伦·布林德（Aaron Brindle）。
1156 01:39:24,091 --> 01:39:25,617 说话人 SPEAKER_10: 非常感谢你们两位。
1157 01:39:31,856 --> 01:39:32,658 说话人 SPEAKER_03: 好的，谢谢。
1158 01:39:32,679 --> 01:39:38,953 说话人 SPEAKER_03: 所以我们在线上有数百名 AI 研究人员，他们是那些以 AI 为核心的公司的人。
1159 01:39:39,092 --> 01:39:43,422 说话人 SPEAKER_03: 因此，最受欢迎的第一个问题是来自本·桑德斯，或者桑德斯。
1160 01:39:44,064 --> 01:39:50,018 说话人 SPEAKER_03：他现在是人工智能初创公司的首席执行官，他的同事实际上是 2008 年 Geoffrey Hinton 的学生。
1161 01:39:49,997 --> 01:39:53,064 说话人 SPEAKER_03：他询问了关于负责任地构建的问题。
1162 01:39:53,083 --> 01:39:55,568 说话人 SPEAKER_03：这些问题中的许多都与负责任地构建有关。
1163 01:39:56,208 --> 01:40:04,545 说话人 SPEAKER_03：他们在思考哪些措施可以帮助他们作为团队成为好的而不是坏的监护人，以及成为监护人的真正含义是什么。
1164 01:40:07,250 --> 01:40:09,052 说话人 SPEAKER_05：好问题。
1165 01:40:10,265 --> 01:40:21,856 说话人 SPEAKER_05：负责任的 AI 框架，有很多框架，我认为几年前有人估计，从国家到企业，大约有 300 个框架。
1166 01:40:21,877 --> 01:40:29,904 说话人 SPEAKER_05：我认为对每家公司来说，建立一个负责任的框架非常重要。
1167 01:40:29,925 --> 01:40:34,770 说话人 SPEAKER_05：你可以借鉴很多东西，甚至 Radical 也在做。
1168 01:40:34,750 --> 01:40:47,305 说话者 SPEAKER_05：构建一个并创建你相信的价值框架，并认识到人工智能产品是一个系统。
1169 01:40:47,425 --> 01:41:03,103 说话者 SPEAKER_05：所以从上游定义问题、数据集、数据完整性、你如何构建模型、部署以及创建一个多利益相关者生态系统或多利益相关者
1170 01:41:03,082 --> 01:41:11,792 说话者 SPEAKER_05：无论哪个团队帮助你构建这个负责任的框架，并建立合作伙伴关系。
1171 01:41:11,912 --> 01:41:24,185 说话者 SPEAKER_05：与公共部门如学术界如我们这样的合作伙伴关系，与关心不同维度从隐私到偏见到这一点的民间社会合作伙伴关系。
1172 01:41:24,706 --> 01:41:27,588 说话人 SPEAKER_05: 所以真正尝试去
1173 01:41:27,569 --> 01:41:36,786 说话人 SPEAKER_05: 两者都有作为公司的观点，但也是生态系统的一部分，并与拥有这种知识的人合作。
1174 01:41:37,546 --> 01:41:40,792 说话人 SPEAKER_05: 所以这是我的当前建议。
1175 01:41:40,813 --> 01:41:42,596 说话人 SPEAKER_10: 我会补充这一点。
1176 01:41:42,617 --> 01:41:45,983 说话人 SPEAKER_12: 不，这比我能给出的答案要好得多，所以。
1177 01:41:46,002 --> 01:41:47,045 说话人 SPEAKER_10: 我只是补充一点。
1178 01:41:47,085 --> 01:41:51,011 说话人 SPEAKER_10: 关于 Fei-Fei 提到的与对此感兴趣的人合作，我认为
1179 01:41:51,515 --> 01:41:56,641 说话人 SPEAKER_10: 投资界有人在思考并引领这一领域。
1180 01:41:57,444 --> 01:42:03,412 说话人 SPEAKER_10: 在我们的案例中，Radical，我们在每个条款中都写入了公司采用负责任 AI 的义务。
1181 01:42:05,314 --> 01:42:08,099 说话人 SPEAKER_10: 刚开始我们这样做的时候，有些阅读条款的律师会问，这是什么？
1182 01：42：08,158 --> 01：42：09,902 议长 SPEAKER_10：我试着把它划掉，但我们把它放回去了。
1183 01:42:10,643 --> 01:42:15,750 说话人 SPEAKER_10: 但我们也在开发一个负责任的 AI 投资框架，我们即将发布
1184 01:42:15,916 --> 01:42:20,140 说话人 SPEAKER_10: 我们将与全球多个不同组织广泛合作
1185 01:42:21,082 --> 01:42:26,027 说话人 SPEAKER_10: 在过去四年中，我们已与 7000 家 AI 公司会面，我想我们投资了大约 40 家
1186 01:42:26,226 --> 01:42:34,095 说话人 SPEAKER_10: 因此，我们看到了很多，并试图构建一个其他人可以继续使用的框架，我们将开源它，以便我们能够开发它并使其更好
1187 01:42:34,195 --> 01:42:40,302 说话人 SPEAKER_10: 我认为许多公司可以通过联系那些有相似想法的人来做很多事情。
1188 01:42:41,025 --> 01:42:42,006 说话人 SPEAKER_10: 你还想问其他问题吗？
1189 01:42:42,167 --> 01:42:42,528 说话人 SPEAKER_03: 是的，太好了。
1190 01:42:42,547 --> 01:42:46,233 说话人 SPEAKER_03: 有很多问题，不幸的是我们只能回答其中几个。
1191 01:42:46,252 --> 01:42:56,427 说话人 SPEAKER_03：但是基于这一点，很多问题都与与产业的关系有关，考虑到产业和私营部门在模型开发中扮演的巨大角色。
1192 01:42:56,908 --> 01:43:05,380 说话人 SPEAKER_03：甚至有人问，研究人员和不同的工程角色今天是否也应该学习管理课程？
1193 01:43:08,953 --> 01:43:15,421 说话人 SPEAKER_12：我必须告诉你一个我在谷歌时的故事。
1194 01:43:16,181 --> 01:43:23,009 说话人 SPEAKER_12：我管理一个小团队，我们每六个月都会收到我们员工的报告。
1195 01:43:23,909 --> 01:43:36,384 说话人 SPEAKER_12: 我收到的一份报告是，Jeff 很好相处，但他可能需要参加一些管理课程。
1196 01:43:36,764 --> 01:43:38,286 说话人 SPEAKER_12: 但那样他就不再是 Jeff 了。
1197 01:43:40,662 --> 01:43:42,203 说话人 SPEAKER_12: 我对管理课程的感觉就是这样。
1198 01:43:49,256 --> 01:43:52,581 说话人 SPEAKER_05: 我没有比这更好的故事了。
1199 01:43:52,600 --> 01:43:54,103 说话人 SPEAKER_10: 我们还剩下大约一分半钟的时间。
1200 01:43:54,163 --> 01:43:57,509 说话人 SPEAKER_10: 那么如果我们能的话，也许我们可以在房间里再进行一个。
1201 01:43:59,591 --> 01:44:00,052 说话人 SPEAKER_10: 让我们看看。
1202 01:44:00,092 --> 01:44:01,135 说话人 SPEAKER_10: 你想接手吗？
1203 01:44:01,515 --> 01:44:02,155 说话人 SPEAKER_10: 嗯。
1204 01:44:02,216 --> 01:44:02,978 说话人 SPEAKER_10: 不，在你旁边。
1205 01:44:03,438 --> 01:44:03,759 说话人 SPEAKER_10: 对不起。
1206 01:44:06,603 --> 01:44:07,024 说话人 SPEAKER_10: 好的。
1207 01:44:08,033 --> 01:44:10,717 说话人 SPEAKER_10: 我们希望尽快提问，然后我们会得到一个快速的回答。
1208 01:44:10,738 --> 01:44:11,137 说话人 SPEAKER_07: 谢谢。
1209 01:44:12,060 --> 01:44:12,900 说话人 SPEAKER_07: 很高兴来到这里。
1210 01:44:13,001 --> 01:44:14,304 说话人 SPEAKER_07: 很高兴见到你，Fei-Fei。
1211 01:44:14,323 --> 01:44:15,565 说话人 SPEAKER_07：我的名字叫高丽莎。
1212 01:44:15,685 --> 01:44:16,426 说话人 SPEAKER_07：我在 Cohere 工作。
1213 01:44:16,847 --> 01:44:28,707 说话人 SPEAKER_07：所以我的问题是，从私营部门的视角来看，我们与所有人合作，将自然语言处理、大型语言模型推广到更广泛的社会。
1214 01:44:28,688 --> 01:44:42,515 说话人 SPEAKER_07：在具体的公共部门和研究机构、大学，那里有很多人才、很多数据，我们如何找到一种互利的关系，我们可以贡献，他们也可以贡献？
1215 01:44:42,835 --> 01:44:44,037 说话人 SPEAKER_07: 谢谢。
1216 01:44:44,198 --> 01:44:45,279 说话人 SPEAKER_12: 给他们一些钱。
1217 01:44:53,613 --> 01:44:55,676 说话人 SPEAKER_05: 或者 H100s。
1218 01:44:56,779 --> 01:44:57,860 说话人 SPEAKER_05: 我们选 H100。
1219 01:44:58,121 --> 01:45:00,145 说话人 SPEAKER_05：但是，这非常重要。
1220 01:45:00,324 --> 01:45:06,456 说话人 SPEAKER_05：我提倡公共部门投资，但实际上我可能更倾向于提倡合作。
1221 01:45:06,596 --> 01:45:12,346 说话人 SPEAKER_05: 我们需要政府、私营部门和公共部门共同努力。
1222 01:45:12,327 --> 01:45:19,238 说话人 SPEAKER_05: 在斯坦福 HAI 的过去四年里，我们主要做的一件事就是创建一个产业生态系统。
1223 01:45:19,840 --> 01:45:22,985 说话人 SPEAKER_05: 我们可以私下讨论很多细节。
1224 01:45:23,145 --> 01:45:29,356 说话人 SPEAKER_05: 但是如果我和大学领导人或高等教育机构谈话，那就是……
1225 01:45:29,337 --> 01:45:31,099 说话人 SPEAKER_05: 我认为我们需要接受这一点。
1226 01:45:31,158 --> 01:45:32,962 说话人 SPEAKER_05: 我们需要负责任地接受这一点。
1227 01:45:32,981 --> 01:45:38,029 说话人 SPEAKER_05: 人们对这事的称呼可能不同，但我认为这个生态系统非常重要。
1228 01:45:38,088 --> 01:45:39,390 说话人 SPEAKER_05: 双方都很重要。
1229 01:45:39,810 --> 01:45:40,993 说话人 SPEAKER_05: 建立那种伙伴关系。
1230 01:45:41,393 --> 01:45:44,497 说话人 SPEAKER_05: 成为彼此的责任伙伴。
1231 01:45:44,997 --> 01:45:46,980 说话人 SPEAKER_05: 资源是一个大问题。
1232 01:45:48,542 --> 01:45:49,944 说话人 SPEAKER_05: 我们会感激的。
1233 01:45:51,122 --> 01:45:51,703 说话人 SPEAKER_10: 谢谢。
1234 01:45:51,724 --> 01:45:53,608 说话人 SPEAKER_10: 好的，就这样，我们的时间正好用完了。
1235 01:45:54,970 --> 01:45:55,832 说话人 SPEAKER_10: 我想感谢你们两位。
1236 01:45:56,432 --> 01:46:04,247 说话人 SPEAKER_10: 我感到非常荣幸，总是能够把你们两位称为朋友，Feifei，你是我的合作伙伴，Jeff，你是我的投资者，并且能够和你们私下里进行这些对话。
1237 01:46:04,328 --> 01:46:08,996 主持人：很高兴把你们两位请到一起，让其他人也能听到你们要说的内容。
1238 01:46:09,015 --> 01:46:11,000 主持人：非常感谢你们两位接受这次采访。
1239 01:46:12,342 --> 01:46:14,667 主持人：希望这次采访对你们和我都有所启发。
1240 01:46:23,759 --> 01:46:28,387 主持人：接下来，我们将把话筒交给多伦多大学艺术与科学学院院长 Melanie Wooden，非常感谢 Jordan。
1241 01:46:28,828 --> 01:46:40,992 主持人：所以 Jeff、Fei-Fei 和 Jordan，代表今晚在火星现场以及在线加入的数千人，我们对今晚这样深刻的对话深表感激。
1242 01:46:41,899 --> 01:46:52,175 主持人：我可以这么说，我想我们很多人都知道，成为大学社区的一员，提供了无尽的参与对话和讲座的机会。
1243 01:46:52,295 --> 01:46:56,360 主持人：作为文理学院院长，我有幸参加其中许多活动。
1244 01:46:57,082 --> 01:47:03,029 主持人：但我可以毫不犹豫地说，今晚的对话确实是无与伦比的。
1245 01:47:03,010 --> 01:47:07,060 说话人 SPEAKER_00：当然，这次对话 couldn't be more timely.
1246 01:47:07,702 --> 01:47:13,475 说话人 SPEAKER_00：杰夫，当你向世界分享你对超级智能威胁的担忧时，
1247 01:47:13,860 --> 01:47:27,564 说话人 SPEAKER_00：我们都听了，我们都尽力去理解这个复杂的问题，无论是阅读观点文章，观看你的视频，还是阅读长篇新闻报道，我们都真的尽力去理解你告诉我们的内容。
1248 01:47:28,204 --> 01:47:37,640 说话人 SPEAKER_00：所以直接从你和你这里听到 Fei-Fei，她现在已经花费了很多年领导以人为中心的 AI 的发展，这真的是非常有力的。
1249 01:47:37,621 --> 01:47:47,157 说话人 SPEAKER_00：因此，感谢两位，感谢今天下午来参加的各位，还要特别感谢使今晚成为可能的 Radical Ventures 和其他合作伙伴。
1250 01:47:47,176 --> 01:47:55,270 说话人 SPEAKER_00：那么，讲座到此结束，我们邀请在场的各位到大厅去享用一些小吃。
1251 01:47:55,511 --> 01:47:56,472 说话人 SPEAKER_00：感谢大家的参与。
1
00:00:00,031 --> 00:00:04,503
Speaker SPEAKER_00: It is known as the Nobel Prize of the computer world.

2
00:00:04,604 --> 00:00:07,833
Speaker SPEAKER_00: It's called the Turing Award, named after Alan Turing.

3
00:00:08,253 --> 00:00:12,787
Speaker SPEAKER_00: You've certainly heard of that name, perhaps seen the movie about the man himself.

4
00:00:12,807 --> 00:00:15,053
Speaker SPEAKER_00: He created the Turing test, which determined

5
00:00:15,032 --> 00:00:20,481
Speaker SPEAKER_00: when artificial intelligence is working in a way that is indistinguishable from human thought.

6
00:00:20,562 --> 00:00:21,603
Speaker SPEAKER_00: That's the backdrop.

7
00:00:21,864 --> 00:00:23,306
Speaker SPEAKER_00: But you may not have heard of the prize.

8
00:00:23,646 --> 00:00:31,237
Speaker SPEAKER_00: It is most prestigious, as you might guess, and every year it's awarded to the best in the field of advanced computer scientists.

9
00:00:31,559 --> 00:00:39,670
Speaker SPEAKER_00: This time, for the first time, there are two Canadians who have won the prize and who are sharing the prize money, which is a $1 million U.S.

10
00:00:40,591 --> 00:00:40,972
Speaker SPEAKER_00: check.

11
00:00:40,953 --> 00:00:43,536
Speaker SPEAKER_00: And I have two of the three winners here, the two Canadians.

12
00:00:43,896 --> 00:00:44,537
Speaker SPEAKER_00: What an honour.

13
00:00:44,899 --> 00:00:53,271
Speaker SPEAKER_00: In studio with me, I have Professor Geoffrey Hinton, who's Chief Scientific Advisor at the Vector Institute, but the profile goes long beyond that.

14
00:00:53,371 --> 00:00:59,100
Speaker SPEAKER_00: Emeritus Professor at U of T, Chief Engineering Fellow at Google as well, Vice President there.

15
00:00:59,540 --> 00:01:00,341
Speaker SPEAKER_00: Welcome to you.

16
00:01:00,502 --> 00:01:01,863
Speaker SPEAKER_00: Wonderful to see you.

17
00:01:01,844 --> 00:01:10,914
Speaker SPEAKER_00: And in San Francisco, I have Yoshua Bengio, who is the director of Quebec's Artificial Intelligence Institute and a professor at the Université de Montréal.

18
00:01:11,414 --> 00:01:13,917
Speaker SPEAKER_00: Welcome to you, professor, there in San Francisco.

19
00:01:13,956 --> 00:01:16,719
Speaker SPEAKER_00: And congratulations, gentlemen, both.

20
00:01:16,799 --> 00:01:17,561
Speaker SPEAKER_00: This is wonderful.

21
00:01:17,600 --> 00:01:19,242
Speaker SPEAKER_00: Professor Bengio, let me start with you.

22
00:01:19,281 --> 00:01:23,126
Speaker SPEAKER_00: I'm wondering, forget the left brain analytical response.

23
00:01:23,387 --> 00:01:25,308
Speaker SPEAKER_00: I want right brain emotion here.

24
00:01:25,629 --> 00:01:30,213
Speaker SPEAKER_00: What was your reaction when you found out you had won this most prestigious prize?

25
00:01:30,515 --> 00:01:34,224
Speaker SPEAKER_02: I mean it was such an emotion and I couldn't believe it.

26
00:01:35,727 --> 00:01:41,519
Speaker SPEAKER_02: It's an honor for us but it's an honor for the whole field that we've been working towards.

27
00:01:41,721 --> 00:01:44,828
Speaker SPEAKER_02: I really felt this was an amazing moment.

28
00:01:45,061 --> 00:01:47,905
Speaker SPEAKER_00: You're in the airport.

29
00:01:47,924 --> 00:01:49,465
Speaker SPEAKER_00: We should probably clarify.

30
00:01:49,546 --> 00:01:54,371
Speaker SPEAKER_00: We're going to get some, who knows, computer-generated voices or things joining us in our interview.

31
00:01:54,712 --> 00:01:58,798
Speaker SPEAKER_00: But anyway, so big excitement on Professor Bengio's part and understandable.

32
00:01:59,158 --> 00:02:06,968
Speaker SPEAKER_00: Help us understand, Professor Hinton, in simple terms for us non-scientists, what exactly have you developed?

33
00:02:07,504 --> 00:02:11,569
Speaker SPEAKER_01: So normally, if you want a computer to do something, you tell it exactly what to do.

34
00:02:11,629 --> 00:02:14,013
Speaker SPEAKER_01: You write a program that tells it in detail what to do.

35
00:02:14,193 --> 00:02:14,413
Speaker SPEAKER_01: Right.

36
00:02:15,093 --> 00:02:25,987
Speaker SPEAKER_01: What we've been working on for the last 40 years or so is making something more like a brain, where you have a bunch of simple processing elements, like the brain cells, and they have connections between them.

37
00:02:26,889 --> 00:02:31,334
Speaker SPEAKER_01: And the system learns to do things by being given examples.

38
00:02:31,875 --> 00:02:35,419
Speaker SPEAKER_01: And what it does inside is just change the strength of the connections, like the brain does.

39
00:02:35,516 --> 00:02:36,038
Speaker SPEAKER_00: Wow.

40
00:02:36,057 --> 00:02:40,024
Speaker SPEAKER_00: So this is not me saying, you know, programming something, this is what you do.

41
00:02:40,564 --> 00:02:43,569
Speaker SPEAKER_00: This is, you call them neural networks.

42
00:02:43,629 --> 00:02:46,234
Speaker SPEAKER_00: This is neural learning like the neurons in our brain.

43
00:02:46,253 --> 00:02:46,354
Speaker SPEAKER_00: Yes.

44
00:02:47,295 --> 00:02:49,419
Speaker SPEAKER_00: And you've been working on this for 40 years.

45
00:02:49,438 --> 00:02:53,205
Speaker SPEAKER_00: In those early days, I mean, people would have thought you were probably

46
00:02:53,472 --> 00:02:56,620
Speaker SPEAKER_00: a little off your rocker maybe a little bit, 40 years ago.

47
00:02:56,659 --> 00:02:59,246
Speaker SPEAKER_01: Yes, for a long time people thought this stuff would never work.

48
00:02:59,387 --> 00:03:06,705
Speaker SPEAKER_01: They thought it was a kind of romantic fantasy that you could take a big neural network and just train it from examples and it would learn everything.

49
00:03:06,937 --> 00:03:08,118
Speaker SPEAKER_00: But it's not, in fact.

50
00:03:08,158 --> 00:03:09,040
Speaker SPEAKER_00: It is not fantasy.

51
00:03:09,200 --> 00:03:10,481
Speaker SPEAKER_00: It is coming to the fore.

52
00:03:10,502 --> 00:03:18,572
Speaker SPEAKER_00: And we have some examples, and Professor Bengio, stay with me, because I'm going to begin with one of the things that is really in Professor Hinton's area of expertise.

53
00:03:19,052 --> 00:03:24,038
Speaker SPEAKER_00: I've been noticing on my Gmail exchanges of late, let's pull this up on the computer.

54
00:03:24,058 --> 00:03:26,801
Speaker SPEAKER_00: I write a note, look at these bottom things.

55
00:03:27,103 --> 00:03:33,890
Speaker SPEAKER_00: Somebody writes a note, and then instead of my having to even write, is this the computer suggesting what I should be responding?

56
00:03:34,207 --> 00:03:47,745
Speaker SPEAKER_01: So what's happened is the computer can see how other people responded to Gmails, and it can understand the relation between the Gmail you got and the Gmails other people have got, even though they're not identical, and it can figure out what's probably an appropriate response.

57
00:03:48,146 --> 00:03:51,872
Speaker SPEAKER_00: So this is an example of precisely what you have come up with.

58
00:03:51,972 --> 00:04:02,705
Speaker SPEAKER_01: This is a neural net that's looking at the words in the Gmail, and inside the neural net, there's neurons getting active, and it's predicting what kind of response you might want to make.

59
00:04:02,967 --> 00:04:04,087
Speaker SPEAKER_00: So I might want to click on it.

60
00:04:04,207 --> 00:04:06,390
Speaker SPEAKER_00: It's doing the thinking and the writing for me.

61
00:04:06,431 --> 00:04:11,778
Speaker SPEAKER_00: Professor Bengio, you're in the area of language as well, and we have some video here.

62
00:04:12,118 --> 00:04:14,823
Speaker SPEAKER_00: This is applied in the area of translation as well.

63
00:04:14,902 --> 00:04:16,584
Speaker SPEAKER_00: Explain that part if you would.

64
00:04:17,290 --> 00:04:18,031
Speaker SPEAKER_02: Yes, of course.

65
00:04:18,872 --> 00:04:45,492
Speaker SPEAKER_02: So, this is not something we expected just a few years ago, that a neural network could take a sequence of words, which is just a bunch of symbols, that don't have an intrinsic meaning by themselves, and going through all the computations that Dr. Hinton talked about with a large neural network, could produce another sequence of words that would correspond to the translation, say, from a sentence in French to a sentence in English.

66
00:04:46,079 --> 00:05:00,639
Speaker SPEAKER_02: And what's interesting is that they're doing all this and at the same time capturing some of the meaning of the words so that words that have similar meaning and similar grammatical role will end up being represented in a similar way.

67
00:05:00,838 --> 00:05:06,887
Speaker SPEAKER_02: And that allows those systems to get the right answer in new sentences that they have never seen before.

68
00:05:06,927 --> 00:05:08,829
Speaker SPEAKER_02: And that's why these things have been deployed.

69
00:05:09,146 --> 00:05:13,552
Speaker SPEAKER_02: for example, in Google Translate and most industrial translation systems.

70
00:05:13,572 --> 00:05:18,980
Speaker SPEAKER_02: But the same technology is being used for all kinds of natural language understanding tasks.

71
00:05:19,439 --> 00:05:24,226
Speaker SPEAKER_00: Okay, this is video that Google's provided, so we're seeing it in terms of how they are depicting that.

72
00:05:24,646 --> 00:05:29,973
Speaker SPEAKER_00: If we want to even look at further application, Professor Hinton, how does this work for healthcare, for example?

73
00:05:30,088 --> 00:05:32,471
Speaker SPEAKER_01: So there's going to be massive applications in healthcare.

74
00:05:32,951 --> 00:05:44,086
Speaker SPEAKER_01: Anytime you need to interpret what's in an image, like a CAT scan or an X-ray, at present that's done by people and people differ from one another and their attention lapses.

75
00:05:44,927 --> 00:05:50,132
Speaker SPEAKER_01: Computers are already as good as people at quite a few of those things and they're getting better all the time.

76
00:05:50,653 --> 00:05:57,901
Speaker SPEAKER_01: So pretty soon what's going to happen is the doctor is going to consult with a computer to agree on an interpretation of an image.

77
00:05:57,882 --> 00:06:02,036
Speaker SPEAKER_01: because computers can see a lot more images than a doctor can in their lifetime.

78
00:06:02,117 --> 00:06:02,838
Speaker SPEAKER_00: That's incredible.

79
00:06:02,858 --> 00:06:06,264
Speaker SPEAKER_00: And again, we're looking at video just to illustrate exactly what you're talking about.

80
00:06:06,283 --> 00:06:07,646
Speaker SPEAKER_00: So you have helped us develop this.

81
00:06:08,427 --> 00:06:28,074
Speaker SPEAKER_00: And as I listen to you, particularly in the healthcare piece, I mean, you see the potential, obviously, but Professor Bengio, you can immediately recognize why people have anxieties about this, why people are concerned about where the future is going to lead, not just working with, but replacing, perhaps, in so many aspects of our life, our society, our economy.

82
00:06:28,055 --> 00:06:31,139
Speaker SPEAKER_00: We're talking a lot about people's worries about AI and robotics.

83
00:06:31,160 --> 00:06:33,002
Speaker SPEAKER_00: How do you answer them, Professor Bengio?

84
00:06:34,745 --> 00:06:41,795
Speaker SPEAKER_02: Well, so I think there needs to be a democratic discussion about how we want to use this technology.

85
00:06:41,956 --> 00:06:48,966
Speaker SPEAKER_02: It could be used for good, for example, in health care and in many other areas, for example, in transportation with self-driving cars.

86
00:06:49,406 --> 00:06:52,391
Speaker SPEAKER_02: But it may also have a negative impact, as you said, on jobs.

87
00:06:52,432 --> 00:06:56,458
Speaker SPEAKER_02: And governments need to start thinking about this right now because it takes time to

88
00:06:56,793 --> 00:07:01,718
Speaker SPEAKER_02: adapt our, say, training systems or social safety net.

89
00:07:02,178 --> 00:07:05,862
Speaker SPEAKER_02: But there are also concerns about the misuse of the technology.

90
00:07:05,882 --> 00:07:22,480
Speaker SPEAKER_02: And for that, again, governments need to think about regulations and laws that would be appropriate to handle that, including international regulations, such as how to manage the use of these technologies in weapons like these killer drones that are worrisome for many of us.

91
00:07:23,185 --> 00:07:23,966
Speaker SPEAKER_00: Absolutely.

92
00:07:24,005 --> 00:07:42,139
Speaker SPEAKER_00: I mean, there's even an institute, the Future of Life Institute, Elon Musk is involved in that, trying to make sure that we use all of the technologies that people like you are developing, but for beneficial ends in humanity, not for exactly some of those worrisome things that you have just signaled.

93
00:07:42,119 --> 00:07:48,250
Speaker SPEAKER_00: You know, you see governments need to look at this because it takes time to come up with a response.

94
00:07:48,670 --> 00:07:50,213
Speaker SPEAKER_00: Time is something they may not have.

95
00:07:50,314 --> 00:07:52,598
Speaker SPEAKER_00: This is advancing very, very quickly now.

96
00:07:52,658 --> 00:07:57,548
Speaker SPEAKER_00: You said you've been in this business for decades, but now it seems that it's moving almost at light speed.

97
00:07:57,848 --> 00:08:00,634
Speaker SPEAKER_00: What does the future look like for you, Professor Hinton?

98
00:08:00,918 --> 00:08:10,774
Speaker SPEAKER_01: So I think this new technology of being able to get computers to do things by just showing them examples is going to be used in more or less every industry.

99
00:08:11,136 --> 00:08:21,353
Speaker SPEAKER_01: Anytime you have data and you want to predict things, you show the computer examples of the historical data and what happened next, and the computer can figure out the regularities and can start making predictions.

100
00:08:21,855 --> 00:08:23,617
Speaker SPEAKER_01: So, for example, it can predict

101
00:08:23,598 --> 00:08:24,560
Speaker SPEAKER_01: It can predict floods.

102
00:08:24,819 --> 00:08:26,463
Speaker SPEAKER_01: It can predict aftershocks of earthquakes.

103
00:08:26,923 --> 00:08:34,096
Speaker SPEAKER_01: It can even look at an image of the back of your eye, and by seeing the blood vessels in your retina, it can predict whether you're going to have a heart attack.

104
00:08:34,615 --> 00:08:35,798
Speaker SPEAKER_01: It's going to happen everywhere.

105
00:08:36,158 --> 00:08:37,902
Speaker SPEAKER_00: In every aspect of our life.

106
00:08:37,922 --> 00:08:38,643
Speaker SPEAKER_01: In every aspect.

107
00:08:38,663 --> 00:08:40,446
Speaker SPEAKER_01: And it's going to increase productivity everywhere.

108
00:08:41,027 --> 00:08:45,274
Speaker SPEAKER_01: The issue is, will that increase in productivity translate to the general good?

109
00:08:45,875 --> 00:08:47,037
Speaker SPEAKER_01: But that's a political question.

110
00:08:47,076 --> 00:08:48,659
Speaker SPEAKER_01: That's not a question about the technology.

111
00:08:49,096 --> 00:08:58,384
Speaker SPEAKER_00: Because, I mean, it's interesting that you raise this, and I don't know if either of you want to chime in on this because we'll have to leave it there, but it's a question for policymakers.

112
00:08:58,445 --> 00:09:09,775
Speaker SPEAKER_00: It's interesting, there is a company I was just reading about that developed a fantastic technology but actually decided not to release it because it was concerned about its application in broader society.

113
00:09:09,836 --> 00:09:16,302
Speaker SPEAKER_00: I mean, are you saying that it's just up to scientists like yourselves to develop whatever you can?

114
00:09:16,621 --> 00:09:18,663
Speaker SPEAKER_00: We have to decide what the limitations are.

115
00:09:18,711 --> 00:09:22,138
Speaker SPEAKER_01: No, I think the scientists should be very well aware of what they're developing.

116
00:09:22,339 --> 00:09:28,529
Speaker SPEAKER_01: I think, for example, they might choose not to work on weapons because the bad use is so obvious.

117
00:09:29,912 --> 00:09:37,446
Speaker SPEAKER_01: But the increase in productivity that you'll get in industries in general, that's something that should be for the general good.

118
00:09:37,426 --> 00:09:42,298
Speaker SPEAKER_01: Of course some jobs will be lost, but many other jobs will be enhanced.

119
00:09:43,562 --> 00:09:50,861
Speaker SPEAKER_01: So if you look at the printing press, the printing press basically got rid of scribes, but I don't think anybody would say we shouldn't have introduced the printing press.

120
00:09:51,567 --> 00:09:52,609
Speaker SPEAKER_00: We'll see for the future.

121
00:09:53,009 --> 00:09:56,034
Speaker SPEAKER_00: What a pleasure to meet you both, and congratulations.

122
00:09:56,195 --> 00:10:04,831
Speaker SPEAKER_00: This conversation could go on to infinity, because I think there's so many aspects of this that we're going to have to address as a society.

123
00:10:04,850 --> 00:10:07,917
Speaker SPEAKER_00: But congratulations to you there in San Francisco, Professor Bengio.

124
00:10:08,216 --> 00:10:10,301
Speaker SPEAKER_00: Safe travels wherever you're headed next.

125
00:10:10,321 --> 00:10:12,063
Speaker SPEAKER_00: Thank you for the time, Professor Hinton.

126
00:10:12,083 --> 00:10:13,225
Speaker SPEAKER_00: A pleasure to meet you as well.

127
00:10:13,346 --> 00:10:13,586
Speaker SPEAKER_00: Thank you.

128
00:10:13,606 --> 00:10:14,427
Speaker SPEAKER_00: Congratulations.

129
00:10:14,528 --> 00:10:15,330
Speaker SPEAKER_01: Thanks very much.


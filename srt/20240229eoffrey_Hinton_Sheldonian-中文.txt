1 00:00:02,765 --> 00:00:09,836 讲者 SPEAKER_00: 好吧，我要让所有计算机科学和机器学习领域的人失望了，因为我将进行一场真正的公开讲座。
2 00:00:10,297 --> 00:00:16,225 讲者 SPEAKER_00: 我将尝试解释神经网络是什么，语言模型是什么，为什么我认为它们能理解。
3 00:00:16,245 --> 00:00:18,629 讲者 SPEAKER_00: 实际上，我有一长串这些东西。
4 00:00:18,609 --> 00:00:25,120 讲者 SPEAKER_00: 最后，我将简要谈谈人工智能的一些威胁。
5 00:00:25,862 --> 00:00:33,034 说话人 SPEAKER_00: 然后，我将讨论数字神经网络和模拟神经网络之间的区别，我认为这种区别是如此令人恐惧。
6 00:00:36,060 --> 00:00:39,707 说话人 SPEAKER_00: 自从 20 世纪 50 年代以来，智能领域有两种范式。
7 00:00:40,378 --> 00:00:47,725 说话人 SPEAKER_00: 受逻辑启发的观点认为，智能的本质是推理，这是通过使用符号规则来操作符号表达式来实现的。
8 00:00:49,726 --> 00:00:51,249 说话人 SPEAKER_00: 他们曾经认为学习可以等待。
9 00:00:51,548 --> 00:00:53,610 说话人 SPEAKER_00: 当我还是学生的时候，有人告诉我不要学习。
10 00:00:53,631 --> 00:00:56,052 说话人 SPEAKER_00: 我们一旦理解了如何表示事物，学习就会随之而来。
11 00:00:57,134 --> 00:00:59,976 说话人 SPEAKER_00: 生物启发的方法非常不同。
12 00:01:00,656 --> 00:01:06,623 说话人 SPEAKER_00: 它认为智能的本质是学习神经网络中连接的强度，推理可以稍后进行。
13 00:01:07,582 --> 00:01:08,724 说话人 SPEAKER_00: 目前先不用担心推理。
14 00:01:08,745 --> 00:01:10,385 说话人 SPEAKER_00: 等我们能够学习东西之后，这个问题会得到解决。
15 00:01:13,218 --> 00:01:17,924 说话人 SPEAKER_00: 现在我将解释什么是人工神经网络，那些了解的人可以感到有趣。
16 00:01:18,965 --> 00:01:23,412 说话人 SPEAKER_00: 一种简单的神经网络有输入神经元和输出神经元。
17 00:01:24,272 --> 00:01:27,378 说话人 SPEAKER_00: 输入神经元可能代表图像中像素的强度。
18 00:01:27,957 --> 00:01:32,745 说话人 SPEAKER_00: 输出神经元可能代表图像中物体的类别，如狗或猫。
19 00:01:33,112 --> 00:01:40,823 说话人 SPEAKER_00: 然后是中间层的神经元，有时被称为隐藏神经元，它们学习检测与找到这些事物相关的特征。
20 00:01:41,183 --> 00:01:51,978 说话人 SPEAKER_00: 所以一种思考方式是，如果你想在图像中找到一只鸟，那么从检测图像中各种位置、各种方向的边缘特征的检测层开始会很好。
21 00:01:52,480 --> 00:01:58,108 说话者 SPEAKER_00: 然后你可能有一层神经元，可以检测到边缘的组合，比如两个边缘以锐角相交。
22 00:01:58,087 --> 00:02:02,835 说话者 SPEAKER_00: 可能是喙，也可能不是，或者一些形成小圆圈的边缘。
23 00:02:03,658 --> 00:02:12,873 说话者 SPEAKER_00: 然后你可能有一层神经元，可以检测到像圆形和两个边缘相交形成喙这样的东西，在正确的空间关系中，这可能是一只鸟的头。
24 00:02:13,594 --> 00:02:19,746 说话者 SPEAKER_00: 最后，你可能有一个输出神经元，它会说，如果我发现鸟的头、脚和翅膀，那么它很可能是一只鸟。
25 00:02:20,747 --> 00:02:22,931 说话人 SPEAKER_00：这就是这些东西将要学会的。
26 00:02:24,581 --> 00:02:29,673 说话人 SPEAKER_00：现在这些小红绿点代表的是连接的权重，问题是，这些权重由谁设置？
27 00:02:32,498 --> 00:02:33,580 说话人 SPEAKER_00：这是做这件事的一种方法。
28 00:02:33,722 --> 00:02:37,088 说话人 SPEAKER_00：对每个人来说都很明显它会起作用，而且很明显它会花费很长时间。
29 00:02:37,569 --> 00:02:41,899 说话人 SPEAKER_00: 你从随机权重开始，然后随机选择一个权重，那个小红点，
30 00:02:42,283 --> 00:02:45,548 说话人 SPEAKER_00: 然后你稍微改变它，看看网络是否表现更好。
31 00:02:46,248 --> 00:02:49,674 说话人 SPEAKER_00: 你必须尝试在许多不同的情况下，才能真正评估它是否表现更好。
32 00:02:50,174 --> 00:02:55,961 说话人 SPEAKER_00: 你做所有这些工作，只是为了看看增加这个权重一点或者减少一点是否能改善情况。
33 00:02:56,442 --> 00:02:58,664 说话者 SPEAKER_00：如果增加它使情况变得更糟，就减少它，反之亦然。
34 00:02:59,526 --> 00:03:00,888 说话者 SPEAKER_00：这就是变异方法。
35 00:03:00,907 --> 00:03:03,110 说话者 SPEAKER_00：这就是进化工作的方式。
36 00:03:03,192 --> 00:03:11,518 说话者 SPEAKER_00：对于进化来说，这样做是有意义的，因为从基因型到表型的转变过程非常复杂，充满了随机的外部事件。
37 00:03:11,919 --> 00:03:13,403 说话人 SPEAKER_00: 你没有那个过程的模型。
38 00:03:14,045 --> 00:03:15,531 说话人 SPEAKER_00: 但对于神经网络来说，这太疯狂了。
39 00:03:17,637 --> 00:03:22,645 说话人 SPEAKER_00: 因为所有的计算都在神经网络中进行，我们有一个关于正在发生什么的模型。
40 00:03:23,146 --> 00:03:26,132 说话人 SPEAKER_00: 因此我们可以利用我们知道那个正向传递中发生什么的事实。
41 00:03:26,733 --> 00:03:32,062 说话人 SPEAKER_00: 我们实际上计算改变权重会如何影响事物，而不是测量改变权重会如何影响事物。
42 00:03:32,703 --> 00:03:37,771 说话人 SPEAKER_00: 有一种叫做反向传播的东西，你会在网络中发送信息。
43 00:03:37,752 --> 00:03:41,435 说话人 SPEAKER_00: 这些信息是关于你得到的结果和你想要的结果之间的差异。
44 00:03:41,996 --> 00:03:49,782 说话人 SPEAKER_00: 你会同时确定网络中的每个权重，是否应该稍微减小或稍微增加，以更接近你想要的结果。
45 00:03:50,903 --> 00:03:52,825 说话人 SPEAKER_00: 这就是反向传播算法。
46 00:03:52,846 --> 00:03:55,207 说话人 SPEAKER_00: 你可以用微积分和链式法则来做。
47 00:03:56,209 --> 00:04:01,174 说话人 SPEAKER_00: 这比变异方法高效，效率比是网络中权重的数量。
48 00:04:01,554 --> 00:04:04,637 说话人 SPEAKER_00: 所以如果你的网络中有万亿个权重，它就比万亿倍高效。
49 00:04:07,974 --> 00:04:12,359 说话人 SPEAKER_00：神经网络经常被用于识别图像中的物体之一。
50 00:04:13,540 --> 00:04:20,807 说话人 SPEAKER_00：现在神经网络可以像展示的那样处理图像，并生成图像的描述作为输出。
51 00:04:21,369 --> 00:04:24,911 说话人 SPEAKER_00：人们尝试用符号人工智能来做这件事已经很多年了，甚至都没有接近成功。
52 00:04:26,353 --> 00:04:27,454 说话人 SPEAKER_00：这是一个困难的任务。
53 00:04:28,095 --> 00:04:33,560 说话人 SPEAKER_00: 我们知道生物系统是通过特征检测器的层次结构来做到这一点的，所以尝试在神经网络上这样做是有意义的。
54 00:04:35,464 --> 00:04:52,653 说话人 SPEAKER_00: 2012 年，我的两位学生，伊利亚·苏特科娃和安吉拉·克日泽夫斯基，在得到我的一点点帮助后，展示了当拥有一百万个训练图像时，你可以通过这种方式制作一个非常好的神经网络来识别一千种不同的物体。
55 00:04:53,254 --> 00:04:57,961 说话人 SPEAKER_00: 在那之前，我们没有足够的训练图像。
56 00:04:58,937 --> 00:05:08,202 说话人 SPEAKER_00: 对于伊利亚来说，他是一位有远见的人，很明显，如果我们把当时的神经网络应用到 ImageNet 上，它们会获胜，而且他是对的。
57 00:05:08,624 --> 00:05:09,766 说话人 SPEAKER_00: 他们赢得相当戏剧性。
58 00:05:10,067 --> 00:05:15,461 说话人 SPEAKER_00: 他们得到了 16%的错误率，而最好的传统计算机视觉系统错误率超过 25%。
59 00:05:15,930 --> 00:05:18,533 说话人 SPEAKER_00: 然后发生的科学现象非常奇怪。
60 00:05:18,995 --> 00:05:24,021 说话人 SPEAKER_00: 在科学中，如果你有两个竞争学派，当你取得一点进步时，另一个学派会说，啊，垃圾。
61 00:05:26,004 --> 00:05:39,862 说话人 SPEAKER_00: 在这种情况下，差距足够大，以至于最好的研究人员，比如 Jitendra Malik 和 Andrew Zissman，只是，Andrew Zissman 给我发邮件说，这太神奇了，然后他改变了正在做的事情，并做了这件事，然后有点烦人地做得比我们更好。
62 00:05:44,314 --> 00:05:45,276 说话人 SPEAKER_00: 语言呢？
63 00:05:46,379 --> 00:05:51,689 说话人 SPEAKER_00: 显然，符号 AI 社区认为他们应该擅长语言。
64 00:05:52,430 --> 00:05:58,762 说话人 SPEAKER_00: 他们中的一些人在出版物中说过，这些特征层次结构无法处理语言。
65 00:05:59,944 --> 00:06:01,968 说话人 SPEAKER_00: 许多语言学家都非常怀疑。
66 00:06:01,949 --> 00:06:07,315 说话人 SPEAKER_00: 乔姆斯基设法说服了他的追随者，语言不是学来的。
67 00:06:07,875 --> 00:06:10,720 说话人 SPEAKER_00: 回过头来看，那完全是胡说八道。
68 00:06:10,779 --> 00:06:15,586 说话人 SPEAKER_00: 如果你能让人们说出明显是假的，那么你就把他们纳入你的教派了。
69 00:06:19,290 --> 00:06:22,314 说话人 SPEAKER_00: 我认为乔姆斯基做出了惊人的成就，但他的时代已经过去了。
70 00:06:25,278 --> 00:06:27,500 说话人 SPEAKER_00: 所以，一个大的神经网络
71 00:06:28,105 --> 00:06:39,509 说话人 SPEAKER_00: 没有先验知识实际上可以通过观察数据来学习语言的语法和语义，这在统计学家和认知科学家看来简直是疯狂。
72 00:06:39,548 --> 00:06:43,036 说话人 SPEAKER_00: 有统计学家向我解释，一个大模型有 100 个参数。
73 00:06:43,336 --> 00:06:45,482 说话人 SPEAKER_00：学习一百万个参数的想法简直是愚蠢的。
74 00:06:46,122 --> 00:06:47,346 说话人 SPEAKER_00：现在我们正在做的是万亿级别的。
75 00:06:51,307 --> 00:06:59,136 说话人 SPEAKER_00：现在我将谈谈我在 1985 年做的一些工作，这是第一个使用反向传播训练的语言模型。
76 00:07:00,137 --> 00:07:03,641 说话人 SPEAKER_00：它实际上是，你可以把它看作是现在这些大型模型的祖先。
77 00:07:04,002 --> 00:07:10,187 说话者 SPEAKER_00：我将详细地谈论它，因为它非常小且简单，你实际上可以了解它是如何工作的。
78 00:07:10,709 --> 00:07:16,454 说话者 SPEAKER_00：一旦你了解了它是如何工作的，它就会让你对更大模型中的情况有所洞察。
79 00:07:17,632 --> 00:07:19,375 说话者 SPEAKER_00：所以有两种非常不同的意义理论。
80 00:07:20,036 --> 00:07:23,680 说话者 SPEAKER_00：有一种结构主义理论，其中词的意义取决于它与其他词的关系。
81 00:07:24,300 --> 00:07:26,122 说话者 SPEAKER_00: 这来自德·索绪尔。
82 00:07:26,142 --> 00:07:29,648 说话者 SPEAKER_00: 而象征式人工智能真正相信那种方法。
83 00:07:29,708 --> 00:07:35,274 说话者 SPEAKER_00: 所以你会有一个关系图，其中包含代表单词的节点和关系弧。
84 00:07:35,956 --> 00:07:38,197 说话者 SPEAKER_00: 你可以像那样捕捉意义。
85 00:07:38,838 --> 00:07:41,262 说话者 SPEAKER_00: 他们认为你必须有一些这样的结构。
86 00:07:41,242 --> 00:07:48,410 说话者 SPEAKER_00: 然后有一个自 1930 年代或更早以来在心理学中存在的理论，即一个词的意义是一大堆特征。
87 00:07:50,132 --> 00:07:56,879 说话者 SPEAKER_00: “狗”这个词的意义是它是生物的，它是捕食者等等。
88 00:07:58,321 --> 00:08:00,944 说话者 SPEAKER_00: 但他们没有说这些特征从哪里来，或者这些特征具体是什么。
89 00:08:01,625 --> 00:08:03,947 说话人 SPEAKER_00: 这两种意义理论听起来完全不同。
90 00:08:05,129 --> 00:08:08,632 说话人 SPEAKER_00: 我想向大家展示如何统一这两种意义理论。
91 00:08:08,831 --> 00:08:14,380 说话人 SPEAKER_00: 我在 1985 年创建了一个简单的模型，它包含了一千多个权重。
92 00:08:19,449 --> 00:08:29,627 说话人 SPEAKER_00: 想法是我们将为每个词学习一组语义特征，我们将学习这些词的特征如何相互作用，以便预测下一个词的特征。
93 00:08:30,608 --> 00:08:34,215 说话人 SPEAKER_00: 所以这是下一个词预测，就像当前的语模型一样，当你微调它们时。
94 00:08:35,849 --> 00:08:41,336 说话人 SPEAKER_00: 但是所有关于事物如何组合的知识都将体现在这些特征交互中。
95 00:08:41,677 --> 00:08:44,221 说话人 SPEAKER_00: 将不会存在任何显式的关联图。
96 00:08:44,621 --> 00:08:47,606 说话人 SPEAKER_00: 如果你想有那样的关系，你可以从你的特征中生成它们。
97 00:08:48,347 --> 00:08:54,717 说话人 SPEAKER_00: 这是一个生成模型，知识存在于你提供给符号的特征以及这些特征相互作用的方式中。
98 00:08:56,908 --> 00:09:00,231 说话人 SPEAKER_00: 我取了一些简单的关联信息，两个家谱。
99 00:09:00,852 --> 00:09:03,456 说话人 SPEAKER_00: 它们故意是同构的。
100 00:09:03,475 --> 00:09:08,100 说话人 SPEAKER_00: 我的意大利研究生总是把意大利家族放在上面。
101 00:09:12,524 --> 00:09:15,847 说话人 SPEAKER_00: 你可以用一组三元组来表达相同的信息。
102 00:09:16,688 --> 00:09:24,856 说话人 SPEAKER_00: 所以如果你使用那里显示的 12 个关系，你可以说像 Colin 的父亲是 James，Colin 的母亲是 Victoria 这样的话，从中你可以推断出
103 00:09:25,376 --> 00:09:32,727 说话人 SPEAKER_00: 在这个美好的、简单的 20 世纪 50 年代的世界里，James 的妻子是 Victoria。
104 00:09:34,130 --> 00:09:35,852 说话人 SPEAKER_00: 你还可以推断出其他的事情。
105 00:09:36,654 --> 00:09:41,461 说话人 SPEAKER_00: 问题在于，如果我只给你一些三元组，你是如何得到这些规则的？
106 00:09:43,205 --> 00:09:51,638 说话人 SPEAKER_00: 所以，一个符号人工智能研究者想要做的是推导出如下形式的规则：如果 X 有母亲 Y，而 Y 有丈夫 Z，那么 X 有父亲 Z。
107 00:09:53,423 --> 00:10:01,232 说话人 SPEAKER_00: 我所做的是使用神经网络来展示它能够学习相同的信息，但全部都是基于这些特征交互。
108 00:10:02,313 --> 00:10:08,341 说话人 SPEAKER_00: 现在，对于像这样永远不会被违反的非常离散的规则，可能不是最好的处理方式。
109 00:10:08,422 --> 00:10:10,725 说话者 SPEAKER_00: 确实，符号派人物尝试用其他方法来做这件事。
110 00:10:11,645 --> 00:10:16,932 说话者 SPEAKER_00: 但是一旦你得到一些不太稳定且并非总是适用的规则，那么神经网络就更好了。
111 00:10:17,167 --> 00:10:24,246 说话者 SPEAKER_00: 因此，问题是，神经网络能否通过反向传播捕获符号派人物会放入规则中的知识？
112 00:10:25,149 --> 00:10:26,413 说话者 SPEAKER_00: 所以神经网络看起来是这样的。
113 00:10:26,432 --> 00:10:32,289 说话人 SPEAKER_00: 有一个代表人物的符号，还有一个代表关系的符号。
114 00:10:32,658 --> 00:10:39,748 说话人 SPEAKER_00: 这个符号通过一些连接，变成了一个特征向量，这些特征是由网络学习的。
115 00:10:40,708 --> 00:10:50,341 说话人 SPEAKER_00: 因此，对于人物一的特征，以及关系的特征，然后这些特征相互作用，预测了输出人物的特征，从这个特征中预测了输出人物。
116 00:10:50,361 --> 00:10:52,404 说话人 SPEAKER_00: 在最后一步找到了最接近的匹配。
117 00:10:54,510 --> 00:11:01,302 讲者：那么这个网络有趣的地方在于，如果你做了正确的正则化，它就能学习到有意义的知识。
118 00:11:01,841 --> 00:11:06,950 讲者：六个特征神经元，现在的这些向量长度通常是300或1000。
119 00:11:07,331 --> 00:11:08,472 讲者：当时它们是六个。
120 00:11:09,735 --> 00:11:15,202 讲者：这是在一种每做一次浮点乘法需要12.5微秒的机器上完成的。
121 00:11:15,756 --> 00:11:21,186 说话人 SPEAKER_00：这比我的苹果 2 好多了，苹果 2 做浮点乘法需要两半毫秒。
122 00:11:22,147 --> 00:11:24,149 说话人 SPEAKER_00：抱歉，这是一个老人。
123 00:11:26,173 --> 00:11:29,458 说话人 SPEAKER_00：所以它学会了像国籍这样的特征。
124 00:11:29,938 --> 00:11:33,524 说话人 SPEAKER_00：因为如果你知道第一个人是英国人，你就知道输出将是英语。
125 00:11:33,644 --> 00:11:35,148 说话人 SPEAKER_00：国籍是一个非常有用的特征。
126 00:11:35,828 --> 00:11:37,672 说话人 SPEAKER_00：它学会了这个人属于哪个时代。
127 00:11:38,172 --> 00:11:41,798 说话人 SPEAKER_00：因为如果你知道关系，如果你学习关系，
128 00:11:42,097 --> 00:11:45,950 说话人 SPEAKER_00：那么答案就是比输入高一代。
129 00:11:46,530 --> 00:11:52,107 说话者 SPEAKER_00：你知道输入的生成，你知道输出是通过这些特征交互生成的。
130 00:11:52,914 --> 00:12:00,985 说话者 SPEAKER_00：所以它学习了所有这些领域的明显特征，并且学会了如何使这些特征相互作用，以便生成输出。
131 00:12:01,524 --> 00:12:14,061 说话者 SPEAKER_00：所以发生的事情是，我向它展示了符号字符串，它创建了特征，这些特征之间的交互可以生成这些符号字符串，但它并没有存储符号字符串。
132 00:12:14,081 --> 00:12:19,226 说话者 SPEAKER_00：就像 GPT-4 一样，它不存储任何单词序列。
133 00:12:19,206 --> 00:12:24,734 说话人 SPEAKER_00: 在其长期知识中，它将它们全部转化为权重，从而可以重新生成序列。
134 00:12:26,154 --> 00:12:29,139 说话人 SPEAKER_00: 但这是一个特别简单的例子，你可以理解它所做的事情。
135 00:12:31,240 --> 00:12:36,587 说话人 SPEAKER_00: 所以，我们今天拥有的大型语言模型，我认为是这种小型语言模型的后代。
136 00:12:37,048 --> 00:12:42,313 说话人 SPEAKER_00: 它们有更多的输入词汇，比如一百万，一百万个词汇片段。
137 00:12:43,296 --> 00:12:46,158 说话人 SPEAKER_00：他们使用了更多的神经元层，
138 00:12:46,139 --> 00:12:51,046 说话人 SPEAKER_00：比如几十层，他们使用了更加复杂的交互。
139 00:12:51,086 --> 00:13:00,640 说话人 SPEAKER_00：所以它们不仅仅是某个特征影响另一个特征，它们会匹配两个特征向量，然后如果相似，那么这个向量会对另一个向量产生很大影响，如果不相似，那么影响就不大，诸如此类。
140 00:13:01,201 --> 00:13:05,548 说话人 SPEAKER_00：所以交互更加复杂，但总体框架是相同的。
141 00:13:05,798 --> 00:13:16,477 说话人 SPEAKER_00: 将符号字符串转换为词片段和特征向量之间的交互特征，这个想法在这些模型中是相同的。
142 00:13:18,360 --> 00:13:21,326 说话人 SPEAKER_00: 理解它们的功能要困难得多。
143 00:13:22,386 --> 00:13:27,235 说话人 SPEAKER_00: 许多人，尤其是乔姆斯基学派的人，认为它们并不真正智能。
144 00:13:27,275 --> 00:13:34,909 说话人 SPEAKER_00: 它们只是使用统计规律将人们创作的文本片段拼凑在一起的一种美化版的自动完成形式。
145 00:13:35,831 --> 00:13:36,852 说话人 SPEAKER_00: 这是一句引用。
146 00:13:40,580 --> 00:13:42,383 说话人 SPEAKER_00: 让我们处理自动补全的反对意见。
147 00:13:42,884 --> 00:13:44,967 说话人 SPEAKER_00: 当有人说这只是自动补全时，
148 00:13:45,758 --> 00:13:50,442 说话人 SPEAKER_00: 他们实际上是在诉诸你对自动补全工作方式的直观理解。
149 00:13:50,462 --> 00:13:54,246 说话人 SPEAKER_00：在以前，自动补全是通过存储，比如说，单词的三元组来工作的。
150 00:13:54,628 --> 00:13:57,971 说话人 SPEAKER_00：如果你看到了前两个，你会计算第三个出现的频率。
151 00:13:58,432 --> 00:14:03,057 说话人 SPEAKER_00：所以如果你看到鱼和薯条经常出现在之后，但猎经常出现。
152 00:14:03,437 --> 00:14:08,543 说话人 SPEAKER_00：所以薯条很可能，猎也很可能，但是虽然不太可能。
153 00:14:09,011 --> 00:14:10,495 说话者 SPEAKER_00: 你可以这样进行自动补全。
154 00:14:11,176 --> 00:14:13,720 说话者 SPEAKER_00: 当人们说这只是自动补全时，他们就是这样做的。
155 00:14:13,740 --> 00:14:18,207 说话者 SPEAKER_00: 我认为这是一种卑鄙的伎俩，因为那根本不是预测下一个单词的方式。
156 00:14:18,609 --> 00:14:19,811 说话者 SPEAKER_00: 他们把单词转换成特征。
157 00:14:19,831 --> 00:14:21,234 说话人 SPEAKER_00: 这些特征相互作用。
158 00:14:22,235 --> 00:14:25,620 说话人 SPEAKER_00: 并且从这些特征交互中，他们预测下一个单词的特征。
159 00:14:27,344 --> 00:14:28,826 说话人 SPEAKER_00: 我想要声明的是
160 00:14:29,953 --> 00:14:40,472 说话人 SPEAKER_00: 是这些数百万个特征以及数十亿个特征之间的交互，它们所学习的是真正理解它们所做的事情，这些大型语言模型。
161 00:14:40,854 --> 00:14:42,275 说话人 SPEAKER_00：他们正在将模型拟合到数据中。
162 00:14:43,038 --> 00:14:47,085 说话人 SPEAKER_00：这不是你最近之前很少考虑的那种模型策略。
163 00:14:47,268 --> 00:14:49,130 说话人 SPEAKER_00：这是一种奇怪的模型。
164 00:14:49,171 --> 00:14:49,971 说话人 SPEAKER_00：它非常大。
165 00:14:50,011 --> 00:14:51,533 说话人 SPEAKER_00: 它有巨大的参数数量。
166 00:14:52,414 --> 00:15:00,486 说话人 SPEAKER_00: 但是它试图通过特征和特征之间的交互来理解这些离散符号的字符串。
167 00:15:00,966 --> 00:15:01,808 说话人 SPEAKER_00: 因此它是一个模型。
168 00:15:03,130 --> 00:15:05,995 说话人 SPEAKER_00: 正是因为这个原因，我认为这些事物真正地理解了。
169 00:15:06,655 --> 00:15:10,059 说话人 SPEAKER_00: 记住一点，如果你问，嗯，我们如何理解？
170 00:15:10,801 --> 00:15:12,464 说话人 SPEAKER_00: 因为显然，我们认为我们理解。
171 00:15:13,644 --> 00:15:16,870 说话人 SPEAKER_00: 嗯，我们中的许多人确实如此。
172 00:15:17,591 --> 00:15:20,157 说话人 SPEAKER_00: 这是我们对如何理解的最佳模型。
173 00:15:21,399 --> 00:15:27,370 说话者 SPEAKER_00：所以并不是说我们对这些 AI 系统的工作方式有一种奇怪的理解，然后这就是大脑的工作方式。
174 00:15:27,389 --> 00:15:32,619 说话者 SPEAKER_00：我们目前对大脑工作方式最好的模型是通过为单词分配特征并让这些特征相互作用。
175 00:15:33,059 --> 00:15:36,686 说话者 SPEAKER_00：最初，这个小语言模型被设计成人类工作方式的模型。
176 00:15:38,489 --> 00:15:41,575 说话者 SPEAKER_00：好的，所以我提出了一个非常强烈的观点，这些事物真的能理解。
177 00:15:44,423 --> 00:15:49,129 演讲者 SPEAKER_00：现在，人们常用的另一个论点是，嗯，GPT-4 只是胡编乱造。
178 00:15:49,730 --> 00:15:52,436 演讲者 SPEAKER_00：当这是由语言模型完成时，实际上应该称之为虚构。
179 00:15:53,476 --> 00:15:55,259 演讲者 SPEAKER_00：他们只是编造东西。
180 00:15:56,442 --> 00:16:01,590 演讲者 SPEAKER_00：现在，心理学家不太这么说，因为心理学家知道人们只是胡编乱造。
181 00:16:02,211 --> 00:16:10,524 说话者 SPEAKER_00：任何研究记忆的人，从 20 世纪 30 年代的巴特莱特开始，都知道人们实际上就像这些大型语言模型。
182 00:16:10,583 --> 00:16:12,287 说话者 SPEAKER_00：他们只是发明东西。
183 00:16:12,402 --> 00:16:19,113 说话者 SPEAKER_00：对我们来说，真实记忆和虚假记忆之间没有明确的界限。
184 00:16:19,913 --> 00:16:26,424 说话者 SPEAKER_00：如果最近发生了某事，并且它似乎与您理解的事物相符，您可能会大致正确地记住它。
185 00:16:26,985 --> 00:16:30,611 说话者 SPEAKER_00：如果很久以前发生了一些事情，或者它很奇怪，你可能会记错。
186 00:16:30,652 --> 00:16:33,836 说话者 SPEAKER_00：而且你通常会非常自信。
187 00:16:34,086 --> 00:16:36,208 说话者 SPEAKER_00：你认为自己记得是正确的，但你其实错了。
188 00:16:36,830 --> 00:16:37,791 说话者 SPEAKER_00：很难证明这一点。
189 00:16:37,890 --> 00:16:41,315 说话人 SPEAKER_00: 但有一个例子可以展示，就是约翰·迪安的记忆。
190 00:16:42,155 --> 00:16:44,418 说话人 SPEAKER_00: 所以约翰·迪安在水门事件中宣誓作证。
191 00:16:45,399 --> 00:16:48,222 说话人 SPEAKER_00: 回顾过去，很明显他在试图说出真相。
192 00:16:49,644 --> 00:16:51,647 说话人 SPEAKER_00: 但他所说的很多都是完全错误的。
193 00:16:52,427 --> 00:16:54,591 说话者 SPEAKER_00: 他会混淆哪些人在哪个会议中。
194 00:16:54,630 --> 00:16:58,054 说话者 SPEAKER_00: 他会把别人的陈述归咎于其他人。
195 00:16:58,095 --> 00:17:00,076 说话者 SPEAKER_00: 实际上，那并不是那个陈述。
196 00:17:00,445 --> 00:17:04,589 说话者 SPEAKER_00: 他把会议完全搞混了。
197 00:17:05,371 --> 00:17:11,199 说话者 SPEAKER_00：但他正确地抓住了白宫发生的事情的要点，正如您可以从录音中看到的那样。
198 00:17:11,599 --> 00:17:15,365 说话者 SPEAKER_00：因为他不知道这些录音，所以您可以通过这种方式进行一个好的实验。
199 00:17:15,444 --> 00:17:21,413 说话者 SPEAKER_00：乌尔里希·内塞尔有一篇关于约翰· Dee 的记忆的精彩文章，他就像一个聊天机器人。
200 00:17:21,432 --> 00:17:22,374 说话者 SPEAKER_00：他只是胡编乱造。
201 00:17:25,298 --> 00:17:26,019 说话人 SPEAKER_00: 这是有可能的。
202 00:17:26,900 --> 00:17:29,423 说话人 SPEAKER_00: 所以他觉得听起来好的东西就是他生产出来的。
203 00:17:30,652 --> 00:17:31,752 说话人 SPEAKER_00: 它们也可以进行推理。
204 00:17:32,413 --> 00:17:37,839 说话人 SPEAKER_00: 我在多伦多有个朋友，他是做符号人工智能的，但非常诚实。
205 00:17:37,960 --> 00:17:40,843 说话者 SPEAKER_00：他对这些事情竟然能工作感到非常困惑。
206 00:17:41,923 --> 00:17:43,365 说话者 SPEAKER_00：他向我提出了一个问题。
207 00:17:43,405 --> 00:17:44,547 说话者 SPEAKER_00：我把这个问题稍微难了一点。
208 00:17:45,667 --> 00:17:49,613 说话者 SPEAKER_00：然后我在它上网之前给了 GPT-4 这个问题。
209 00:17:49,673 --> 00:17:56,440 说话人 SPEAKER_00：所以当它只是一堆冻结在 2021 年的权重时，所有知识都体现在特征之间的相互作用强度中。
210 00:17:57,601 --> 00:17:59,946 说话人 SPEAKER_00：所以我家里的房间都是涂成蓝色、白色或黄色的。
211 00:18:00,548 --> 00:18:01,952 说话人 SPEAKER_00：黄色的油漆一年后就会褪成白色。
212 00:18:02,252 --> 00:18:03,817 说话人 SPEAKER_00：两年后，我希望它们都变成白色。
213 00:18:03,837 --> 00:18:04,759 说话人 SPEAKER_00：我应该做什么，为什么？
214 00:18:05,382 --> 00:18:06,885 说话人 SPEAKER_00：赫克托认为他做不到这一点。
215 00:18:08,990 --> 00:18:10,515 说话人 SPEAKER_00：这是 GPT-4 说的。
216 00:18:11,637 --> 00:18:13,482 说话人 SPEAKER_00：它完全击中了要点。
217 00:18:15,099 --> 00:18:21,288 说话人 SPEAKER_00: 首先，它开始说，假设蓝色油漆不会褪成白色，因为在我告诉你们黄色油漆会褪成白色之后，嗯，也许蓝色油漆也会。
218 00:18:22,450 --> 00:18:30,660 说话人 SPEAKER_00: 假设它不会，那些不需要粉刷的白色房间，那些不需要粉刷的黄色房间，因为它们将在一年内褪成白色，你需要把蓝色房间粉刷成白色。
219 00:18:32,102 --> 00:18:36,807 说话人 SPEAKER_00: 有一回我试过，它说你需要把蓝色房间粉刷成黄色，因为它意识到那样会褪成白色。
220 00:18:37,328 --> 00:18:40,432 说话人 SPEAKER_00: 这更像是一位数学家为了将其简化为先前问题而提出的解决方案。
221 00:18:44,817 --> 00:18:51,969 说话人 SPEAKER_00: 所以，既然我已经声称这些事物真的能理解，那么我想现在谈谈其中的一些风险。
222 00:18:53,250 --> 00:18:55,734 说话人 SPEAKER_00: 所以，强大的 AI 有很多风险。
223 00:18:56,737 --> 00:19:02,746 说话人 SPEAKER_00: 有虚假的图像、声音和视频，这些将在下一次选举中使用。
224 00:19:03,307 --> 00:19:07,193 说话人 SPEAKER_00: 今年有很多选举，它们将有助于破坏民主。
225 00:19:07,674 --> 00:19:08,536 说话人 SPEAKER_00: 我对此非常担忧。
226 00:19:08,936 --> 00:19:11,601 说话人 SPEAKER_00: 大公司正在为此做些什么，但可能还不够。
227 00:19:12,609 --> 00:19:14,594 说话人 SPEAKER_00: 有可能出现大量失业。
228 00:19:14,874 --> 00:19:15,996 说话人 SPEAKER_00: 我们对此并不真正了解。
229 00:19:16,656 --> 00:19:18,661 说话人 SPEAKER_00: 我的意思是，过去的技术往往创造了工作。
230 00:19:19,402 --> 00:19:26,976 说话人 SPEAKER_00: 但是这些东西，嗯，我们曾经比动物更强壮，我们曾经是最强壮的东西。
231 00:19:27,857 --> 00:19:31,144 说话人 SPEAKER_00: 而当我们进入工业革命时，我们有了比人强壮得多的机器。
232 00:19:31,785 --> 00:19:34,128 说话人 SPEAKER_00: 手工劳动的工作消失了。
233 00:19:34,632 --> 00:19:41,018 说话人 SPEAKER_00：当有了比我们更聪明的东西时，智力领域的体力劳动工作将消失。
234 00:19:41,698 --> 00:19:43,339 说话人 SPEAKER_00：我认为将会出现大量的失业。
235 00:19:44,039 --> 00:19:45,080 说话人 SPEAKER_00：我的朋友 Jan 不同意。
236 00:19:47,143 --> 00:19:50,685 说话人 SPEAKER_00：必须区分两种失业，两种工作损失。
237 00:19:51,146 --> 00:19:56,652 说话人 SPEAKER_00：会有一些工作，你可以无限期地增加完成的工作量，比如在医疗保健领域。
238 00:19:57,192 --> 00:20:04,638 说话人 SPEAKER_00：每个人都希望有一个私人医生一直和他们交谈，这样当他们有点痒的时候，医生就会说，不，那不是癌症。
239 00:20:04,618 --> 00:20:09,804 说话人 SPEAKER_00：所以医学领域的工作量有很大的扩展空间，因此那里不会出现失业。
240 00:20:10,365 --> 00:20:12,686 说话人 SPEAKER_00：但在其他方面，可能会有显著的失业。
241 00:20:13,968 --> 00:20:16,550 说话人 SPEAKER_00: 将会有大规模的监控，这已经在中国发生了。
242 00:20:17,791 --> 00:20:22,856 说话人 SPEAKER_00: 将会出现致命的自主武器，它们将会非常恶劣，并且它们将会真正地自主。
243 00:20:23,237 --> 00:20:25,519 说话人 SPEAKER_00: 美国人已经非常明确地做出了决定。
244 00:20:25,900 --> 00:20:31,925 说话人 SPEAKER_00: 他们说人们将负责，但当你问他们这意味着什么时，并不意味着人们将参与决定杀戮的决策过程。
245 00:20:33,239 --> 00:20:38,885 说话人 SPEAKER_00: 就我所知，美国人打算到 2030 年让一半的士兵成为机器人。
246 00:20:40,487 --> 00:20:43,069 说话人 SPEAKER_00: 现在，我并不确定这是真的。
247 00:20:43,109 --> 00:20:53,962 说话人 SPEAKER_00: 我问过 Chuck Schumer 的国家情报顾问，他说，嗯，如果房间里有人知道的话，那应该是我。
248 00:20:55,104 --> 00:21:00,089 说话人 SPEAKER_00: 所以我认为这是美国人的说法，你可能这么想，但我不可能发表评论。
249 00:21:02,887 --> 00:21:06,653 说话人 SPEAKER_00: 将会有网络犯罪和故意制造的流行病。
250 00:21:08,214 --> 00:21:19,048 说话人 SPEAKER_00: 我很高兴在英国，尽管他们在监管方面没有做太多，但他们已经拨出了一些资金，以便他们可以尝试开源模型，看看让它们犯网络犯罪有多容易。
251 00:21:20,190 --> 00:21:21,132 说话人 SPEAKER_00: 这将非常重要。
252 00:21:21,592 --> 00:21:23,173 说话人 SPEAKER_00: 将会有歧视和偏见。
253 00:21:23,855 --> 00:21:29,082 说话人 SPEAKER_00: 我认为这些威胁不如其他威胁重要，但我是位年迈的白人男性。
254 00:21:30,344 --> 00:21:40,258 说话人 SPEAKER_00: 我认为，如果你的目标是不要无偏见，而是要减少你取代的系统中的偏见，那么处理歧视和偏见比其他事情更容易。
255 00:21:40,979 --> 00:21:46,287 说话人 SPEAKER_00: 原因是，如果你冻结 AI 系统的权重，你可以测量其偏见，但你不能对人这样做。
256 00:21:47,028 --> 00:21:49,412 说话人 SPEAKER_00: 一旦开始检查，他们就会改变他们的行为。
257 00:21:50,272 --> 00:21:55,119 说话人 SPEAKER_00：我认为我们可以做很多事情来解决这个问题，那就是歧视和偏见。
258 00:21:57,090 --> 00:22:03,391 说话人 SPEAKER_00：但我真正担心的是长期存在的生存威胁，这也是我离开谷歌后谈论的事情。
259 00:22:04,374 --> 00:22:06,882 说话人 SPEAKER_00：这就是这些事情可能会消灭人类的那种威胁。
260 00:22:07,721 --> 00:22:10,525 说话人 SPEAKER_00：人们说，这只是一本科幻小说。
261 00:22:11,285 --> 00:22:13,287 说话人 SPEAKER_00：嗯，我认为这不再是科幻小说了。
262 00:22:13,307 --> 00:22:16,309 说话人 SPEAKER_00：关于它的科幻小说有很多，但我觉得这已经不再是科幻小说了。
263 00:22:17,089 --> 00:22:24,037 说话人 SPEAKER_00：其他人，大公司说这样的话是为了转移人们对其他所有坏事的注意力。
264 00:22:24,797 --> 00:22:29,662 说话人 SPEAKER_00：这也是我必须离开谷歌才能说出这些话的原因之一，这样我就不会被指责是谷歌的走狗。
265 00：22：30,481 --> 00：22：35,686 议长 SPEAKER_00：虽然我必须承认，我仍然有一些 Google 的分享。
266 00：22：36,932 --> 00：22：39,256 议长 SPEAKER_00：他们有几种方法可以消灭我们。
267 00：22：41,319 --> 00：23：02,570 议长 SPEAKER_00：所以超级智能会被普京、习或特朗普等坏人利用，他们会想用它来纵选民和发动战争，他们会让它做非常坏的事情，他们可能会走得太远，它可能会接管。
268 00：23：03,873 --> 00：23：06,416 议长 SPEAKER_00：可能最让我担心的是
269 00:23:07,762 --> 00:23:15,513 说话人 SPEAKER_00：如果你想拥有一个能够完成任务的智能代理，你需要给它创建子目标的能力。
270 00:23:17,215 --> 00:23:20,601 说话人 SPEAKER_00：所以如果你想前往美国，你有一个子目标就是到达机场。
271 00:23:21,382 --> 00:23:24,467 说话人 SPEAKER_00：然后你可以专注于这个子目标，暂时不必担心其他所有事情。
272 00:23:25,989 --> 00:23:30,295 说话人 SPEAKER_00：因此，如果允许它们创建子目标，超级智能将更加有效。
273 00:23:32,063 --> 00:23:41,855 说话人 SPEAKER_00: 一旦允许他们这样做，他们很快就会意识到有一个几乎普遍的子目标，这个目标几乎对任何事情都有帮助，那就是获得更多的控制权。
274 00:23:44,278 --> 00:23:53,630 说话人 SPEAKER_00: 我和欧盟的一位副总裁谈了这些事情，我们想要控制这些事情，以便它们能做得更好，我们想要这些事情，以便它们能做得更好。
275 00:23:54,191 --> 00:23:55,853 说话人 SPEAKER_00: 她的反应是，嗯，为什么不会呢？
276 00:23:55,913 --> 00:23:56,913 说话人 SPEAKER_00: 我们把它搞得一团糟。
277 00:23:57,915 --> 00:24:00,618 说话人 SPEAKER_00: 她把这当作理所当然。
278 00:24:02,117 --> 00:24:04,060 说话人 SPEAKER_00: 所以他们的子目标是获得更多权力。
279 00:24:04,101 --> 00:24:06,804 说话人 SPEAKER_00: 他们更有效地实现对我们有益的事情。
280 00:24:08,145 --> 00:24:11,630 说话人 SPEAKER_00: 他们将更容易获得更多权力，因为他们将能够操纵人们。
281 00:24:12,270 --> 00:24:16,696 讲者 SPEAKER_00：例如，特朗普可以入侵国会大厦，而他自己从未去过那里。
282 00:24:16,817 --> 00:24:18,720 讲者 SPEAKER_00：只需通过谈话，他就能入侵国会大厦。
283 00:24:19,400 --> 00:24:22,944 讲者 SPEAKER_00：这些超级智能，只要它们能与人交流，
284 00:24:22,924 --> 00:24:26,135 讲者 SPEAKER_00：当它们的智慧远超我们时，它们将能够说服我们做各种事情。
285 00:24:26,999 --> 00:24:30,430 说话者 SPEAKER_00: 我认为没有希望有一个大开关来关闭它们。
286 00:24:30,770 --> 00:24:35,567 说话者 SPEAKER_00: 任何想要关闭这个开关的人都会被超级智能说服，这是一个非常糟糕的想法。
287 00:24:39,361 --> 00:24:46,491 说话者 SPEAKER_00: 另一个让许多人担忧的问题是，如果超级智能相互竞争会发生什么？
288 00:24:47,113 --> 00:24:47,973 说话者 SPEAKER_00: 你将会有进化。
289 00:24:47,993 --> 00:24:51,138 说话者 SPEAKER_00: 能够获取最多资源的那一个将变得最聪明。
290 00:24:52,099 --> 00:24:58,230 说话者 SPEAKER_00: 一旦它们有了任何自我保护的感觉，那么就会发生进化。
291 00:24:58,769 --> 00:25:02,455 说话者 SPEAKER_00: 具有更多自我保护意识的将获胜，更具侵略性的也将获胜。
292 00:25:03,017 --> 00:25:07,123 说话者 SPEAKER_00: 然后你就会遇到像我们这样的跳上来的黑猩猩所遇到的所有问题。
293 00:25:07,103 --> 00:25:11,652 说话人 SPEAKER_00：我们是在小部落中进化而来的，与其他部落有很多敌意和竞争。
294 00:25:15,339 --> 00:25:21,772 说话人 SPEAKER_00：我想以我在 2023 年初的一个顿悟来结束我的讲话。
295 00:25:21,894 --> 00:25:26,884 说话人 SPEAKER_00：我以前一直认为
296 00:25:30,626 --> 00:25:32,909 说话人 SPEAKER_00：我们离超级智能还非常遥远。
297 00:25:33,288 --> 00:25:37,232 说话人 SPEAKER_00: 我以前告诉人们 50 到 100 年，也许 30 到 100 年。
298 00:25:37,292 --> 00:25:38,054 说话人 SPEAKER_00: 这还很长。
299 00:25:38,314 --> 00:25:39,634 说话人 SPEAKER_00: 我们现在不用担心这个。
300 00:25:41,757 --> 00:25:46,000 说话人 SPEAKER_00: 我还认为让 AI 模型更像大脑会使它们变得更好。
301 00:25:46,381 --> 00:25:48,863 说话人 SPEAKER_00: 我认为大脑比我们现有的 AI 要好得多。
302 00:25:49,324 --> 00:25:59,772 说话人 SPEAKER_00: 如果我们能让 AI 更像是大脑，例如，通过拥有三个时间尺度，我们目前的大多数模型只有两个时间尺度，一个用于改变权重，这是缓慢的，
303 00:26:00,377 --> 00:26:04,644 说话人 SPEAKER_00: 另一个用于输入的词语，这是快速的，改变神经活动。
304 00:26:05,025 --> 00:26:07,107 说话人 SPEAKER_00: 所以神经活动的变化和权重的变化。
305 00:26:07,689 --> 00:26:09,330 讲者 SPEAKER_00：大脑的时标比这要多。
306 00:26:09,652 --> 00:26:12,796 讲者 SPEAKER_00：大脑中权重快速变化并迅速衰减。
307 00:26:13,336 --> 00:26:15,099 讲者 SPEAKER_00：这可能就是它处理很多短期记忆的方式。
308 00:26:15,460 --> 00:26:20,468 讲者 SPEAKER_00：由于技术原因，我们无法在我们的模型中实现这一点，这与矩阵-矩阵乘法有关。
309 00:26:21,169 --> 00:26:27,397 说话人 SPEAKER_00: 我仍然相信，一旦我们将其纳入我们的模型，它们就会变得更好。
310 00:26:29,200 --> 00:26:45,005 说话人 SPEAKER_00: 因为在那之前我做了两年，我突然相信，我们现在拥有的东西，我们现在拥有的数字模型，可能已经非常接近大脑，并且将变得比大脑更好。
311 00:26:45,486 --> 00:26:47,209 说话人 SPEAKER_00: 我将解释我为什么相信这一点。
312 00:26:49,467 --> 00:26:51,569 说话人 SPEAKER_00: 所以数字计算很棒。
313 00:26:52,510 --> 00:26:58,579 说话人 SPEAKER_00: 你可以在不同的计算机、不同的硬件上运行相同的程序，或者在不同的硬件上运行相同的神经网络。
314 00:26:58,940 --> 00:27:00,261 说话人 SPEAKER_00: 你只需要保存权重。
315 00:27:01,282 --> 00:27:02,444 说话人 SPEAKER_00: 这意味着它是永恒的。
316 00:27:02,945 --> 00:27:09,494 说话人 SPEAKER_00: 一旦你有了权重，它们就是永恒的，因为如果硬件损坏了，只要你有权重，你就可以制造更多的硬件并运行相同的神经网络。
317 00:27:11,196 --> 00:27:17,465 说话人 SPEAKER_00: 但要这样做，我们需要将晶体管运行在非常高的功率下，以便它们表现出数字特性。
318 00:27:17,647 --> 00:27:20,730 说话人 SPEAKER_00: 我们必须拥有能够完全按照你的指示执行的硬件。
319 00:27:21,511 --> 00:27:25,416 说话人 SPEAKER_00: 当我们通过告诉计算机如何做事情来指导计算机时，那是非常棒的。
320 00:27:26,778 --> 00:27:31,403 说话人 SPEAKER_00: 但我们现在有了另一种让计算机做事的方法。
321 00:27:31,864 --> 00:27:39,832 说话人 SPEAKER_00: 因此，我们现在有可能利用硬件丰富的类比特性，以远低于能源消耗的方式完成计算。
322 00:27:40,794 --> 00:27:46,660 说话人 SPEAKER_00: 所以，这些大型语言模型在训练时，学习需要像兆瓦一样，使用也需要像兆瓦一样，而我们只需要 30 瓦。
323 00:27:50,335 --> 00:28:07,602 说话人 SPEAKER_00: 因此，因为我们知道如何训练，也许我们可以使用类比硬件，每块硬件都有所不同，但我们训练它以利用其独特的特性，使其完成我们想要的工作，从而为输入提供正确的输出。
324 00:28:09,151 --> 00:28:15,037 说话人 SPEAKER_00: 如果我们这样做，那么我们就可以放弃硬件和软件必须分开的想法。
325 00:28:16,458 --> 00:28:21,844 说话人 SPEAKER_00: 我们可以拥有只在那一小块硬件上工作的权重，这样我们就可以更加节能。
326 00:28:25,106 --> 00:28:30,813 说话人 SPEAKER_00: 所以我开始思考我所说的“凡人计算”，在那里你放弃了硬件和软件之间的区别。
327 00:28:31,472 --> 00:28:39,141 说话人 SPEAKER_00: 使用非常低功耗的模拟计算，你可以并行处理以导纳形式存储的万亿个权重。
328 00:28:40,825 --> 00:28:44,087 说话人 SPEAKER_00: 此外，硬件也不需要那么可靠。
329 00:28:44,169 --> 00:28:48,272 说话人 SPEAKER_00: 你不需要硬件在指令层面上总是按照你的要求去做。
330 00:28:49,134 --> 00:28:54,398 说话人 SPEAKER_00: 你可以有粘稠的硬件，你可以让它生长，然后你只需要学会让它做正确的事情。
331 00:28:55,941 --> 00:28:57,722 说话人 SPEAKER_00: 所以你应该能够以更低的成本使用硬件。
332 00:28:58,143 --> 00:29:04,710 说话人 SPEAKER_00: 甚至可以对神经元进行基因工程，使其由回收的神经元制成。
333 00:29:06,632 --> 00:29:09,414 说话人 SPEAKER_00：我想给你们举一个例子，看看这有多高效。
334 00:29:10,339 --> 00:29:23,589 说话人 SPEAKER_00：所以在神经网络中，你一直在做的事情就是将一个神经活动向量与权重矩阵相乘，以得到下一层的神经活动向量，至少得到下一层的输入。
335 00:29:23,609 --> 00:29:26,998 说话人 SPEAKER_00：因此，向量矩阵乘法是你要使其高效化的东西。
336 00:29:28,210 --> 00:29:39,163 说话人 SPEAKER_00：所以我们在数字计算机中是这样做的，我们有一些高功率驱动的晶体管来表示 32 位数字中的位。
337 00:29:40,104 --> 00:29:48,434 说话者 SPEAKER_00: 然后要乘以两个 32 位数字，你需要执行，我从未上过任何计算机科学课程，但我认为你需要执行大约 1,000 次单比特数字操作。
338 00:29:48,654 --> 00:29:52,338 说话者 SPEAKER_00: 这大约是比特长度的平方，如果你想快速完成的话。
339 00:29:55,123 --> 00:29:56,984 说话者 SPEAKER_00: 所以你需要执行很多这样的数字操作。
340 00:29:58,248 --> 00:30:10,839 说话者 SPEAKER_00: 有一种更简单的方法来做这件事，那就是你让神经活动成为电压，让权重成为电导，电压乘以电导就是每单位时间的电荷，电荷只是简单地相加。
341 00:30:11,932 --> 00:30:18,279 说话人 SPEAKER_00: 你可以通过在导线上施加一些电压来执行向量矩阵乘法。
342 00:30:18,940 --> 00:30:24,266 说话人 SPEAKER_00: 下一个层中的每个神经元接收到的将是这个向量与这些权重的乘积。
343 00:30:26,107 --> 00:30:26,627 说话人 SPEAKER_00: 这很棒。
344 00:30:27,148 --> 00:30:28,509 说话人 SPEAKER_00: 它的能量效率高得多。
345 00:30:28,730 --> 00:30:30,231 说话人 SPEAKER_00: 你已经可以买到能做这个的薯片了。
346 00:30:31,093 --> 00:30:33,776 说话人 SPEAKER_00: 但每次你这样做，它都会略有不同。
347 00:30:35,958 --> 00:30:37,839 说话人 SPEAKER_00: 此外，做非线性的事情很难。
348 00:30:40,519 --> 00:30:42,742 说话人 SPEAKER_00: 所以在 mortal computation 方面有几个大问题。
349 00:30:44,326 --> 00:31:00,611 讲者 SPEAKER_00：一方面，使用反向传播算法比较困难，因为如果你正在利用某件特定硬件的古怪模拟特性，你可以假设硬件不知道自己的特性，因此现在使用反向传播算法在自己身上变得很困难。
350 00:31:00,830 --> 00:31:06,058 讲者 SPEAKER_00：使用调整权重并查看是否有所帮助的强化算法要容易得多，但它们非常低效。
351 00:31:06,920 --> 00:31:09,243 讲者 SPEAKER_00：对于小型网络，
352 00:31:09,680 --> 00:31:13,786 讲者 SPEAKER_00：我们已经提出了与反向传播算法相当高效的方法，但略逊一筹。
353 00:31:14,326 --> 00:31:17,451 说话人 SPEAKER_00: 但这些方法还没有实现规模化，我不知道它们是否能够实现。
354 00:31:17,872 --> 00:31:20,255 说话人 SPEAKER_00: 从某种意义上说，反向传播就是正确的事情去做。
355 00:31:20,296 --> 00:31:25,804 说话人 SPEAKER_00: 对于大型、深度网络，我不确定我们是否能够得到像反向传播一样好的效果。
356 00:31:26,263 --> 00:31:33,595 说话人 SPEAKER_00: 因此，这些模拟系统中的学习算法可能不会像我们为大型语言模型所拥有的那样好。
357 00:31:33,575 --> 00:31:39,804 说话人 SPEAKER_00: 另一个相信大型语言模型的原因是它可能有上万亿个权重。
358 00:31:40,664 --> 00:31:42,848 说话人 SPEAKER_00: 你有一百万亿个权重。
359 00:31:42,868 --> 00:31:46,053 说话人 SPEAKER_00: 即使你只使用其中的 10%来获取知识，那也是 1000 万亿个权重。
360 00:31:46,773 --> 00:31:51,902 说话人 SPEAKER_00: 但是大型语言模型在其万亿个权重中，所知道的信息比你们多上千倍。
361 00:31:52,962 --> 00:31:54,885 说话人 SPEAKER_00: 它的知识多得多。
362 00:31:55,827 --> 00:31:57,789 说话人 SPEAKER_00: 这部分原因是因为它看到了多得多的数据。
363 00:31:58,111 --> 00:32:00,273 说话人 SPEAKER_00: 但可能是因为它有一个更好的学习算法。
364 00:32:01,080 --> 00:32:02,403 说话人 SPEAKER_00: 我们并没有针对这一点进行优化。
365 00:32:02,442 --> 00:32:09,034 说话人 SPEAKER_00: 我们并不擅长将大量经验压缩到少量连接中，现在几万亿已经算是少量了。
366 00:32:10,296 --> 00:32:13,580 说话人 SPEAKER_00: 我们优化的是拥有不多经验。
367 00:32:14,122 --> 00:32:15,663 说话人 SPEAKER_00: 人的一生大约只有一亿秒。
368 00:32:16,645 --> 00:32:19,490 说话人 SPEAKER_00: 假设你在 30 岁之后不再学习任何东西，这基本上是真实的。
369 00：32：19,971 --> 00：32：25,941 演讲者 SPEAKER_00：所以你活了大约 10 亿秒，你有 1000 万亿个连接。
所以你拥有的参数数量比你拥有的经验疯狂得多。
371 00：32：30,608 --> 00：32：34,635 演讲者 SPEAKER_00：所以我们的大脑已经优化了，可以最好地利用不多的经验。
372 00:32:38,362 --> 00:32:48,679 说话者 SPEAKER_00：人类计算的一个大问题是，如果软件与硬件不可分割，一旦系统学习完毕，如果硬件损坏，就会丢失所有知识。
373 00:32:49,079 --> 00:32:50,561 说话人 SPEAKER_00: 在这种意义上是凡人的。
374 00:32:51,065 --> 00:32:54,230 说话人 SPEAKER_00: 那么你们如何将这种知识转移到另一个凡人系统中呢？
375 00:32:55,352 --> 00:33:05,248 说话人 SPEAKER_00: 嗯，你可以让老系统做一个讲座，让新系统去思考如何改变它们大脑中的权重，以便它们也能说出那样的话。
376 00:33:06,410 --> 00:33:07,491 说话人 SPEAKER_00: 这被称为蒸馏。
377 00:33:07,853 --> 00:33:12,279 说话人 SPEAKER_00: 你尝试让学生模型模仿教师模型的输出。
378 00:33:12,749 --> 00:33:15,532 说话人 SPEAKER_00: 这样做是可行的，但效率不高。
379 00:33:16,914 --> 00:33:20,199 说话人 SPEAKER_00: 有些同学可能注意到，大学并不那么有效率。
380 00:33:20,239 --> 00:33:23,282 说话人 SPEAKER_00: 将知识从教授传递给学生非常困难。
381 00:33:26,647 --> 00:33:31,734 说话人 SPEAKER_00：这种蒸馏方法，比如一句话，可能包含几百比特的信息。
382 00:33:32,035 --> 00:33:35,640 说话人 SPEAKER_00：即使你以最优的方式学习，也无法传达超过几百比特。
383 00:33:37,041 --> 00:33:40,426 说话人 SPEAKER_00：但是如果你使用这些大型数字模型，那么
384 00:33:42,584 --> 00:33:55,023 说话人 SPEAKER_00：如果你观察一群所有都具有完全相同的神经网络、完全相同的权重，并且它们是数字的，所以它们会以完全相同的方式使用这些权重。
385 00:33:56,405 --> 00:34:01,233 说话人 SPEAKER_00: 这一千个不同的代理都去查看互联网的不同部分并学习东西。
386 00:34:02,057 --> 00:34:05,382 说话人 SPEAKER_00: 现在你想让它们中的每一个都知道其他代理学到了什么。
387 00:34:06,262 --> 00:34:09,186 说话人 SPEAKER_00: 你可以通过平均梯度或平均权重来实现这一点。
388 00:34:09,806 --> 00:34:14,152 说话人 SPEAKER_00: 因此，你可以将一个代理学到的内容大规模地传达给所有其他代理。
389 00:34:15,032 --> 00:34:23,202 说话人 SPEAKER_00: 所以当你分享权重或梯度时，你正在传达的是万亿个数字，而不仅仅是几百比特，而是万亿个实数。
390 00:34:24,284 --> 00:34:27,507 说话人 SPEAKER_00: 因此，它们的通信能力非常强。
391 00:34:27,673 --> 00:34:29,657 说话人 SPEAKER_00: 这就是它们超越我们的地方。
392 00:34:30,778 --> 00:34:36,126 说话人 SPEAKER_00: 它们在多个相同模型的副本之间通信得更好。
393 00:34:36,387 --> 00:34:39,692 说话人 SPEAKER_00: 正是因为这个，GPT-4 才知道比人类多得多的东西。
394 00:34:39,711 --> 00:34:41,574 说话人 SPEAKER_00: 并不是单一模型做到了这一点。
395 00:34:41,655 --> 00:34:44,599 说话人 SPEAKER_00: 而是一大堆相同模型的副本在不同的硬件上运行。
396 00:34:48,144 --> 00:34:56,297 说话人 SPEAKER_00: 所以我的结论，虽然我不太喜欢，就是数字计算
397 00:34:56,530 --> 00:34:57,632 说话人 SPEAKER_00: 需要大量的能量。
398 00:34:58,172 --> 00:34:59,353 说话人 SPEAKER_00: 因此它永远不会进化。
399 00:35:00,175 --> 00:35:04,141 说话人 SPEAKER_00: 我们必须通过利用硬件的怪癖来进化，以实现非常低的能耗。
400 00:35:05,844 --> 00:35:09,789 说话人 SPEAKER_00: 但一旦拥有它，代理之间的共享就变得非常容易。
401 00:35:09,809 --> 00:35:16,500 说话人 SPEAKER_00: GPT-4 在大约 2%的权重中拥有数千倍的知识。
402 00:35:16,519 --> 00:35:17,922 说话人 SPEAKER_00: 这相当令人沮丧。
403 00:35:19,804 --> 00:35:24,833 说话人 SPEAKER_00: 生物计算非常适合进化，因为它需要的能量非常少。
404 00:35:25,960 --> 00:35:30,768 说话人 SPEAKER_00: 但我的结论是，数字计算只是更好。
405 00:35:31,831 --> 00:35:43,273 说话人 SPEAKER_00: 我认为在接下来的 20 年里，大概有 50%的概率，它可能会比我们更聪明。
406 00:35:44,034 --> 00:35:47,681 说话人 SPEAKER_00: 而且很可能在接下来的 100 年里，它将远远比我们聪明。
407 00:35:48,117 --> 00:35:52,083 说话人 SPEAKER_00: 因此，我们需要考虑如何应对这种情况。
408 00:35:52,844 --> 00:35:58,554 说话人 SPEAKER_00: 而且很少有例子是更智能的事物被不那么智能的事物所控制。
409 00:35:59,655 --> 00:36:02,860 说话人 SPEAKER_00: 一个很好的例子是一个母亲被婴儿控制。
410 00:36:02,880 --> 00:36:07,768 说话人 SPEAKER_00: 进化为此付出了很多努力，以确保婴儿能够生存。
411 00:36:07,929 --> 00:36:10,193 说话人 SPEAKER_00: 对于婴儿来说，能够控制母亲非常重要。
412 00:36:11,474 --> 00:36:13,077 说话人 SPEAKER_00: 但这样的例子并不多。
413 00:36:14,239 --> 00:36:18,286 说话人 SPEAKER_00: 有些人认为我们可以让这些东西变得仁慈。
414 00:36:19,568 --> 00:36:25,036 说话人 SPEAKER_00: 但如果它们相互竞争，我认为它们会开始像黑猩猩一样行事。
415 00:36:26,077 --> 00:36:33,869 说话人 SPEAKER_00: 我并不确信，如果它们变得非常聪明并且有了自我保护的概念，你还能让它们保持仁慈。
416 00:36:35,231 --> 00:36:37,434 说话人 SPEAKER_00: 它们可能会决定它们比我们更重要。
417 00：36：39,016 --> 00：36：43,081 演讲者 SPEAKER_00：我想，我在创纪录的时间内完成了讲座。

1 00:00:00,031 --> 00:00:05,945 说话人 SPEAKER_00: 我们的主要限制是我们的体力，而工业革命消除了这个限制。
2 00:00:06,367 --> 00:00:11,880 说话人 SPEAKER_00: 现在我们主要的限制是我们的智力，而人工智能将消除这个限制。
3 00:00:24,937 --> 00:00:28,626 说话人 SPEAKER_00: 因此，深度学习是受大脑工作方式的启发。
4 00:00:28,987 --> 00:00:32,655 说话人 SPEAKER_00: 我们并不完全清楚大脑是如何工作的，但我们对其了解很多。
5 00:00:33,679 --> 00:00:39,673 说话人 SPEAKER_00：在你的大脑中，有很多被称为神经元的脑细胞，这些脑细胞之间有连接。
6 00:00:40,884 --> 00:00:44,192 说话人 SPEAKER_00：当你学习某样东西时，你会改变这些连接的强度。
7 00:00:44,993 --> 00:00:47,156 说话人 SPEAKER_00：神经元通过“ping”的方式向其他神经元发送信号。
8 00:00:48,179 --> 00:00:51,064 说话人 SPEAKER_00：神经元只需决定何时“ping”即可。
9 00:00:51,646 --> 00:00:58,079 说话者 SPEAKER_00: 它会根据来自其他神经元的输入来决定何时进行 ping 操作，其中一些是感觉神经元，就像在您的视网膜中一样。
10 00:00:59,161 --> 00:01:01,104 说话者 SPEAKER_00: 如果它获得了足够的输入，
11 00:01:01,085 --> 00:01:02,106 说话者 SPEAKER_00: 那么它就会进行 ping 操作。
12 00:01:02,606 --> 00:01:05,412 说话者 SPEAKER_00: 但是它获得的输入量取决于连接的权重。
13 00:01:05,671 --> 00:01:10,900 说话者 SPEAKER_00：所以，一个权重较小的连接意味着当一个神经元向另一个神经元发送信号时，它的影响不大。
14 00:01:11,600 --> 00:01:18,471 说话者 SPEAKER_00：一个权重较大的连接意味着当一个神经元向另一个神经元发送信号时，它的影响很大，很可能会让它“砰”地一下。
15 00:01:18,492 --> 00:01:21,777 说话者 SPEAKER_00：而学习就是改变连接的强度。
16 00:01:22,111 --> 00:01:27,843 说话者 SPEAKER_00：在学习中，大脑使用什么原则来改变连接的强度，这是主要问题。
17 00:01:28,185 --> 00:01:36,001 讲者 SPEAKER_00：在 20 世纪 80 年代，人们提出了一种称为反向传播的方法，这是决定如何改变连接强度的一种方式。
18 00:01:36,965 --> 00:01:40,272 讲者 SPEAKER_00：结果在 20 世纪 80 年代，它表现还算不错。
19 00:01:40,706 --> 00:01:47,316 讲者 SPEAKER_00：当时我们没有意识到，如果你给它大量的数据和计算量，它会表现得非常好。
20 00:01:47,337 --> 00:01:55,409 讲者 SPEAKER_00：现在大型聊天机器人使用反向传播来改变计算机中模拟的神经网络中连接的强度。
21 00:01:56,450 --> 00:01:58,192 说话人 SPEAKER_00: 深度学习就是这样工作的。
22 00:01:58,614 --> 00:02:02,799 说话人 SPEAKER_00: 比如说，如果你想有一个能识别鸟类的神经网络，
23 00:02:03,522 --> 00:02:06,947 说话人 SPEAKER_00: 在第一层，它可能有一些能识别边缘的神经元。
24 00:02:07,828 --> 00:02:13,014 说话人 SPEAKER_00: 然后在下一层，它可能有一些能识别边缘组合的神经元，比如两条边缘以锐角相交。
25 00:02:13,716 --> 00:02:15,078 说话人 SPEAKER_00: 那可能是鸟的喙。
26 00:02:15,579 --> 00:02:21,948 说话人 SPEAKER_00: 然后在下一层，可能有识别组合的神经元，这些组合可能是喙和眼睛。
27 00:02:22,467 --> 00:02:25,812 说话人 SPEAKER_00: 如果它们彼此之间的关系正确，它就会认为那可能是鸟的头。
28 00:02:26,213 --> 00:02:27,876 说话人 SPEAKER_00: 而技巧在于
29 00:02:27,855 --> 00:02:42,742 说话人 SPEAKER_00：它具有越来越复杂的特征检测器层，直到它看到，好吧，我们可能看到了一个鸟头，可能看到了一个像这样的鸟的脚，可能看到了鸟翅膀的尖端，如果我们看到所有这些，我们就说它可能是一只鸟。
30 00:02:43,043 --> 00:02:48,657 说话人 SPEAKER_00：但是所有的权重，连接强度都是在神经网络中学习的。
31 00:02:48,676 --> 00:02:53,530 说话人 SPEAKER_00：所以从 20 世纪 50 年代人工智能开始以来，有两种人工智能的方法。
32 00:02:54,131 --> 00:02:58,301 说话人 SPEAKER_00：有一种基于逻辑的方法，还有一种基于生物的方法。
33 00:02:58,282 --> 00:03:07,411 说话人 说话人_00：所以生物学方法试图模拟大脑中的神经网络，而逻辑方法试图模拟逻辑进行推理。
34 00:03:07,831 --> 00:03:15,038 说话人 说话人_00：直到最近，使用神经网络的生物学方法才比逻辑方法好得多。
35 00:03:15,778 --> 00:03:23,326 说话人 说话人_00：在人工智能的前50年里，许多年，几乎每个人都相信逻辑方法，但这并没有真正奏效。
36 00:03:23,306 --> 00:03:34,008 说话人 说话人_00：然后从本世纪初开始，大约在2009年，使用神经网络的生物学方法突然开始奏效。
37 00:03:35,192 --> 00:03:36,495 说话人 说话人_00：这带来了巨大的变化。
38 00:03:37,395 --> 00:03:41,485 说话人 说话人_00：所以一个重大的突破发生在2012年，
39 00:03:41,465 --> 00:03:53,181 说话人 说话人_00：当时模拟生物学的神经网络突然在识别物体方面变得非常出色，真正让神经网络工作得好的有三件事。
40 00:03:55,122 --> 00:04:01,491 说话人 说话人_00：第一点是来自像为游戏开发的 NVIDIA 芯片等东西带来的巨大计算机能力。
41 00:04:02,592 --> 00:04:06,038 说话人 说话人_00：第二点是来自互联网的大量数据。
42 00:04:06,354 --> 00:04:16,086 说话人 说话人_00：第三点是技术发展，例如，2017年，谷歌推出了使语言模型工作得更好的转换器。
43 00:04:16,487 --> 00:04:19,071 说话人 说话人_00：然后，神经网络引起了巨大的兴趣。
44 00:04:19,732 --> 00:04:30,725 说话人 说话人_00：随后，当 ChatGPT 发布时，人们发现神经网络真的能理解你说的话，并能合理地回答问题，引起了巨大的兴趣。
45 00:04:31,266 --> 00:04:34,589 说话人 说话人_00：我认为人工智能将在所有行业中得到应用。
46 00:04:34,569 --> 00:04:39,459 说话人 说话人_00：有些领域，它的作用显然会非常好，比如在医疗保健领域。
47 00:04:40,781 --> 00:04:45,310 说话人 说话人_00：目前，如果我有一些症状，我会去看家庭医生。
48 00:04:46,271 --> 00:04:51,461 说话人 说话人_00：如果我有某种罕见疾病，我的家庭医生可能从未见过这种罕见疾病的病例。
49 00:04:52,603 --> 00:04:56,189 说话人 SPEAKER_00：我更愿意去看一位看过上亿人的家庭医生
50 00:04:56,389 --> 00:05:02,759 说话人 SPEAKER_00：并且这位家庭医生了解我的整个基因组，知道我所有医疗测试的历史结果。
51 00:05:03,721 --> 00:05:07,648 说话人 SPEAKER_00：我们将得到这样的 AI 医生，他们的能力远超普通医生。
52 00:05:08,389 --> 00:05:11,213 说话人 SPEAKER_00：他们阅读医学图像的能力也将更出色。
53 00:05:11,954 --> 00:05:15,040 说话人 说话人_00：所以人工智能将能够使用更多信息。
54 00:05:15,600 --> 00:05:23,173 说话人 说话人_00：已经，如果你遇到难以诊断的病例，你可以把它们交给医生，
55 00:05:23,778 --> 00:05:25,622 说话人 说话人_00：他们大约能正确诊断40%。
56 00:05:26,365 --> 00:05:30,877 说话人 说话人_00：或者你可以把它们交给人工智能系统，它大约能正确诊断50%。
57 00:05:30,896 --> 00:05:37,314 说话人 SPEAKER_00: 或者你可以把它们交给医生和 AI 系统的组合，他们一起工作，正确率大约能达到 60%。
58 00:05:38,189 --> 00:05:42,494 说话人 SPEAKER_00: 在北美，每年大约有 20 万人因误诊而死亡。
59 00:05:43,314 --> 00:05:44,937 说话人 SPEAKER_00: 这已经是在生命上的巨大节省了。
60 00:05:45,617 --> 00:05:53,646 说话人 SPEAKER_00: 在教育方面，我们知道如果你有一个私人辅导老师，你学习东西的速度会比在和其他孩子一起的教室里快一倍。
61 00:05:54,887 --> 00:05:58,312 说话人 说话人_00: 利用人工智能，每个人都能拥有自己的私人导师。
62 00:05:58,851 --> 00:06:01,074 说话人 说话人_00: 这样可以让孩子们的学习速度提高一倍。
63 00:06:02,134 --> 00:06:04,137 说话人 说话人_00: 这将对教育产生巨大的影响。
64 00:06:05,098 --> 00:06:07,000 说话人 说话人_00: 对大学可能不是那么有利。
65 00:06:07,281 --> 00:06:12,526 说话人 说话人_00：但在任何有数据的领域，人工智能都将是有帮助的。
66 00:06:13,507 --> 00:06:14,668 说话人 说话人_00：所以让我给你举一个例子。
67 00:06:14,968 --> 00:06:25,341 说话人 说话人_00：我有一个邻居开发了一个 AI 系统，这个系统为一家从事采矿的公司，利用他们拥有的数据
68 00:06:25,692 --> 00:06:33,406 说话人 说话人_00：来记录执行某些操作所需的时间，比如挖掘特定长度的矿井，在不同情况下。
69 00:06:34,307 --> 00:06:37,132 说话人 SPEAKER_00: 他们对此有大约 100 亿个数据点。
70 00:06:37,574 --> 00:06:39,718 说话人 SPEAKER_00: 他们现在将这些数据放入了人工智能系统中。
71 00:06:40,406 --> 00:06:44,874 说话人 SPEAKER_00: 现在它们可以非常快速地回答你提出的任何问题。
72 00:06:45,654 --> 00:06:56,711 说话人 SPEAKER_00: 以往如果他们想知道，如果一个承包商说他能完成这项工作，他实际上能否在他说的时间内完成，这种情况的概率是多少。
73 00:06:58,052 --> 00:07:06,786 说话人 SPEAKER_00: 在过去，如果你想了解这个答案，你会雇佣一家大型咨询公司，他们会为你写一份报告，三周后你才能得到报告。
74 00:07:07,322 --> 00:07:11,430 说话人 SPEAKER_00: 现在的情况是，四秒后，他的系统就会给你答案。
75 00:07:11,930 --> 00:07:16,399 说话人 SPEAKER_00: 这只是一个例子，但所有拥有大量数据的行业都将如此。
76 00:07:16,980 --> 00:07:24,151 说话人 SPEAKER_00: 你将能够利用公司拥有的所有这些数据，并且能够非常有效地、快速地利用这些数据。
77 00:07:24,932 --> 00:07:31,845 说话人 SPEAKER_00：已经，当我想要处理一些新问题时，我会请 GPT-4 来帮助我。
78 00:07:32,939 --> 00:07:36,946 说话人 SPEAKER_00：所以我有一座小屋，小屋被某种蚂蚁侵占了。
79 00:07:37,706 --> 00:07:42,375 说话人 SPEAKER_00：我和 GPT-4 谈了，它告诉我可能是什么蚂蚁以及如何处理它们。
80 00:07:43,177 --> 00:07:44,197 说话人 SPEAKER_00：这非常有帮助。
81 00:07:44,639 --> 00:07:50,769 说话人 SPEAKER_00：就像有一个非常非常博学且非常非常有耐心的朋友。
82 00:07:51,630 --> 00:07:53,014 说话人 SPEAKER_00：但我们确实看到了他们的幻觉，对吧？
83 00:07:53,033 --> 00:07:55,737 说话人 SPEAKER_00：有时它会出错，但人们也会犯错。
84 00:07:56,439 --> 00:07:58,423 说话人 SPEAKER_00：它产生幻觉的事实
85 00:07:58,639 --> 00:08:00,184 说话人 说话人_00：这就是人们也会做的事情。
86 00:08:00,725 --> 00:08:02,449 说话人 说话人_00：当人们这样做的时候，这被称为虚构。
87 00:08:02,891 --> 00:08:04,675 说话人 说话人_00：人们一直在虚构。
88 00:08:05,237 --> 00:08:10,913 说话人 说话人_00：所以如果你看任何人回忆很久以前发生的事情，他们不会记得细节。
89 00:08:11,153 --> 00:08:14,201 说话人 SPEAKER_00: 他们会非常自信，但他们会把细节搞错。
90 00:08:14,721 --> 00:08:18,348 说话人 SPEAKER_00: 所以人们就像这些大型聊天机器人一样。
91 00:08:18,509 --> 00:08:19,690 说话人 SPEAKER_00: 他们总是胡编乱造。
92 00:08:20,232 --> 00:08:24,238 说话人 SPEAKER_00: 预测未来非常困难，尤其是当事物变化很快的时候。
93 00:08:25,000 --> 00:08:32,333 说话人 SPEAKER_00: 如果你想知道 10 年后会是什么样子，最好的办法是回顾 10 年前，看看那时是什么样子。
94 00:08:33,076 --> 00:08:38,264 说话人 SPEAKER_00: 10 年前，没有人会预料到会有像 GPT-4 这样的东西。
95 00:08:38,245 --> 00:08:41,708 说话人 SPEAKER_00: 或者像谷歌开发的 Gemini 这样的大型聊天机器人。
96 00:08:42,470 --> 00:08:46,315 说话人 SPEAKER_00: 从现在起 10 年后，我们将拥有人们无法预料到的东西。
97 00:08:47,355 --> 00:08:49,138 说话人 SPEAKER_00: 它们会比人们预期的要好得多。
98 00:08:50,019 --> 00:08:53,583 说话人 SPEAKER_00: 如果你展望几年后，事情不会发生重大变化。
99 00:08:54,004 --> 00:08:58,870 说话人 SPEAKER_00: 在 10 年的时间里，我们将看到人工智能能做什么的巨大变化。
100 00:08:59,238 --> 00:09:10,360 说话人 SPEAKER_00: 人工智能革命的意义，与之前的技术革命相比，在我们文明史上的意义有多大？
101 00:09:10,701 --> 00:09:13,145 说话人 SPEAKER_00: 所以如果你看看工业革命，
102 00:09:13,480 --> 00:09:18,986 说话人 SPEAKER_00: 工业革命中发生的事情是人力不再那么重要了。
103 00:09:19,668 --> 00:09:22,672 说话人 说话人_00：在那之前，如果你想挖沟渠，你需要人去挖沟渠。
104 00:09:23,754 --> 00:09:31,024 说话人 说话人_00：那时候有动物，有风车、水车之类的，但基本上还是依靠人力。
105 00:09:31,683 --> 00:09:34,467 说话人 说话人_00：工业革命之后，人力变得不再重要。
106 00:09:35,089 --> 00:09:38,052 说话人 说话人_00：现在正在发生的是，人的智力变得重要。
107 00:09:39,096 --> 00:09:47,268 说话人 SPEAKER_00：当人工智能变得比我们聪明得多时，人类智慧将变得不再重要，就像工业革命后人类力量变得不再重要一样。
108 00:09:47,628 --> 00:09:51,192 说话人 SPEAKER_00：你可以把历史看作是消除人的限制。
109 00:09:51,833 --> 00:09:56,278 说话人 SPEAKER_00：长期以来，我们主要的限制是我们的体力。
110 00:09:57,160 --> 00:10:00,565 说话人 SPEAKER_00：现在我们主要的限制是我们的智力。
111 00:10:00,950 --> 00:10:02,913 说话人 SPEAKER_00: 人工智能将消除这种限制。
112 00:10:03,514 --> 00:10:10,964 说话人 SPEAKER_00: 如果你问，你见过多少例子是更智能的东西被不那么智能的东西控制的？
113 00:10:11,565 --> 00:10:15,129 说话人 SPEAKER_00: 我只知道一个例子，那就是母亲和孩子。
114 00:10:15,910 --> 00:10:20,115 说话人 SPEAKER_00: 进化让婴儿控制母亲投入了大量的努力。
115 00:10:20,897 --> 00:10:22,820 说话人 SPEAKER_00：对物种的生存来说，这非常重要。
116 00:10:23,160 --> 00:10:27,125 说话人 SPEAKER_00：但婴儿和母亲在智力上基本上是一样的。
117 00:10:27,105 --> 00:10:31,716 说话人 SPEAKER_00：我们不知道当这些事物比我们更聪明时，我们是否还能保持控制。
118 00:10:32,717 --> 00:10:38,289 说话人 SPEAKER_00：有些人，比如我的朋友 Yann LeCun，认为这会没事，我们创造了这些事物，它们总是会按照我们告诉它们的去做。
119 00:10:39,192 --> 00:10:40,354 说话人 说话人_00: 我不相信。
120 00:10:40,433 --> 00:10:41,976 说话人 说话人_00: 我认为我们无法对此有信心。
121 00:10:42,619 --> 00:10:46,025 说话人 说话人_00: 我认为我们是物质实体。
122 00:10:46,158 --> 00:10:53,546 说话人 说话人_00: 一个人在计算机中无法开发的东西是没有的。
123 00:10:54,086 --> 00:10:57,811 说话人 SPEAKER_00: 从长远来看，计算机可以拥有我们所有的感知能力。
124 00:10:58,010 --> 00:10:59,613 说话人 SPEAKER_00: 我认为人并没有什么特别之处。
125 00:11:00,153 --> 00:11:01,595 说话人 SPEAKER_00: 他们只是非常、非常复杂。
126 00:11:02,014 --> 00:11:03,576 说话人 SPEAKER_00: 他们经过非常漫长的进化。
127 00:11:04,258 --> 00:11:05,578 说话人 说话人_00：我们对其他人来说非常特别。
128 00:11:07,140 --> 00:11:10,725 说话人 说话人_00：但我们没有任何东西是不能在机器中模拟的。
129 00:11:11,384 --> 00:11:14,629 说话人 说话人_00：有人担心人工智能会夺走你的工作。
130 00:11:15,097 --> 00:11:17,821 说话人 说话人_00：因此，人工智能存在许多不同的风险。
131 00:11:18,221 --> 00:11:20,023 说话人 说话人_00: 我们不应该混淆这些不同的风险。
132 00:11:20,783 --> 00:11:22,385 说话人 说话人_00: 不同的风险有不同的解决方案。
133 00:11:23,066 --> 00:11:27,610 说话人 说话人_00: 当然，人工智能将会取代很多工作。
134 00:11:28,331 --> 00:11:35,619 说话人 说话人_00: 所以像律师助理、为律师做研究的人这样的工作，人工智能现在可以做到，而且做得更好。
135 00:11:36,419 --> 00:11:38,702 说话人 说话人_00：所以我们不需要那么多的法律助理。
136 00:11:39,783 --> 00:11:42,826 说话人 说话人_00：对于许多普通办公室工作，
137 00:11:43,447 --> 00:11:45,230 说话人 说话人_00：将需要的人数要少得多。
138 00:11:46,791 --> 00:11:52,495 说话人 说话人_00：我有一个亲戚在一家医疗服务机构工作，负责回复投诉信。
139 00:11:53,417 --> 00:11:58,640 说话人 SPEAKER_00: 她以前需要 25 分钟来写一封投诉信的回复。
140 00:11:59,461 --> 00:12:06,207 说话人 SPEAKER_00: 现在她只需将信件输入 ChatGBT，它就会生成答案，即回复。
141 00:12:06,768 --> 00:12:07,989 说话人 SPEAKER_00: 她只需检查一下，然后发送出去。
142 00:12:08,009 --> 00:12:08,870 说话人 SPEAKER_00: 这只需要 5 分钟。
143 00:12:09,711 --> 00:12:12,493 说话人 说话人_00：所以他们需要的人比那样少五倍。
144 00:12:13,267 --> 00:12:16,972 说话人 说话人_00：这在许多普通办公室工作中很典型。
145 00:12:18,375 --> 00:12:31,096 说话人 说话人_00：现在有些人说人工智能将创造许多新的工作，它确实会创造许多新的工作，但并不清楚它是否会创造出足够的新工作来替代由人工智能而不是人完成的普通办公室工作。
146 00:12:31,898 --> 00:12:33,139 说话人 说话人_00：因此这是一个严重的问题。
147 00:12:33,660 --> 00:12:35,123 说话人 SPEAKER_00：会发生什么？
148 00:12:35,102 --> 00:12:37,865 说话人 SPEAKER_00：对于所有那些工作被人工智能取代的人。
149 00:12:38,346 --> 00:12:40,467 说话人 SPEAKER_00：我认为这是政府需要担心的问题。
150 00:12:40,989 --> 00:12:52,360 说话人 SPEAKER_00：我认为我最担心的是短期内网络攻击和生物武器因人工智能而变得更容易制造。
151 00:12:53,221 --> 00:13:03,591 说话人 SPEAKER_00：例如，去年钓鱼攻击增长了 1200%，这主要是因为这些聊天机器人可以被用于钓鱼攻击。
152 00:13:03,756 --> 00:13:07,705 说话人 SPEAKER_00：我担心人们会利用这些工具制造新的病原体。
153 00:13:08,405 --> 00:13:11,392 说话人 SPEAKER_00：目前对此几乎没有控制。
154 00:13:11,591 --> 00:13:12,394 说话人 SPEAKER_00：这是短期内的。
155 00:13:12,894 --> 00:13:17,342 说话人 SPEAKER_00：从长远来看，我非常担心失业和自主致命武器。
156 00:13:18,365 --> 00:13:24,297 说话人 SPEAKER_00：而从更长远的视角来看，我真的很担心这些事物变得比我们更聪明，并最终取代我们。
157 00:13:24,850 --> 00:13:30,004 说话人 SPEAKER_00：我认为在两到三年内，我们可能会看到自主致命武器。
158 00:13:30,046 --> 00:13:31,028 说话人 SPEAKER_00：那将是负面影响。
159 00:13:31,590 --> 00:13:35,181 说话人 SPEAKER_00：所有的大型国防部门都在尝试开发它们。
160 00:13:36,359 --> 00:13:44,991 说话人 SPEAKER_00：如果你看看到目前为止关于人工智能的法规，它们都有一条条款说这些法规都不适用于人工智能的军事用途。
161 00:13:45,734 --> 00:13:52,303 说话人 SPEAKER_00：所以，例如，欧洲法规有一个明确的条款，说明它们不适用于人工智能的军事用途。
162 00:13:53,325 --> 00:13:55,649 说话人 SPEAKER_00：因此，政府不愿意自我监管。
163 00:13:56,509 --> 00:13:59,615 说话人 SPEAKER_00: 他们正急于开发自主致命武器。
164 00:13:59,975 --> 00:14:01,437 说话人 SPEAKER_00: 一旦这些武器被使用，
165 00:14:01,957 --> 00:14:03,460 说话人 SPEAKER_00: 我们将看到它们有多么糟糕。
166 00:14:04,181 --> 00:14:10,509 说话人 SPEAKER_00: 在它们被使用并造成可怕后果之后，我们可能能够得到类似日内瓦公约那样的东西，就像我们为化学武器所得到的那样。
167 00:14:11,390 --> 00:14:12,751 说话人 说话人_00：这些惯例已经起作用了。
168 00:14:13,413 --> 00:14:15,475 说话人 说话人_00：在乌克兰，他们没有使用化学武器。
169 00:14:16,778 --> 00:14:22,745 说话人 说话人_00：但我们直到看到第一次世界大战中它们的糟糕程度，才得到了这些惯例。
170 00:14:23,366 --> 00:14:28,894 说话人 说话人_00：我们需要做的是在事物发展的过程中进行更多关于安全性的工作。
171 00:14:29,886 --> 00:14:32,750 说话人 SPEAKER_00: 只有大型公司才有资源去做这件事。
172 00:14:34,232 --> 00:14:38,440 说话人 SPEAKER_00: 因此，我们需要政府迫使大型公司加强安全工作。
173 00:14:39,640 --> 00:14:41,784 说话人 SPEAKER_00: 显然，你需要大量的计算机能力。
174 00:14:42,645 --> 00:14:44,489 说话人 SPEAKER_00: 显然，你需要大量的技术人才。
175 00:14:45,429 --> 00:14:47,552 说话人 SPEAKER_00：所以你需要保留你的技术科学家。
176 00:14:48,794 --> 00:14:54,222 说话人 SPEAKER_00：而保留技术科学家的方法是给他们一个可以进行研究的环境。
177 00:14:55,283 --> 00:15:02,875 说话人 SPEAKER_00：以加拿大为例，就其经济体量而言，在人工智能领域做得非常好。
178 00:15:03,296 --> 00:15:08,264 说话人 SPEAKER_00：它在人工智能领域做得很好，因为它有一项资助基础科学研究的政策。
179 00:15:08,725 --> 00:15:13,613 人工智能领域的领先研究者像我、Yoshua Bengio 和 Rich Sutton
180 00:15:13,592 --> 00:15:20,402 他们来到加拿大部分原因是社会制度，但主要是由于加拿大愿意资助基础科学研究。
181 00:15:20,942 --> 00:15:24,846 所以你们需要做的一件事就是好好资助基础科学研究。
182 00:15:25,567 --> 00:15:41,067 我对韩国的建议是，如果你们为有创造力的研究人员提供很好的基础科学研究资助，他们实际上会更愿意从事基础科学研究，而不是仅仅为了在银行工作而赚很多钱。
183 00:15:41,048 --> 00:15:43,892 有些人会离开去银行工作，赚很多钱。
184 00:15:44,131 --> 00:15:45,373 对于这一点，你也没办法。
185 00:15:46,495 --> 00:15:54,945 但对基础科学研究的良好支持，现在包括对计算能力的良好支持，这是留住优秀研究人员的途径。
186 00:15:56,346 --> 00:16:00,633 你认为我们会随着人工智能的出现而出现机器人吗？
187 00:16:00,653 --> 00:16:01,774 说话人 说话人_00：这是一个类人机器人吗？
188 00:16:03,086 --> 00:16:04,830 说话人 说话人_00：现在很多人正在做这个。
189 00:16:06,832 --> 00:16:07,494 说话人 说话人_00：这是可能的。
190 00:16:08,335 --> 00:16:12,059 说话人 说话人_00：我们可能会这样做的原因是因为工厂是为人类设计的。
191 00:16:13,322 --> 00:16:16,527 说话人 说话人_00：我的意思是，他们在工厂里设计所有机器，以便人们可以操作它们。
192 00:16:17,788 --> 00:16:22,014 说话人 说话人_00：因此，与其重新设计所有机器，你可能会想重新设计一个人。
193 00:16:22,897 --> 00:16:24,239 说话人 说话人_00：然后你可以使用相同的机器。
194 00:16:25,600 --> 00:16:27,403 说话人 说话人_00：现在人们正在尝试这样做。
195 00:16:27,583 --> 00:16:28,804 说话人 SPEAKER_00: 我不知道那会发生什么。
196 00:16:29,466 --> 00:16:31,929 说话人 SPEAKER_00: 我认为这是一个充满不确定性的时期。
197 00:16:32,821 --> 00:16:36,008 说话人 SPEAKER_00: 当你处于充满不确定性的时期，你应该谨慎。
198 00:16:36,889 --> 00:16:39,995 说话人 SPEAKER_00: 会有一段调整期或衰退期吗？
199 00:16:40,998 --> 00:16:42,421 说话人 SPEAKER_00: 关于这一点有两种不同的观点。
200 00:16:43,744 --> 00:16:45,548 说话人 SPEAKER_00: 所以我的观点是它将继续下去。
201 00:16:46,591 --> 00:16:56,581 说话人 SPEAKER_00: 有些人，尤其是那些相信人工智能逻辑方法、传统方法的人，他们从来不喜欢神经网络，一直在说，这一切即将结束。
202 00:16:57,422 --> 00:17:01,525 说话人 SPEAKER_00: 但神经网络，多年来，人们一直在说它们被过度炒作。
203 00:17:02,086 --> 00:17:03,087 说话人 说话人_00：但他们已经做到了。
204 00:17:03,347 --> 00:17:04,630 说话人 说话人_00：现在他们做得非常出色。
205 00:17:05,150 --> 00:17:06,290 说话人 说话人_00：所以我认为他们并没有被过度炒作。
206 00:17:06,951 --> 00:17:10,055 说话人 说话人_00：每隔几年，他们就会说，嘿，神经网络被过度炒作。
207 00:17:10,596 --> 00:17:12,538 说话人 说话人_00：一切即将崩溃并停止。
208 00:17:13,239 --> 00:17:14,640 说话人 说话人_00：他们每次都错了。
209 00:17:16,290 --> 00:17:17,761 说话人 说话人_00：我认为他们还会继续错下去。
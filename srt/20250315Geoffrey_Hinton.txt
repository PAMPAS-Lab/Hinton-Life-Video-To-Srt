1
00:00:00,031 --> 00:00:06,641
Speaker SPEAKER_00: in between 5 and 20 years from now, there's a good chance, a 50% chance, we'll get AI smarter than us.

2
00:00:11,587 --> 00:00:21,481
Speaker SPEAKER_00: So, I was more or less asleep in a hotel room in California and I had my phone upside down on the bedside table with the sound turned off.

3
00:00:22,289 --> 00:00:30,022
Speaker SPEAKER_00: And I just happened to be sleeping on my side when the phone was in my line of sight and it got bright.

4
00:00:30,042 --> 00:00:37,075
Speaker SPEAKER_00: So I showed this little slit of brightness and it started vibrating.

5
00:00:37,256 --> 00:00:40,661
Speaker SPEAKER_00: I was in California, and almost everybody I know is on the East Coast.

6
00:00:41,362 --> 00:00:43,567
Speaker SPEAKER_00: So I wondered who on earth could be calling me.

7
00:00:44,088 --> 00:00:50,237
Speaker SPEAKER_00: So I picked it up, and there was this long phone number that I didn't recognize the area code, the country code.

8
00:00:51,039 --> 00:00:53,883
Speaker SPEAKER_00: And then somewhere the Swedish accent asked if I was Geoffrey Hinton.

9
00:00:54,204 --> 00:00:56,067
Speaker SPEAKER_00: And then he told me I won the Nobel Prize in Physics.

10
00:00:56,087 --> 00:00:59,311
Speaker SPEAKER_00: And my first reaction was, well, wait a minute, I don't do physics.

11
00:01:00,232 --> 00:01:02,015
Speaker SPEAKER_00: This could be a prank.

12
00:01:01,996 --> 00:01:03,338
Speaker SPEAKER_00: I thought it might well be a prank.

13
00:01:04,739 --> 00:01:07,082
Speaker SPEAKER_00: But then what he said sounded very plausible.

14
00:01:07,763 --> 00:01:09,706
Speaker SPEAKER_00: And then other Swedish accents came on.

15
00:01:11,087 --> 00:01:12,469
Speaker SPEAKER_00: And I was convinced it was real.

16
00:01:12,950 --> 00:01:14,453
Speaker SPEAKER_00: Well, I was sort of convinced it was real.

17
00:01:15,274 --> 00:01:19,399
Speaker SPEAKER_00: But for a couple of days after that, I thought I might be in a dream.

18
00:01:20,140 --> 00:01:22,582
Speaker SPEAKER_00: And so I did a bit of statistical reasoning.

19
00:01:23,084 --> 00:01:25,287
Speaker SPEAKER_00: And the statistical reasoning goes like this.

20
00:01:25,704 --> 00:01:32,293
Speaker SPEAKER_00: What's the chance that someone who's really a psychologist, trying to understand how the brain works, would get the Nobel Prize in physics?

21
00:01:33,114 --> 00:01:35,058
Speaker SPEAKER_00: Well, let's say the chance is 1 in 2 million.

22
00:01:35,659 --> 00:01:37,641
Speaker SPEAKER_00: That's a fairly generous estimate of the chance.

23
00:01:39,784 --> 00:01:44,391
Speaker SPEAKER_00: What's the chance that if it's my dream, I get the Nobel Prize in physics?

24
00:01:45,212 --> 00:01:46,594
Speaker SPEAKER_00: Well, let's say the chance is 1 in 2.

25
00:01:47,977 --> 00:01:54,665
Speaker SPEAKER_00: So now you've got, it's a million times more likely that this is a dream than that it's reality.

26
00:01:55,557 --> 00:02:04,570
Speaker SPEAKER_00: So I thought it might be like those dreams you have when you're younger, that you can fly, and you have this dream that you can fly and it's wonderful, and then you wake up and it was only a dream.

27
00:02:05,171 --> 00:02:11,902
Speaker SPEAKER_00: And then a month later you have a dream that you can fly again, and you remember that you had a dream that you could fly and it wasn't true, but this time it's real.

28
00:02:13,044 --> 00:02:17,973
Speaker SPEAKER_00: I thought it might be one of those, and so for a couple of days I was sort of waiting to see if I would wake up.

29
00:02:20,135 --> 00:02:21,277
Speaker SPEAKER_00: I haven't woken up yet.

30
00:02:23,771 --> 00:02:33,627
Speaker SPEAKER_00: There was a lot of pressure to succeed academically, so I kind of knew from a very early age that I had to be a successful academic or a failure.

31
00:02:35,971 --> 00:02:42,860
Speaker SPEAKER_00: I had a friend at high school who was always much cleverer than me, who when we were about 16 or 17,

32
00:02:43,364 --> 00:02:51,417
Speaker SPEAKER_00: He came into school one day and started talking about memories in the brain and how they might be distributed over the brain in the same way as holograms.

33
00:02:51,456 --> 00:02:52,998
Speaker SPEAKER_00: Holograms had just been invented then.

34
00:02:53,500 --> 00:02:55,122
Speaker SPEAKER_00: This would have been about 1965.

35
00:02:56,084 --> 00:03:06,319
Speaker SPEAKER_00: And he got very interested in the idea that came from a psychologist called Lashley, that memories were distributed across many neurons.

36
00:03:07,201 --> 00:03:11,228
Speaker SPEAKER_00: And I got very interested in that and ever since then I've been thinking about how the brain might work.

37
00:03:13,655 --> 00:03:16,521
Speaker SPEAKER_00: So there's kind of two kinds of risks of AI.

38
00:03:17,403 --> 00:03:25,264
Speaker SPEAKER_00: There's relatively short-term risks, which are very important and very urgent, and they're mainly to do with people misusing AI.

39
00:03:26,727 --> 00:03:29,554
Speaker SPEAKER_00: So people are still in charge, but they're misusing it.

40
00:03:29,719 --> 00:03:44,776
Speaker SPEAKER_00: The risks include things like replacing lots of jobs and causing an increase in the gap between the rich and the poor, because when productivity increases using AI, it's not shared equally.

41
00:03:45,317 --> 00:03:47,862
Speaker SPEAKER_00: Some people lose their jobs and other people get rich.

42
00:03:47,842 --> 00:03:50,706
Speaker SPEAKER_00: So that's bad for society.

43
00:03:51,508 --> 00:03:55,774
Speaker SPEAKER_00: That's one kind of risk, and we need to figure out what to do about that, although it's not clear what to do.

44
00:03:56,514 --> 00:04:00,180
Speaker SPEAKER_00: Another kind of risk is fake videos, where they'll corrupt elections.

45
00:04:00,199 --> 00:04:01,102
Speaker SPEAKER_00: They're already doing that.

46
00:04:02,122 --> 00:04:11,376
Speaker SPEAKER_00: Another kind of risk is cyber attacks, where bad actors use these big AI models for crafting better attacks.

47
00:04:11,717 --> 00:04:13,718
Speaker SPEAKER_00: To begin with, it's just for doing better phishing.

48
00:04:14,379 --> 00:04:17,665
Speaker SPEAKER_00: So last year, phishing attacks went up 1,200%.

49
00:04:17,863 --> 00:04:21,730
Speaker SPEAKER_00: Probably largely because the large language models can make them much more effective.

50
00:04:23,975 --> 00:04:32,329
Speaker SPEAKER_00: Then there's designing things like COVID, which you can do much more efficiently using AI.

51
00:04:32,971 --> 00:04:36,437
Speaker SPEAKER_00: And it's soon going to be relatively easy to design things like that.

52
00:04:37,257 --> 00:04:40,564
Speaker SPEAKER_00: And that means one crazy person can cause endless chaos.

53
00:04:41,134 --> 00:04:47,593
Speaker SPEAKER_00: It gets much easier if you release the weights of a large model, so they can take a large model and then fine-tune it.

54
00:04:48,053 --> 00:04:50,983
Speaker SPEAKER_00: And people are now releasing weights of large models, which I think is crazy.

55
00:04:52,947 --> 00:04:54,391
Speaker SPEAKER_00: There's other short-term risks.

56
00:04:54,471 --> 00:04:56,899
Speaker SPEAKER_00: There's obviously things like discrimination and bias.

57
00:04:57,908 --> 00:05:06,956
Speaker SPEAKER_00: So if you train a model, suppose you're training a model to decide whether prisoners should get parole.

58
00:05:07,858 --> 00:05:15,906
Speaker SPEAKER_00: If the historical data is that white prisoners get parole and black prisoners don't, and you train an AI model on historical data, it'll say the same thing.

59
00:05:16,908 --> 00:05:27,678
Speaker SPEAKER_00: I'm not as worried about that as other people, I think, because with an AI model, you can freeze the weights and you can measure the discrimination, which you can't do with people.

60
00:05:27,827 --> 00:05:35,259
Speaker SPEAKER_00: If you try and measure discrimination in people, they realize they're being measured, and you get the Volkswagen effect, where they behave differently when they're being measured.

61
00:05:36,802 --> 00:05:44,733
Speaker SPEAKER_00: So, I think actually for discrimination bias, it's going to be much easier to measure them in AI systems than in people.

62
00:05:45,954 --> 00:05:50,502
Speaker SPEAKER_00: And I think our goal should be not to make things that don't discriminate and aren't biased.

63
00:05:51,382 --> 00:05:57,091
Speaker SPEAKER_00: Our aim should be to make things that discriminate a lot less and are a lot less biased than the systems they replace.

64
00:05:57,680 --> 00:06:06,031
Speaker SPEAKER_00: So I think that's one of the, it's a very important problem, but it's something where it's fairly clear we can make progress there.

65
00:06:06,052 --> 00:06:07,454
Speaker SPEAKER_00: So that's short-term problems.

66
00:06:08,696 --> 00:06:14,464
Speaker SPEAKER_00: There's also a longer-term problem of these things taking over.

67
00:06:14,725 --> 00:06:17,548
Speaker SPEAKER_00: So what we're doing is we're making things more intelligent than ourselves.

68
00:06:19,692 --> 00:06:27,483
Speaker SPEAKER_00: Researchers differ on when that will happen, but among the leading researchers, there's very little disagreement on the fact that it will happen.

69
00:06:28,021 --> 00:06:30,571
Speaker SPEAKER_00: Unless, of course, we blow ourselves up.

70
00:06:32,257 --> 00:06:37,237
Speaker SPEAKER_00: So the question is, what's going to happen when we've created beings that are more intelligent than us?

71
00:06:37,824 --> 00:06:40,949
Speaker SPEAKER_00: And we don't know what's going to happen.

72
00:06:40,970 --> 00:06:42,812
Speaker SPEAKER_00: We've never been in that situation before.

73
00:06:43,593 --> 00:06:45,776
Speaker SPEAKER_00: Anybody who says it's all going to be fine is crazy.

74
00:06:46,057 --> 00:06:49,742
Speaker SPEAKER_00: And anybody who says they're inevitably going to take over, they're crazy too.

75
00:06:50,062 --> 00:06:51,024
Speaker SPEAKER_00: We really don't know.

76
00:06:51,564 --> 00:07:03,242
Speaker SPEAKER_00: But because we really don't know, it will make a lot of sense to do a lot of basic research now on whether we can stay in control of things that we create that are more intelligent than us.

77
00:07:05,803 --> 00:07:11,790
Speaker SPEAKER_00: There aren't many examples we know of, of more intelligent things being controlled by less intelligent things.

78
00:07:13,752 --> 00:07:17,656
Speaker SPEAKER_00: The only good example I know of is a baby controlling a mother.

79
00:07:18,156 --> 00:07:23,401
Speaker SPEAKER_00: There's not much difference in intelligence, and evolution had to put a lot of work into making that happen.

80
00:07:23,422 --> 00:07:26,504
Speaker SPEAKER_00: It's very important that the baby can control the mother.

81
00:07:26,524 --> 00:07:32,571
Speaker SPEAKER_00: But if you look around, on the whole, more intelligent things are not controlled by less intelligent things.

82
00:07:33,192 --> 00:07:37,516
Speaker SPEAKER_00: Now some people think it'll be fine because we make them and we'll build them in such a way that we can always control them.

83
00:07:38,357 --> 00:07:41,300
Speaker SPEAKER_00: But these things will be intelligent, they'll be like us.

84
00:07:41,821 --> 00:07:44,244
Speaker SPEAKER_00: And in fact, the way they work is very like the way we work.

85
00:07:44,564 --> 00:07:45,966
Speaker SPEAKER_00: They're not like computer code.

86
00:07:46,586 --> 00:07:48,889
Speaker SPEAKER_00: People refer to them sometimes as computer programs.

87
00:07:49,088 --> 00:07:50,370
Speaker SPEAKER_00: They're not computer programs at all.

88
00:07:50,850 --> 00:07:55,315
Speaker SPEAKER_00: You write a computer program to tell a neural network how to learn, a simulated neural network.

89
00:07:56,136 --> 00:07:59,519
Speaker SPEAKER_00: But once it starts learning, it extracts structure from data,

90
00:07:59,500 --> 00:08:04,264
Speaker SPEAKER_00: And the system you've got at the end has extracted its structure from the data.

91
00:08:04,565 --> 00:08:06,187
Speaker SPEAKER_00: It's not something that anybody programmed.

92
00:08:06,767 --> 00:08:08,389
Speaker SPEAKER_00: We don't exactly know how it's going to work.

93
00:08:09,531 --> 00:08:10,572
Speaker SPEAKER_00: And it'll be like us.

94
00:08:10,872 --> 00:08:21,564
Speaker SPEAKER_00: So, making these programs behave in a reasonable way, sorry, these systems behave in a reasonable way, is much like making a child behave in a reasonable way.

95
00:08:21,845 --> 00:08:27,752
Speaker SPEAKER_00: The controls you really have are, you can reinforce, you can reward it for good behavior, punish it for bad behavior.

96
00:08:27,985 --> 00:08:35,417
Speaker SPEAKER_00: But the main control you have is demonstrating good behaviour, training it on good behaviour, so that's what it observes and that's what it mimics.

97
00:08:36,097 --> 00:08:37,419
Speaker SPEAKER_00: And it's the same for these systems.

98
00:08:38,000 --> 00:08:43,909
Speaker SPEAKER_00: And so it's very important we train them on the kind of behaviour that we would like to see in them.

99
00:08:44,370 --> 00:08:51,841
Speaker SPEAKER_00: At present, the big chatbots are trained on all the data they can get, which includes things like the diaries of serial killers.

100
00:08:52,743 --> 00:08:55,748
Speaker SPEAKER_00: Well, if you were raising a child, would you

101
00:08:56,470 --> 00:09:00,078
Speaker SPEAKER_00: get your child to learn to read on the Diaries of Sir Iroconus.

102
00:09:00,119 --> 00:09:01,702
Speaker SPEAKER_00: I think you'd realise that was a bad idea.

103
00:09:04,671 --> 00:09:05,712
Speaker SPEAKER_00: Well, that's what we don't know.

104
00:09:05,773 --> 00:09:12,370
Speaker SPEAKER_00: My guess is, in between 5 and 20 years from now,

105
00:09:12,518 --> 00:09:16,302
Speaker SPEAKER_00: there's a good chance, a 50% chance, we'll get AI smarter than us.

106
00:09:17,105 --> 00:09:17,985
Speaker SPEAKER_00: It may be much longer.

107
00:09:18,447 --> 00:09:19,828
Speaker SPEAKER_00: It's just possible it's a bit shorter.

108
00:09:21,610 --> 00:09:25,035
Speaker SPEAKER_00: But I think it's quite likely to have happened in 20 years' time.

109
00:09:25,417 --> 00:09:27,318
Speaker SPEAKER_00: Other researchers think it's shorter or longer.

110
00:09:29,201 --> 00:09:29,822
Speaker SPEAKER_00: That's my guess.

111
00:09:30,803 --> 00:09:34,870
Speaker SPEAKER_00: Actually, that was my guess a year ago, so I guess my guess now is between four and 19 years.

112
00:09:34,909 --> 00:09:41,580
Speaker SPEAKER_00: I think it depends what field you're in.

113
00:09:43,399 --> 00:09:47,943
Speaker SPEAKER_00: whether you're trying to do something that's very different from the standards in the field.

114
00:09:50,025 --> 00:09:54,389
Speaker SPEAKER_00: So for neural networks, for a long time they were regarded as ridiculous.

115
00:09:55,672 --> 00:09:58,594
Speaker SPEAKER_00: And it was clear to many people that they would never work.

116
00:10:00,456 --> 00:10:07,863
Speaker SPEAKER_00: So to work in a field like that, you have to be confident that you're right even when everybody else says you're wrong.

117
00:10:09,585 --> 00:10:10,065
Speaker SPEAKER_00: And

118
00:10:11,327 --> 00:10:15,977
Speaker SPEAKER_00: I had, actually a couple of things happened when I was very young that helped.

119
00:10:17,481 --> 00:10:26,822
Speaker SPEAKER_00: One was my parents, who were both atheists, sent me to a Christian school, a Christian private school, from the age of seven.

120
00:10:27,089 --> 00:10:29,152
Speaker SPEAKER_00: And everybody at the school believed in God.

121
00:10:29,812 --> 00:10:32,076
Speaker SPEAKER_00: The teachers believed in God and the other kids believed in God.

122
00:10:32,817 --> 00:10:35,902
Speaker SPEAKER_00: And it seemed to me it was just obvious nonsense.

123
00:10:35,922 --> 00:10:38,325
Speaker SPEAKER_00: And it turns out I was right.

124
00:10:39,346 --> 00:10:55,188
Speaker SPEAKER_00: And so that experience of everybody else around you believing one thing and it being clear to you they're wrong, and then it turning out that as you get older it turns out there's other people also don't believe in God, that was a very useful experience.

125
00:10:55,168 --> 00:10:59,513
Speaker SPEAKER_00: That was one thing that helped keep me going when everybody said neural networks was nonsense.

126
00:10:59,533 --> 00:11:02,436
Speaker SPEAKER_00: It wasn't everybody, but it was almost everybody in computer science.

127
00:11:03,496 --> 00:11:17,610
Speaker SPEAKER_00: Another experience that I haven't talked about much, when I was about, I think probably about nine, but I don't know exactly what age, I heard a radio program with my father talking about continental drift.

128
00:11:18,292 --> 00:11:23,657
Speaker SPEAKER_00: So at that point, there was a lot of controversy about whether the continents moved around.

129
00:11:24,379 --> 00:11:27,023
Speaker SPEAKER_00: Nearly all geologists thought it was complete rubbish.

130
00:11:28,744 --> 00:11:37,432
Speaker SPEAKER_00: So the theory was first introduced, I think, around 1920 by a climatologist called Wegener, who had lots of evidence that the continents moved around.

131
00:11:38,153 --> 00:11:39,355
Speaker SPEAKER_00: But he wasn't a geologist.

132
00:11:40,255 --> 00:11:42,899
Speaker SPEAKER_00: And the geologists just thought this was complete rubbish.

133
00:11:43,658 --> 00:11:44,580
Speaker SPEAKER_00: And they poo-pooed it.

134
00:11:45,620 --> 00:11:49,565
Speaker SPEAKER_00: And they, for example, refused to allow it to be in textbooks.

135
00:11:50,025 --> 00:11:51,667
Speaker SPEAKER_00: They said it would only mislead the students.

136
00:11:52,067 --> 00:11:54,370
Speaker SPEAKER_00: And it was complete nonsense.

137
00:11:56,594 --> 00:12:06,645
Speaker SPEAKER_00: So I saw a debate in which there was a theory that was regarded as complete nonsense by geologists, nearly all geologists, and turned out to be correct.

138
00:12:08,248 --> 00:12:09,349
Speaker SPEAKER_00: So that was also very helpful.

139
00:12:09,788 --> 00:12:15,875
Speaker SPEAKER_00: And in fact, what happened with neural nets, the most similar thing I know, is what happened with continental drift.

140
00:12:16,937 --> 00:12:25,807
Speaker SPEAKER_00: That there was, with continental drift there was this idea that, you know, that the South America fitted nicely into the armpit of Africa, and

141
00:12:28,100 --> 00:12:29,100
Speaker SPEAKER_00: But it wasn't just that.

142
00:12:29,140 --> 00:12:35,508
Speaker SPEAKER_00: It was that the soil types all down the coast of America matched the soil types from Norway all down to South Africa.

143
00:12:37,230 --> 00:12:38,750
Speaker SPEAKER_00: There were fossils that linked up.

144
00:12:39,532 --> 00:12:43,797
Speaker SPEAKER_00: There were glacial scrapes on rocks in the tropics.

145
00:12:44,256 --> 00:12:45,918
Speaker SPEAKER_00: And there were coal deposits in the Arctic.

146
00:12:46,960 --> 00:12:51,524
Speaker SPEAKER_00: So there's all this evidence that the continents had moved around.

147
00:12:52,078 --> 00:12:55,703
Speaker SPEAKER_00: the geologists as a field completely dismissed it.

148
00:12:56,485 --> 00:12:58,407
Speaker SPEAKER_00: They just couldn't believe that the Earth had moved.

149
00:12:59,248 --> 00:13:02,292
Speaker SPEAKER_00: And it was the same with neural networks.

150
00:13:03,192 --> 00:13:07,698
Speaker SPEAKER_00: We had this evidence that neural networks have to be able to learn to do complicated things, because we've got a brain.

151
00:13:09,682 --> 00:13:15,269
Speaker SPEAKER_00: But most people in AI said, if you take neural networks and try and learn everything in the neural networks, it's hopeless.

152
00:13:15,589 --> 00:13:19,875
Speaker SPEAKER_00: The knowledge has to be innate, or you have to do it by learning symbolic rules.

153
00:13:20,850 --> 00:13:27,499
Speaker SPEAKER_00: and they basically refused to allow people to publish neural network stuff in their journals.

154
00:13:30,282 --> 00:13:33,144
Speaker SPEAKER_00: So it was a very similar paradigm shift.

155
00:13:33,384 --> 00:13:37,570
Speaker SPEAKER_00: And now there's been a more or less complete paradigm shift.

156
00:13:37,590 --> 00:13:46,419
Speaker SPEAKER_00: I don't know if I'm in a good position to give advice, but the piece of advice I normally give is, if you have an idea and it seems right to you,

157
00:13:47,176 --> 00:13:52,844
Speaker SPEAKER_00: and it's different from what everybody else believes, don't give up on it until you've figured out why it's wrong.

158
00:13:53,764 --> 00:13:58,873
Speaker SPEAKER_00: So most ideas you have like that, you're wrong, there's something you haven't thought of, or there's something you didn't understand.

159
00:14:01,196 --> 00:14:13,793
Speaker SPEAKER_00: Just very occasionally, you have an idea that's different from what other people believe and is right, you're never gonna discover that unless you keep going with your beliefs until you discover why they're wrong.

160
00:14:14,774 --> 00:14:16,778
Speaker SPEAKER_00: And you should just ignore what other people say.

161
00:14:17,230 --> 00:14:19,308
Speaker SPEAKER_00: I'm very good at ignoring what other people say.

162
00:14:23,068 --> 00:14:33,184
Speaker SPEAKER_00: I think the scientists have a much better understanding of what this technology is than politicians do, or the general public do.

163
00:14:34,486 --> 00:14:35,889
Speaker SPEAKER_00: The scientists still disagree.

164
00:14:36,089 --> 00:14:41,376
Speaker SPEAKER_00: So there's still some scientists who say, these big chapels, they don't really understand what they're saying.

165
00:14:42,619 --> 00:14:47,506
Speaker SPEAKER_00: Despite all the evidence that they do understand what they're saying, some scientists say it's just a statistical trick.

166
00:14:49,410 --> 00:14:54,436
Speaker SPEAKER_00: I would like to have been concerned about this existential threat sooner.

167
00:14:56,004 --> 00:14:57,109
Speaker SPEAKER_00: I always thought

168
00:14:57,932 --> 00:15:03,878
Speaker SPEAKER_00: superintelligence was a long way off and we could worry about it later, and the problem for now was just to make things more intelligent.

169
00:15:04,958 --> 00:15:08,162
Speaker SPEAKER_00: I wish I'd thought sooner about what was going to happen.

170
00:15:08,722 --> 00:15:24,157
Speaker SPEAKER_00: If you go back to Turing in the early 1950s, he talks about making things smarter than us, and he has about one sentence which says, of course when they get smarter than us, we're finished.

171
00:15:24,878 --> 00:15:26,538
Speaker SPEAKER_00: He doesn't say it like that, but

172
00:15:26,519 --> 00:15:27,884
Speaker SPEAKER_00: he implies that.

173
00:15:29,048 --> 00:15:35,092
Speaker SPEAKER_00: But most people just don't think about that problem until it gets close, and the problem is it's close now.

174
00:15:35,956 --> 00:15:37,322
Speaker SPEAKER_00: So I wish I'd thought about that sooner.

175
00:15:38,821 --> 00:15:58,298
Speaker SPEAKER_00: Okay, so half of the prize money, half of my share of the prize money, I donated to an organization in Canada that trains people who live in indigenous communities on the technology of producing safe drinking water.

176
00:15:58,278 --> 00:16:02,621
Speaker SPEAKER_00: And that's good because those people will then stay in the communities and they'll have safe drinking water.

177
00:16:03,623 --> 00:16:15,475
Speaker SPEAKER_00: It's ridiculous at this time, in a rich country like Canada, that, for example, in Ontario, 20% of the indigenous communities do not have safe drinking water.

178
00:16:16,014 --> 00:16:17,015
Speaker SPEAKER_00: That's just crazy.

179
00:16:17,616 --> 00:16:25,504
Speaker SPEAKER_00: And I'm sort of sympathetic to this problem because I adopted a child in Peru and I was there for two months and you can't drink the tap water there.

180
00:16:26,264 --> 00:16:27,947
Speaker SPEAKER_00: It's poisonous.

181
00:16:29,311 --> 00:16:32,855
Speaker SPEAKER_00: So your whole life revolves around how do you get safe water.

182
00:16:33,756 --> 00:16:37,120
Speaker SPEAKER_00: It's just makes, it's a huge extra burden on everyday life.

183
00:16:37,562 --> 00:16:40,065
Speaker SPEAKER_00: And so it's crazy that people in Canada have to suffer that.

184
00:16:41,647 --> 00:16:43,369
Speaker SPEAKER_00: So that's, I donated half of it to that.

185
00:16:44,210 --> 00:16:55,825
Speaker SPEAKER_00: The other half, back in the 1980s, I worked with someone called Terry Sanofsky, who was actually a student of Hopfield's.

186
00:16:56,750 --> 00:16:58,354
Speaker SPEAKER_00: on this theory of Boltzmann machines.

187
00:16:58,894 --> 00:17:01,059
Speaker SPEAKER_00: We worked on it equally.

188
00:17:01,922 --> 00:17:06,832
Speaker SPEAKER_00: I wouldn't have had the theory unless I'd been talking to him, and he wouldn't have had it unless he'd been talking to me.

189
00:17:08,957 --> 00:17:14,269
Speaker SPEAKER_00: He was a physicist originally, and then went into neuroscience.

190
00:17:15,109 --> 00:17:17,474
Speaker SPEAKER_00: We thought this must be how the brain works.

191
00:17:17,515 --> 00:17:21,763
Speaker SPEAKER_00: It was such an elegant learning algorithm that we were convinced that it had to be how the brain works.

192
00:17:22,484 --> 00:17:26,653
Speaker SPEAKER_00: So we thought we might get the Nobel Prize in Physiology or Medicine for discovering how the brain worked.

193
00:17:27,695 --> 00:17:34,569
Speaker SPEAKER_00: And we had an agreement back in the 1980s, which was that if they gave it to one of us and not the other one, we'd split it.

194
00:17:35,173 --> 00:17:46,382
Speaker SPEAKER_00: And so when they gave me the Nobel Prize totally unexpectedly, and one of the reasons was Boltzmann machines, I got in touch with Terry and said, where would he like me to send his half?

195
00:17:47,392 --> 00:17:55,682
Speaker SPEAKER_00: And he said, well look, he didn't feel right about it because it wasn't just for Boltzmann machines, it was for other subsequent stuff that he wasn't so involved in.

196
00:17:56,604 --> 00:17:57,904
Speaker SPEAKER_00: So he refused to take the money.

197
00:17:58,826 --> 00:18:07,778
Speaker SPEAKER_00: And so in the end we compromised that I took that half of my share, and we used it to set up a prize in his name for young researchers.

198
00:18:08,438 --> 00:18:14,546
Speaker SPEAKER_00: And it'll be for young researchers with crazy theories of how the brain works, like we were.

199
00:18:15,507 --> 00:18:16,048
Speaker SPEAKER_00: And

200
00:18:17,345 --> 00:18:20,308
Speaker SPEAKER_00: It'll be handed out at the annual conference in our field.

201
00:18:21,371 --> 00:18:24,976
Speaker SPEAKER_00: And that seemed like a good compromise.

202
00:18:25,997 --> 00:18:32,605
Speaker SPEAKER_00: So I feel he could easily have been the third person named in the Nobel Prize.

203
00:18:32,987 --> 00:18:33,467
Speaker SPEAKER_00: He wasn't.

204
00:18:34,147 --> 00:18:36,451
Speaker SPEAKER_00: I'm not complaining about that, but he could have been.

205
00:18:37,752 --> 00:18:42,019
Speaker SPEAKER_00: But this is a way of recognizing that he made a huge contribution.

206
00:18:44,851 --> 00:18:50,894
Unknown Speaker: you


1 00:00:05,684 --> 00:00:16,539 说话人 说话人_00: 谢谢。
2 00:00:17,039 --> 00:00:30,018 说话人 说话人_00: 很高兴再次回到这里，我也很高兴能与杰弗里·辛顿同台，他是这个领域中最聪明、最可爱、最有能力、最善良的人之一。
3 00:00:30,419 --> 00:00:31,640 说话人 说话人_00: 你好，杰弗里·辛顿？
4 00:00:32,835 --> 00:00:33,396 说话人 说话人_01: 我很好。
5 00:00:33,417 --> 00:00:35,238 说话人 SPEAKER_01: 感谢你那超凡脱俗的介绍。
6 00:00:35,560 --> 00:00:37,101 说话人 SPEAKER_00: 好吧。
7 00:00:37,121 --> 00:00:43,350 说话人 SPEAKER_00: 嗯，杰夫，我想从我们大约一年前的一次简短对话开始。
8 00:00:44,152 --> 00:00:47,036 说话人 SPEAKER_00: 我们当时在多伦多，即将上台。
9 00:00:48,057 --> 00:00:49,820 说话人 说话人_00：我的两个孩子当时和我在一起。
10 00:00:49,920 --> 00:00:51,662 说话人 说话人_00：他们当时分别是14岁和12岁。
11 00:00:52,484 --> 00:00:57,692 说话人 说话人_00：然后你看了那个大的，你说，你会像你父亲一样进入媒体行业吗？
12 00:00:57,712 --> 00:00:59,433 说话人 说话人_00：他回答了。
13 00:00:59,936 --> 00:01:00,216 说话人 说话人_00: 不。
14 00:01:00,957 --> 00:01:02,359 说话人 说话人_00: 你说，好。
15 00:01:02,399 --> 00:01:07,885 说话人 说话人_00: 然后，我说，如果他不去媒体行业，他应该做什么？
16 00:01:08,566 --> 00:01:11,490 说话人 说话人_00: 你说，他应该成为一名水管工。
17 00:01:12,451 --> 00:01:16,816 说话人 SPEAKER_00: 所以那个儿子刚刚申请了学校报纸。
18 00:01:17,358 --> 00:01:24,426 说话人 SPEAKER_00: 我很好奇你是否认为他犯了一个严重的错误，我应该真的下楼去帮他修鸭子。
19 00:01:26,245 --> 00:01:27,968 说话人 SPEAKER_01: 不，我只是在开个玩笑。
20 00:01:28,549 --> 00:01:32,659 说话人 SPEAKER_01: 但我认为管道工这个职业会比大多数职业存在的时间更长。
21 00:01:33,701 --> 00:01:38,432 说话人 SPEAKER_01：我认为目前，在 AI 方面，
22 00:01:38,698 --> 00:01:40,941 说话人 SPEAKER_01：它最不擅长的是物理操作。
23 00:01:41,001 --> 00:01:44,566 说话人 SPEAKER_01：它正在快速进步，但与人类相比，这正是它最不擅长的地方。
24 00:01:45,227 --> 00:01:46,168 说话人 SPEAKER_00：好的，太棒了。
25 00:01:46,408 --> 00:01:52,594 说话人 SPEAKER_00：好吧，我想从 Hinton 博士的背景开始，先把这个基础打好。
26 00:01:53,055 --> 00:01:58,402 说话人 SPEAKER_00：然后我想进入一个环节，我会问他一些最有趣的技术问题，一些我们在舞台上讨论过的问题。
27 00:01:58,783 --> 00:02:04,849 说话人 SPEAKER_00：我们将讨论一点“AI 向善”，然后是“AI 向恶”，接着我们会讨论一下监管框架。
28 00:02:05,049 --> 00:02:05,870 说话人 SPEAKER_00：杰夫，这样可以吗？
29 00:02:06,813 --> 00:02:07,894 说话人 SPEAKER_00: 好的，太棒了。
30 00:02:07,914 --> 00:02:30,745 说话人 SPEAKER_00: 好吧，我想从 40 年前开始讲起，那时你是一个孤独的科学家，你意识到，要制造一台极其强大的计算机，你应该通过模仿人脑的架构来实现这一点，这最终被证明是这个领域最重要的洞察之一，可能是 20 世纪后期的。
31 00:02:31,286 --> 00:02:33,528 说话人 SPEAKER_00: 现在听起来这似乎很显然。
32 00:02:33,508 --> 00:02:34,409 说话人 SPEAKER_00: 但当时并不是这样。
33 00:02:34,430 --> 00:02:38,415 说话人 SPEAKER_00：那么，请告诉我那个真正推动这个领域发展的洞察时刻。
34 00:02:40,218 --> 00:02:44,125 说话人 SPEAKER_01：这是一个美好的神话，但很多人都有这样的想法。
35 00:02:44,585 --> 00:02:49,132 说话人 SPEAKER_01：特别是在 20 世纪 50 年代，冯·诺伊曼和图灵都这么认为。
36 00:02:49,733 --> 00:02:54,419 说话人 SPEAKER_01：他们俩都英年早逝非常不幸，否则我们这个领域的历史可能完全不同。
37 00:02:55,008 --> 00:03:01,334 说话人 SPEAKER_01：但在我看来，如果你想理解智能，你就需要理解我们所知的最高级的智能，那就是我们自己。
38 00:03:02,455 --> 00:03:09,862 说话人 SPEAKER_01：我们的智能并非来自人们编程的大量命题，然后通过逻辑对这些命题进行推理。
39 00:03:10,563 --> 00:03:15,848 说话人 SPEAKER_01：它源自一个主要设计用于视觉和运动控制的头脑。
40 00:03:16,729 --> 00:03:23,597 说话人 SPEAKER_01：显然，随着你的学习，那个头脑中的连接强度会发生变化，我们只需弄清楚这是如何发生的。
41 00:03:24,336 --> 00:03:26,439 说话人 说话人_00: 好吧，这很有道理。
42 00:03:26,460 --> 00:03:27,322 说话人 说话人_00: 你在历史上有根基。
43 00:03:27,342 --> 00:03:28,423 说话人 说话人_00: 现在我们快速进行。
44 00:03:28,824 --> 00:03:29,444 说话人 说话人_00: 你来处理这个。
45 00:03:30,265 --> 00:03:32,169 说话人 SPEAKER_00：人们说你走错了路。
46 00:03:32,468 --> 00:03:33,090 说话人 SPEAKER_00：你继续走下去。
47 00:03:33,450 --> 00:03:34,292 说话人 SPEAKER_00：其他人会加入你。
48 00:03:34,513 --> 00:03:36,354 说话人 SPEAKER_00：最终，你会明白你走的是一条正确的路。
49 00:03:36,435 --> 00:03:37,537 说话人 说话人_00：它不清楚会去哪里。
50 00:03:38,057 --> 00:03:39,139 说话人 说话人_00：你赢得了图灵奖。
51 00:03:39,620 --> 00:03:40,300 说话人 说话人_00：你加入了谷歌。
52 00:03:40,320 --> 00:03:41,483 说话人 说话人_00：你把公司卖给了谷歌。
53 00:03:42,204 --> 00:03:45,990 说话人 说话人_00：你当时，大约一年半以前，
54 00:03:46,156 --> 00:03:47,296 说话人 说话人_00：你离开了谷歌。
55 00:03:47,317 --> 00:03:52,462 说话人 说话人_00：请告诉我你离开的那一幕，那是在 Chat GPT 发布几个月后。
56 00:03:52,962 --> 00:03:56,706 说话人 说话人_00：请告诉我你最后工作的内容以及你离开的那一刻。
57 00:03:58,188 --> 00:03:59,389 说话人 SPEAKER_01：首先，让我先把一些事情说清楚。
58 00:03:59,550 --> 00:04:04,616 说话人 SPEAKER_01：我离开的原因有几个，其中一个原因是我 75 岁了，我决定那时候无论如何都应该退休。
59 00:04:05,377 --> 00:04:14,627 说话人 SPEAKER_01：我离开不仅仅是为了谈论 AI 的危险，这又是另一个原因。
60 00:04:14,606 --> 00:04:22,538 说话人 SPEAKER_01：而且我在 2023 年初，大约在 3 月份，突然意识到 AI 的危险，这是存在的威胁。
61 00:04:23,139 --> 00:04:30,610 说话人 SPEAKER_01：我开始和其他对存在性威胁感到恐惧的人交谈，比如罗杰·格罗斯，他们鼓励我公开表态。
62 00:04:31,190 --> 00:04:34,776 说话人 SPEAKER_01：然后我决定离开谷歌，这样我就可以自由地发言了。
63 00:04:34,755 --> 00:04:45,447 说话人 SPEAKER_01：我感到恐惧的原因是我正在尝试弄清楚如何让模拟计算机在 30 瓦而不是兆瓦的功率下实现大型语言模型。
64 00:04:46,367 --> 00:04:55,055 说话人 SPEAKER_01：在这样做的时候，我确信数字计算有一些东西使得它比大脑做得更好。
直到那时，我花了50年时间思考，如果我们能让它更像大脑，那就更好了。
66 00:05:01,502 --> 00:05:07,076 发言人 SPEAKER_01：我终于在 2023 年初意识到，它拥有大脑永远无法拥有的东西。
因为它是数字的，你可以制作许多相同模型，它们以完全相同的方式工作，并且每个副本可以查看数据集的不同部分。
他们可以得到一个梯度，并且可以将这些梯度结合起来，这使得他们能够学习到更多、更多。
69 00:05:24,473 --> 00:05:27,459 说话人 SPEAKER_01：这就是为什么 GPT-4 能知道比人更多的原因。
70 00:05:28,161 --> 00:05:34,276 说话人 SPEAKER_01：这是在多种不同的硬件上运行的多份不同副本，它们查看了整个互联网。
71 00:05:34,257 --> 00:05:36,259 说话人 SPEAKER_01：这是我们永远无法拥有的。
72 00:05:36,338 --> 00:05:41,365 说话人 SPEAKER_01：所以他们拥有的而我们没有的是，他们可以非常高效地共享。
73 00:05:41,865 --> 00:05:44,007 说话人 SPEAKER_01：我们可以非常低效地分享。
74 00:05:44,367 --> 00:05:45,428 说话人 SPEAKER_01：现在就是这样做的。
75 00:05:46,009 --> 00:05:51,255 说话人 SPEAKER_01：我产生句子，你尝试找出如何改变你大脑中的突触，以便你可能说出那样的话。
76 00:05:51,896 --> 00:05:53,877 说话人 SPEAKER_01：这是一种非常缓慢且低效的分享方式。
77 00:05:54,838 --> 00:06:01,766 说话人 SPEAKER_01：数字智能，如果是同一模型的副本，可以以万亿比特的带宽进行共享。
78 00:06:02,826 --> 00:06:09,413 说话人 SPEAKER_00：所以你有了这个时刻，这个突然的认识，这些系统可以比你想象的强大得多。
79 00:06:09,434 --> 00:06:12,096 说话人 SPEAKER_00：这肯定是一个既令人兴奋又令人激动的时刻。
80 00:06:12,757 --> 00:06:15,259 说话人 SPEAKER_00：那么为什么这种巨大的恐惧也会如此普遍呢？
81 00:06:17,442 --> 00:06:22,706 说话人 SPEAKER_01：嗯，这让我觉得它们会比我想的更快地变得比我们更聪明。
82 00:06:24,189 --> 00:06:27,552 说话人 SPEAKER_01：这也让我觉得它们是更高级的智能形式。
83 00:06:28,762 --> 00:06:32,913 说话人 SPEAKER_00：让我问你关于其他两位人工智能之父的问题。
84 00:06:32,973 --> 00:06:34,978 说话人 SPEAKER_00：所以你和三个人一起获得了图灵奖。
85 00:06:35,418 --> 00:06:39,870 说话人 说话人_00: Meta 人工智能负责人 Yann LeCun，以及 Yoshua Bengio。
86 00:06:40,692 --> 00:06:42,456 说话人 说话人_00: 我试图弄清楚你们之间的区别。
87 00:06:42,475 --> 00:06:43,619 说话人 说话人_00: 让我知道这行不行。
88 00:06:43,639 --> 00:06:45,101 说话人 说话人_00: 你们都是教父。
89 00:06:45,418 --> 00:06:51,709 说话人 SPEAKER_00: 简认为 AI 就像弗雷多·科莱昂，不太能干，容易控制。
90 00:06:52,471 --> 00:06:56,838 说话人 SPEAKER_00: 约书亚可能把它看作桑尼，你知道的，可能相当危险。
91 00:06:57,298 --> 00:07:01,987 说话人 SPEAKER_00: 你把它看作迈克尔·科莱昂，可能极其危险。
92 00:07:02,468 --> 00:07:03,569 说话人 SPEAKER_00: 这样理解对吗？
93 00:07:03,610 --> 00:07:05,333 说话人 SPEAKER_00: 我认为不是这样。
94 00:07:05,353 --> 00:07:08,338 说话人 SPEAKER_00: 我认为约书亚和我对危险的观点非常相似。
95 00:07:08,942 --> 00:07:14,410 说话人 SPEAKER_00: 但你和简的不同之处在于，你将这视为一个比他更强大的系统。
96 00:07:14,709 --> 00:07:17,012 说话人 SPEAKER_00: 正因如此，你比他更担心。
97 00:07:19,937 --> 00:07:21,499 说话人 SPEAKER_01：这是一个区别，是的。
98 00:07:21,978 --> 00:07:23,721 说话人 SPEAKER_01：这是一个主要区别。
99 00:07:23,742 --> 00:07:26,685 说话人 SPEAKER_01：所以我认为它已经很智能了。
100 00:07:27,386 --> 00:07:29,007 说话人 SPEAKER_01：而且 Jan 认为猫更聪明。
101 00:07:29,689 --> 00:07:29,889 说话人 SPEAKER_01: 对。
102 00:07:30,430 --> 00:07:34,274 说话人 SPEAKER_00: 好吧，让我们来探讨一下这个智能问题，我认为这是最有趣的问题之一。
103 00:07:35,233 --> 00:07:42,937 说话人 SPEAKER_00: 你认为人类大脑中有什么是这些机器和人工智能系统无法复制的吗？
104 00:07:43,016 --> 00:07:47,610 说话人 SPEAKER_00: 我们的大脑能做些什么是机器无法复制的吗？
105 00:07:48,694 --> 00:07:48,754 说话人 SPEAKER_01: 不。
106 00:07:50,793 --> 00:07:59,322 说话人 SPEAKER_00: 那这意味着我们无法做到的事情，这些智能机器都能超越吗？
107 00:08:00,182 --> 00:08:07,672 说话人 SPEAKER_00: 比如说，他们最终能够创作出更美的音乐。
108 00:08:08,172 --> 00:08:13,658 说话人 SPEAKER_00: 他们将能够比我们做得更好，涉及简单认知的所有事情。
109 00:08:14,920 --> 00:08:16,040 说话人 SPEAKER_01：这就是我的信念，是的。
110 00:08:17,742 --> 00:08:29,074 说话人 SPEAKER_00：你不认为有什么精神上的或者超出了神经网络所能捕捉的范围的东西吗？
111 00:08:30,617 --> 00:08:35,261 我认为我们所说的“精神”可能被这些外星智能所捕捉。
112 00:08:35,903 --> 00:08:38,225 我同意 Sam Altman 的观点，那就是外星智能。
113 00:08:38,284 --> 00:08:39,225 说话人 SPEAKER_01：它并不完全像我们。
114 00:08:39,706 --> 00:08:40,868 说话人 SPEAKER_01：它与我们有一些不同。
115 00:08:41,948 --> 00:08:45,133 说话人 SPEAKER_01：但如果你看
116 00:08:46,260 --> 00:08:47,182 说话人 SPEAKER_01：像宗教这样的东西。
117 00:08:47,423 --> 00:08:49,225 说话人 SPEAKER_01：我不明白为什么你不应该得到宗教的。
118 00:08:51,250 --> 00:08:58,582 说话人 SPEAKER_00：昨天，当我问 Altman 这个问题时，他说，嗯，可能有一个区别，那就是主观体验。
119 00:08:59,543 --> 00:09:02,590 说话人 说话人_00：一个机器人，一个系统无法体验世界。
120 00:09:03,331 --> 00:09:07,938 说话人 说话人_00：您认为 AI 系统可以拥有主观体验吗？
121 00:09:07,999 --> 00:09:08,559 说话人 说话人_00：是的，我相信。
122 00:09:08,779 --> 00:09:10,243 说话人 说话人_00：我认为它们已经拥有了。
123 00:09:10,712 --> 00:09:12,174 说话人 SPEAKER_00: 好吧，让我们更深入地探讨一下。
124 00:09:12,215 --> 00:09:12,735 说话人 SPEAKER_00: 解释一下。
125 00:09:12,775 --> 00:09:14,938 说话人 SPEAKER_00: 这是一个有争议的提议，杰夫。
126 00:09:14,958 --> 00:09:16,682 说话人 SPEAKER_00: 你不能只用一句话来回答。
127 00:09:16,942 --> 00:09:18,183 说话人 SPEAKER_00: 请跟进，Hinton 博士。
128 00:09:19,365 --> 00:09:22,109 说话人 SPEAKER_01: 好的，我试图给出清晰、尖锐的回答来回答你的问题。
129 00:09:22,129 --> 00:09:24,933 说话人 SPEAKER_01: 很好，因为我有... 在 Altman 没有的方式。
130 00:09:25,394 --> 00:09:28,538 说话人 SPEAKER_01: 但是，是的，我们需要跟进那一点。
131 00:09:29,220 --> 00:09:39,774 说话人 SPEAKER_01：所以，我的观点是，几乎每个人都对心灵有一个完全错误的模型。
132 00:09:40,480 --> 00:09:43,267 说话人 SPEAKER_01：这是一件很难推销的事情。
133 00:09:43,307 --> 00:09:49,081 说话人 SPEAKER_01：我现在处于一个信念与大多数人坚信的信念不一致的位置。
134 00:09:49,942 --> 00:09:52,649 说话人 SPEAKER_01：我总是在那个位置上非常快乐。
135 00:09:53,929 --> 00:09:59,597 说话人 SPEAKER_01：所以大多数人把思维看作是一种内部剧院。
136 00:10:00,538 --> 00:10:05,424 说话人 SPEAKER_01：事实上，人们如此确信这种观点是正确的，以至于他们甚至不认为这是一种观点。
137 00:10:05,785 --> 00:10:07,347 说话人 SPEAKER_01：他们甚至不认为这是一种他们拥有的模型。
138 00:10:07,628 --> 00:10:08,669 说话人 SPEAKER_01：他们认为这是显而易见的。
139 00:10:09,410 --> 00:10:12,955 说话人 SPEAKER_01：就像人们认为太阳绕地球转一样明显。
140 00:10:13,756 --> 00:10:16,360 说话人 SPEAKER_01：我的意思是，你一看就知道它绕地球转。
141 00:10:17,774 --> 00:10:24,965 说话人 SPEAKER_01：最终，人们意识到太阳不是绕地球转，而是地球自转。
142 00:10:26,126 --> 00:10:30,732 说话人 SPEAKER_01：那是山姆犯的一个小技术错误，由于我比较挑剔，我喜欢指出他的错误。
143 00:10:31,173 --> 00:10:38,725 说话人 SPEAKER_01：起初他们认为太阳绕着地球转，后来他们意识到地球绕着太阳转。
144 00:10:39,125 --> 00:10:40,388 说话人 SPEAKER_01：这不是正确的对比。
145 00:10:40,828 --> 00:10:45,115 说话人 SPEAKER_01：他们原本认为太阳绕着地球转，后来他们意识到地球自转。
146 00:10:45,095 --> 00:10:47,577 说话人 SPEAKER_01：地球绕太阳转与年有关，而不是与日有关。
147 00:10:48,558 --> 00:10:51,842 说话人 SPEAKER_01：但无论如何，太阳绕地球转是很明显的，我们错了。
148 00:10:51,942 --> 00:10:52,663 说话人 SPEAKER_01：我们有一个模型。
149 00:10:53,225 --> 00:10:54,466 说话人 SPEAKER_01：这是一个简单的模型。
150 00:10:54,966 --> 00:10:56,268 说话人 SPEAKER_01：这显然是正确的。
151 00:10:56,327 --> 00:10:57,629 说话人 SPEAKER_01：你几乎能看见它发生。
152 00:10:58,210 --> 00:10:59,631 说话人 SPEAKER_01：我们关于那个模型是错的。
153 00:11:00,533 --> 00:11:03,976 说话人 SPEAKER_01：我认为大多数人关于心智的看法也是一样。
154 00:11:04,437 --> 00:11:08,503 说话人 SPEAKER_01：大多数人认为有一个内在的剧场，他们在这方面是错误的。
155 00:11:08,562 --> 00:11:12,267 说话人 SPEAKER_01：他们还没有理解心理状态的语言是如何运作的。
156 00:11:12,922 --> 00:11:15,866 说话人 SPEAKER_00：但请解释这如何适用于人工智能系统。
157 00:11:15,966 --> 00:11:29,730 说话人 SPEAKER_00：请解释一下，如果我告诉 GPT-4，我说，你刚刚经历了一个巨大的声响，有什么东西撞到了你。
158 00:11:30,390 --> 00:11:33,395 说话人 SPEAKER_00：它并没有感到疼痛或伤害，它的耳朵也不痛。
159 00:11:33,416 --> 00:11:36,380 说话人 SPEAKER_00：它有什么主观体验的感觉？
160 00:11:37,373 --> 00:11:39,918 说话人 SPEAKER_01：好的，那么让我们举一个简单易懂的例子。
161 00:11:40,860 --> 00:11:47,493 说话人 SPEAKER_01：我不假装自己能完全回答什么是意识，尽管我认为我在这个领域取得了一点点进步。
162 00:11:48,134 --> 00:11:51,642 说话人 SPEAKER_01：实际上，这一进步是在上个世纪由哲学家们取得的。
163 00:11:54,587 --> 00:11:59,297 说话人 SPEAKER_01：所以如果我对你说，我看到一些粉红色的象在我面前飘浮，
164 00:12:00,340 --> 00:12:09,955 说话人 SPEAKER_01：关于这一点的一种思考方式是，有一个内在的剧院，在我的内在剧院里有一些粉红色的象，我似乎可以直接看到这些粉红色的象。
165 00:12:10,635 --> 00:12:13,659 说话人 SPEAKER_01：如果你问它们是由什么构成的，它们是由一种叫做“质料”的东西构成的。
166 00:12:14,081 --> 00:12:21,010 说话人 SPEAKER_01：也许有一些粉红色的质料，一些象的质料，一些直立方向的质料和一些运动的质料，它们以某种方式结合在一起。
167 00:12:21,532 --> 00:12:23,053 说话人 SPEAKER_01：这是关于正在发生的事情的一个理论。
168 00:12:23,173 --> 00:12:25,538 说话人 SPEAKER_01：这是一个内部剧院，里面有搞笑的恐怖事物。
169 00:12:26,462 --> 00:12:32,553 说话人 SPEAKER_01：还有一个完全不同的理论，我正在试图告诉你我的感知系统告诉我什么。
170 00:12:33,375 --> 00:12:40,347 说话人 SPEAKER_01：我的感知系统告诉我，空中漂浮着粉红色的小象，我知道这是错误的。
171 00:12:41,649 --> 00:12:50,125 说话者 SPEAKER_01：那么，我告诉你们我的感知系统告诉我什么，就是通过说，我的感知系统要正常工作，需要满足什么条件？
172 00:12:50,307 --> 00:13:01,142 说话者 SPEAKER_01：实际上，当我说我有在眼前飘浮的小粉象的主观体验时，我可以说同样的话，而不必使用“主观体验”这个词。
173 00:13:01,662 --> 00:13:10,914 说话者 SPEAKER_01：我可以说我感知系统告诉我的正确性，如果世界上真的有在我面前飘浮的小粉象的话。
174 00:13:11,182 --> 00:13:18,029 说话者 SPEAKER_01：换句话说，这些小粉象有趣的地方不在于它们存在于一个由有趣的东西叫做“质料”构成的内在剧院中。
175 00:13:18,769 --> 00:13:20,873 说话人 SPEAKER_01：这是假设的世界状态。
176 00:13:21,774 --> 00:13:24,255 说话人 SPEAKER_01：这只是间接引用技巧。
177 00:13:24,275 --> 00:13:31,624 说话人 SPEAKER_01：我无法直接描述我的感知系统告诉我什么，但我可以说出世界必须具备哪些条件才能是正确的。
178 00:13:32,926 --> 00:13:38,471 说话人 SPEAKER_00：那么机器在感知方面可以或多或少做到同样的事情吗？
179 00:13:39,092 --> 00:13:41,657 说话人 SPEAKER_01：是的，让我给您举一个例子。
180 00:13:41,836 --> 00:13:47,985 说话人 SPEAKER_01：我想给您举一个显然有主观体验的聊天机器人的例子。
181 00:13:48,004 --> 00:13:59,441 说话人 SPEAKER_01：假设我有一个多模态聊天机器人，它有一个摄像头和一个机械臂，我训练它，它可以说话，可以看到东西，我在它面前放一个物体，然后说指向这个物体。
182 00:14:00,221 --> 00:14:01,263 说话人 SPEAKER_01：它会指向这个物体。
183 00:14:02,504 --> 00:14:08,273 说话人 SPEAKER_01：现在我把它放在镜头前面，而它却不知道，
184 00:14:09,129 --> 00:14:13,395 说话人 SPEAKER_01：现在我把它放在镜头前面，然后对它说，指向那个物体，它却指向了一边。
185 00:14:14,177 --> 00:14:16,080 说话人 SPEAKER_01：我说，不，那个物体不是在那里。
186 00:14:16,139 --> 00:14:18,763 说话人 SPEAKER_01：物体就在你正前方，但我把它放在了你的镜头前面。
187 00:14:19,563 --> 00:14:22,849 说话人 SPEAKER_01：哦，我明白了，棱镜弯曲了光线。
188 00:14:23,570 --> 00:14:29,518 说话人 SPEAKER_01：所以物体实际上就在我正前方，但我有一种主观感觉它偏到了一边。
189 00:14:30,173 --> 00:14:36,145 说话人 SPEAKER_01：如果聊天机器人这么说，我认为它将“主观体验”这个词用得和我们一样。
190 00:14:36,886 --> 00:14:40,294 说话人 SPEAKER_01：它并不是指聊天机器人无法拥有的神秘内在的东西。
191 00:14:40,916 --> 00:14:46,528 说话人 SPEAKER_01：这指的是一个假设的世界状态，使得聊天机器人的感知是正确的。
192 00:14:47,520 --> 00:14:54,087 说话人 SPEAKER_00：哇，好吧，这是第一次有人和我争论这个问题，但这是一个非常有趣、非常有趣的案例。
193 00:14:54,128 --> 00:15:10,625 说话人 SPEAKER_00：让我们谈谈互操作性，这是我向 Altman 询问的事情，因为他认为，理解 AI 系统的核心内部机制将是我们避免灾难性后果的最大保障。
194 00:15:10,605 --> 00:15:12,932 说话人 SPEAKER_00：你帮助设计这些系统。
195 00:15:13,014 --> 00:15:18,190 说话人 SPEAKER_00：为什么看透它们并理解它们在做什么这么难？
196 00:15:19,836 --> 00:15:21,480 说话人 SPEAKER_01：好的，让我们来看一个极端的例子。
197 00:15:21,541 --> 00:15:23,366 说话人 SPEAKER_01：假设我们有一个大数据集
198 00:15:24,307 --> 00:15:26,509 说话人 SPEAKER_01：并且我们试图回答一个是非问题。
199 00:15:27,291 --> 00:15:30,052 说话人 SPEAKER_01：在这个数据集中，有很多弱规则性。
200 00:15:30,774 --> 00:15:35,798 说话人 SPEAKER_01：可能有 30 万个弱规则性表明答案应该是“否”。
201 00:15:36,778 --> 00:15:40,481 说话人 SPEAKER_01：还有 60 万个弱规则性表明答案应该是“是”。
202 00:15:41,263 --> 00:15:43,544 说话人 SPEAKER_01：这些规则性的强度大致相等。
203 00:15:44,044 --> 00:15:45,807 说话人 SPEAKER_01：答案非常明确，是的。
204 00:15:46,287 --> 00:15:48,448 说话人 SPEAKER_01：有压倒性的证据表明答案应该是肯定的。
205 00:15:48,929 --> 00:15:51,192 说话人 SPEAKER_01：但这种证据都存在于这些微弱的规律中。
206 00:15:51,272 --> 00:15:53,673 说话人 SPEAKER_01：这只是它们综合作用的结果。
207 00:15:53,653 --> 00:15:55,076 发言人 SPEAKER_01：当然，这是一个极端的情况。
如果您然后问某人，好吧，解释为什么它说“是”。
唯一解释它为什么说“是”的方法就是深入研究这600,000个弱规律。
当你在这样一个领域，有很多很多微弱的规律，而且它们的数量如此之多以至于实际上变得很重要，它们的综合效应是显著的，没有理由预期你能得到对事物的简单解释。
211 00:16:21,024 --> 00:16:28,174 说话人 SPEAKER_00: 昨天的对话中，Altman 指出了一份来自 Anthropic 的论文，我觉得非常有趣。
212 00:16:28,816 --> 00:16:40,533 说话人 SPEAKER_00: 论文讨论了分析 Claude（Anthropic 的模型）的内部运作，以及找到所有与金门大桥概念相关的神经连接。
213 00:16:40,552 --> 00:16:44,658 说话人 SPEAKER_00: 然后你对所有这些连接进行加权，创建出“金门 Claude”。
214 00:16:44,999 --> 00:16:47,582 说话人 SPEAKER_00: 然后你进入那个聊天机器人，你说，给我讲一个爱情故事。
215 00:16:47,822 --> 00:16:50,307 说话人 SPEAKER_00：这是一个发生在金门大桥的爱情故事。
216 00:16:50,287 --> 00:16:52,910 说话人 SPEAKER_00：你知道，你问它是什么，它就会描述金门大桥。
217 00:16:54,212 --> 00:17:12,800 说话人 SPEAKER_00：鉴于这一点，为什么我们不能进入一个大型语言模型并调整权重，不是针对金门大桥，而是针对，比如说，同情的概念，怜悯的概念，然后创建一个更有可能为世界做好事的的大型语言模型呢？
218 00:17:13,961 --> 00:17:18,208 说话人 SPEAKER_01：我认为你可以创建一个有同情心的模型，但不是通过直接调整权重。
219 00:17:18,248 --> 00:17:20,791 说话人 SPEAKER_01：你只需在表现出同理心的数据上训练它。
220 00:17:22,835 --> 00:17:24,136 说话人 SPEAKER_00：然后你会得到相同的结果。
221 00:17:24,979 --> 00:17:25,179 说话人 SPEAKER_01：是的。
222 00:17:25,779 --> 00:17:26,901 说话人 SPEAKER_00：我们应该这样做吗？
223 00:17:27,623 --> 00:17:33,912 发言人 SPEAKER_01：过去人们已经有很多例子试图了解单个神经元在做什么。
224 00:17:35,343 --> 00:17:37,465 发言者 SPEAKER_01：我已经这样做了大约 50 年。
225 00:17:38,106 --> 00:17:46,798 发言人 SPEAKER_01：如果神经元直接连接到输入或直接连接到输出，你就有机会了解单个神经元在做什么。
但是一旦有多个层级，就非常非常难以理解系统内部深处的神经元到底在做什么，因为起作用的是边际效应。
227 00:17:58,695 --> 00:18:03,442 说话者 SPEAKER_01：其边际效应因其他神经元的行为而异，取决于输入。
228 00:18:03,423 --> 00:18:06,567 说话者 SPEAKER_01：因此，随着输入的变化，所有这些神经元的边际效应也会变化。
229 00:18:06,989 --> 00:18:11,076 说话者 SPEAKER_01：而且，要获得关于它们行为的良好理论极其困难。
230 00:18:11,096 --> 00:18:24,637 说话者 SPEAKER_00：所以，我可以尝试调整后台构建的神经网络中的权重以实现同情心，结果却可能制造出一种可怕的杀戮机器，因为我不知道我做了什么以及一切是如何相互关联的？
231 00:18:24,736 --> 00:18:28,180 说话人 SPEAKER_01：是的，我可能是少数真正尝试过这样做的人之一。
232 00:18:28,240 --> 00:18:36,509 说话人 SPEAKER_01：所以在神经网络非常早期的阶段，当学习算法效果不佳时，我有一台 Lisp 机器，鼠标有三个按钮。
233 00:18:37,731 --> 00:18:42,277 说话人 SPEAKER_01：我想出了一个方法来显示一个小型神经网络中的所有权重。
234 00:18:42,297 --> 00:18:45,361 说话人 SPEAKER_01：我还让它这样，如果你按左键，权重就会稍微小一点。
235 00:18:45,381 --> 00:18:47,663 说话人 SPEAKER_01：按下右侧按钮，重量会稍微增加一点。
236 00:18:48,444 --> 00:18:52,009 说话人 SPEAKER_01：按下中间按钮，你可以看到重量的值。
237 00:18:52,461 --> 00:18:53,748 将打印出重量的值。
238 00:18:54,471 --> 00:18:57,888 我尝试调整神经网络，调整权重。
239 00:18:57,970 --> 00:18:59,416 说话人 SPEAKER_01：这真的很困难。
240 00:18:59,498 --> 00:19:01,681 说话人 SPEAKER_01：反向传播要好得多。
241 00:19:02,402 --> 00:19:09,710 说话人 SPEAKER_00：嗯，我们得等待一个比杰弗里·辛顿（Geoffrey Hinton）更聪明的下一级 AI 来找出如何做到这一点。
242 00:19:11,251 --> 00:19:13,013 说话人 SPEAKER_00：让我们谈谈一些有益于 AI 的事情。
243 00:19:13,335 --> 00:19:16,519 说话人 SPEAKER_00：你经常谈论医疗领域将带来的好处。
244 00:19:16,818 --> 00:19:24,067 说话人 SPEAKER_00：当你浏览可持续发展目标时，似乎健康和医药是你认为人工智能将带来很多好处的一个领域。
245 00:19:24,107 --> 00:19:24,587 说话人 SPEAKER_00：这公平吗？
246 00:19:24,808 --> 00:19:27,632 说话人 SPEAKER_00：请告诉我原因。
247 00:19:28,726 --> 00:19:29,626 说话人 SPEAKER_01: 哎，我有点困惑。
248 00:19:29,666 --> 00:19:30,567 说话人 SPEAKER_01: 这很明显。
249 00:19:31,169 --> 00:19:34,473 说话人 SPEAKER_01: 它在解读医学影像方面将会更加出色。
250 00:19:35,013 --> 00:19:41,583 说话人 SPEAKER_01: 2016 年，我说到 2021 年，它将在解读医学影像方面比临床医生更加出色。
251 00:19:41,943 --> 00:19:42,565 说话人 SPEAKER_01：我错了。
252 00:19:42,644 --> 00:19:47,991 说话人 SPEAKER_01：这可能还需要另外五到十年，部分原因是医学对新事物的接受非常缓慢。
253 00:19:49,093 --> 00:19:52,417 说话人 SPEAKER_01：而且，我也高估了短期进步的速度。
254 00:19:53,640 --> 00:19:56,943 说话人 SPEAKER_01：所以这是我做出的一个错误预测。
255 00:19:56,923 --> 00:19:58,526 说话人 SPEAKER_01：但很明显它正在变得越来越好。
256 00:19:58,546 --> 00:20:05,194 说话人 SPEAKER_01：现在它在许多种类的医学图像上可以与相当好的医学专家相媲美。
257 00:20:05,415 --> 00:20:06,676 说话人 SPEAKER_01：不是全部，但很多都是。
258 00:20:07,258 --> 00:20:08,500 说话人 SPEAKER_01：而且它一直在变得越来越好。
259 00:20:09,201 --> 00:20:11,584 说话人 SPEAKER_01：它可以看到比任何临床医生更多的数据。
260 00:20:12,105 --> 00:20:14,248 说话人 SPEAKER_01：所以最终它肯定会变得更好。
261 00:20:14,688 --> 00:20:16,130 说话人 SPEAKER_01：我只是觉得它会来得更快一些。
262 00:20:16,490 --> 00:20:21,498 说话人 SPEAKER_01：但它也擅长处理诸如整合大量患者数据之类的事情。
263 00:20:21,518 --> 00:20:25,242 说话人 SPEAKER_01：结合基因组数据，所有医疗测试的结果。
264 00:20:25,222 --> 00:20:35,618 说话人 SPEAKER_01：我的家庭医生如果看过一亿个病人，并且能记住他们所有人的信息，或者能整合他们的信息，那我会非常喜欢的。
265 00:20:36,039 --> 00:20:48,439 说话人 SPEAKER_01：所以当我带着一些奇怪的症状去的时候，医生可以立刻说出是什么，因为她已经看过在那一亿个病人中像这样的有 500 个病人了。
266 00:20:48,619 --> 00:20:50,843 说话人 SPEAKER_01：这即将到来，这将是非常惊人的。
267 00:20:51,076 --> 00:20:58,411 说话人 说话人_00: 那么医疗福利的未来是 A、看过更多病人并对其进行培训的医生。
268 00:20:58,711 --> 00:21:02,279 说话人 说话人_00: B、像分析图像这样的特定任务。
269 00:21:02,760 --> 00:21:04,284 说话人 说话人_00: 那么，关于科学突破呢？
270 00:21:04,644 --> 00:21:08,712 说话人 说话人_00: 我喜欢你们的老同事在 AlphaFold 3、AlphaFold 2 上所做的工作。
271 00:21:09,384 --> 00:21:10,707 说话人 SPEAKER_01：当然会有很多这样的。
272 00:21:10,787 --> 00:21:18,297 说话人 SPEAKER_01：这将有助于理解正在发生的事情，以及设计新药。
273 00:21:18,356 --> 00:21:21,560 说话人 SPEAKER_01：显然，这有助于设计新药。
274 00:21:21,820 --> 00:21:23,743 说话人 SPEAKER_01：我认为 Demis 现在非常相信这一点。
275 00:21:25,346 --> 00:21:28,710 说话人 SPEAKER_01：但这将帮助我们理解基础科学。
276 00:21:28,690 --> 00:21:36,318 说话人 SPEAKER_01：在许多情况下，存在大量我们进化过程中未曾处理过的数据。
277 00:21:36,338 --> 00:21:42,125 说话人 SPEAKER_01：所以这不是视觉数据，也不是声学数据，是基因组和其他数据。
278 00:21:42,906 --> 00:21:51,036 说话人 SPEAKER_01：我认为这些 AI 系统将更擅长处理大量数据，发现其中的模式并理解它们。
279 00:21:51,319 --> 00:21:57,867 说话人 SPEAKER_00：这触及了我对 AI 领域的主要批评之一，我想知道你是否也有同样的看法。
280 00:21:57,887 --> 00:22:13,445 说话人 SPEAKER_00：我明白为什么那么多研究人员以及你的一些前学生，许多领域的先驱，都在努力制造出像人类一样，甚至无法与人类区分的机器。
281 00:22:13,425 --> 00:22:22,517 说话人 SPEAKER_00：但还有很多人在尝试构建非常具体的东西，比如 AlphaFold3，或者试图弄清楚如何利用 AI 推动癌症研究。
282 00:22:23,458 --> 00:22:35,653 说话人 SPEAKER_00：你认为我感到在 AGI 方面投入了过多的关注和权重，而在具体科学效益方面关注不足，这是不是错的？
283 00:22:36,832 --> 00:22:38,634 说话人 SPEAKER_01：我认为你在这方面可能确实是对的。
284 00:22:39,035 --> 00:22:44,560 说话人 SPEAKER_01：长期以来，我认为 AGI 不会出现，不会有一个时刻这些事物会突然比我们聪明。
285 00:22:45,101 --> 00:22:47,785 说话人 SPEAKER_01：它们会在不同时间在不同的事情上比我们做得更好。
286 00:22:48,525 --> 00:22:55,733 说话人 SPEAKER_01：所以如果你玩国际象棋或围棋，显然，人类永远不可能像 AlphaGo 或 AlphaZero 那样出色。
287 00:22:56,453 --> 00:22:57,635 说话人 SPEAKER_01：他们已经远远超过我们了。
288 00:22:59,178 --> 00:23:04,864 说话人 SPEAKER_01：我们可以从他们玩游戏的方式中学到很多东西，人们也在学习这一点，但他们在这方面远远领先于我们。
289 00:23:05,012 --> 00:23:07,817 说话人 SPEAKER_01：而且在编程方面，他们可能已经远远超过了我。
290 00:23:09,299 --> 00:23:10,382 说话人 SPEAKER_01：我并不是一个很好的程序员。
291 00:23:11,763 --> 00:23:16,712 说话人 SPEAKER_01：我认为他们突然在所有事情上都变得更好这个想法很荒谬。
292 00:23:17,133 --> 00:23:22,261 说话人 SPEAKER_01：他们将在不同时间擅长不同的事情，我相信身体操作将是其中较晚的一项。
293 00:23:23,271 --> 00:23:39,752 说话人 SPEAKER_00：所以当你的前学生向你寻求项目时，你是否经常引导他们去做更多基础科学研究，推动更多发现，而不是继续追求类似人类的智能？
294 00:23:39,772 --> 00:23:44,298 说话人 SPEAKER_01：我的前学生现在都太老了，不再向我提问了。
295 00:23:46,580 --> 00:23:50,145 说话人 SPEAKER_00: 他以前的学生几乎运营着世界上所有的 AI 公司。
296 00:23:50,165 --> 00:23:52,788 说话人 SPEAKER_00: 这就像是一种微妙的方式来触及那个问题。
297 00:23:53,089 --> 00:23:53,671 说话人 SPEAKER_00: 我们就让它这样吧。
298 00:23:53,911 --> 00:23:55,392 说话人 SPEAKER_00: 那就回到 AI 的正面价值上来。
299 00:23:56,573 --> 00:24:10,866 讲者 SPEAKER_00：观察可持续发展目标，观察房间里人们的抱负，您觉得人工智能会以何种方式改变教育，从而帮助实现公平，尤其是当这些系统精通地球上每一种语言时？
300 00:24:12,028 --> 00:24:12,327 讲者 SPEAKER_01：是的。
301 00:24:13,429 --> 00:24:15,250 讲者 SPEAKER_01：那么，让我给你们讲一个小故事。
302 00:24:15,371 --> 00:24:22,017 讲者 SPEAKER_01：在我上学的时候，我父亲坚持让我学习德语，因为他认为那将是科学的语言。
303 00:24:22,789 --> 00:24:30,222 讲者 SPEAKER_01：那是因为在化学领域，我认为在上个世纪的中间部分，或者说是早期，德语是科学界的语言。
304 00:24:31,005 --> 00:24:32,227 讲者 SPEAKER_01：我的德语不是很好。
305 00:24:32,907 --> 00:24:34,270 讲者 SPEAKER_01：我在德语方面表现不佳。
306 00:24:34,852 --> 00:24:41,202 讲者 SPEAKER_01：于是我的父母为我请了一位私人教师，不久我就成为了德语课上的尖子生。
307 00:24:41,909 --> 00:24:55,944 讲者 SPEAKER_01：私人辅导比坐在教室里听老师广播要有效率得多，因为私人辅导老师能看到你具体哪里不懂，并直接给你你需要的那一点信息，让你正确理解。
308 00:24:56,786 --> 00:24:59,148 讲者 SPEAKER_01：所以我认为每个人都会请私人辅导。
309 00:24:59,489 --> 00:25:04,194 讲者 SPEAKER_01：直到现在，私人辅导一直是富人或中产阶级有抱负的人的领域。
310 00:25:06,396 --> 00:25:08,378 讲者 SPEAKER_01：从这个意义上说，这将非常有帮助。
311 00:25:08,400 --> 00:25:11,442 说话者 SPEAKER_01：我认为可汗学院也这么认为。
312 00:25:12,097 --> 00:25:13,118 说话人 SPEAKER_00：这是一件大事。
313 00:25:13,138 --> 00:25:23,670 说话人 SPEAKER_00：我的意思是，如果每个人都能拥有这些极其出色的私人导师，他们可以讲他们的语言，我们总有一天会做到的，愿上帝保佑，很快就会实现。
314 00:25:23,690 --> 00:25:25,332 说话人 SPEAKER_00：这在这里已经成为一个热门话题。
315 00:25:26,432 --> 00:25:29,316 说话人 SPEAKER_00：你没看到世界正在变得更加平等吗？
316 00:25:30,758 --> 00:25:32,078 说话人 SPEAKER_01：从这个意义上说，是的。
317 00:25:32,359 --> 00:25:36,384 说话人 SPEAKER_01：关于教育机会，我认为它将变得更加平等。
318 00:25:37,105 --> 00:25:40,147 说话人 SPEAKER_01：精英大学可能不会喜欢这一点，
319 00:25:41,173 --> 00:25:43,676 说话人 SPEAKER_01：我认为它将变得更加平等，是的。
320 00:25:43,717 --> 00:25:44,218 说话人 SPEAKER_00: 我们不在这里。
321 00:25:44,258 --> 00:25:46,000 说话人 SPEAKER_00: 我们更感兴趣的是人类的未来。
322 00:25:46,020 --> 00:25:49,125 说话人 SPEAKER_00: 这不是，你知道的，只为精英大学的 AI。
323 00:25:49,184 --> 00:25:49,986 说话人 SPEAKER_00: 这是为善的 AI。
324 00:25:50,006 --> 00:25:52,328 说话人 SPEAKER_00：我认为我们可以把这个看作是这个阶段的胜利。
325 00:25:52,970 --> 00:25:53,431 说话人 SPEAKER_00：当然。
326 00:25:53,671 --> 00:26:08,270 说话人 SPEAKER_00：但是你的回答中有一个空缺，暗示你感觉 AI 总体上不会成为促进平等的力量，反而可能成为加剧不平等的力量。
327 00:26:08,411 --> 00:26:10,354 说话人 SPEAKER_00：我解读你的回答是否错了？
328 00:26:11,431 --> 00:26:15,516 说话人 SPEAKER_01：嗯，我们生活在一个资本主义体系，资本主义体系为我们带来了很多。
329 00:26:16,237 --> 00:26:18,278 说话人 SPEAKER_01：但我们知道一些关于资本主义体系的事情。
330 00:26:18,799 --> 00:26:30,795 说话人 SPEAKER_01：如果你看看像大石油、大烟草、石棉，或者各种其他东西，我们知道在资本主义体系中，人们试图赚取利润。
331 00:26:31,675 --> 00:26:39,345 说话人 SPEAKER_01：因此你需要强有力的监管，以确保他们在追求利润的过程中，不会破坏环境，例如。
332 00:26:40,422 --> 00:26:44,788 说话人 SPEAKER_01：我们显然需要这个用于人工智能，但我们得到的速度远远不够快。
333 00:26:45,830 --> 00:26:53,824 说话人 SPEAKER_01：所以如果你看萨姆·奥特曼昨天说的话，他给人的印象是，是的，他们非常关注安全和如此等等。
334 00:26:54,364 --> 00:26:56,227 说话人 SPEAKER_01：但现在我们已经有了关于这个的实验。
335 00:26:56,708 --> 00:27:02,817 说话人 SPEAKER_01：我们看到了一个实验的结果，这个实验是把安全与利润对立起来。
336 00:27:02,798 --> 00:27:05,401 说话人 SPEAKER_01：现在实验是在相当糟糕的条件下进行的。
337 00:27:05,500 --> 00:27:15,212 说话人 SPEAKER_01：这是在所有 OpenAI 员工即将能够将纸币换成真钱的时候进行的，因为即将有一轮大融资，他们将要能够出售他们的股份。
338 00:27:15,814 --> 00:27:18,576 说话人 SPEAKER_01：所以这并不是在理想条件下进行的实验。
339 00:27:19,178 --> 00:27:21,922 说话人 SPEAKER_01：但利润和安全方面的赢家是显而易见的。
340 00:27:22,521 --> 00:27:24,704 说话人 SPEAKER_01：现在很清楚，OpenAI 拥有一个新的安全小组。
341 00:27:25,376 --> 00:27:28,881 说话人 SPEAKER_01：OpenAI 拥有的，是一个新的安全小组。
342 00:27:30,342 --> 00:27:34,326 说话人 SPEAKER_01：它雇佣了一些经济学家，至少有一位经济学家。
343 00:27:34,346 --> 00:27:36,909 说话人 SPEAKER_01：我认为经济学家是资本主义的祭司。
344 00:27:37,789 --> 00:27:46,818 说话人 SPEAKER_01：我认为它对存在性威胁的担忧远不如伊利亚和他合作的人那么大。
345 00:27:51,022 --> 00:27:53,645 说话人 SPEAKER_01：我也这么认为，
346 00:27:54,148 --> 00:27:59,663 说话人 SPEAKER_01：问题是资本主义是关于盈利的，我并不完全反对。
347 00:27:59,702 --> 00:28:08,344 说话人 SPEAKER_01：我的意思是，它为我们做了许多好事，这种推动力，但它需要得到监管，以免也造成不好的事情。
348 00:28:10,045 --> 00:28:11,207 说话人 SPEAKER_01: 创造大量财富。
349 00:28:11,788 --> 00:28:16,959 说话人 SPEAKER_01: 我认为对几乎每个人来说都很清楚，人工智能将会提高生产力。
350 00:28:17,599 --> 00:28:21,407 说话人 SPEAKER_01: 问题在于，那额外的财富将流向何方？
351 00:28:21,448 --> 00:28:24,053 说话人 SPEAKER_01: 我认为它不会流向穷人。
352 00:28:24,073 --> 00:28:25,415 说话人 SPEAKER_01: 我认为这会流向富人。
353 00:28:25,435 --> 00:28:27,941 说话人 SPEAKER_01: 所以我认为这会加剧贫富差距。
354 00:28:28,280 --> 00:28:28,982 说话人 SPEAKER_01: 我相信这一点。
355 00:28:30,365 --> 00:28:31,647 说话人 SPEAKER_00: 你没有希望吗？
356 00:28:33,400 --> 00:28:46,773 说话人 SPEAKER_00：你似乎在说 AI，它的力量，它可能因为训练这些大型语言模型所需的资源而只属于少数企业，AI 似乎与资本主义和公平不相容。
357 00:28:47,414 --> 00:29:02,951 说话人 SPEAKER_00：对于我们刚才讨论的公平和教育，每个人都能获得极其强大的机器的能力，如果不是像最昂贵的机器那样完全强大，你对此没有希望吗？
358 00:29:04,180 --> 00:29:12,672 说话人 SPEAKER_01：对此有一些希望，但在我大部分生活中，我一直认为随着人们受教育程度的提高，他们会变得更加理智。
359 00:29:13,952 --> 00:29:15,996 说话人 SPEAKER_01：但这并没有真正发生。
360 00:29:16,436 --> 00:29:22,424 Speaker SPEAKER_01: 现在看看共和党，他们只是在散播谎言，而且都是疯狂的谎言。
361 00:29:23,738 --> 00:29:27,101 Speaker SPEAKER_00: 这是一个好时机。
362 00:29:27,121 --> 00:29:28,303 Speaker SPEAKER_00: 让我们进入问题。
363 00:29:29,384 --> 00:29:33,428 Speaker SPEAKER_00: 我想探讨如何进行监管，以及您的相关想法。
364 00:29:33,907 --> 00:29:38,532 说话人 SPEAKER_00：但我还想谈谈您对人工智能发展方向的一些其他担忧。
365 00:29:39,193 --> 00:29:49,964 说话人 SPEAKER_00：您能否在这里列举一两点，不是您担心的，而是您对经济存在的担忧，以及您对未来 12 个月的担忧。
366 00:29:51,446 --> 00:29:57,214 说话人 SPEAKER_01：好的，我担心的是我对它知之甚少的东西，那就是网络犯罪。
367 00:29:58,355 --> 00:30:04,324 说话人 SPEAKER_01：我最近听了 Dawn Song 的演讲，她说去年的钓鱼攻击增加了 1200%。
368 00:30:05,164 --> 00:30:16,240 说话人 SPEAKER_01：当然，它们变得越来越好，因为你再也分辨不出它们是通过拼写错误或奇怪的语法，因为现在都是由聊天机器人完成的。
369 00:30:17,266 --> 00:30:19,009 说话人 SPEAKER_01：或者很多都是。
370 00:30:19,029 --> 00:30:21,433 说话人 SPEAKER_01：所以我对此感到担忧，但对此了解不多。
371 00:30:21,974 --> 00:30:26,443 说话人 SPEAKER_01：我非常担心的另一件事是虚假视频破坏选举。
372 00:30:27,246 --> 00:30:34,380 说话人 SPEAKER_01：我认为在每次选举前，显然会有很多假视频，因为那时没有时间来反驳它们。
373 00:30:34,781 --> 00:30:39,849 说话人 SPEAKER_01：而且我认为对公众进行假视频的预防接种是个好主意。
374 00:30:40,651 --> 00:30:41,933 说话人 SPEAKER_01：所以把它们当作一种疾病来对待。
375 00:30:41,993 --> 00:30:47,162 说话人 SPEAKER_01：而预防疾病的方法是提供一种减弱版本。
376 00:30:47,942 --> 00:30:50,326 说话人 SPEAKER_01：所以我认为
377 00:30:50,307 --> 00:30:53,109 说话人 SPEAKER_01：世界上有很多慈善的亿万富翁。
378 00:30:53,130 --> 00:30:59,718 说话人 SPEAKER_01：我认为他们应该花钱，或者至少一部分，在选举前一个月左右在广播上播出。
379 00:31:00,417 --> 00:31:03,142 说话人 SPEAKER_01：有很多非常令人信服的假视频。
380 00:31:03,162 --> 00:31:05,403 说话人 SPEAKER_01：最后他们说，但这却是假的。
381 00:31:06,085 --> 00:31:09,087 说话人 SPEAKER_01：那不是特朗普在说话，特朗普从未说过那样的话。
382 00:31:10,410 --> 00:31:13,073 说话人 SPEAKER_01：也不是拜登在说话，拜登从未说过那样的话。
383 00:31:13,432 --> 00:31:15,015 说话人 SPEAKER_01：这是一个假视频。
384 00:31:15,738 --> 00:31:18,961 说话人 SPEAKER_01：然后你会让人对几乎所有事情都产生怀疑。
385 00:31:19,522 --> 00:31:21,746 说话人 SPEAKER_01：如果到处都是假视频，这是个好主意。
386 00:31:22,626 --> 00:31:25,671 说话人 SPEAKER_01：但这样就需要一种方法让人们检查视频是否真实。
387 00:31:27,653 --> 00:31:32,318 说话人 SPEAKER_01：如果他们愿意花上 30 秒的时间，这比检查视频是否为假要容易。
388 00:31:32,960 --> 00:31:38,386 说话人 SPEAKER_01：例如，Jan Tallinn 建议，可以在每个视频的开头放置一个二维码。
389 00:31:38,406 --> 00:31:41,391 说话人 SPEAKER_01：你可以使用这个二维码访问一个网站。
390 00:31:41,624 --> 00:31:47,536 说话人 SPEAKER_01：如果网站上有相同的视频，你就知道该网站声称这个视频是真实的。
391 00:31:48,116 --> 00:31:52,967 说话人 SPEAKER_01：现在，你将视频是否真实的问题简化为网站是否真实的问题。
392 00:31:53,307 --> 00:31:54,388 说话人 SPEAKER_01: 网站是独一无二的。
393 00:31:55,050 --> 00:32:03,227 说话人 SPEAKER_01: 所以如果你确定那真的是特朗普竞选网站，那么你就知道特朗普竞选团队真的发布了那个视频。
394 00:32:03,460 --> 00:32:05,402 说话人 SPEAKER_00: 那我们能不能暂停一下？
395 00:32:05,663 --> 00:32:07,684 说话人 SPEAKER_00: 这就是为什么我喜欢采访杰弗里·辛顿（Geoffrey Hinton）的原因。
396 00:32:07,885 --> 00:32:22,243 说话人 SPEAKER_00：我们已经从关于意识的全新理论，一个关于主观感受的极具争议的理论，转变到我们应该通过播放低剂量的虚假视频来对公众进行虚假新闻的免疫接种的想法。
397 00:32:22,423 --> 00:32:26,368 说话人 SPEAKER_00：让我们先来谈谈第一部分，因为我觉得你的解决方案那里有两个部分。
398 00:32:26,890 --> 00:32:30,134 说话人 SPEAKER_00：所以第一个是让公众对虚假视频进行免疫接种。
399 00:32:30,153 --> 00:32:32,656 说话人 SPEAKER_00：所以你是说具体来说？
400 00:32:32,636 --> 00:32:42,670 说话人 SPEAKER_00: 应该有人制作数百万个短小、虚假但不太有害的视频并发布到推特帖子中吗？
401 00:32:42,971 --> 00:32:44,512 说话人 SPEAKER_01: 它们可能有一定的破坏性。
402 00:32:44,532 --> 00:32:47,696 说话人 SPEAKER_01: 除非它们看起来像真实的政治广告，否则它们不会令人信服。
403 00:32:50,140 --> 00:32:58,611 说话人 SPEAKER_01: 但在广告结束时，它们是短广告，所以你希望人们看到最后，广告结束时会说这是假的。
404 00:32:59,249 --> 00:33:01,573 说话人 SPEAKER_01：这就是让你应对的衰减。
405 00:33:01,753 --> 00:33:02,074 说话人 SPEAKER_00：我明白了。
406 00:33:02,213 --> 00:33:05,420 说话人 SPEAKER_00：所以你看，你就像，啊，这证明了我的观点。
407 00:33:05,461 --> 00:33:06,682 说话人 SPEAKER_00：哦，等等，那是假的。
408 00:33:06,903 --> 00:33:08,266 说话人 SPEAKER_00: 然后你更不信任了。
409 00:33:08,286 --> 00:33:08,665 说话人 SPEAKER_00: 我喜欢这个。
410 00:33:08,767 --> 00:33:08,987 说话人 SPEAKER_00: 好的。
411 00:33:09,126 --> 00:33:09,587 说话人 SPEAKER_00: 正确的。
412 00:33:09,828 --> 00:33:13,315 说话人 SPEAKER_00：那么第二部分是每个视频都应该有一个二维码。
413 00:33:13,635 --> 00:33:14,497 说话人 SPEAKER_00：然后你看到一些东西。
414 00:33:14,557 --> 00:33:15,880 说话人 SPEAKER_00：现在你已经知道了。
415 00:33:16,401 --> 00:33:17,983 说话人 SPEAKER_00：然后你扫描这个小二维码。
416 00:33:18,023 --> 00:33:18,965 说话人 SPEAKER_00: 你去网站。
417 00:33:19,185 --> 00:33:19,807 说话人 SPEAKER_00: 哎，是真的。
418 00:33:19,846 --> 00:33:20,788 说话人 SPEAKER_00: 它真的在网站上。
419 00:33:20,888 --> 00:33:22,471 说话人 SPEAKER_00: 那是想法吗？
420 00:33:22,451 --> 00:33:29,300 说话人 SPEAKER_01：仅仅能带你去一个真实网站是不够的，因为假视频也可能带你去同一个真实网站。
421 00:33:30,102 --> 00:33:31,744 说话人 SPEAKER_01：视频必须在那里。
422 00:33:31,765 --> 00:33:34,147 说话人 SPEAKER_01：那个网站上必须要有相同的视频。
423 00:33:34,628 --> 00:33:35,028 说话人 SPEAKER_00：公平。
424 00:33:35,730 --> 00:33:37,893 说话人 SPEAKER_00：让我们来谈谈偏见以及如何防止它们。
425 00:33:37,952 --> 00:33:43,279 说话人 SPEAKER_00：人们谈论的风险之一是，在存在偏见的训练数据上训练的人工智能系统将产生有偏见的成果。
426 00:33:44,422 --> 00:33:48,847 说话人 SPEAKER_00：让我们回到医学领域，在那里你提出了一个令人信服的论点，
427 00:33:48,827 --> 00:33:51,172 说话人 SPEAKER_00：网络人工智能将带来巨大的益处。
428 00:33:51,732 --> 00:34:05,974 说话者 SPEAKER_00：你可以想象一个刚刚接受过美国人群医疗记录培训的医生，因为他们的医疗问题、DNA 等不同，所以不能给赞比亚人提供正确的医疗建议。
429 00:34:06,435 --> 00:34:10,481 说话者 SPEAKER_00：你对这个问题有多担心？我们应该如何解决这个问题？
430 00:34:11,844 --> 00:34:17,293 说话者 SPEAKER_01：好吧，我对偏见和歧视问题不太担心，我更担心其他问题。
431 00:34:17,974 --> 00:34:22,101 说话者 SPEAKER_01：而且我知道我是一个年老的白色男性，这可能与此有关。
432 00:34:22,400 --> 00:34:23,882 说话人 SPEAKER_01：这种情况很少发生在我身上。
433 00:34:25,726 --> 00:34:28,791 说话人 SPEAKER_01：但我认为如果你设定目标
434 00:34:28,771 --> 00:34:34,561 说话人 SPEAKER_01：用不那么有偏见的系统或人去替换有偏见的系统。
435 00:34:34,581 --> 00:34:38,208 说话人 SPEAKER_01：不是无偏见的系统，而是不那么有偏见的系统。
436 00:34:38,228 --> 00:34:39,710 说话人 SPEAKER_01：这似乎完全可行。
437 00:34:40,552 --> 00:34:49,226 说话人 SPEAKER_01：如果我有所有白人决定年轻黑人女性是否应该获得抵押贷款的数据，我预计其中会有一些偏见。
438 00:34:49,206 --> 00:34:56,940 说话人 SPEAKER_01：一旦我用这些数据训练了一个 AI 系统，我实际上可以冻结权重，然后去检查偏见，这是用人类无法做到的。
439 00:34:57,641 --> 00:35:04,311 说话人 SPEAKER_01：对于人类来说，如果你试图检查他们的偏见，你会得到类似大众汽车效应的结果，他们会意识到你在检查他们，然后以完全不同的方式行事。
440 00:35:06,916 --> 00:35:10,842 说话人 SPEAKER_01：我刚发明了这个名字，“大众效应”，就是这样。
441 00:35:12,646 --> 00:35:19,577 说话人 SPEAKER_01：在使用 AI 系统时，如果你冻结权重，你可以更好地测量偏差，并采取措施克服它，改善它。
442 00:35:19,976 --> 00:35:21,539 说话人 SPEAKER_01：你永远无法完全摆脱它。
443 00:35:22,041 --> 00:35:23,141 说话人 SPEAKER_01：我觉得这太难了。
444 00:35:24,403 --> 00:35:30,353 说话人 SPEAKER_01：但是假设你使目标，使新系统比被取代的系统大大减少偏见。
445 00:35:30,813 --> 00:35:32,476 说话人 SPEAKER_01：我认为这是完全可行的。
446 00:35:32,693 --> 00:35:33,193 说话人 SPEAKER_00：了不起。
447 00:35:33,594 --> 00:35:57,755 说话人 SPEAKER_00：并且你是否觉得，在行业中关于偏见的关注，虽然是一个主要话题，但实际上这些系统可能会变得更加公正，我们实际上应该这样说，而不是说我们必须消除所有偏见，我们应该说，让我们使它们比人类更少有偏见，然后从这里开始。
448 00:35:58,951 --> 00:36:00,773 说话人 SPEAKER_01：我认为那是合理的。
449 00:36:00,932 --> 00:36:04,657 说话人 SPEAKER_01：我不认为在政治上……我不确定这是否在政治上可接受。
450 00:36:04,677 --> 00:36:07,842 说话人 SPEAKER_01：我的意思是，假设你说，我们要引入自动驾驶汽车。
451 00:36:08,461 --> 00:36:11,826 说话人 SPEAKER_01：它们在道路上杀死了很多人，但只有普通汽车的一半那么多。
452 00:36:12,547 --> 00:36:13,829 说话人 SPEAKER_01：我觉得你这么做是逃不掉的。
453 00:36:14,128 --> 00:36:16,351 说话人 SPEAKER_01：他们几乎不用为你杀人，你就能逃脱。
454 00:36:18,574 --> 00:36:24,141 所以，我认为这里存在一个政治问题，那就是以理性的方式接受新技术。
455 00:36:26,202 --> 00:36:26,764 说话人 SPEAKER_01：但是...
456 00:36:28,516 --> 00:36:33,882 说话人 SPEAKER_01：我认为我们应该追求显著减少偏见的系统，并对此感到满意。
457 00:36:34,824 --> 00:36:42,592 说话人 SPEAKER_00：好的，让我们谈谈你在采访中描述的 AI 最大的风险，那就是它们会得到次级目标，对吧？
458 00:36:42,853 --> 00:36:49,201 说话人 SPEAKER_00：以及它们会得到一个超越它们创造者或用户赋予它们的初始目标的目标。
459 00:36:49,661 --> 00:36:56,429 说话人 SPEAKER_00：解释一下 A，你认为的次级目标是什么，B，为什么那这么糟糕，以及 C，我们能做些什么来解决这个问题。
460 00:36:57,742 --> 00:37:10,762 说话人 SPEAKER_01：所以一个无害的子目标就是，如果我想让一个 AI 代理为我规划一次旅行，我会说，你必须把我带到北美，假设我在欧洲，我会说，你必须把我带到北美。
461 00:37:11,543 --> 00:37:15,550 说话人 SPEAKER_01：所以它的子目标就是想出如何把我带到机场。
462 00:37:16,356 --> 00:37:18,400 说话人 SPEAKER_01：这只是一个典型的子目标。
463 00:37:19,143 --> 00:37:22,349 说话人 SPEAKER_01：如果你想要制作智能代理，它们必须要有这样的子目标。
464 00:37:22,690 --> 00:37:27,902 说话人 SPEAKER_01：他们必须能够专注于问题的一个小部分，并解决它，而不必担心其他一切。
465 00:37:29,231 --> 00:37:38,364 说话人 SPEAKER_01：现在，一旦你有一个可以创建自己的子目标的系统，就有一个非常有帮助的特定子目标。
466 00:37:38,824 --> 00:37:41,329 说话人 SPEAKER_01：这个子目标就是获得更多控制。
467 00:37:42,070 --> 00:37:46,996 说话人 SPEAKER_01：如果我获得更多控制，我就能更好地完成用户希望我做的各种事情。
468 00:37:47,777 --> 00:37:49,760 说话人 SPEAKER_01：所以，获得更多控制权是理所当然的。
469 00:37:51,222 --> 00:37:53,264 说话人 SPEAKER_01：而且，我们担心的是，
470 00:37:54,088 --> 00:38:01,318 说话人 SPEAKER_01：最终，一个 AI 系统会意识到，如果我能控制一切，我就可以给这些愚蠢的人类他们想要的东西，而他们却没有任何控制权。
471 00:38:02,380 --> 00:38:05,103 说话人 SPEAKER_01：这可能确实是正确的。
472 00:38:06,286 --> 00:38:15,117 说话人 SPEAKER_01：但是，担心的是，如果 AI 系统决定它对自己比对人类更感兴趣，那我们就完了。
473 00:38:15,958 --> 00:38:20,146 说话人 SPEAKER_00：实际上，在我们完蛋之前，正如你描述的那样，我已经非常担心了。
474 00:38:20,166 --> 00:38:23,490 说话人 SPEAKER_00：所以，你有一个 AI，目标是让尼克准时到达机场。
475 00:38:24,132 --> 00:38:26,876 说话人 SPEAKER_00：在某个未来状态下，AI 将是全能的。
476 00:38:27,257 --> 00:38:32,525 说话人 SPEAKER_00：嗯，把尼克弄晕，让他双手背后，然后直接把他扔进车里，可能是把尼克送到机场的最佳方式。
477 00:38:32,565 --> 00:38:35,251 说话人 SPEAKER_00：这样效率更高，因为那样的话，他在出去的路上就不会和任何人说话了。
478 00:38:35,711 --> 00:38:39,637 说话人 SPEAKER_00：所以你可以看到这些子目标正在走向失败。
479 00:38:40,833 --> 00:38:43,295 说话人 SPEAKER_01：是的，但记住它是一个非常智能的系统。
480 00:38:43,835 --> 00:38:49,402 说话人 SPEAKER_01：到那时，它应该不会以明显违反人类利益的方式出错。
481 00:38:49,422 --> 00:38:54,327 说话人 SPEAKER_01：它应该被训练成对人类利益感兴趣。
482 00:38:54,648 --> 00:38:56,829 说话人 SPEAKER_00：好吧，太好了，因为我真的不希望这种情况发生在我身上。
483 00:38:57,451 --> 00:39:01,574 说话人 SPEAKER_00：好吧，让我们来过一遍，我想过一遍一些监管框架。
484 00:39:01,735 --> 00:39:07,802 说话人 SPEAKER_00：我有一个问题想问你，我觉得你可能比大多数人更能回答，那就是，
485 00:39:09,182 --> 00:39:32,186 说话人 SPEAKER_00：阻碍大型 AI 公司和 AI 研究人员在安全性或放缓方面工作的因素，不仅仅是力量，不仅仅是金钱，还有实现伟大事业的梦想，或者说，像程序员说的，找到一些美好的东西。
486 00:39:32,166 --> 00:39:44,942 说话人 SPEAKER_00：请告诉我一个时刻，让监管者能够理解，作为突破边缘的开发者，那是什么感觉，以及监管者在思考政策时应该如何考虑这一点。
487 00:39:47,005 --> 00:39:52,891 说话人 SPEAKER_01：我不确定能否给你提供好的见解。
对于一名研究人员，对于被好奇心驱使的研究人员，致力于如何使某物更强大，引入一些戏剧性的新功能。
489 00:40:05,884 --> 00:40:17,059 发言人 SPEAKER_01：就像之前的发言人谈到的，你学习一种语言的模型，你学习另一种语言的模型，然后你可以采用内部表示并将它们相互旋转。
490 00:40:18,260 --> 00:40:19,222 发言人 SPEAKER_01：这真是太神奇了。
491 00:40:19,242 --> 00:40:22,085 发言人 SPEAKER_01：看到这样的事情你会感到很高兴。
492 00:40:22,065 --> 00:40:27,775 说话人 SPEAKER_01：我不确定你是否从安全工作中获得同样的快乐。
493 00:40:29,358 --> 00:40:31,260 说话人 SPEAKER_01：所以我在某种程度上同意你的观点。
494 00:40:31,601 --> 00:40:33,625 说话人 SPEAKER_01：然而，从事安全工作非常重要。
495 00:40:34,347 --> 00:40:39,996 说话人 SPEAKER_01：而且有一些非常优秀的研究人员热衷于将他们的职业生涯投入到安全工作中。
496 00:40:40,356 --> 00:40:45,846 说话人 SPEAKER_01：我认为我们应该尽一切努力让这条职业道路成为一条有回报的职业道路。
497 00:40:46,400 --> 00:40:51,737 说话人 SPEAKER_00：所以你会对这间屋子的年轻企业编码者说，这是上帝的工作。
498 00:40:52,400 --> 00:40:59,362 说话人 SPEAKER_00：从事安全工作，那将是一件好事，也许甚至比当水管工更好。
499 00:41:00,693 --> 00:41:04,157 说话人 SPEAKER_01：哦，是的，如果你能在安全方面取得进展，那将是非常惊人的，是的。
500 00:41:04,516 --> 00:41:06,639 说话人 SPEAKER_00: 好的，太棒了，我要和我的孩子们谈谈。
501 00:41:08,641 --> 00:41:10,603 说话人 SPEAKER_00: 我们来谈谈你想要的监管框架。
502 00:41:11,264 --> 00:41:17,590 说话人 SPEAKER_00: 你提到的一件事是，我相信你去了唐宁街 10 号，说英国应该有普遍基本收入。
503 00:41:18,012 --> 00:41:23,797 说话人 SPEAKER_00: 你能解释一下为什么吗，然后解释一下你在那里推荐的其他法规？
504 00:41:25,043 --> 00:41:34,072 说话人 SPEAKER_01：是的，我被邀请参加了唐宁街，那里有一大批 Sunax 顾问，他的首席助手以及许多其他为他提供 AI 建议的人。
505 00:41:34,793 --> 00:41:36,094 说话人 SPEAKER_01：我和他们谈了好一会儿。
506 00:41:36,976 --> 00:41:38,998 说话人 SPEAKER_01：那时，我并没有坐着。
507 00:41:39,099 --> 00:41:49,829 说话人 SPEAKER_01：所以我走进了这个房间，那里有一大群顾问，我和他们谈了好一会儿，包括说，我认为人工智能创造的就业机会不会像它所取代的那么多。
508 00:41:50,510 --> 00:41:54,054 说话人 SPEAKER_01: 因此他们需要像全民基本收入这样的东西。
509 00:41:55,570 --> 00:42:05,822 说话人 SPEAKER_01: 当会议结束时，我开始出门，意识到我正站在一张巨大的玛格丽特·撒切尔画像前，向人们解释他们应该有社会主义。
510 00:42:06,362 --> 00:42:08,804 说话人 SPEAKER_01: 我站在一张大玛格丽特·撒切尔的画像前，这相当有趣。
511 00:42:11,188 --> 00:42:11,407 说话人 SPEAKER_00: 好的。
512 00:42:11,967 --> 00:42:14,650 说话人 SPEAKER_00：所以是全民基本收入。
513 00:42:14,891 --> 00:42:20,498 说话人 SPEAKER_00：那么，在人工智能的世界里，杰弗里·辛顿的监管计划还包括了哪些内容？
514 00:42:20,478 --> 00:42:29,074 说话人 SPEAKER_01：我认为一个很直接的想法，萨姆·艾尔顿可能不会喜欢，就是应该投入相当的资源来确保安全。
515 00:42:30,077 --> 00:42:39,215 说话人 SPEAKER_01：如果你看看至少一个离开 OpenAI 的人的声明，因为他们认为 OpenAI 对安全的重视程度不够，那是因为资源问题。
516 00:42:41,581 --> 00:42:42,181 说话人 SPEAKER_01：我认为
517 00:42:42,802 --> 00:42:49,771 说话人 SPEAKER_01：如果政府能够的话，应该坚持投入更多资源用于安全。
518 00:42:50,152 --> 00:42:51,574 说话人 SPEAKER_01：这有点像石油公司。
519 00:42:51,614 --> 00:43:00,704 说话人 SPEAKER_01：你可以坚持让他们投入大量资源清理垃圾填埋场和清理他们排放的东西。
520 00:43:01,246 --> 00:43:02,266 说话人 SPEAKER_01：政府可以做到这一点。
521 00:43:02,286 --> 00:43:04,610 说话人 SPEAKER_01：如果政府做不到这一点，他们就会继续胡言乱语。
522 00:43:05,190 --> 00:43:12,539 说话人 SPEAKER_01：这显然是政府的作用，让资本主义在没有摧毁一切的情况下运转。
523 00:43:13,010 --> 00:43:14,110 说话人 SPEAKER_01：这正是他们应该做的事情。
524 00:43:15,873 --> 00:43:17,355 说话人 SPEAKER_00: 但有一个更简单的方法来做这件事，对吧？
525 00:43:17,394 --> 00:43:23,501 说话人 SPEAKER_00: 我的意思是，政府可以监管这些大公司，要求它们关注安全，我们需要审计并确保它们这样做。
526 00:43:24,362 --> 00:43:35,494 说话人 SPEAKER_00: 但政府也可以资助大量的安全研究，收集大量的政府数据，并将其提供给安全研究人员，资助大量的计算资源并分配给安全研究人员。
527 00:43:35,554 --> 00:43:42,983 说话人 SPEAKER_00: 那么这里的政府官员都应该建立人工智能吗？联合国应该建立一个人工智能安全研究所吗？
528 00:43:44,650 --> 00:43:50,365 说话人 SPEAKER_01：我认为联合国资金相当紧张，联合国不得不做一些像在加沙给人们提供食物的事情。
529 00:43:51,990 --> 00:43:53,835 说话人 SPEAKER_01：我宁愿它用这些钱在加沙给人们提供食物。
530 00:43:53,856 --> 00:43:55,942 说话人 SPEAKER_01：我认为联合国没有这些资源。
531 00:43:56,784 --> 00:44:00,094 说话人 SPEAKER_01：也许它应该有这些资源，但它没有。
532 00:44:00,074 --> 00:44:02,657 说话人 SPEAKER_01：我认为加拿大没有这样的资源。
533 00:44:02,737 --> 00:44:10,070 说话人 SPEAKER_01：加拿大已经做出了严肃的努力，为大学和初创企业投入资金用于计算。
534 00:44:10,710 --> 00:44:17,641 说话人 SPEAKER_01：所以他们最近投入了 200 亿美元，这在加拿大来说是一笔巨款。
535 00:44:17,661 --> 00:44:20,547 说话人 SPEAKER_01：但这与大型公司能做的事情相比，微不足道。
536 00:44:21,588 --> 00:44:27,077 说话者 SPEAKER_01：也许像沙特阿拉伯这样的国家可以投入相当的资金，但我并不确定他们是否对安全感兴趣。
537 00:44:27,445 --> 00:44:34,610 说话者 说话者_00：所以杰夫，我们只剩下一分钟了，尽管你给出了精彩、精彩、迅速的回答，我还有14个问题。
538 00:44:35,110 --> 00:44:36,797 说话者 SPEAKER_00：所以我将在最后问一个大问题。
从所有这些人工智能研究中，你一直在研究大脑是如何工作的。
540 00:44:42,797 --> 00:44:45,161 说话人 SPEAKER_00：你对为什么我们睡觉有令人难以置信的理论。
541 00:44:46,304 --> 00:44:49,429 说话人 SPEAKER_00：如果你有机会和 Hinton 博士交谈，我建议你问他那个问题。
542 00:44:50,512 --> 00:44:58,025 说话人 SPEAKER_00：在过去一年半的 AI 爆炸中，关于大脑我们学到了什么让你感到惊讶的？
543 00:44:59,844 --> 00:45:00,827 说话人 SPEAKER_01：哦，那让我感到惊讶。
544 00:45:01,507 --> 00:45:06,695 说话人 SPEAKER_01：我宁愿回顾几年前的经历，真正让我惊讶的是这些大型语言模型有多么出色。
545 00:45:07,577 --> 00:45:18,255 说话人 SPEAKER_01：我认为我在 1985 年创建了第一个使用反向传播来尝试预测一系列单词中下一个单词的语言模型。
546 00:45:18,735 --> 00:45:20,297 说话人 SPEAKER_01：这个序列只有三个单词长。
547 00:45:20,338 --> 00:45:26,527 说话人 SPEAKER_01：这是一个只有几千个权重的整个系统，但它是这种类型模型的第一例。
548 00:45:26,507 --> 00:45:35,324 说话人 SPEAKER_01：当时，我对它似乎能够统一两种关于词义的理论感到非常兴奋。
549 00:45:35,965 --> 00:45:38,572 说话人 SPEAKER_01：一种理论是它与其他词的关系有关。
550 00:45:39,052 --> 00:45:40,635 说话人 SPEAKER_01：这就是德·索绪尔的理论。
551 00:45:41,277 --> 00:45:45,445 说话人 SPEAKER_01：另一个来自心理学家的理论是，这是一个庞大的语义特征集合。
552 00:45:45,862 --> 00:45:57,967 说话人 SPEAKER_01：我们现在通过学习嵌入并让不同单词或词片段的嵌入特征之间进行交互，我们已经成功统一了这两种不同的意义理论。
553 00:45:58,427 --> 00:46:05,442 说话人 SPEAKER_01：我相信，我们现在拥有真正理解它们所说内容的巨大语言模型，几乎和人类一样。
554 00:46:05,422 --> 00:46:15,016 说话人 SPEAKER_01：所以我想说的最后一个观点是，这些语言模型的起源，使用反向传播来预测下一个单词，并不是为了创造一项好的技术。
555 00:46:15,318 --> 00:46:17,221 说话人 SPEAKER_01：这是为了尝试理解人们是如何做到的。
556 00:46:17,842 --> 00:46:24,873 说话人 SPEAKER_01：所以我认为人们理解语言的方式，我们最好的模型就是这些大型 AI 模型。
557 00:46:25,833 --> 00:46:29,659 说话人 SPEAKER_01：那些说“不”，他们实际上并不理解，这是胡说。
558 00:46:29,719 --> 00:46:32,103 说话人 SPEAKER_01：他们理解的方式和我们理解的方式一样。
559 00:46:33,130 --> 00:46:34,753 说话人 SPEAKER_00：好吧，嗯，我们得在这个话题上结束。
560 00:46:34,793 --> 00:46:42,574 说话人 SPEAKER_00：得知杰夫·辛顿那令人难以置信的智慧以某种方式支撑着我们今天使用的所有 AI 模型，这让我感到一丝欣慰。
561 00:46:42,594 --> 00:46:44,117 Speaker SPEAKER_00: 非常感谢，Hinton 博士。
562 00:46:44,358 --> 00:46:45,400 Speaker SPEAKER_00: 感谢您今天加入我们。
563 00:46:46,384 --> 00:46:46,784 Speaker SPEAKER_01: 谢谢。
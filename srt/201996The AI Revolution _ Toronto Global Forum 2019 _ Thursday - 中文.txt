1 00:00:05,278 --> 00:00:12,183 说话人 SPEAKER_00: 好吧，如果在这个房间里有人认为在接受这次采访之前，我有一丝一毫的畏惧。
2 00:00:12,721 --> 00:00:13,641 说话人 SPEAKER_00: 你们都对了。
3 00:00:14,683 --> 00:00:16,806 说话人 SPEAKER_00: 话虽如此，我们在这里将尝试进行一场精彩的讨论。
4 00:00:17,106 --> 00:00:22,794 说话人 SPEAKER_00: 我知道 25 分钟后，人们会离开这里，比进来时知道得多得多。
5 00:00:22,934 --> 00:00:23,675 说话人 说话人_00：让我们开始吧。
6 00:00:24,315 --> 00:00:32,326 说话人 说话人_00：我想这会很有帮助，鉴于你们两位和一位同事刚刚获得了图灵奖。
7 00:00:32,826 --> 00:00:39,234 说话人 说话人_00：我以为这会是因为你们在神经网络和深度学习方面的发现，杰夫，
8 00:00:39,383 --> 00:00:46,076 说话人 说话人_00：给观众一个关于深度学习究竟是什么以及神经网络是什么的好理解。
9 00:00:46,537 --> 00:00:55,216 说话人 SPEAKER_03：好吧，60 年前或者更早，在人工智能的初期，关于如何制造智能系统有两种想法。
10 00:00:55,898 --> 00:00:59,906 说话人 SPEAKER_03：有一种受逻辑启发的想法，即通过推理规则处理符号串。
11 00:01:00,409 --> 00:01:01,451 说话人 SPEAKER_03：使用推理规则。
12 00:01:01,551 --> 00:01:08,581 说话人 SPEAKER_03：还有一种受生物学启发的想法，即尝试模仿大脑细胞的大网络，并学习连接的强度。
13 00:01:09,043 --> 00:01:10,525 说话人 SPEAKER_03：这些是非常不同的范例。
14 00:01:11,186 --> 00:01:17,375 说话人 SPEAKER_03：在很长的一段时间里，基于试图模仿大脑的神经网络范例并没有很好地工作。
15 00:01:18,376 --> 00:01:19,438 说话人 SPEAKER_03：我们实际上并不清楚原因。
16 00:01:19,737 --> 00:01:26,468 说话人 SPEAKER_03：最终，我们发现它没有很好地工作，因为我们没有提供足够的数据，也没有足够的计算机能力。
17 00:01:26,784 --> 00:01:32,021 说话人 SPEAKER_03：从本世纪初开始，我们获得了越来越多的计算能力，越来越多的数据。
18 00:01:32,421 --> 00:01:39,242 说话人 SPEAKER_03：突然之间，系统学会了如何做事，而不是像以前那样需要编程。
19 00:01:39,509 --> 00:01:40,349 说话人 SPEAKER_03：变得有效。
20 00:01:40,709 --> 00:01:42,453 说话人 SPEAKER_03：这就是过去十年发生的事情。
21 00:01:42,873 --> 00:01:49,281 说话人 SPEAKER_03：我们已经看到它们在语音识别方面做得更好，在图像识别方面做得更好，在机器翻译方面也做得更好。
22 00:01:50,221 --> 00:02:00,834 说话人 SPEAKER_03：所有这些是通过使用由相互连接的模拟脑细胞组成的大网络，并修改连接强度来实现的，以便网络的行为更符合您的期望。
23 00:02:01,856 --> 00:02:08,604 说话人 SPEAKER_03：因此，要让它执行某项任务，您不需要编写一个程序来告诉它如何执行该特定任务。
24 00:02:08,719 --> 00:02:20,514 说话人 SPEAKER_03：您编写一个程序来告诉大网络如何学习，然后对于任何特定的任务，您只需给它一些输入数据，展示正确的输出数据，然后它就会找出如何改变所有连接强度。
25 00:02:20,533 --> 00:02:22,780 说话人 SPEAKER_03：所以如果你给它那些输入，它会给出正确的输出。
26 00:02:23,046 --> 00:02:26,532 说话人 SPEAKER_03：并且它对新例子的一般化效果很好。
27 00:02:27,114 --> 00:02:30,639 说话人 SPEAKER_03：所以如果你有一个大数据集并且想要预测任何东西，这就是你要走的路。
28 00:02:30,980 --> 00:02:36,509 说话人 SPEAKER_00：所以本质上，你是在模仿你认为我们大脑工作的方式。
29 00:02:36,530 --> 00:02:39,194 说话人 SPEAKER_03：在非常抽象的层面上，我们正在模拟大脑的工作方式。
30 00:02:39,414 --> 00:02:47,889 说话人 SPEAKER_03：所有细节都不同，但通过调整连接强度从例子中学习这一基本思想是大脑的工作方式。
31 00:02:47,870 --> 00:02:49,793 说话人 SPEAKER_03：请 Joshua，是你吗？
32 00:02:49,812 --> 00:03:03,510 说话人 SPEAKER_02：我只是想更深入地说明学习为什么如此重要，以及基于符号、规则和人类给出的事实进行经典人工智能方法为什么不起作用。
33 00:03:05,794 --> 00:03:10,781 说话人 SPEAKER_02：因为我们知道很多事情，但无法编程让计算机去做。
34 00:03:11,162 --> 00:03:13,305 说话人 SPEAKER_02：我们无法意识到这种知识。
35 00:03:13,944 --> 00:03:17,370 说话人 SPEAKER_02：我和你知道如何识别这是一杯水。
36 00:03:17,485 --> 00:03:21,469 说话人 SPEAKER_02：但我们无法告诉计算机如何完成这项工作。
37 00:03:21,490 --> 00:03:38,834 说话人 SPEAKER_02：结果我们发现，关于我们大脑的能力，有很多东西我们无法用简单的解释来剖析，就像我们无法向电脑解释一样，甚至无法向另一个人解释，因为我们无法获取那种知识。
38 00:03:38,854 --> 00:03:39,895 说话人 SPEAKER_02：它隐藏在我们的大脑中。
39 00:03:40,436 --> 00:03:46,884 说话人 SPEAKER_02：因此，计算机获得这种知识的方法是像孩子一样从数据中学习。
40 00:03:46,865 --> 00:03:48,586 说话人 SPEAKER_02：这就是为什么这如此重要的原因。
41 00:03:49,048 --> 00:03:53,573 说话人 SPEAKER_00: 这是最接近模拟我们的大脑，而不是逻辑方法。
42 00:03:54,054 --> 00:03:55,354 说话人 SPEAKER_00: 我们的大脑是随机工作的。
43 00:03:55,375 --> 00:04:04,686 说话人 SPEAKER_02: 这并不是在模拟我们的大脑，因为神经科学家会说大脑与我们正在做的事情非常不同，但它显然受到了我们对大脑了解的许多启发。
44 00:04:04,746 --> 00:04:09,632 说话人 SPEAKER_00: 你的背景是认知心理学吗？我对此很好奇，
45 00:04:09,611 --> 00:04:13,036 说话人 说话人_00：这种认知心理学与计算机科学的结合怎么样？
46 00:04:13,377 --> 00:04:20,745 说话人 说话人_00：是认知心理学背景让你有了这样的思考方式吗？
47 00:04:21,887 --> 00:04:25,411 说话人 说话人_03：实际上，我在心理学上并不顺利。
48 00:04:26,052 --> 00:04:26,473 说话人 说话人_03：你辞职了？
49 00:04:27,694 --> 00:04:32,661 说话人 SPEAKER_03：这种灵感来源于认为那些认知心理学家所说的完全是胡说八道，根本不会奏效。
50 00:04:34,163 --> 00:04:35,084 说话人 SPEAKER_03：好吧，好吧。
51 00:04:35,545 --> 00:04:37,307 说话人 SPEAKER_00：所以我们不得不想出其他办法。
52 00:04:37,877 --> 00:04:38,978 说话人 SPEAKER_00：来解决这个问题。
53 00:04:40,682 --> 00:04:51,154 说话人 说话人_00：你们俩在这个领域工作了这么多年，杰夫，你时间更长一些，因为你年纪大一点，有点像在荒野中，意思是你的方法没有被认真对待。
54 00:04:51,535 --> 00:04:52,697 说话人 说话人_00：是什么让你继续前进的？
55 00:04:52,757 --> 00:04:55,761 说话人 说话人_00：我认为科学家们的求知欲非常有趣。
56 00:04:56,201 --> 00:04:57,523 说话人 说话人_00：是什么让你继续前进的？
57 00:04:57,562 --> 00:05:07,074 说话人 说话人_00：当主流计算机科学家似乎在忽视你，甚至更糟的时候，是什么让你继续努力，达到突破？
58 00:05:07,848 --> 00:05:08,949 说话人 说话人_00：显然，我们是对的。
59 00:05:11,875 --> 00:05:13,096 说话人 说话人_00：但是需要一些东西，对吧？
60 00:05:13,396 --> 00:05:13,697 说话人 说话人_00：是这样的吗？
61 00:05:14,178 --> 00:05:14,418 说话人 SPEAKER_02: 嗯。
62 00:05:14,800 --> 00:05:16,101 说话人 SPEAKER_02: 我的意思是，除了脑力。
63 00:05:16,182 --> 00:05:24,136 说话人 SPEAKER_02: 我认为如果你想在研究上取得成功，你必须愿意做别人不愿意做的事情，因为研究是探索。
64 00:05:24,175 --> 00:05:25,156 说话人 SPEAKER_02: 是发现，对吧？
65 00:05:25,838 --> 00:05:29,865 说话人 SPEAKER_02：在你进行探索之前，很多人可能不会相信这一点。
66 00:05:30,666 --> 00:05:32,589 说话人 SPEAKER_02：所以你必须要有这一点。
67 00:05:32,872 --> 00:05:39,252 说话人 SPEAKER_02：在一定程度上有自信和愿意冒险去做这样的事情。
68 00:05:39,694 --> 00:05:40,997 说话人 SPEAKER_00：我认为这是一个很大的想法，对吧？
69 00:05:41,038 --> 00:05:43,947 说话人 SPEAKER_00：愿意做别人不愿意做的事情。
70 00:05:44,365 --> 00:05:47,588 说话人 SPEAKER_03：尤其是在看起来不切实际的时候。
71 00:05:47,687 --> 00:05:54,053 说话人 SPEAKER_03：所以正如 Yoshua 所说，传统人工智能领域的人们所做的是将事实输入到计算机中。
72 00:05:54,093 --> 00:06:04,283 说话人 SPEAKER_03：他们会观察世界，写下一些事实，用计算机能够编程的逻辑语言表达它们，然后将这些事实输入到计算机中。
73 00:06:04,302 --> 00:06:10,007 说话人 SPEAKER_03：另一种选择是计算机仅从数据中推导出所有这些知识，而这些知识根本不是显式的。
74 00:06:10,689 --> 00:06:14,372 说话人 SPEAKER_03：这似乎有些困难，做起来似乎很困难。
75 00:06:14,351 --> 00:06:24,867 说话人 SPEAKER_03：特别是，有人认为你可以得到一台有很多随机连接的计算机，并且它能学会做复杂的事情，比如机器翻译，这在几乎每个人看来都是完全不可能的。
76 00:06:25,307 --> 00:06:30,074 说话人 SPEAKER_02：让我再补充一点，以澄清这些术语。
77 00:06:30,355 --> 00:06:34,721 说话人 SPEAKER_02：所以人工智能是关于构建可能最终与我们一样聪明的机器。
78 00:06:36,163 --> 00:06:43,312 说话人 SPEAKER_02：而机器学习是实现人工智能的一种方法，我们希望计算机学会如何做事并理解世界。
79 00:06:43,562 --> 00:06:48,750 说话人 SPEAKER_02：深度学习和神经网络是受大脑启发的机器学习的一种特定形式。
80 00:06:48,930 --> 00:06:51,374 说话人 SPEAKER_02：好的，所以这些术语有时可能会让人感到困惑。
81 00:06:51,394 --> 00:06:51,694 说话人 说话人_00：一起。
82 00:06:51,814 --> 00:06:53,257 说话人 说话人_00：不，这非常有帮助。
83 00:06:55,720 --> 00:07:04,151 说话人 说话人_00：您认为目前最令人激动的应用深度学习的项目是什么？
84 00:07:04,833 --> 00:07:06,415 说话人 说话人_00：最大的项目有哪些？
85 00:07:07,711 --> 00:07:12,177 说话人 SPEAKER_03：这很难回答，因为应用领域非常广泛。
86 00:07:12,197 --> 00:07:18,509 说话人 SPEAKER_03：例如，为了拯救地球，我们需要提高太阳能电池板的效率。
87 00:07:19,410 --> 00:07:21,353 说话人 SPEAKER_03：为此，我们需要纳米技术。
88 00:07:22,314 --> 00:07:26,422 说话人 SPEAKER_03：深度学习现在被应用于预测材料的性质。
89 00:07:26,783 --> 00:07:28,665 说话人 SPEAKER_03：我认为这可能会产生重大影响。
90 00:07:29,086 --> 00:07:32,351 说话人 SPEAKER_03：如果你能让太阳能电池板提高 10%的效率，那将产生巨大的影响。
91 00:07:32,567 --> 00:07:36,016 说话人 SPEAKER_00：这将有助于改变该方案的可行性。
92 00:07:36,237 --> 00:07:43,314 说话人 SPEAKER_02：实际上，同样的技术可能被用来构建更好的碳捕集技术，更好的电池，尽管这还没有实现。
93 00:07:43,413 --> 00:07:48,947 说话人 SPEAKER_02：气候变化的应用是新的，但具有很大的潜力。
94 00:07:48,966 --> 00:07:51,031 说话人 SPEAKER_02：但这不仅仅是在材料方面，还包括
95 00:07:51,012 --> 00:08:00,567 说话人 SPEAKER_02：例如，提高电力使用效率和使用预测来更有效地利用可再生能源。
96 00:08:01,869 --> 00:08:06,716 说话人 SPEAKER_02：它在更好的气候模型中，因为预测未来非常困难，这只是在变化。
97 00:08:07,276 --> 00:08:08,238 说话人 SPEAKER_02：所以有很多方法。
98 00:08:08,319 --> 00:08:12,404 说话人 SPEAKER_02：但这些是我们更期待的方向。
99 00:08:12,745 --> 00:08:16,310 说话人 SPEAKER_02：目前，大多数深度学习应用
100 00:08:16,408 --> 00:08:25,769 说话人 SPEAKER_02：都是在使用它来改善与客户互动的公司中。
101 00:08:25,790 --> 00:08:26,711 说话人 SPEAKER_00：所以预测。
102 00:08:27,213 --> 00:08:34,570 说话人 SPEAKER_02：是的，比如，搜索引擎、推荐、广告。
103 00:08:34,549 --> 00:08:36,552 说话人 SPEAKER_02：提出客户需要的商品。
104 00:08:36,852 --> 00:08:42,522 说话人 SPEAKER_00：我想要在一分钟内回到那个话题，预测机器、广告以及提出客户想要的商品。
105 00:08:43,302 --> 00:08:47,688 说话人 说话人_00：让我来问问您关于无人驾驶汽车的事情，因为这是我们经常听说的一种应用。
106 00:08:48,629 --> 00:08:52,216 说话人 说话人_00：谁愿意来给我们介绍一下无人驾驶汽车的现状？
107 00:08:53,317 --> 00:08:55,419 说话人 说话人_03：我认为这是不可避免的。
108 00:08:56,120 --> 00:08:58,784 说话人 说话人_03：我认为当它们到来时，它们将挽救许多生命。
109 00:08:58,764 --> 00:09:07,696 说话人 SPEAKER_03：我认为可能存在一种转变，这只是我个人的看法，那就是我们如何看待交通。
110 00:09:07,756 --> 00:09:10,767 说话人 SPEAKER_03：比如，目前我们有汽车和火车。
111 00:09:10,982 --> 00:09:23,559 说话人 SPEAKER_03：那么，如果我们能拥有像火车这样的东西，它们可以随时到你想要的地方，去你想要的地方，但这些不是你拥有的东西，而是社会共有的东西，而且是高度协调的。
112 00:09:23,600 --> 00:09:25,081 说话人 SPEAKER_03：所以，你需要大量的中央协调。
113 00:09:25,582 --> 00:09:29,548 说话人 SPEAKER_03：所以你可以让很多车辆非常接近地以非常快的速度行驶而不会出现问题。
114 00:09:29,528 --> 00:09:34,672 说话人 SPEAKER_03：我认为整个交通方式可能会在更长的时间内发生转变。
115 00:09:34,712 --> 00:09:39,537 说话人 SPEAKER_00：所以拥有个人汽车的概念，这种模式将不复存在。
116 00:09:39,777 --> 00:09:43,419 说话人 SPEAKER_00：我认为这个概念将会消失，但需要时间。
117 00:09:43,539 --> 00:09:45,682 说话人 SPEAKER_02：顺便说一下，我们还没有到那里。
118 00:09:45,981 --> 00:09:51,626 说话人 SPEAKER_00：那么让我们谈谈自动驾驶汽车，即使是那些我们在谷歌园区看到鸣笛的汽车。
119 00:09:52,087 --> 00:09:55,169 说话人 SPEAKER_00：你想象这些汽车要多久才能商业化？
120 00:09:56,051 --> 00:09:59,433 说话人 SPEAKER_03：我认为是在几年到 50 年之间。
121 00:10:01,591 --> 00:10:04,014 说话人 SPEAKER_03：真的，它仍然是个未知数。
122 00:10:04,153 --> 00:10:06,557 说话人 SPEAKER_03：我认为预测未来非常困难。
123 00:10:06,857 --> 00:10:10,501 说话人 SPEAKER_03：你可以相当准确地预测未来几年，然后就像雾一样。
124 00:10:10,542 --> 00:10:15,467 说话人 SPEAKER_03：你可以看得相当清楚，突然你会撞到一堵墙，你不知道墙那边发生了什么。
125 00:10:15,869 --> 00:10:17,129 说话人 SPEAKER_03：我认为预测未来就像那样。
126 00:10:17,169 --> 00:10:21,174 说话人 SPEAKER_03：我认为在接下来的 10 年里，对于无人驾驶汽车，我相当有信心，我们周围会有很多。
127 00:10:21,195 --> 00:10:21,755 说话人 SPEAKER_00：我们会有的。
128 00:10:21,936 --> 00:10:25,000 说话人 SPEAKER_00：但我们不能确定。
129 00:10:24,980 --> 00:10:28,703 说话人 说话人_00：我很好奇，因为我以为那会先进得多。
130 00:10:28,745 --> 00:10:30,886 说话人 说话人_00：你好像是在说那马上就要发生了。
131 00:10:31,587 --> 00:10:32,970 说话人 说话人_00：是什么造成了这种模糊感？
132 00:10:33,149 --> 00:10:36,192 说话人 说话人_02：把它想象成一个 80-20 的问题。
133 00:10:37,014 --> 00:10:41,019 说话人 SPEAKER_02：在自动驾驶汽车方面取得进展最初非常快。
134 00:10:41,879 --> 00:10:52,572 说话人 SPEAKER_02：实际上，只要有点技术知识，几个月内就能搞出点能勉强工作的东西，只要驾驶情况简单。
135 00:10:52,552 --> 00:11:00,932 说话人 SPEAKER_02：但要让这些事物达到人类水平的安全和可靠性，还需要做更多的工作。
136 00:11:01,936 --> 00:11:03,458 说话人 SPEAKER_02：所以这真的很困难。
137 00:11:03,479 --> 00:11:07,548 说话人 说话人_00：所以这并不一定是我们所想的那样快。
138 00:11:07,750 --> 00:11:09,955 说话人 说话人_02：正在进行大量的投资。
139 00:11:09,934 --> 00:11:16,162 说话人 说话人_02：但也有许多不确定性，因为还有一些基本挑战需要解决。
140 00:11:16,221 --> 00:11:17,524 说话人 说话人_03：让我给你举一个例子。
141 00:11:17,943 --> 00:11:19,886 说话人 SPEAKER_03：现在的机器翻译已经相当不错了。
142 00:11:21,609 --> 00:11:24,272 说话人 SPEAKER_03：但还有一些事情我们离做到还相当遥远。
143 00:11:24,511 --> 00:11:26,735 说话人 SPEAKER_03：而且我们不知道何时能够做到。
144 00:11:26,754 --> 00:11:33,562 说话人 SPEAKER_03：例如，如果让我翻译成法语，奖杯会因为太小而放不进箱子。
145 00:11:34,318 --> 00:11:37,865 说话人 SPEAKER_03：你认为“它”指的是手提箱，因为它太小了。
146 00:11:38,385 --> 00:11:44,254 说话人 SPEAKER_03：但如果我说奖杯放不进手提箱，因为它太大，你认为“它”指的是奖杯，因为它太大。
147 00:11:44,755 --> 00:11:46,318 说话人 SPEAKER_03：而在法语中，它们的性别是不同的。
148 00:11:46,339 --> 00:11:48,601 说话人 SPEAKER_03：所以你必须知道“它”指的是哪一个才能翻译“它”。
149 00:11:49,322 --> 00:11:50,524 说话人 SPEAKER_03：谷歌翻译做不到这一点。
150 00:11:50,586 --> 00:11:51,606 说话人 SPEAKER_03：你试试谷歌翻译。
151 00:11:51,626 --> 00:11:52,227 说话人 SPEAKER_03：它不会翻译正确。
152 00:11:52,649 --> 00:11:54,572 说话人 SPEAKER_03：我的意思是，它有一半的时间能翻译正确。
153 00:11:54,552 --> 00:11:57,280 说话人 SPEAKER_03：这对机器翻译来说是可接受的。
154 00:11:57,562 --> 00:11:58,885 说话人 SPEAKER_03：偶尔犯错误是可以的。
155 00:11:59,528 --> 00:12:02,236 说话人 SPEAKER_03：但对于自动驾驶汽车来说则不可接受。
156 00:12:02,717 --> 00:12:08,054 说话人 SPEAKER_03：以及那种需要深入理解情况才能做出正确决策的情况。
157 00:12:08,389 --> 00:12:12,077 说话人 SPEAKER_03：有很多这样的不同情况，它们都很罕见，但你必须全部做对。
158 00:12:12,317 --> 00:12:23,100 说话人 SPEAKER_00：那么您想象，当自动驾驶汽车或车辆被接受时，假设它们的性能将是完美的吗？
159 00:12:23,120 --> 00:12:25,245 说话人 SPEAKER_00：不，我认为，我的意思是，公众会喜欢这样的。
160 00:12:25,264 --> 00:12:26,027 说话人 SPEAKER_02：比人类更好。
161 00:12:26,006 --> 00:12:27,448 说话人 SPEAKER_03：比人类更好。
162 00:12:27,509 --> 00:12:29,692 说话人 SPEAKER_03：但这并不足以让他们比人类更好。
163 00:12:29,812 --> 00:12:38,664 说话人 SPEAKER_03：也就是说，如果它们平均比人类更好，如果它们杀的人比人类少得多，但它们会犯人类不会犯的错误，公众会非常不满。
164 00:12:39,706 --> 00:12:49,019 说话人 SPEAKER_00：这本身可能是一个两小时的讨论，我想讨论很多问题，但它确实引发了许多问题，比如，如果发生了这种情况，如果有人被杀，谁负责？
165 00:12:49,480 --> 00:12:50,542 说话人 说话人_00：这是自动驾驶汽车吗？
166 00:12:51,062 --> 00:12:51,623 说话人 说话人_00：这些公司。
167 00:12:52,323 --> 00:12:53,466 说话人 说话人_00：这些公司。
168 00:12:53,850 --> 00:12:54,631 说话人 说话人_00：建造汽车。
169 00:12:55,113 --> 00:12:56,595 说话人 说话人_00：构建算法的公司。
170 00:12:57,095 --> 00:12:57,294 说话人 说话人_00：是的。
171 00:12:57,696 --> 00:13:01,640 说话人 说话人_00：不，我认为是开发算法的研究人员对抗公司。
172 00:13:01,660 --> 00:13:04,705 说话人 说话人_00：所以正如我所说的，这本身可能是一个两小时的讨论。
173 00:13:04,945 --> 00:13:21,647 说话人 SPEAKER_00：让我回到我们更熟悉的部分，那就是那些本质上在向我们销售东西或服务的公司使用深度学习，比如亚马逊、谷歌，或者那些在社交上吸引我们的公司，比如 Facebook 等等。
174 00:13:21,626 --> 00:13:29,996 说话人 SPEAKER_00：他们正在进行一场巨大的竞争，以变得越来越好，他们显然是在一场积累尽可能多数据的竞赛中。
175 00:13:30,857 --> 00:13:31,479 说话人 SPEAKER_02：以及研究人员。
176 00:13:31,799 --> 00:13:32,900 说话人 SPEAKER_00：以及研究人员，是的。
177 00:13:32,921 --> 00:13:33,662 说话人 说话人_00：所以有两部分。
178 00:13:33,721 --> 00:13:43,533 他们的在积累研究人员方面的主导地位，以及他们在积累数据方面的巨大主导地位。
179 00:13:43,868 --> 00:13:45,549 帮助我们理解这一切将走向何方。
180 00:13:45,649 --> 00:13:54,139 我想象，我们越想知道我们做什么，我们吃什么，我们住在哪里，我们和谁有交往，我们看什么，等等等等。
181 00:13:54,158 --> 00:14:04,549 说话人 说话人_00: 在什么时候，他们关于我们的数据如此之多，以至于他们影响我们的能力超过了我们影响自己的能力？
182 00:14:04,831 --> 00:14:13,419 说话人 说话人_00: 当这些预测机器变得越来越厉害时，我们应该关注人类能动性的更大问题吗？
183 00:14:13,518 --> 00:14:14,099 说话人 说话人_02: 当然。
184 00:14:15,941 --> 00:14:34,284 说话人 说话人_02: 我们不希望其他组织，无论这些机器是由特定个人、公司还是政府控制，对我们有过多的影响，从而基本操纵我们。
185 00:14:34,946 --> 00:14:36,788 说话人 SPEAKER_02：这在道德上是不可以接受的。
186 00:14:37,729 --> 00:14:42,816 说话人 SPEAKER_02：我认为政治家最终必须面对
187 00:14:43,066 --> 00:14:57,062 说话人 SPEAKER_02：关于我们将社会规范、法律和法规置于何处的决策，以明确在多大程度上，AI 可以利用关于我们每个人的大量数据来影响我们，什么是可以接受的，什么是不可以接受的。
188 00:14:58,764 --> 00:15:04,090 说话人 SPEAKER_02：就我个人而言，我会把标准定得很低，以确保我们保持我们的自主权。
189 00:15:04,110 --> 00:15:05,412 说话人 SPEAKER_02：如果没有法规，那么结果会怎样？
190 00:15:05,392 --> 00:15:22,184 说话人 SPEAKER_02：我的意思是，我认为我们不应该允许人工智能影响人们去做那些显然不是他们自己选择的事情，而是可能符合某些想要向你推销东西或其他目的的组织利益。
191 00:15:22,164 --> 00:15:28,552 说话人 SPEAKER_00：那么这需要一些隐私法或透明度法吗？
192 00:15:28,611 --> 00:15:32,677 说话人 SPEAKER_00：我将就此问题询问 Jeff，因为他对此有看法。
193 00:15:32,716 --> 00:15:38,624 说话人 SPEAKER_00: 但您认为它需要走向某种对隐私的监管吗？
194 00:15:38,644 --> 00:15:40,725 说话人 SPEAKER_02: 不仅仅是隐私问题。
195 00:15:40,765 --> 00:15:43,870 说话人 SPEAKER_02: 所以隐私是另一个问题，它是
196 00:15:44,355 --> 00:15:49,182 说话人 SPEAKER_02: 我们允许他人使用哪些数据，以及用于什么目的？
197 00:15:49,241 --> 00:16:01,355 说话人 SPEAKER_02：所以其中一部分原因是，我们可能不想让我们的数据被用来对付我们，本质上，无论是你的医疗数据还是你去哪里以及和谁在一起等等。
198 00:16:03,658 --> 00:16:10,566 说话人 SPEAKER_02：所以这与隐私有关，但更多的是关于我们对 AI 使用的道德可接受性所设定的限制。
199 00:16:11,591 --> 00:16:16,616 说话人 SPEAKER_03：我认为在医疗数据方面，有一个很大的权衡。
200 00:16:16,756 --> 00:16:30,870 说话人 SPEAKER_03：我不一定想让人们知道我的医疗状况太多，但我真的很想得到更好的预测和更好的治疗，这些预测和治疗是基于从许多人的医疗状况中学习的程序。
201 00:16:31,410 --> 00:16:40,299 说话人 SPEAKER_03：我必须做出权衡，那就是如果你们能利用大量数据做出预测，我就能得到更好的医疗服务。
202 00:16:40,279 --> 00:16:54,363 说话人 SPEAKER_03：区块链等技术将非常有帮助，这样我可以控制我的数据，并做出决定，比如，我想让你能准确预测我需要什么治疗，你可以用我的数据来学习，然后做出准确的预测。
203 00:16:54,623 --> 00:17:00,852 说话人 SPEAKER_03：如果我不喜欢这样，我可以说，好吧，我愿意接受较差的预测，而且你们不能看到我所有的个人数据。
204 00:17:00,832 --> 00:17:04,435 说话人 SPEAKER_03：我认为如果人们能够做出这样的权衡，那会非常好。
205 00:17:04,777 --> 00:17:06,397 说话人 SPEAKER_03：直到最近，这似乎还不可行。
206 00:17:06,458 --> 00:17:08,460 说话人 SPEAKER_03：看起来政治家不得不为所有人做出权衡。
207 00:17:08,799 --> 00:17:10,922 说话人 SPEAKER_00：但我认为有了区块链，你可以给人们更多的控制权。
208 00:17:11,522 --> 00:17:28,798 说话人 SPEAKER_00：但你会认为大多数人，如果你说，看，如果你分享你的数据，而且没有人知道是你个人，他们不能用它来对付你，也就是说，他们不能不雇佣你或给你贷款或和你出去约会或 whatever，如果你基于这个前提分享它，你是在相信这将会发生。
209 00:17:28,778 --> 00:17:34,984 说话人 SPEAKER_02：好吧，我们需要一个机制来确保这些愿望能够得到满足。
210 00:17:35,365 --> 00:17:47,880 说话人 SPEAKER_02：也许有一种可能，就是拥有中立的第三方，就像我们所说的数据信托，来捍卫我的需求。
211 00:17:48,480 --> 00:17:49,201 说话人 SPEAKER_02：我不是律师。
212 00:17:49,301 --> 00:17:55,308 说话人 SPEAKER_02：我无法理解关于我的数据如何被使用的数十页法律语言。
213 00:17:55,288 --> 00:18:06,480 说话人 SPEAKER_02：但一个理解我的需求和数百万人的需求的组织，可能有能力在我数据被用于某些目的时代表我签字。
214 00:18:06,759 --> 00:18:09,083 说话人 SPEAKER_00：所以用户之间的一些中间人。
215 00:18:09,143 --> 00:18:09,222 说话人 SPEAKER_00：是的。
216 00:18:10,423 --> 00:18:13,287 说话人 SPEAKER_00：我对你的观点有点着迷和感兴趣。
217 00:18:13,747 --> 00:18:17,111 说话人 说话人_00：我们西方世界确实有隐私的概念。
218 00:18:17,171 --> 00:18:18,192 说话人 说话人_00：我们认为我们有权利拥有它。
219 00:18:18,231 --> 00:18:18,531 说话人 说话人_00：我们应该有。
220 00:18:19,492 --> 00:18:21,914 说话人 说话人_00：中国没有这样的隐私问题。
221 00:18:22,236 --> 00:18:23,457 说话人 说话人_00：事实上，
222 00:18:23,436 --> 00:18:25,819 说话人 说话人_00：那里的人们似乎意识到他们没有隐私。
223 00:18:25,839 --> 00:18:27,301 说话人 说话人_00：政府知道他们所做的一切。
224 00:18:28,002 --> 00:18:43,884 说话人 说话人_00：如果我们试图理解适当的立法、监管水平，而中国没有，这难道不可能会给他们带来巨大的优势，在他们的科学家所做的工作以及他们能做的事情上超过我们吗？
225 00:18:44,163 --> 00:18:44,825 说话人 SPEAKER_02：这有什么问题吗？
226 00:18:45,285 --> 00:18:45,826 说话人 SPEAKER_02：我认为没有。
227 00:18:45,865 --> 00:18:48,869 说话人 SPEAKER_02：我认为这主要给我们控制他们人口提供了巨大的优势。
228 00:18:51,313 --> 00:18:53,375 说话人 SPEAKER_00：您不认为这给他们带来了优势吗？
229 00:18:53,778 --> 00:18:57,040 说话人 SPEAKER_00：因为他们有更多数据而正在推进他们的人工智能吗？
230 00:18:57,300 --> 00:18:57,501 说话人 SPEAKER_00：不。
231 00:18:58,522 --> 00:18:58,903 说话人 SPEAKER_00：没有顾虑？
232 00:18:58,923 --> 00:18:59,523 说话人 SPEAKER_00：你有顾虑吗？
233 00:19:01,285 --> 00:19:06,630 说话人 SPEAKER_03：我的主要担忧是，我有一个朋友刚从中国西部回来，他在那里认识了一些维吾尔人。
234 00:19:07,351 --> 00:19:08,833 说话人 SPEAKER_03：那里发生的事情非常可怕。
235 00:19:09,452 --> 00:19:10,954 说话人 SPEAKER_03：这取决于大规模监控。
236 00:19:13,376 --> 00:19:16,019 说话人 SPEAKER_03：而且让我担忧的是 AI 可以被这样使用。
237 00:19:16,480 --> 00:19:21,625 说话人 SPEAKER_03：我认为我们需要很多保护措施来阻止它在西方被这样使用。
238 00:19:22,144 --> 00:19:34,163 说话人 SPEAKER_02：但在当前的国际协调框架下，各国在处理内部事务方面拥有很大的自主权，这将很困难。
239 00:19:34,243 --> 00:19:45,621 说话人 SPEAKER_02：我认为最终这和其他问题，比如气候变化，都会引发一个问题：我们如何为地球制定全球规则，其中
240 00:19:47,490 --> 00:19:56,424 说话人 SPEAKER_02：人权、环境和财政公平或需要在全球层面解决的问题，如何能够得到实施。
241 00:19:56,484 --> 00:20:02,654 说话人 SPEAKER_02：我认为我们目前拥有的联合国等机构对于这一点来说是不够的。
242 00:20:03,636 --> 00:20:08,605 说话人 SPEAKER_00：确实感觉到了这种不对称性。
243 00:20:08,720 --> 00:20:24,348 说话人 SPEAKER_00：尽管你说在人工智能这个不断发展的世界中有很多事情要做，以及我们作为社会在结构、决策方面的能力，
244 00:20:24,328 --> 00:20:33,839 说话人 SPEAKER_00：整个美国还在纠结是否合法化堕胎，这是我们 40 年前认为已经解决的案件，他们仍在审理。
245 00:20:33,859 --> 00:20:36,544 说话人 说话人_00: 他们难以接受气候变化是真实存在的这一事实。
246 00:20:37,365 --> 00:20:40,288 说话人 说话人_00: 我们看到世界各地不同政府都在发生着什么。
247 00:20:40,909 --> 00:20:49,459 说话人 说话人_00: 你能就这种不对称性发表一点看法吗？即科学所处的位置和作为人类以及有组织的我们社会的位置之间的不对称性？
248 00:20:50,401 --> 00:20:53,905 说话人 说话人_03: 我认为社会正在倒退，对吧？
249 00:20:53,926 --> 00:20:54,467 说话人 SPEAKER_03：好的，很有趣。
250 00:20:54,487 --> 00:20:57,772 说话人 SPEAKER_03：技术正在向前发展，我们将大大提高生产力。
251 00:20:58,453 --> 00:21:10,873 说话人 SPEAKER_03：这种生产力的巨大提高是否会使大众受益，将很大程度上取决于政治决策，而且我们正在得到这些民粹主义政府，这非常令人担忧。
252 00:21:11,156 --> 00:21:14,324 说话人 SPEAKER_03：这些政府欺骗了大多数人。
253 00:21:15,546 --> 00:21:16,808 说话人 SPEAKER_04：我不喜欢它。
254 00:21:16,989 --> 00:21:22,480 说话人 SPEAKER_00：你不喜欢，我的意思是，正如你所说，我知道你说过很多次你是科学家，而不是社会政策制定者。
255 00:21:22,500 --> 00:21:31,178 说话人 SPEAKER_00：我认为你确实在考虑社会政策，但我觉得我们需要你以某种方式参与社会政策讨论。
256 00:21:31,238 --> 00:21:32,540 说话人 SPEAKER_02：整个，
257 00:21:33,314 --> 00:21:34,734 说话人 SPEAKER_02：社会需要参与。
258 00:21:35,496 --> 00:21:41,701 说话人 SPEAKER_02：在蒙特利尔，我们制定了这份关于人工智能负责任发展的蒙特利尔宣言。
259 00:21:42,481 --> 00:21:59,116 说话人 SPEAKER_02：我们当然在桌边邀请了人工智能科学家，还有社会科学家、政治科学家、法律人士、医学专家，以及来图书馆为我们提供反馈的普通市民，共同制定一套关于人工智能社会规范的原则。
260 00:21:59,557 --> 00:22:00,598 说话人 SPEAKER_02：我想回到你的问题。
261 00:22:01,799 --> 00:22:03,320 说话人 SPEAKER_02：我非常喜欢一句话。
262 00:22:03,300 --> 00:22:09,026 说话人 SPEAKER_02：这句话与讨论相关，即智慧竞赛。
263 00:22:09,046 --> 00:22:10,988 说话人 SPEAKER_02：所以有一种智慧竞赛。
264 00:22:11,228 --> 00:22:20,556 说话人 SPEAKER_02：现在有一种集体个人智慧与我们所引入世界的科技力量之间的竞赛。
265 00:22:21,636 --> 00:22:28,702 说话人 SPEAKER_02：想想看，如果把大枪给孩子们，可能会发生一些不好的事情。
266 00:22:28,903 --> 00:22:31,806 说话人 SPEAKER_02：如果你给他们大炸弹，可能会更糟。
267 00:22:32,528 --> 00:22:34,851 说话人 SPEAKER_02：AI 可能会被以很大的方式滥用。
268 00:22:34,871 --> 00:22:37,914 说话人 SPEAKER_02：它越强大，就越容易被滥用。
269 00:22:37,934 --> 00:22:38,936 说话人 SPEAKER_02：越来越少的人。
270 00:22:38,957 --> 00:22:40,858 说话人 SPEAKER_02：因此我们需要关注智慧的部分。
271 00:22:41,279 --> 00:22:46,066 说话人 SPEAKER_03：是的，我认为您需要尊重儿童的第二修正案权利。
272 00:22:46,727 --> 00:22:49,770 说话人 SPEAKER_00：我的意思是，这种智慧，这是一个美丽的词组。
273 00:22:49,891 --> 00:22:52,114 说话人 说话人_00：智慧，智慧权利？
274 00:22:52,355 --> 00:22:52,494 说话人 说话人_00：种族。
275 00:22:52,515 --> 00:22:53,496 说话人 说话人_00：种族，智慧之种族。
276 00:22:53,916 --> 00:22:56,460 说话人 说话人_00：这是一个美丽的短语。
277 00:22:56,440 --> 00:22:59,584 说话人 说话人_00：今天我要带回家的所有东西中，那将是最重要的。
278 00:22:59,604 --> 00:23:03,627 说话人 说话人_00：我知道今天在这里有很多国际客人，但我们是在加拿大。
279 00:23:04,388 --> 00:23:06,632 说话人 说话人_00：我们确实有你们两位加拿大人。
280 00:23:07,132 --> 00:23:19,246 说话人 说话人_00：我很想了解一下，我们加拿大能做些什么来继续您已经开始的这项美好领导。
281 00:23:19,266 --> 00:23:24,152 说话人 SPEAKER_00：在加拿大，我们能做哪些最重要的事情来巩固这一领先地位？
282 00:23:24,807 --> 00:23:28,171 说话人 SPEAKER_03：嗯，我个人认为最重要的事情就是重新选举特鲁多。
283 00:23:29,051 --> 00:23:29,633 说话人 SPEAKER_00：好的。
284 00:23:29,653 --> 00:23:31,555 说话人 SPEAKER_00：嗯，就这样了。
285 00:23:33,396 --> 00:23:33,797 说话人 说话人_00: 哦，好的。
286 00:23:33,856 --> 00:23:34,518 说话人 说话人_00: 你觉得呢？
287 00:23:35,779 --> 00:23:49,836 说话人 说话人_02: 此外，我认为加拿大在国际舞台上有着强大、中立、积极、和平的传统。
288 00:23:50,153 --> 00:24:00,567 说话人 说话人_02: 人工智能和其他问题，我认为，需要加拿大在桌面上有所作为，发挥领导作用，而不仅仅是跟随南边的邻国。
289 00:24:01,288 --> 00:24:02,871 说话人 说话人_00：那会很好。
290 00:24:03,412 --> 00:24:04,292 说话人 说话人_00：我们想这么做。
291 00:24:04,814 --> 00:24:10,321 说话人 说话人_00：你提到了选举和政治，所以我感觉我有权参与这个话题。
292 00:24:10,301 --> 00:24:20,715 说话人 说话人_00：现在有证据表明上一次选举被黑客攻击了，黑客攻击可以是一个我们可以深入讨论的话题，我很乐意和你讨论 AI 系统的黑客攻击风险。
293 00:24:21,277 --> 00:24:31,771 说话人 SPEAKER_00：有没有办法避免在任何选举中达到这种程度的干扰，但让我们谈谈美国选举，因为它对世界来说非常重要。
294 00:24:32,373 --> 00:24:36,659 说话人 SPEAKER_00：有没有办法避免在下次选举中达到这种程度的干扰？
295 00:24:37,179 --> 00:24:37,660 说话人 SPEAKER_03：是的。
296 00:24:38,382 --> 00:24:39,364 说话人 SPEAKER_03：那怎么办？
297 00:24:40,005 --> 00:24:52,829 说话人 SPEAKER_03：机器学习研究人员可以对此做出一些事情，比如剑桥分析公司是由一位名叫鲍勃·默瑟的机器学习研究人员和其他机器学习研究人员创建的
298 00:24:53,484 --> 00:24:55,227 说话人 SPEAKER_03：不喜欢剑桥分析公司所做的事情。
299 00:24:56,167 --> 00:24:58,971 说话人 SPEAKER_03：鲍勃·默瑟成为了他所在公司的耻辱。
300 00:25:00,252 --> 00:25:04,538 说话人 SPEAKER_03：他不得不辞去公司联合首席执行官的职务。
301 00:25:05,460 --> 00:25:16,894 讲者 SPEAKER_03：据我所知，在这个周期内，由于其他机器学习研究人员反应带来的尴尬，Bob Mercer 不会资助特朗普。
302 00:25:16,958 --> 00:25:23,468 讲者 SPEAKER_03：这只是我们能做的一件小事，但至少我认为这次我们把他排除在外了。
303 00:25:23,567 --> 00:25:24,288 讲者 SPEAKER_00：所以这是可行的。
304 00:25:24,630 --> 00:25:30,077 讲者 SPEAKER_00：我认为应该有一些像《纽约时报》那样的重磅文章来讨论我们能做什么。
305 00:25:30,598 --> 00:25:33,682 说话人 说话人_00：时间不多了，我就问您这个问题。
306 00:25:33,983 --> 00:25:35,605 说话人 说话人_00：我们离奇点还有多远？
307 00:25:37,910 --> 00:25:39,712 说话人 说话人_03：这是假设存在奇点。
308 00:25:39,732 --> 00:25:40,153 说话人 说话人_00：好的。
309 00:25:40,814 --> 00:25:43,258 说话人 说话人_00：那么这个词应该被丢弃吗？
310 00:25:44,299 --> 00:25:44,619 说话人 说话人_00：是的。
311 00:25:44,988 --> 00:25:50,582 说话人 说话人_03：我认为奇点肯定已经远远超出了雾气阻挡你看到任何事物的程度。
312 00:25:52,586 --> 00:25:52,906 说话人 说话人_00：好的。
313 00:25:53,367 --> 00:25:53,890 Speaker SPEAKER_00: 谢谢。
314 00:25:53,990 --> 00:25:56,095 Speaker SPEAKER_00: 能和你们在这里感到非常荣幸。

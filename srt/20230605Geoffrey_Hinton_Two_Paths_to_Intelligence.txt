1
00:00:04,232 --> 00:00:05,354
Speaker SPEAKER_12: Thank you for the introduction.

2
00:00:06,375 --> 00:00:09,217
Speaker SPEAKER_12: The title's different from the advertised one, but the content's the same.

3
00:00:09,977 --> 00:00:11,099
Speaker SPEAKER_12: There's two questions.

4
00:00:11,980 --> 00:00:13,420
Speaker SPEAKER_12: If I have my glasses, I can read them.

5
00:00:15,682 --> 00:00:20,027
Speaker SPEAKER_12: The first question is, will artificial intelligence soon be smarter than us?

6
00:00:21,207 --> 00:00:29,894
Speaker SPEAKER_12: And for a long time, for like 50 years, I was working on trying to make artificial neural nets in order to understand how the real brain might work.

7
00:00:30,335 --> 00:00:33,277
Speaker SPEAKER_12: And I always assumed that the real brain was better,

8
00:00:33,578 --> 00:00:35,119
Speaker SPEAKER_12: And the artificial neural nets were worse.

9
00:00:35,159 --> 00:00:36,862
Speaker SPEAKER_12: And if you made them more like the brain, they'd work better.

10
00:00:37,402 --> 00:00:39,164
Speaker SPEAKER_12: And a few months ago, I suddenly changed my mind.

11
00:00:40,185 --> 00:00:45,892
Speaker SPEAKER_12: And that's why I decided to leave Google and talk about the risks.

12
00:00:46,593 --> 00:00:50,517
Speaker SPEAKER_12: And the risks, obviously, will people be able to stay in control if you get super intelligent things?

13
00:00:51,177 --> 00:01:00,008
Speaker SPEAKER_12: Because there's no examples of a more intelligent thing being controlled by a less intelligent thing since Biden won the election.

14
00:01:02,265 --> 00:01:04,287
Speaker SPEAKER_12: I'm going to make a few speculations about that.

15
00:01:05,469 --> 00:01:11,096
Speaker SPEAKER_12: But the main content of the talk is just going to be about the reason why I think these things are getting more intelligent than us.

16
00:01:12,158 --> 00:01:13,138
Speaker SPEAKER_12: So the first question.

17
00:01:14,180 --> 00:01:15,701
Speaker SPEAKER_12: I don't know much about risk.

18
00:01:15,721 --> 00:01:18,185
Speaker SPEAKER_12: I talked to a whole bunch of risk people today who knew much more than me.

19
00:01:19,787 --> 00:01:20,548
Speaker SPEAKER_12: So I won't talk about that.

20
00:01:22,310 --> 00:01:28,897
Speaker SPEAKER_12: So in conventional computing, you use computers that are designed to follow instructions.

21
00:01:29,418 --> 00:01:34,742
Speaker SPEAKER_12: The fundamental property of a computer is that you can run the same program on different pieces of hardware.

22
00:01:36,024 --> 00:01:41,028
Speaker SPEAKER_12: And so the knowledge in the program is immortal in the sense that if a piece of hardware dies, the knowledge doesn't die.

23
00:01:41,688 --> 00:01:42,989
Speaker SPEAKER_12: You can run it on another piece of hardware.

24
00:01:45,212 --> 00:01:51,158
Speaker SPEAKER_12: To do that, you need to run transistors at very high power so they behave in a reliable digital way.

25
00:01:52,117 --> 00:01:59,424
Speaker SPEAKER_12: And you can't make use of the analog properties, the analog and highly variable properties of a particular piece of hardware.

26
00:01:59,775 --> 00:02:02,278
Speaker SPEAKER_12: Because then you wouldn't be able to do the same thing on a different piece of hardware.

27
00:02:05,302 --> 00:02:22,061
Speaker SPEAKER_12: So I got interested, because of the very high power used by these large language models, the big energy consumption, I got interested in whether there was a much cheaper way to do computation in terms of power that we could use now and we couldn't use previously.

28
00:02:23,483 --> 00:02:26,526
Speaker SPEAKER_12: And digital computers were designed to be programmed by people.

29
00:02:27,586 --> 00:02:29,729
Speaker SPEAKER_12: which is why they had to follow instructions precisely.

30
00:02:29,990 --> 00:02:35,919
Speaker SPEAKER_12: But now we know a different way to make general purpose computers do particular tasks, which is learn from examples.

31
00:02:37,341 --> 00:02:44,611
Speaker SPEAKER_12: And if they learn from examples, maybe we can get rid of this basic principle of computer science, which is you have to separate the software from the hardware.

32
00:02:45,513 --> 00:02:54,066
Speaker SPEAKER_12: Maybe we can use analog computers that make use of all these analog properties and aren't reproducible,

33
00:02:54,417 --> 00:02:57,780
Speaker SPEAKER_12: So you just can't transfer the knowledge to another computer.

34
00:02:58,381 --> 00:03:00,643
Speaker SPEAKER_12: But you can run at very low power, like 30 watts.

35
00:03:03,705 --> 00:03:08,229
Speaker SPEAKER_12: So obviously there's big advantages to separating software from hardware.

36
00:03:08,248 --> 00:03:11,793
Speaker SPEAKER_12: You can write one program and it'll run on all your iPhones.

37
00:03:12,592 --> 00:03:19,299
Speaker SPEAKER_12: It also means you can have a separate department where you don't know anything about electrical engineering and you call yourself computer science.

38
00:03:20,020 --> 00:03:23,242
Speaker SPEAKER_12: If it's all mixed up together, you can't do that.

39
00:03:26,193 --> 00:03:32,539
Speaker SPEAKER_12: And so when you don't separate the software from the hardware, you get what I call mortal computation.

40
00:03:34,322 --> 00:03:45,133
Speaker SPEAKER_12: That is, you're going to learn in the hardware, and what you learn is only going to be any good for that particular piece of hardware, because you're making use of all its weird analog properties.

41
00:03:47,695 --> 00:03:54,663
Speaker SPEAKER_12: Now, that's great, because we can use low power analog computation, and we can maybe grow hardware in 3D,

42
00:03:55,133 --> 00:04:00,941
Speaker SPEAKER_12: where we don't know the connectivity or the precise properties of the individual neurons if it's neural net-like hardware.

43
00:04:02,423 --> 00:04:10,914
Speaker SPEAKER_12: But we've obviously got one big problem, which is we're going to have to have a learning procedure that can learn in this hardware without knowing the exact properties of the hardware.

44
00:04:11,276 --> 00:04:12,858
Speaker SPEAKER_12: And I'll talk a bit about how you might do that.

45
00:04:13,699 --> 00:04:19,927
Speaker SPEAKER_12: It's probably not going to be backpropagation, because in backpropagation, you have a forward pass through layers or through time.

46
00:04:20,788 --> 00:04:24,735
Speaker SPEAKER_12: And you have to know the properties of the forward pass in order to

47
00:04:25,389 --> 00:04:27,112
Speaker SPEAKER_12: use backpropagation to get gradients.

48
00:04:27,533 --> 00:04:32,480
Speaker SPEAKER_12: If you don't actually know the properties of the hardware, you need a different kind of learning algorithm.

49
00:04:36,166 --> 00:04:40,072
Speaker SPEAKER_12: So advantages of mortal computation, you can use weight-level parallelism.

50
00:04:40,112 --> 00:04:42,255
Speaker SPEAKER_12: So you can have trillion-way parallelism.

51
00:04:43,036 --> 00:04:45,540
Speaker SPEAKER_12: And that means your computing elements don't need to be very fast.

52
00:04:46,122 --> 00:04:46,923
Speaker SPEAKER_12: That's what the brain does.

53
00:04:47,865 --> 00:04:49,247
Speaker SPEAKER_12: And we can use hugely less energy.

54
00:04:49,968 --> 00:04:53,153
Speaker SPEAKER_12: And we can grow the hardware.

55
00:04:53,976 --> 00:04:57,401
Speaker SPEAKER_12: It might be good not to start again from scratch.

56
00:04:58,062 --> 00:05:04,069
Speaker SPEAKER_12: I was guessing that we would end up actually re-engineering biological neurons as the hardware.

57
00:05:05,572 --> 00:05:07,394
Speaker SPEAKER_12: But as you'll see, we may not have time for that.

58
00:05:10,379 --> 00:05:18,088
Speaker SPEAKER_12: So I just want to give you one example of a computation that's obviously much more sensibly done analog than digital.

59
00:05:18,108 --> 00:05:22,514
Speaker SPEAKER_12: So suppose you want to multiply a vector of neural activities by a matrix of weights.

60
00:05:23,153 --> 00:05:24,595
Speaker SPEAKER_12: And that's what we want to do all the time.

61
00:05:24,634 --> 00:05:26,817
Speaker SPEAKER_12: That's the main computation we do.

62
00:05:28,180 --> 00:05:38,915
Speaker SPEAKER_12: So you could drive transistors at very high power and represent the activities as digital numbers, and then perform a whole bunch of operations on the bits to multiply them together.

63
00:05:40,416 --> 00:05:46,004
Speaker SPEAKER_12: Or you could make the neural activities just be voltages, and you can make the weights be conductances.

64
00:05:46,862 --> 00:05:49,867
Speaker SPEAKER_12: And there's physicists around, so I hope I got the units right.

65
00:05:50,408 --> 00:05:55,375
Speaker SPEAKER_12: If you multiply voltage by conductance, I think you get a charge per unit time.

66
00:05:56,317 --> 00:05:58,901
Speaker SPEAKER_12: And charges add themselves up.

67
00:05:59,942 --> 00:06:06,913
Speaker SPEAKER_12: So now you can multiply a vector by a matrix very simply without ever representing anything digitally.

68
00:06:07,836 --> 00:06:09,237
Speaker SPEAKER_12: And it's very efficient.

69
00:06:09,577 --> 00:06:11,300
Speaker SPEAKER_12: There are chips around that do that now.

70
00:06:11,786 --> 00:06:15,732
Speaker SPEAKER_12: The problem they have is then they try and digitize things afterwards before they do anything else with it.

71
00:06:16,814 --> 00:06:18,877
Speaker SPEAKER_12: But this particular computation can be done efficiently.

72
00:06:23,502 --> 00:06:27,048
Speaker SPEAKER_02: So, I'll get back to the learning problem in a minute.

73
00:06:27,127 --> 00:06:34,057
Speaker SPEAKER_12: The other big problem for mortal computation is that hardware dies and then everything it knows gets lost.

74
00:06:35,660 --> 00:06:40,646
Speaker SPEAKER_12: And so, the solution to that problem in mortal computation

75
00:06:41,317 --> 00:06:43,220
Speaker SPEAKER_12: There's a completely different solution in digital computation.

76
00:06:43,240 --> 00:06:51,009
Speaker SPEAKER_12: But in mortal computation, the solution is the teacher tries to distill the knowledge into a student.

77
00:06:51,591 --> 00:07:00,081
Speaker SPEAKER_12: And the way you do that is the teacher shows the student the correct responses to some input, and the student tries to mimic those responses.

78
00:07:00,961 --> 00:07:03,225
Speaker SPEAKER_12: And that's what was going on with Trump's tweets.

79
00:07:03,762 --> 00:07:08,149
Speaker SPEAKER_12: Left-wing people kept complaining that all the things he said were false.

80
00:07:08,769 --> 00:07:09,851
Speaker SPEAKER_12: That was completely irrelevant.

81
00:07:09,872 --> 00:07:10,673
Speaker SPEAKER_12: That wasn't the point.

82
00:07:11,274 --> 00:07:18,365
Speaker SPEAKER_12: The point was, you take a situation and you show people how to react to it, and your followers try and react like that.

83
00:07:19,266 --> 00:07:21,630
Speaker SPEAKER_12: It turns out that's much more effective than reasoning with people.

84
00:07:24,875 --> 00:07:27,339
Speaker SPEAKER_12: That's how distillation works for distilling prejudice.

85
00:07:30,475 --> 00:07:35,362
Speaker SPEAKER_12: So let's think about an agent that's classifying images into 1,024 categories.

86
00:07:36,725 --> 00:07:42,875
Speaker SPEAKER_12: If you just tell the agent the right answer, you're only giving them 10 bits of information.

87
00:07:43,877 --> 00:07:49,045
Speaker SPEAKER_12: So you're only constraining the weights by 10 bits when you say you should give this class for this input.

88
00:07:50,627 --> 00:07:52,630
Speaker SPEAKER_12: But suppose you already had a teacher who was trained.

89
00:07:53,833 --> 00:07:56,516
Speaker SPEAKER_12: The teacher will give probabilities for all the outputs.

90
00:07:57,862 --> 00:08:00,887
Speaker SPEAKER_12: And that's 1,023 real numbers, because they add up to one.

91
00:08:01,569 --> 00:08:11,223
Speaker SPEAKER_12: And as long as those probabilities aren't too small, if you train a student to mimic a teacher, each training example is far more valuable.

92
00:08:11,704 --> 00:08:17,372
Speaker SPEAKER_12: Because the student is trying not just to get the right answer to be the most probable thing, assuming the teacher got it right.

93
00:08:17,952 --> 00:08:22,098
Speaker SPEAKER_12: The student is trying to match the probabilities of the teacher for all the other training examples.

94
00:08:22,619 --> 00:08:30,415
Speaker SPEAKER_12: And actually, in many cases, there's much more information in the relative probabilities of wrong answers than there is in the right answer.

95
00:08:30,435 --> 00:08:38,994
Speaker SPEAKER_12: So for example, if I show you a BMW, the system will say, 0.8 is a BMW, and 0.1, it's an Audi.

96
00:08:39,855 --> 00:08:43,243
Speaker SPEAKER_12: And there's a one in a million chance that it's a garbage truck.

97
00:08:44,253 --> 00:08:45,875
Speaker SPEAKER_12: There's no German manufacturers here.

98
00:08:47,017 --> 00:08:48,499
Speaker SPEAKER_12: One in a million chance it's a garbage truck.

99
00:08:48,519 --> 00:08:52,946
Speaker SPEAKER_12: But the point is, it'll also say there's a one in a billion chance that it's a carrot.

100
00:08:54,828 --> 00:08:58,835
Speaker SPEAKER_12: And if you look at all the things that get a one in a million chance, they're other vehicles.

101
00:08:59,196 --> 00:09:04,764
Speaker SPEAKER_12: So it's telling you a lot about the classes of things by all the things that get low but not totally zero probabilities.

102
00:09:05,404 --> 00:09:07,248
Speaker SPEAKER_12: And vegetables is not.

103
00:09:07,447 --> 00:09:09,572
Speaker SPEAKER_12: And so they get very much smaller probabilities.

104
00:09:09,912 --> 00:09:13,197
Speaker SPEAKER_12: And that ratio between the garbage truck and the carrot is telling you a whole lot.

105
00:09:14,138 --> 00:09:21,148
Speaker SPEAKER_12: So training a student to mimic the probabilities of a teacher is good, especially if the probabilities aren't too small.

106
00:09:21,969 --> 00:09:25,774
Speaker SPEAKER_12: And we can make the probabilities not too small by using a high temperature.

107
00:09:26,655 --> 00:09:43,200
Speaker SPEAKER_12: So normally, if you have multiple alternative outputs, you use a softmax, which you say the probability of each answer is proportional to e to the logit for that answer, where the logit is the amount of evidence you've got for that answer.

108
00:09:44,176 --> 00:09:47,861
Speaker SPEAKER_12: If you just scale that by temperature, you get much softer probabilities.

109
00:09:49,484 --> 00:09:54,831
Speaker SPEAKER_12: And so I've got an example of that where it may surprise some of you to know I used the MNIST digits.

110
00:09:56,695 --> 00:10:02,363
Speaker SPEAKER_12: And I scaled, and I've got a good teacher, and then I scaled down the probabilities so they're much softer.

111
00:10:03,524 --> 00:10:05,148
Speaker SPEAKER_12: And if you look at this, these are all twos.

112
00:10:06,208 --> 00:10:13,440
Speaker SPEAKER_12: But if you look at that middle row, the teacher thought, yes, it's a two.

113
00:10:14,028 --> 00:10:16,711
Speaker SPEAKER_12: But it's just slightly like a 0.

114
00:10:16,792 --> 00:10:19,535
Speaker SPEAKER_12: It's much more like a 0 than any of the other 2s.

115
00:10:20,376 --> 00:10:27,307
Speaker SPEAKER_12: And so you can see on that one training example, you're teaching the system things about 0s as well, by saying this is a 2 that looks a bit like a 0.

116
00:10:27,327 --> 00:10:30,873
Speaker SPEAKER_12: Or if you look at the second row, that's a 2 that looks a bit like an 8.

117
00:10:32,014 --> 00:10:34,298
Speaker SPEAKER_12: And that's very useful extra information.

118
00:10:35,711 --> 00:10:41,118
Speaker SPEAKER_12: So the reason I'm talking about distillation quite a lot is because that's how we get knowledge between ourselves.

119
00:10:41,538 --> 00:10:49,447
Speaker SPEAKER_12: It's also how you get knowledge between one digital system and a different neural network that's got a completely different architecture.

120
00:10:49,907 --> 00:10:56,313
Speaker SPEAKER_12: So often you want to train a great big model and then get that knowledge into a smaller model and use distillation for that.

121
00:11:01,298 --> 00:11:04,942
Speaker SPEAKER_02: And one nice thing about distillation is when you're

122
00:11:05,327 --> 00:11:16,102
Speaker SPEAKER_12: training the student from a teacher and training the student to get the same probabilities for wrong answers as the teacher, you're actually training the student to generalize the same way as the teacher.

123
00:11:16,764 --> 00:11:20,789
Speaker SPEAKER_12: So it's one of the very rare cases where you're actually training the student on how to generalize.

124
00:11:21,490 --> 00:11:24,615
Speaker SPEAKER_12: Normally, you just train it to get the right answer and hope it'll get the right answer in other cases.

125
00:11:25,035 --> 00:11:26,738
Speaker SPEAKER_12: But here, we're actually training it to generalize.

126
00:11:30,384 --> 00:11:33,427
Speaker SPEAKER_12: And obviously, a single label isn't a very rich answer.

127
00:11:33,778 --> 00:11:42,119
Speaker SPEAKER_12: So you'd be better off training the student by giving it an image and then giving it a great long caption for the image and training it to predict all the words in the caption.

128
00:11:42,740 --> 00:11:45,447
Speaker SPEAKER_12: And that way you can get more information for a training case.

129
00:11:47,150 --> 00:11:48,514
Speaker SPEAKER_02: So that makes distillation work better.

130
00:11:53,049 --> 00:11:58,456
Speaker SPEAKER_12: So that's how distillation is, how you get information between two analog systems.

131
00:11:59,116 --> 00:12:00,658
Speaker SPEAKER_12: And that's what we were doing then.

132
00:12:01,139 --> 00:12:03,260
Speaker SPEAKER_12: You've got a very old analog system here.

133
00:12:03,881 --> 00:12:07,426
Speaker SPEAKER_12: And you've got a few old ones and a lot of young analog systems there.

134
00:12:07,905 --> 00:12:12,171
Speaker SPEAKER_12: And we're trying to get information from this old analog system into these young analog systems.

135
00:12:12,971 --> 00:12:17,736
Speaker SPEAKER_12: And the whole point of this talk is that process isn't very efficient, as I just demonstrated.

136
00:12:17,777 --> 00:12:21,740
Speaker SPEAKER_12: It's a slow and painful process.

137
00:12:22,177 --> 00:12:28,784
Speaker SPEAKER_12: We can't get, wouldn't it be great if I could just sort of go chunk and everything I knew went into your brain.

138
00:12:29,326 --> 00:12:30,126
Speaker SPEAKER_12: That would be so good.

139
00:12:31,427 --> 00:12:33,390
Speaker SPEAKER_12: Wouldn't be much good for the university, but it would be.

140
00:12:34,851 --> 00:12:39,778
Speaker SPEAKER_12: But that's what biological systems have to do.

141
00:12:41,740 --> 00:12:43,543
Speaker SPEAKER_12: Now let's talk a bit about the learning algorithm.

142
00:12:43,743 --> 00:12:52,077
Speaker SPEAKER_12: Because thinking about this was one of the things that made me change my mind about which is better, digital systems or biological systems.

143
00:12:53,379 --> 00:13:07,404
Speaker SPEAKER_12: So if you don't want to back propagate because you don't actually know what the hardware is doing, you can use a simple algorithm which has got an evolutionary flavor, which is you generate a small random perturbation to the weights.

144
00:13:08,160 --> 00:13:10,244
Speaker SPEAKER_12: And you have some global objective function.

145
00:13:10,745 --> 00:13:12,067
Speaker SPEAKER_12: You put some examples through.

146
00:13:12,106 --> 00:13:16,933
Speaker SPEAKER_12: And you see whether you do better or worse on this objective function after you've made the perturbation.

147
00:13:17,615 --> 00:13:21,841
Speaker SPEAKER_12: And then you take a step in the direction of perturbation that's proportional to how much better you did.

148
00:13:22,861 --> 00:13:26,868
Speaker SPEAKER_12: And the thing about that is, on average, it's going in the right direction.

149
00:13:27,168 --> 00:13:29,652
Speaker SPEAKER_12: If you do it enough times, you'll go in the same direction as backpropagation.

150
00:13:29,932 --> 00:13:31,094
Speaker SPEAKER_12: But it's got very high variance.

151
00:13:31,835 --> 00:13:33,577
Speaker SPEAKER_02: So it'll only work in very small systems.

152
00:13:41,894 --> 00:13:47,623
Speaker SPEAKER_12: Also, if you use the same weight perturbation for everything in a mini-batch of examples, you get even worse variants.

153
00:13:48,043 --> 00:13:52,769
Speaker SPEAKER_12: And if you use different weight perturbations, you can't use matrix-matrix multiplies, but that's just for insiders.

154
00:13:54,773 --> 00:14:01,263
Speaker SPEAKER_12: It's much better to use activity perturbation, where you perturb the input to a neuron, and you do the same thing.

155
00:14:01,322 --> 00:14:07,412
Speaker SPEAKER_12: We perturb the inputs to all the neurons, so they get the inputs they're getting from the rest of the net, plus this additional perturbation.

156
00:14:08,049 --> 00:14:15,158
Speaker SPEAKER_12: And then you see how much better you get as a result of that perturbation and take a step in that direction proportional to how much better you got.

157
00:14:15,217 --> 00:14:20,684
Speaker SPEAKER_12: And obviously, there's far fewer neurons than there are weights, so it's got much less variance.

158
00:14:21,105 --> 00:14:23,288
Speaker SPEAKER_12: And that's good enough to learn problems like MNIST.

159
00:14:24,168 --> 00:14:27,753
Speaker SPEAKER_12: It learns slower than backpropagation, but it learns at a reasonable speed.

160
00:14:30,436 --> 00:14:33,940
Speaker SPEAKER_12: The problem is, once you start trying to scale that to big nets,

161
00:14:34,190 --> 00:14:35,172
Speaker SPEAKER_12: It's just hopeless.

162
00:14:35,231 --> 00:14:36,533
Speaker SPEAKER_12: You can learn things like MNIST.

163
00:14:36,594 --> 00:14:39,178
Speaker SPEAKER_12: You can learn things like CIFAR-10 if you're very determined.

164
00:14:39,940 --> 00:14:44,727
Speaker SPEAKER_12: But ImageNet, where you have millions of images or a million images, it's just much too slow.

165
00:14:49,216 --> 00:14:58,711
Speaker SPEAKER_12: So one way around it that avoids doing global backpropagation is to say what we're going to do is we're going to actually have gazillions of local objective functions.

166
00:14:59,772 --> 00:15:03,956
Speaker SPEAKER_12: So the way we're going to scale things, you naturally think, I get a little neural net, it works.

167
00:15:05,018 --> 00:15:06,919
Speaker SPEAKER_12: I have a little neural net with this objective function.

168
00:15:06,980 --> 00:15:08,241
Speaker SPEAKER_12: I can train it, that's fine.

169
00:15:08,282 --> 00:15:10,203
Speaker SPEAKER_12: What if I want a big neural net?

170
00:15:10,244 --> 00:15:12,166
Speaker SPEAKER_12: You try the same training algorithm, it doesn't work.

171
00:15:13,187 --> 00:15:19,214
Speaker SPEAKER_12: But what if I had lots of little neural nets, each with its own objective function, and envision they could all be spatially local?

172
00:15:20,235 --> 00:15:26,402
Speaker SPEAKER_12: I give it its own objective function, so I'm never trying to learn lots of parameters all at once from one objective function.

173
00:15:27,563 --> 00:15:28,884
Speaker SPEAKER_12: And that works.

174
00:15:29,523 --> 00:15:35,312
Speaker SPEAKER_12: It doesn't work as well as backpropagation, but it does scale up much better to larger nets made of lots of local groups.

175
00:15:36,474 --> 00:15:38,998
Speaker SPEAKER_12: So the question is, where do you get these local objective functions?

176
00:15:41,020 --> 00:15:43,625
Speaker SPEAKER_12: And here's one possibility of revision.

177
00:15:45,489 --> 00:15:52,559
Speaker SPEAKER_12: You take patches of image, and you have a little net that extracts something, a vector that's going to represent what's going on in the patch of image.

178
00:15:53,552 --> 00:16:03,524
Speaker SPEAKER_12: And you say, I'd like to extract vectors that agree with the vectors extracted by other patches for this same image, but disagree with the vectors extracted for other images.

179
00:16:04,726 --> 00:16:07,149
Speaker SPEAKER_12: So this is called contrastive unsupervised learning.

180
00:16:07,890 --> 00:16:10,573
Speaker SPEAKER_12: And it turns out you can do that in multiple levels.

181
00:16:11,455 --> 00:16:17,962
Speaker SPEAKER_12: So you're doing learning on local patches, trying to get them to agree with other local patches at the same level, trying to get the outputs to agree.

182
00:16:19,284 --> 00:16:21,147
Speaker SPEAKER_12: You're also doing that at multiple levels.

183
00:16:22,222 --> 00:16:25,768
Speaker SPEAKER_12: And if you're very persistent, you can make that work moderately well.

184
00:16:26,168 --> 00:16:32,096
Speaker SPEAKER_12: It works better than all the other biologically plausible learning algorithms I know about.

185
00:16:32,817 --> 00:16:35,440
Speaker SPEAKER_12: It still doesn't work nearly as well as backpropagation.

186
00:16:36,562 --> 00:16:41,548
Speaker SPEAKER_12: And there's a paper about that in ICLR by Mengyi Ren, who did all the difficult work.

187
00:16:45,533 --> 00:16:51,201
Speaker SPEAKER_12: The description I gave of it is slightly false, so you could understand it easier.

188
00:16:51,282 --> 00:16:53,264
Speaker SPEAKER_12: But the paper's there, so I don't feel it's dishonest.

189
00:16:56,066 --> 00:16:58,268
Speaker SPEAKER_02: It took a lot of hard work, and it's a fairly dense paper.

190
00:17:03,653 --> 00:17:13,141
Speaker SPEAKER_12: OK, now let's go to the central issue of this talk is how agents share knowledge.

191
00:17:13,181 --> 00:17:15,363
Speaker SPEAKER_12: It's all about communication between different agents.

192
00:17:16,323 --> 00:17:20,547
Speaker SPEAKER_12: So what I'm doing now is trying to communicate knowledge to you, and I'm doing it

193
00:17:21,032 --> 00:17:21,973
Speaker SPEAKER_12: very inefficiently.

194
00:17:21,993 --> 00:17:24,896
Speaker SPEAKER_12: And the way I'm doing it is I'm producing strings of words.

195
00:17:25,817 --> 00:17:36,349
Speaker SPEAKER_12: In old-fashioned AI, what you would have done is said, what's going on is you're then going to clean up those words into a logically and ambiguous language, and you're going to put them in your brain.

196
00:17:36,990 --> 00:17:39,292
Speaker SPEAKER_12: And that's what teaching is.

197
00:17:41,015 --> 00:17:42,596
Speaker SPEAKER_12: That's actually wrong.

198
00:17:42,676 --> 00:17:43,718
Speaker SPEAKER_12: That's not how it works.

199
00:17:44,719 --> 00:17:46,862
Speaker SPEAKER_12: How it really works, I believe, is this.

200
00:17:47,561 --> 00:17:48,963
Speaker SPEAKER_12: I produce strings of words.

201
00:17:49,416 --> 00:17:54,642
Speaker SPEAKER_12: And you try and figure out how to change the connection strengths in your brain so that you would have said that.

202
00:17:55,742 --> 00:17:57,025
Speaker SPEAKER_12: And that's a very different process.

203
00:17:57,065 --> 00:17:57,845
Speaker SPEAKER_12: That's distillation.

204
00:17:57,865 --> 00:18:03,191
Speaker SPEAKER_12: That's a very different process from just storing strings of words or storing even cleaned up strings of words.

205
00:18:03,711 --> 00:18:10,037
Speaker SPEAKER_12: You're trying to figure out how to change trillions of weights in your brain so that that will be a reasonable thing to say.

206
00:18:10,076 --> 00:18:12,839
Speaker SPEAKER_02: That is, if you believe me.

207
00:18:16,983 --> 00:18:17,684
Speaker SPEAKER_02: So.

208
00:18:18,576 --> 00:18:20,298
Speaker SPEAKER_02: That's a difficult way to share knowledge.

209
00:18:21,138 --> 00:18:23,721
Speaker SPEAKER_02: And digital computers have a much better way to share knowledge.

210
00:18:27,164 --> 00:18:29,007
Speaker SPEAKER_12: So they can do weight or gradient sharing.

211
00:18:29,047 --> 00:18:33,671
Speaker SPEAKER_12: So suppose I have a big artificial neural net with a trillion connections.

212
00:18:35,593 --> 00:18:41,759
Speaker SPEAKER_12: If I have it on a digital computer, I can make exact copies of it on lots of different digital computers.

213
00:18:42,880 --> 00:18:46,743
Speaker SPEAKER_12: And each copy can go off and look at a different bit of the internet.

214
00:18:47,028 --> 00:18:50,432
Speaker SPEAKER_12: and figure out how it would change its weight so that it would have said that.

215
00:18:52,536 --> 00:19:01,229
Speaker SPEAKER_12: And then it can take that weight change that it would like to make, and it can talk to all the other computers, and they can all agree to average all their weight changes.

216
00:19:01,950 --> 00:19:07,237
Speaker SPEAKER_12: This is simplification, but if you know, yeah, it's a simplification, but it's basically that.

217
00:19:07,738 --> 00:19:15,869
Speaker SPEAKER_12: They average all their weight changes, and then everybody knows what each person, each digital computer learned.

218
00:19:16,474 --> 00:19:24,083
Speaker SPEAKER_12: Basically, it's what educators would love, which is, I can take what's in my brain and just plunk it in your brain.

219
00:19:24,683 --> 00:19:26,806
Speaker SPEAKER_12: The hell with all this trying to predict what I would say.

220
00:19:26,826 --> 00:19:29,388
Speaker SPEAKER_12: I just want to take what's in my brain and put it in your brain.

221
00:19:30,089 --> 00:19:32,172
Speaker SPEAKER_12: And these digital intelligences can do that.

222
00:19:32,913 --> 00:19:36,436
Speaker SPEAKER_12: And they do it by just, they all agree on the weight change.

223
00:19:36,696 --> 00:19:38,378
Speaker SPEAKER_12: And now they all know what all of them learned.

224
00:19:39,480 --> 00:19:41,942
Speaker SPEAKER_12: So imagine if we had 10,000 people.

225
00:19:43,163 --> 00:19:45,807
Speaker SPEAKER_12: And whenever one of us learned something, all of us knew it.

226
00:19:46,714 --> 00:19:49,798
Speaker SPEAKER_12: That will give you a tremendous advantage.

227
00:19:49,817 --> 00:19:52,020
Speaker SPEAKER_12: And that's the advantage that digital computation has.

228
00:19:52,820 --> 00:20:04,354
Speaker SPEAKER_12: In addition, it's got the advantage that it can use backpropagation, which is probably a better learning algorithm, because that can go through lots and lots of layers of neurons and compute exactly the correct gradient.

229
00:20:04,854 --> 00:20:09,519
Speaker SPEAKER_12: None of this guess the direction and see how well it works and hope you can average away the variance.

230
00:20:10,240 --> 00:20:12,182
Speaker SPEAKER_12: It's a better learning algorithm.

231
00:20:13,984 --> 00:20:22,836
Speaker SPEAKER_12: So all of this came as a tremendous relief to me, because I spent the last few years trying to come up with biologically plausible learning algorithms that work as well as backpropagation.

232
00:20:23,517 --> 00:20:25,819
Speaker SPEAKER_12: And I finally decided, maybe there aren't any.

233
00:20:25,839 --> 00:20:27,382
Speaker SPEAKER_12: Maybe backpropagation is actually better.

234
00:20:28,282 --> 00:20:30,065
Speaker SPEAKER_12: And that's what I think I now believe.

235
00:20:34,289 --> 00:20:38,174
Speaker SPEAKER_12: So I just want to remind you that you pay a tremendous cost to get this.

236
00:20:38,434 --> 00:20:41,558
Speaker SPEAKER_12: And the cost you pay is that they have to be digital computers.

237
00:20:41,578 --> 00:20:43,721
Speaker SPEAKER_12: They have to be fabricated precisely so they

238
00:20:44,173 --> 00:20:50,522
Speaker SPEAKER_12: do exactly what they're told to do at the level of the instructions, and it uses a lot of energy.

239
00:20:55,490 --> 00:21:01,318
Speaker SPEAKER_12: Distillation is what we use if you've got biological neural networks, or if you've got two digital networks that have completely different architectures.

240
00:21:04,323 --> 00:21:07,448
Speaker SPEAKER_12: But it's got a much lower bandwidth, so if you've got a

241
00:21:07,984 --> 00:21:12,548
Speaker SPEAKER_12: a digital model with a trillion connections, they all go off and look at some data, then they average their weights.

242
00:21:12,589 --> 00:21:17,755
Speaker SPEAKER_12: When they're averaging their weight changes, that's the trillions of bits that are being communicated.

243
00:21:18,756 --> 00:21:26,222
Speaker SPEAKER_12: When you try and predict a sentence that I say, when you try and change your weight so you predict the sentence, that's hundreds of bits at best.

244
00:21:27,344 --> 00:21:30,626
Speaker SPEAKER_12: If you're Carl, it's about two bits because he knew what I was going to say anyway.

245
00:21:34,049 --> 00:21:35,612
Speaker SPEAKER_02: So it's much lower bandwidth.

246
00:21:37,988 --> 00:21:39,288
Speaker SPEAKER_02: So here's the story so far.

247
00:21:41,270 --> 00:21:43,493
Speaker SPEAKER_12: There's two very distinct ways to do computation.

248
00:21:44,654 --> 00:21:50,520
Speaker SPEAKER_12: And the primary way in which they differ is in how you communicate knowledge between different agents.

249
00:21:51,541 --> 00:21:53,344
Speaker SPEAKER_12: So in digital computation, you use weight sharing.

250
00:21:54,684 --> 00:21:58,209
Speaker SPEAKER_12: And you've got this tremendous bandwidth for sharing what each agent learned.

251
00:21:58,990 --> 00:22:05,717
Speaker SPEAKER_12: In biological computation, you can use very low power if you make use of the analog properties of the hardware.

252
00:22:06,202 --> 00:22:09,086
Speaker SPEAKER_02: But now sharing knowledge is a slow and painful business.

253
00:22:13,633 --> 00:22:15,295
Speaker SPEAKER_02: So now let's look at large language models.

254
00:22:16,896 --> 00:22:19,760
Speaker SPEAKER_12: Because every AI talk now has to get to large language models eventually.

255
00:22:22,064 --> 00:22:22,805
Speaker SPEAKER_12: They're quite interesting.

256
00:22:23,165 --> 00:22:24,647
Speaker SPEAKER_12: They use digital computation.

257
00:22:25,087 --> 00:22:29,453
Speaker SPEAKER_12: So you have lots of different copies of the same set of weights running on different computers when it's learning.

258
00:22:30,174 --> 00:22:31,636
Speaker SPEAKER_12: And they all look at different bits of the internet.

259
00:22:32,308 --> 00:22:39,439
Speaker SPEAKER_12: And that allows them to see a huge amount of data and consolidate all that knowledge because they can share what they learn.

260
00:22:40,299 --> 00:22:47,770
Speaker SPEAKER_12: And so if you look at large language models, they have about a trillion weights, and they know probably a thousand times as much as any one of us.

261
00:22:48,050 --> 00:22:49,413
Speaker SPEAKER_12: They kind of know everything.

262
00:22:49,653 --> 00:22:54,119
Speaker SPEAKER_12: GPT-4 sort of knows every plausible thing, every reasonable thing it knows.

263
00:22:55,241 --> 00:23:01,911
Speaker SPEAKER_12: Now, we have a hundred trillion connections, so we've got a hundred times as many connections.

264
00:23:02,886 --> 00:23:06,451
Speaker SPEAKER_12: So we're really not using all that power.

265
00:23:07,453 --> 00:23:09,036
Speaker SPEAKER_12: But it's because we can't see enough data.

266
00:23:09,777 --> 00:23:12,661
Speaker SPEAKER_12: And if only we could get the knowledge from other people, maybe we could use it all.

267
00:23:13,281 --> 00:23:22,255
Speaker SPEAKER_12: But they've got 1,000 times more knowledge in 1% of the connections, which sort of confirms the argument they've got a better learning algorithm.

268
00:23:22,454 --> 00:23:31,627
Speaker SPEAKER_12: The combination of backpropagation with this easy communication between different agents with the same weights means they basically got a much better learning algorithm.

269
00:23:33,126 --> 00:23:37,696
Speaker SPEAKER_12: Now, it's presently being used to steal all our knowledge.

270
00:23:39,881 --> 00:23:43,509
Speaker SPEAKER_12: Sorry, steal is not the right word, particularly in the current political context.

271
00:23:43,528 --> 00:23:47,998
Speaker SPEAKER_12: It's to acquire our knowledge by using distillation.

272
00:23:48,398 --> 00:23:50,763
Speaker SPEAKER_12: So these digital agents that can share knowledge

273
00:23:51,419 --> 00:23:56,546
Speaker SPEAKER_12: Each agent, when it tries to get knowledge from the web, is using distillation to get the knowledge.

274
00:23:56,566 --> 00:24:03,015
Speaker SPEAKER_12: It's looking what people said and trying to change its way so it would have said the same thing, which isn't a very efficient way to get knowledge.

275
00:24:03,776 --> 00:24:08,542
Speaker SPEAKER_12: But there's lots of them, and we run them for a very long time.

276
00:24:09,364 --> 00:24:16,834
Speaker SPEAKER_12: And so it can basically learn everything people know in a few months on a lot of computers.

277
00:24:18,696 --> 00:24:21,240
Speaker SPEAKER_12: It's even using an inefficient form of distillation.

278
00:24:21,810 --> 00:24:27,876
Speaker SPEAKER_12: Because distillation is quite efficient if you look at the teacher's probabilities for a large bunch of alternative classes.

279
00:24:31,701 --> 00:24:39,750
Speaker SPEAKER_12: What the digital models are doing when they acquire knowledge from documents on the web is the document on the web is the teacher.

280
00:24:40,089 --> 00:24:44,214
Speaker SPEAKER_12: And they just look at the word that the person, the writer generated next.

281
00:24:44,535 --> 00:24:46,317
Speaker SPEAKER_12: They don't get to see the whole distribution.

282
00:24:46,718 --> 00:24:48,118
Speaker SPEAKER_12: They could learn much faster if they did.

283
00:24:48,619 --> 00:24:51,262
Speaker SPEAKER_12: But they just get to see a stochastic choice from that distribution.

284
00:24:51,462 --> 00:24:52,784
Speaker SPEAKER_02: But that's good enough so they can learn.

285
00:24:58,170 --> 00:25:07,940
Speaker SPEAKER_12: Now, if you had these large digital neural nets running on multiple different computers, and they got knowledge directly from the world, they could probably get knowledge much faster.

286
00:25:08,441 --> 00:25:15,627
Speaker SPEAKER_12: So if, for example, they were predicting the next frame in a video, if they wandered around with the camera on their head and tried to predict the next frame in a video,

287
00:25:15,894 --> 00:25:22,663
Speaker SPEAKER_12: or if they got a robot arm and try and predict what will happen when they move their arms around, they could probably learn much faster.

288
00:25:23,384 --> 00:25:31,153
Speaker SPEAKER_12: So the large language models are kind of, they're learning fairly abstract stuff, which is good, but they don't have much bandwidth because they're just learning from a low bandwidth string of words.

289
00:25:32,336 --> 00:25:37,962
Speaker SPEAKER_12: I suspect that these large models will get a lot better.

290
00:25:38,502 --> 00:25:40,885
Speaker SPEAKER_12: We know that they'll get a lot better if you make them multimodal.

291
00:25:41,287 --> 00:25:45,311
Speaker SPEAKER_12: So GPT-4 was trained with images as well as words.

292
00:25:45,915 --> 00:25:47,877
Speaker SPEAKER_12: It's possible that Google's doing the same thing.

293
00:25:53,163 --> 00:26:05,656
Speaker SPEAKER_12: So I think, particularly when they're multimodal, they could learn much, much more than us.

294
00:26:07,097 --> 00:26:09,560
Speaker SPEAKER_12: And if you play with GPT-4,

295
00:26:10,367 --> 00:26:14,213
Speaker SPEAKER_12: It's very hard not to believe that it's already fairly intelligent.

296
00:26:15,336 --> 00:26:23,166
Speaker SPEAKER_12: So there's people I respect a lot, like Yann LeCun, who think it doesn't really understand what it's saying.

297
00:26:24,048 --> 00:26:28,695
Speaker SPEAKER_12: But I don't understand how he can believe that, because you can give it little puzzles.

298
00:26:29,297 --> 00:26:38,130
Speaker SPEAKER_12: And if it doesn't really understand, if it's just a sort of stochastic parrot that's doing autocomplete, I don't see how it can solve puzzles of a form it's never seen before.

299
00:26:38,936 --> 00:26:41,118
Speaker SPEAKER_12: So I've got a friend who's in symbolic AI.

300
00:26:42,240 --> 00:26:46,724
Speaker SPEAKER_12: And he's called Hector Levesque.

301
00:26:47,105 --> 00:26:48,365
Speaker SPEAKER_12: And he's got a lot of integrity.

302
00:26:48,405 --> 00:26:50,367
Speaker SPEAKER_12: So he doesn't want to change the goalposts all the time.

303
00:26:51,348 --> 00:26:54,031
Speaker SPEAKER_12: He's now very surprised that they can do this.

304
00:26:54,112 --> 00:26:56,534
Speaker SPEAKER_12: And he admits he's very surprised that neural nets can do it.

305
00:26:56,773 --> 00:27:02,480
Speaker SPEAKER_12: And he can't understand how such a stupid method can deal with reasoning, little bits of reasoning.

306
00:27:03,421 --> 00:27:05,923
Speaker SPEAKER_12: So he asked me to give GPT-4 a problem.

307
00:27:06,358 --> 00:27:09,465
Speaker SPEAKER_12: And I made the problem more difficult because I knew it would be able to do his problem.

308
00:27:10,428 --> 00:27:17,001
Speaker SPEAKER_12: And I gave it the problem, the rooms in my house are painted white or blue or yellow.

309
00:27:19,387 --> 00:27:21,511
Speaker SPEAKER_12: Yellow paint fades to white within a year.

310
00:27:22,914 --> 00:27:25,000
Speaker SPEAKER_12: And in two years time, I'd like them all to be white.

311
00:27:25,259 --> 00:27:26,021
Speaker SPEAKER_12: What should I do?

312
00:27:27,519 --> 00:27:30,544
Speaker SPEAKER_12: And what you would say is you should paint the blue rooms white.

313
00:27:32,125 --> 00:27:40,134
Speaker SPEAKER_12: But if you're a mathematician, you might say you should paint the blue rooms yellow, because that reduces it to an already solved problem, because you know the yellow goes to white.

314
00:27:41,416 --> 00:27:43,819
Speaker SPEAKER_12: And GBT4 actually gave the mathematician a solution.

315
00:27:43,859 --> 00:27:45,080
Speaker SPEAKER_12: It said paint the blue rooms yellow.

316
00:27:46,643 --> 00:27:49,385
Speaker SPEAKER_12: But the point is, I don't see how he could have done that without understanding.

317
00:27:49,826 --> 00:27:54,152
Speaker SPEAKER_12: And there's all these other things where you tell it to write code to produce a diagram, and it produces a diagram.

318
00:27:54,757 --> 00:27:57,384
Speaker SPEAKER_12: So I don't understand how Jan can think they don't understand.

319
00:27:57,423 --> 00:28:02,217
Speaker SPEAKER_02: He's probably giving a lecture the other way around right now.

320
00:28:08,153 --> 00:28:08,714
Speaker SPEAKER_02: So.

321
00:28:09,843 --> 00:28:14,867
Speaker SPEAKER_12: This made me believe that these things can get more intelligent than us, and it might happen quite soon.

322
00:28:15,269 --> 00:28:19,752
Speaker SPEAKER_12: I always believed it was like 50 to 100 years, or 30 to 100 years, or 30 to 50 years.

323
00:28:20,193 --> 00:28:21,915
Speaker SPEAKER_12: I think I said different things at different times.

324
00:28:23,356 --> 00:28:25,239
Speaker SPEAKER_12: But now I believe it's like 5 to 20.

325
00:28:25,740 --> 00:28:27,161
Speaker SPEAKER_12: I think it's going to happen fairly soon.

326
00:28:27,741 --> 00:28:33,768
Speaker SPEAKER_12: And if it's going to happen in five years' time, we can't just leave it to philosophers to decide what to do about it.

327
00:28:34,388 --> 00:28:38,532
Speaker SPEAKER_12: It's time we actually got some practical experience

328
00:28:39,086 --> 00:28:49,316
Speaker SPEAKER_12: So what I believe is, well, let me finish this slide, that people are going to not be able to resist giving these things goals.

329
00:28:49,636 --> 00:28:51,218
Speaker SPEAKER_12: Obviously, you want to do things, you give them goals.

330
00:28:52,118 --> 00:28:56,182
Speaker SPEAKER_12: And if you want to be good at achieving goals, you give them the ability to create sub-goals.

331
00:28:57,884 --> 00:29:08,554
Speaker SPEAKER_12: And as soon as you have the ability to create a sub-goal, if you're intelligent, you'll realize that a very good sub-goal is to get more control, because that helps you achieve all your other goals.

332
00:29:09,412 --> 00:29:12,596
Speaker SPEAKER_12: So I'll give you an example where you can see yourself doing it.

333
00:29:13,277 --> 00:29:14,919
Speaker SPEAKER_12: You're sitting in a very boring seminar.

334
00:29:15,099 --> 00:29:16,060
Speaker SPEAKER_12: Not this one.

335
00:29:16,642 --> 00:29:17,623
Speaker SPEAKER_12: A very boring seminar.

336
00:29:18,203 --> 00:29:20,226
Speaker SPEAKER_12: And you see a little patch of light on the ceiling.

337
00:29:20,866 --> 00:29:21,969
Speaker SPEAKER_12: And you kind of go, what's that?

338
00:29:22,569 --> 00:29:24,531
Speaker SPEAKER_12: And you listen to the boring seminar for a bit.

339
00:29:24,551 --> 00:29:26,914
Speaker SPEAKER_12: And then you notice that when you move, the light moves.

340
00:29:27,576 --> 00:29:30,019
Speaker SPEAKER_12: And then you realize it's the reflection of the sun off your watch.

341
00:29:30,881 --> 00:29:32,061
Speaker SPEAKER_12: And so what do you do next?

342
00:29:32,583 --> 00:29:34,105
Speaker SPEAKER_12: Do you say, OK, I've solved that problem.

343
00:29:34,125 --> 00:29:34,905
Speaker SPEAKER_12: I know what that is now.

344
00:29:35,205 --> 00:29:36,448
Speaker SPEAKER_12: I'm going to go and listen to the seminar.

345
00:29:36,868 --> 00:29:37,950
Speaker SPEAKER_12: No, that's not what you do.

346
00:29:38,538 --> 00:29:40,279
Speaker SPEAKER_12: Well, if you do that, you're not a real scientist.

347
00:29:41,461 --> 00:29:45,345
Speaker SPEAKER_12: What you do next is you go, oh, I wonder if I, how do I make it move that way?

348
00:29:45,365 --> 00:29:46,586
Speaker SPEAKER_12: And how do I make it move this way?

349
00:29:46,906 --> 00:29:49,990
Speaker SPEAKER_12: And you try and figure out how you rotate your wrist to make it move in different directions.

350
00:29:50,490 --> 00:29:52,313
Speaker SPEAKER_12: And once you've done that, you go back and listen to the seminar.

351
00:29:53,693 --> 00:29:57,818
Speaker SPEAKER_12: We have this very sensible and strong urge to get control of things.

352
00:29:58,259 --> 00:30:06,147
Speaker SPEAKER_12: Because obviously, if you can control things, then on some future occasion, when you need to make the spotlight move, you'll know how to do it.

353
00:30:06,922 --> 00:30:12,411
Speaker SPEAKER_12: And I can't see how we're going to prevent a superintelligence wanting to get control of things.

354
00:30:13,932 --> 00:30:17,097
Speaker SPEAKER_12: And then it's sort of tricky.

355
00:30:17,939 --> 00:30:22,125
Speaker SPEAKER_12: You might imagine you could air gap it so it can't actually press red buttons or pull big levers.

356
00:30:22,946 --> 00:30:28,695
Speaker SPEAKER_12: But if it can output text, then it can manipulate people.

357
00:30:28,715 --> 00:30:33,604
Speaker SPEAKER_12: So it turns out if you want to invade a building in Washington, all you need to be able to do is output text.

358
00:30:34,141 --> 00:30:38,207
Speaker SPEAKER_12: And you can persuade gullible people that they're saving democracy by invading this building.

359
00:30:39,347 --> 00:30:42,633
Speaker SPEAKER_12: And this thing is going to be much smarter than us.

360
00:30:43,314 --> 00:30:49,162
Speaker SPEAKER_12: So as long as we're reading what it says, it's sort of Medusa, you need to hide your eyes from it.

361
00:30:50,282 --> 00:30:53,728
Speaker SPEAKER_12: As long as you're reading what it says, it's going to be able to manipulate you.

362
00:30:54,608 --> 00:30:57,834
Speaker SPEAKER_12: So this makes me very depressed.

363
00:30:58,535 --> 00:31:03,961
Speaker SPEAKER_12: I wish I had an easy solution to this and I don't.

364
00:31:04,532 --> 00:31:22,480
Speaker SPEAKER_12: So when I sort of changed my mind about how soon these things are going to be super intelligent, and actually how much better digital intelligence is than biological intelligence, I'd always thought it was the other way around, I decided I ought to at least sort of shout fire.

365
00:31:22,981 --> 00:31:28,109
Speaker SPEAKER_12: I don't know what to do about it or which way to run, but we need to worry about this seriously.

366
00:31:28,951 --> 00:31:32,717
Speaker SPEAKER_12: And there's lots of people who've been thinking about these risks for much longer than me.

367
00:31:33,355 --> 00:31:34,696
Speaker SPEAKER_12: and have various proposals.

368
00:31:36,099 --> 00:31:39,065
Speaker SPEAKER_12: I haven't seen a really plausible one for how we can keep it under control yet.

369
00:31:39,145 --> 00:31:52,630
Speaker SPEAKER_12: But my best bet is that the companies that are developing this should be forced to put a lot of work into checking out the safety of it as they're developing it and before it's smarter than us.

370
00:31:53,166 --> 00:31:57,771
Speaker SPEAKER_12: So they should be putting a similar amount of work into seeing how it tries to get out of your control.

371
00:31:58,752 --> 00:32:09,144
Speaker SPEAKER_12: Because anybody who's programmed a computer knows that just theorizing and thinking about things is not very good compared with actually trying things out.

372
00:32:09,163 --> 00:32:12,268
Speaker SPEAKER_12: When you try things out, they just behave like you didn't expect.

373
00:32:13,107 --> 00:32:15,971
Speaker SPEAKER_12: And things you thought were big problems turn out not to be problems.

374
00:32:16,632 --> 00:32:19,275
Speaker SPEAKER_12: So like for many, many years,

375
00:32:19,626 --> 00:32:24,055
Speaker SPEAKER_12: Many, many people didn't investigate neural networks because they were going to get stuck in local minima.

376
00:32:25,157 --> 00:32:27,481
Speaker SPEAKER_12: It turned out they never actually checked if that was true.

377
00:32:27,541 --> 00:32:28,805
Speaker SPEAKER_12: They just assumed it was true.

378
00:32:29,346 --> 00:32:29,866
Speaker SPEAKER_12: And it's not.

379
00:32:30,607 --> 00:32:34,414
Speaker SPEAKER_12: And even if it was, there'd be good local minima, so it doesn't matter.

380
00:32:35,136 --> 00:32:36,318
Speaker SPEAKER_12: But it's not actually true.

381
00:32:37,160 --> 00:32:42,450
Speaker SPEAKER_12: So we need to get practical experience with these things and how they try and escape and how you might control them.

382
00:32:43,132 --> 00:32:51,607
Speaker SPEAKER_12: And I'd have much more belief in someone telling me how to keep them under control if they had a little one and they could keep it under control, rather than if they were just theorizing.

383
00:32:55,556 --> 00:32:56,096
Speaker SPEAKER_02: So...

384
00:32:57,493 --> 00:32:57,835
Speaker SPEAKER_02: Yes.

385
00:32:59,096 --> 00:33:14,085
Speaker SPEAKER_12: If we cease to be the apex of intelligence, they'll need us for a bit, because we're very low power, so we can run computations very cheaply, the sort of intellectual equivalent of digging ditches, and we can keep the power stations running.

386
00:33:14,825 --> 00:33:17,887
Speaker SPEAKER_12: But they can probably design better computers than us.

387
00:33:18,087 --> 00:33:22,310
Speaker SPEAKER_12: They can certainly sort of take neurons and re-engineer them genetically and make better things than us.

388
00:33:23,813 --> 00:33:28,557
Speaker SPEAKER_12: So my conclusion is maybe we're just a passing stage in the evolution of intelligence.

389
00:33:29,617 --> 00:33:32,380
Speaker SPEAKER_12: And actually, maybe that's good for all the other species.

390
00:33:35,482 --> 00:33:40,586
Speaker SPEAKER_12: I think if we can keep them under control, they could be of tremendous value.

391
00:33:40,626 --> 00:33:44,190
Speaker SPEAKER_12: The reason people are going to keep developing this stuff, even despite all the risks,

392
00:33:44,692 --> 00:33:46,234
Speaker SPEAKER_12: is because it can do tremendous good.

393
00:33:46,815 --> 00:33:55,064
Speaker SPEAKER_12: Like in medicine, wouldn't you like to go and see a general practitioner who'd seen 100 million patients, including thousands with your rare condition?

394
00:33:55,324 --> 00:33:56,484
Speaker SPEAKER_12: It would just be so much better.

395
00:33:58,567 --> 00:34:05,555
Speaker SPEAKER_12: Wouldn't you like to be able to take a CAT scan and extract from it hugely more information than any doctor knew could be extracted from it?

396
00:34:08,717 --> 00:34:13,063
Speaker SPEAKER_12: So I've got to the end, and I managed to get there fast enough

397
00:34:13,431 --> 00:34:16,295
Speaker SPEAKER_12: So I can talk about some really flaky stuff.

398
00:34:16,315 --> 00:34:16,576
Speaker SPEAKER_12: OK.

399
00:34:17,697 --> 00:34:22,704
Speaker SPEAKER_12: So this was the serious stuff, OK?

400
00:34:24,326 --> 00:34:26,409
Speaker SPEAKER_12: You need to worry about these things getting control.

401
00:34:27,030 --> 00:34:32,838
Speaker SPEAKER_12: And if you're young and you want to do research on neural networks, see if you can figure out a way to be sure they won't get control.

402
00:34:34,179 --> 00:34:36,842
Speaker SPEAKER_12: Now, many people believe that.

403
00:34:39,186 --> 00:34:39,385
Speaker SPEAKER_12: Yes.

404
00:34:40,186 --> 00:34:42,190
Speaker SPEAKER_12: Many people believe that.

405
00:34:42,608 --> 00:34:53,867
Speaker SPEAKER_12: There's one reason why we don't have to worry, and that reason is that these things don't have subjective experience, or consciousness, or sentience, or whatever you want to call it.

406
00:34:54,628 --> 00:34:56,210
Speaker SPEAKER_12: These things are just dumb computers.

407
00:34:56,391 --> 00:35:04,465
Speaker SPEAKER_12: They can manipulate symbols and they can do things, but they don't actually have real experience, so they're not like us.

408
00:35:06,840 --> 00:35:16,518
Speaker SPEAKER_12: Now, I was strongly advised that if you've got a good reputation, you can say one crazy thing and you can get away with it and people will actually listen.

409
00:35:17,639 --> 00:35:20,465
Speaker SPEAKER_12: So I'm relying on that fact for you to listen so far.

410
00:35:21,346 --> 00:35:25,032
Speaker SPEAKER_12: But if you say two crazy things, people just say he's crazy and they won't listen.

411
00:35:25,974 --> 00:35:27,938
Speaker SPEAKER_02: So I'm not expecting you to listen to the next bit.

412
00:35:33,065 --> 00:35:42,097
Speaker SPEAKER_12: So people definitely have a tendency to think they're special, like we were made in the image of God, so of course he put us at the center of the universe.

413
00:35:44,561 --> 00:35:53,954
Speaker SPEAKER_12: And many people think there's still something special about people that a digital computer couldn't possibly have, which is we have subjective experience.

414
00:35:55,556 --> 00:35:58,701
Speaker SPEAKER_12: And they think that's one of the reasons we don't need to worry.

415
00:35:59,474 --> 00:36:01,936
Speaker SPEAKER_12: And I wasn't sure whether many people actually think that.

416
00:36:02,438 --> 00:36:06,402
Speaker SPEAKER_12: So I asked chat GPG for what people think, and it told me that's what they think.

417
00:36:10,208 --> 00:36:11,509
Speaker SPEAKER_12: It's actually good.

418
00:36:11,528 --> 00:36:12,590
Speaker SPEAKER_12: I mean, this is good.

419
00:36:13,311 --> 00:36:15,894
Speaker SPEAKER_12: This is probably an N of 100 million, right?

420
00:36:17,175 --> 00:36:19,338
Speaker SPEAKER_12: And I just had to say, what do people think?

421
00:36:22,204 --> 00:36:25,690
Speaker SPEAKER_12: So I'm going to now try and undermine the sentience defense.

422
00:36:26,713 --> 00:36:32,804
Speaker SPEAKER_12: I don't think there's anything special about people, except they're very complicated, and they're wonderful, and they're very interesting to other people.

423
00:36:37,534 --> 00:36:44,786
Speaker SPEAKER_12: So if you're a philosopher, you can classify me as in the sort of Dandenic camp.

424
00:36:48,697 --> 00:36:55,186
Speaker SPEAKER_12: I think people have completely misunderstood what the mind is and what consciousness, what subjective experience is.

425
00:36:56,047 --> 00:37:08,322
Speaker SPEAKER_12: So let's suppose that I just took a lot of LSD and now I'm seeing little pink elephants.

426
00:37:10,005 --> 00:37:13,608
Speaker SPEAKER_12: And I want to tell you what's going on in my perceptual system.

427
00:37:14,668 --> 00:37:19,056
Speaker SPEAKER_12: So I would say something like, I've got the subjective experience of a little pink elephant floating in front of me.

428
00:37:19,958 --> 00:37:21,481
Speaker SPEAKER_12: And let's unpack what that means.

429
00:37:22,744 --> 00:37:25,969
Speaker SPEAKER_12: What I'm doing is I'm trying to tell you what's going on in my perceptual system.

430
00:37:27,952 --> 00:37:34,625
Speaker SPEAKER_12: And the way I'm doing it is not by telling you neuron 52 is highly active, because that wouldn't do you any good.

431
00:37:34,644 --> 00:37:37,150
Speaker SPEAKER_12: And actually, I don't even know that.

432
00:37:38,057 --> 00:37:47,333
Speaker SPEAKER_12: But we have this idea that there's things out there in the world, and there's normal perception, so that things out there in the world give rise to percepts in a normal kind of a way.

433
00:37:47,373 --> 00:37:55,525
Speaker SPEAKER_12: And now I've got this percept, and I can tell you what would have to be out there in the world for this to be the result of normal perception.

434
00:37:56,628 --> 00:38:02,918
Speaker SPEAKER_12: And what would have to be out there in the world for this to be the result of normal perception is little pink elephants floating around.

435
00:38:04,027 --> 00:38:14,623
Speaker SPEAKER_12: And so when I say I have the subjective experience of little pink elephants, it's not that there's an inner theater with little pink elephants in it made of funny stuff called qualia.

436
00:38:15,423 --> 00:38:16,266
Speaker SPEAKER_12: It's not like that at all.

437
00:38:16,286 --> 00:38:17,306
Speaker SPEAKER_12: That's completely wrong.

438
00:38:18,329 --> 00:38:23,635
Speaker SPEAKER_12: I'm trying to tell you about my perceptual system via the idea of normal perception.

439
00:38:24,536 --> 00:38:31,166
Speaker SPEAKER_12: And I'm saying, what's going on here would be normal perception if there were little pink elephants.

440
00:38:31,670 --> 00:38:36,596
Speaker SPEAKER_12: But the little pink elephants, what's funny about them is not that they're made of qualia and they're in an inner world.

441
00:38:36,976 --> 00:38:40,340
Speaker SPEAKER_12: What's funny about them is they're counterfactual.

442
00:38:40,360 --> 00:38:41,541
Speaker SPEAKER_12: But they're in the real world.

443
00:38:42,382 --> 00:38:45,847
Speaker SPEAKER_12: Or rather, they're not in the real world, but they're the kinds of things that could be.

444
00:38:46,989 --> 00:38:50,612
Speaker SPEAKER_12: So they're not made of spooky stuff in an inner theater.

445
00:38:51,393 --> 00:38:54,077
Speaker SPEAKER_12: They're made of counterfactual stuff in a perfectly normal world.

446
00:38:55,059 --> 00:38:59,905
Speaker SPEAKER_12: And that's what I think is going on when people talk about subjective experience.

447
00:39:01,505 --> 00:39:08,074
Speaker SPEAKER_12: So in that sense, I think these models can have subjective experience.

448
00:39:08,115 --> 00:39:10,159
Speaker SPEAKER_12: So let's suppose we make a multimodal model.

449
00:39:11,340 --> 00:39:12,682
Speaker SPEAKER_12: It's like GPT-4.

450
00:39:12,983 --> 00:39:14,204
Speaker SPEAKER_12: It's got a camera, let's say.

451
00:39:15,387 --> 00:39:22,458
Speaker SPEAKER_12: And when it's not looking, I don't know how you do that, but when it's not looking, you put a prism in front of the camera.

452
00:39:22,958 --> 00:39:24,240
Speaker SPEAKER_12: But it doesn't know about the prism.

453
00:39:26,264 --> 00:39:28,507
Speaker SPEAKER_12: And now you put an object in front of it.

454
00:39:29,230 --> 00:39:31,492
Speaker SPEAKER_12: And you say, where's the object?

455
00:39:32,373 --> 00:39:33,536
Speaker SPEAKER_12: And it says, the object's there.

456
00:39:33,976 --> 00:39:34,818
Speaker SPEAKER_12: Let's suppose it can point.

457
00:39:34,938 --> 00:39:35,739
Speaker SPEAKER_12: It says, the object's there.

458
00:39:36,719 --> 00:39:37,621
Speaker SPEAKER_12: And you say, you're wrong.

459
00:39:37,681 --> 00:39:41,947
Speaker SPEAKER_12: And it says, well, I got the subjective experience of the objects there.

460
00:39:41,967 --> 00:39:42,728
Speaker SPEAKER_12: And you say, that's right.

461
00:39:42,748 --> 00:39:46,974
Speaker SPEAKER_12: You got the subjective experience of the objects there, but it's actually there because I put a prism in front of your lens.

462
00:39:48,315 --> 00:39:51,840
Speaker SPEAKER_12: And I think that's the same use of subjective experiences we use for people.

463
00:39:53,143 --> 00:39:57,128
Speaker SPEAKER_12: I've got one more example to convince you there's nothing special about people.

464
00:39:57,967 --> 00:40:09,530
Speaker SPEAKER_12: Suppose I'm talking to a chatbot, and I suddenly realize that the chatbot thinks that I'm a teenage girl.

465
00:40:10,893 --> 00:40:17,766
Speaker SPEAKER_12: There's various clues to that, like the chatbot's telling me about somebody called Beyonce, who I've never heard of.

466
00:40:19,804 --> 00:40:21,447
Speaker SPEAKER_12: all sorts of other stuff about makeup.

467
00:40:23,210 --> 00:40:24,150
Speaker SPEAKER_12: Sorry, I didn't say that.

468
00:40:26,333 --> 00:40:27,215
Speaker SPEAKER_02: You have to be very careful.

469
00:40:29,438 --> 00:40:37,309
Speaker SPEAKER_12: So I could ask the chatbot, what demographics do you think I am?

470
00:40:37,771 --> 00:40:39,612
Speaker SPEAKER_12: And it'll say, you're a teenage girl.

471
00:40:40,875 --> 00:40:42,757
Speaker SPEAKER_12: That'll be more evidence it thinks I'm a teenage girl.

472
00:40:43,038 --> 00:40:49,407
Speaker SPEAKER_12: I can look back over the conversation and see how it misinterpreted something I said, and that's why it thought I was a teenage girl.

473
00:40:50,027 --> 00:41:03,572
Speaker SPEAKER_12: And my claim is, when I say the chatbot thought I was a teenage girl, that use of the word thought is exactly the same as the use of the word thought when I say you thought I should maybe have stopped the lecture before I got into the really freaky stuff.

474
00:41:05,777 --> 00:41:08,061
Speaker SPEAKER_12: OK, that's all I really wanted to say.

475
00:41:09,545 --> 00:41:10,786
Speaker SPEAKER_13: We're going to try.

476
00:41:11,172 --> 00:41:12,393
Speaker SPEAKER_13: something a little bit complicated.

477
00:41:12,432 --> 00:41:17,018
Speaker SPEAKER_13: We have a full room in the overflow room as well, and we'd like them to have a chance to ask questions.

478
00:41:17,039 --> 00:41:26,630
Speaker SPEAKER_13: So we're going to be switching, asking questions in this room, and then taking questions online on my phone from the other room.

479
00:41:27,230 --> 00:41:28,351
Speaker SPEAKER_13: Let's hope this works.

480
00:41:28,711 --> 00:41:30,333
Speaker SPEAKER_13: But we are going to start with this room.

481
00:41:30,393 --> 00:41:33,538
Speaker SPEAKER_13: So could we see some hands for questions?

482
00:41:34,117 --> 00:41:35,119
Speaker SPEAKER_09: I'm Christian Maffidan.

483
00:41:36,059 --> 00:41:37,322
Speaker SPEAKER_09: I have a confession to make.

484
00:41:37,943 --> 00:41:40,925
Speaker SPEAKER_09: We used AI to produce a book.

485
00:41:41,378 --> 00:41:43,762
Speaker SPEAKER_09: on Princess Diana over two years ago.

486
00:41:43,782 --> 00:41:53,317
Speaker SPEAKER_09: And we'll put a label in front of the book saying it was generated by AI, Fred Intelligence, and we only edited it.

487
00:41:53,336 --> 00:41:58,023
Speaker SPEAKER_09: Yet it still went on to be bestseller in Barnes and Nobles and all the other stores.

488
00:41:59,065 --> 00:42:08,601
Speaker SPEAKER_09: The ethical issue is really, did we really write the book or did we give an inspiration

489
00:42:09,778 --> 00:42:11,721
Speaker SPEAKER_09: to Fred.ai to write the book.

490
00:42:13,242 --> 00:42:16,668
Speaker SPEAKER_09: I think that's the biggest headache we have in terms of looking.

491
00:42:16,688 --> 00:42:20,552
Speaker SPEAKER_09: So because of that, I signed the letter to say AI should be paused.

492
00:42:20,952 --> 00:42:22,394
Speaker SPEAKER_09: The research should be paused.

493
00:42:22,414 --> 00:42:23,195
Speaker SPEAKER_12: Thank you.

494
00:42:23,215 --> 00:42:24,818
Speaker SPEAKER_12: OK, I have several comments on that.

495
00:42:26,800 --> 00:42:29,583
Speaker SPEAKER_12: First, I didn't sign the letter because I think there's no hope of that happening.

496
00:42:30,405 --> 00:42:35,132
Speaker SPEAKER_12: Retrospectively, I think it was a good letter because it drew political attention, even though there was no hope of it happening.

497
00:42:35,152 --> 00:42:37,795
Speaker SPEAKER_12: So it was a sensible thing to do.

498
00:42:37,943 --> 00:42:39,905
Speaker SPEAKER_12: but I don't think there's any hope of people pausing AI.

499
00:42:40,186 --> 00:42:41,547
Speaker SPEAKER_12: Maybe they should, but I don't think they will.

500
00:42:43,710 --> 00:42:48,297
Speaker SPEAKER_12: On the ethical issue, there's a lot of problems there.

501
00:42:48,338 --> 00:43:00,576
Speaker SPEAKER_12: You might say that the whole of humanity wrote the book, because a chatbot trains on what the whole of humanity said, and then from that, it produces more stuff.

502
00:43:01,163 --> 00:43:05,347
Speaker SPEAKER_12: I don't really have much to say about those ethical issues.

503
00:43:05,887 --> 00:43:19,059
Speaker SPEAKER_12: I really want to focus on this existential risk of these things getting smarter than us and taking over because there's lots of people have done much more work on the ethical issues and I don't have anything of any interest to say about them I'm afraid.

504
00:43:20,900 --> 00:43:28,047
Speaker SPEAKER_13: Can we take one more question from this room and then I'll switch to the other room and then back again.

505
00:43:29,041 --> 00:43:31,043
Speaker SPEAKER_02: Against the wall there.

506
00:43:34,786 --> 00:43:35,407
Speaker SPEAKER_11: Herbie Bradley.

507
00:43:36,148 --> 00:43:43,336
Speaker SPEAKER_11: Now, I was wondering how you see the trade-off between open source and closed development of increasingly more capable AI systems.

508
00:43:43,958 --> 00:43:51,346
Speaker SPEAKER_11: Obviously, open development has the benefit that lots more people are looking at the system and figuring out its flaws, but maybe there are too many risks.

509
00:43:51,425 --> 00:43:51,987
Speaker SPEAKER_11: What are your thoughts?

510
00:43:53,068 --> 00:43:53,929
Speaker SPEAKER_12: Right.

511
00:43:54,510 --> 00:43:57,612
Speaker SPEAKER_12: How do you feel about open source development in nuclear weapons?

512
00:44:01,086 --> 00:44:04,112
Speaker SPEAKER_12: So that's the danger of open source, right?

513
00:44:04,132 --> 00:44:06,335
Speaker SPEAKER_12: There's more crazy out there to do crazy things with it.

514
00:44:07,115 --> 00:44:10,400
Speaker SPEAKER_12: Also, I don't actually know the answer to this, and I should.

515
00:44:11,021 --> 00:44:18,994
Speaker SPEAKER_12: I believe that you still need, like, at least tens of millions of dollars to train one of these big chatbots.

516
00:44:19,315 --> 00:44:23,601
Speaker SPEAKER_12: And the open source stuff is just modifying it, having got the chatbot.

517
00:44:23,822 --> 00:44:26,166
Speaker SPEAKER_12: I don't think you can open source train from scratch, can you?

518
00:44:28,677 --> 00:44:30,679
Speaker SPEAKER_11: Right, OK.

519
00:44:31,340 --> 00:44:38,427
Speaker SPEAKER_12: So if these things are going to be dangerous, it might actually be better for a few big companies.

520
00:44:38,588 --> 00:44:41,291
Speaker SPEAKER_12: I don't work at Google anymore, so I'm not saying this on Google's behalf.

521
00:44:41,952 --> 00:44:53,164
Speaker SPEAKER_12: But it might work out better for a few big companies, preferably in several different countries, to develop this stuff and at the same time be developing ways of keeping it under control.

522
00:44:53,885 --> 00:44:55,527
Speaker SPEAKER_12: As soon as you open source everything,

523
00:44:55,795 --> 00:44:58,278
Speaker SPEAKER_12: People will do all sorts of crazy things with it.

524
00:44:59,139 --> 00:45:02,224
Speaker SPEAKER_12: It'll be a very quick way to learn how it can go wrong.

525
00:45:06,130 --> 00:45:07,793
Speaker SPEAKER_13: All right, question from the overflow room.

526
00:45:08,554 --> 00:45:13,880
Speaker SPEAKER_13: Given your views on the sentience defense, do you think there's a major worry about artificial suffering?

527
00:45:14,561 --> 00:45:18,527
Speaker SPEAKER_13: Many people are concerned about the impacts that AI could take on taking control of humans.

528
00:45:18,989 --> 00:45:21,632
Speaker SPEAKER_13: But should we be worried about the harms that humans could do to AI?

529
00:45:23,516 --> 00:45:24,536
Speaker SPEAKER_12: OK.

530
00:45:25,072 --> 00:45:30,605
Speaker SPEAKER_12: Sort of the worst suffering people have is, are they getting this in the other one?

531
00:45:31,686 --> 00:45:32,148
Speaker SPEAKER_12: Good.

532
00:45:32,168 --> 00:45:33,911
Speaker SPEAKER_12: The worst suffering people have is pain.

533
00:45:33,972 --> 00:45:37,398
Speaker SPEAKER_12: And these things don't have pain, at least not yet.

534
00:45:38,121 --> 00:45:41,648
Speaker SPEAKER_12: So we don't have to worry about physical pain.

535
00:45:42,724 --> 00:45:47,112
Speaker SPEAKER_12: that I imagine they can get frustrated, and we have to worry about things like frustration.

536
00:45:48,094 --> 00:45:52,682
Speaker SPEAKER_12: And this is getting in, this is just new territory, right?

537
00:45:52,804 --> 00:45:55,447
Speaker SPEAKER_12: I don't, I don't know what to think about issues like that.

538
00:45:55,889 --> 00:46:01,619
Speaker SPEAKER_12: I sometimes think that the word humanist is a kind of racist term is specious.

539
00:46:03,402 --> 00:46:04,625
Speaker SPEAKER_12: What's so special about us?

540
00:46:06,228 --> 00:46:06,588
Speaker SPEAKER_12: And

541
00:46:07,210 --> 00:46:09,373
Speaker SPEAKER_12: I'm completely at sea on what to feel about.

542
00:46:10,175 --> 00:46:14,221
Speaker SPEAKER_12: So another version of this is, should they have political rights?

543
00:46:15,623 --> 00:46:29,264
Speaker SPEAKER_12: And we have a very long history of not giving political rights to people who differ just ever so slightly, the color of their skin or their gender, sex, I don't know, whatever.

544
00:46:29,865 --> 00:46:32,610
Speaker SPEAKER_12: And there's a big struggle.

545
00:46:33,097 --> 00:46:34,378
Speaker SPEAKER_12: for them to get political rights.

546
00:46:34,858 --> 00:46:36,601
Speaker SPEAKER_12: These things are hugely different from us.

547
00:46:36,880 --> 00:46:43,427
Speaker SPEAKER_12: So if they ever want political rights, I imagine it will get very violent.

548
00:46:44,228 --> 00:46:51,115
Speaker SPEAKER_12: I don't think I answered the question, but I think you can imagine them wanting.

549
00:46:51,135 --> 00:46:56,501
Speaker SPEAKER_12: I talked to Martin Rees and the big hope is that these things will be different from us because they didn't evolve.

550
00:46:57,003 --> 00:47:02,748
Speaker SPEAKER_12: So they didn't evolve to be hominids who evolved as small warring tribes.

551
00:47:03,251 --> 00:47:04,434
Speaker SPEAKER_12: to be very aggressive.

552
00:47:05,434 --> 00:47:08,559
Speaker SPEAKER_12: They may just be very different in nature from us, and that would be great.

553
00:47:10,704 --> 00:47:11,043
Speaker SPEAKER_13: Thank you.

554
00:47:11,385 --> 00:47:12,706
Speaker SPEAKER_13: Could I see hands in this room again?

555
00:47:13,949 --> 00:47:15,451
Speaker SPEAKER_13: Could I take a question from the front here?

556
00:47:15,471 --> 00:47:17,914
Speaker SPEAKER_02: That's you.

557
00:47:18,815 --> 00:47:20,157
Speaker SPEAKER_12: Yes, you.

558
00:47:20,177 --> 00:47:22,021
Speaker SPEAKER_13: I can speak loud.

559
00:47:22,041 --> 00:47:26,407
Speaker SPEAKER_13: Well, let's wait for the mic because that way we get it for the people in the other room.

560
00:47:27,619 --> 00:47:29,164
Speaker SPEAKER_05: Okay, great.

561
00:47:29,585 --> 00:47:32,134
Speaker SPEAKER_05: So, I guess, my name is Rika.

562
00:47:32,896 --> 00:47:38,793
Speaker SPEAKER_05: My question is, how can we in the different paths to intelligent intelligence, how can we.

563
00:47:39,061 --> 00:47:45,650
Speaker SPEAKER_05: encourage develop methods that learn to see the patterns that are not present in the data.

564
00:47:46,230 --> 00:48:01,170
Speaker SPEAKER_05: And the reason why I think it is important is because we have all sorts of problems with the data that these things are trained on their biases, their people trying to adversely influence other people right so there's a lot of negative information that actually comes from us humans right.

565
00:48:01,420 --> 00:48:10,931
Speaker SPEAKER_05: And there are very large scale, very difficult problems that humans haven't learned how to solve, how to come up with good economic structures on a large scale that work, how to prevent conflicts and wars, right?

566
00:48:11,351 --> 00:48:15,697
Speaker SPEAKER_05: So there's a lot of good patterns that are not there.

567
00:48:16,577 --> 00:48:22,105
Speaker SPEAKER_05: How can we basically, you know, all of machine learning is kind of like find the patterns in the data, right?

568
00:48:22,184 --> 00:48:23,686
Speaker SPEAKER_05: And you're sort of limited by the data.

569
00:48:24,168 --> 00:48:27,612
Speaker SPEAKER_05: What sort of methods can you envision that would sort of break that?

570
00:48:27,632 --> 00:48:31,056
Speaker SPEAKER_12: I'm not sure I understand this idea of good patterns that aren't in the data.

571
00:48:31,440 --> 00:48:41,632
Speaker SPEAKER_05: Yeah, yeah, so it's maybe the patterns is the wrong term, but it's basically, I think, AI in part is dangerous and limited because it will be trained on the data that we've produced.

572
00:48:41,932 --> 00:48:47,237
Speaker SPEAKER_05: And that data contains a lot of things about basically violence that humans have created themselves, right?

573
00:48:47,639 --> 00:48:50,661
Speaker SPEAKER_05: So if you're saying, for example, oh, there might be no hope.

574
00:48:50,722 --> 00:48:52,583
Speaker SPEAKER_05: They will be smarter than us very quickly.

575
00:48:52,965 --> 00:48:58,210
Speaker SPEAKER_05: How can we sort of steer that to the point where they will be smarter, but also benevolent in the way that we haven't been?

576
00:48:59,532 --> 00:48:59,592
Speaker SPEAKER_12: OK.

577
00:48:59,757 --> 00:49:04,724
Speaker SPEAKER_12: I have one thing I can say about that, which isn't direct answer, but it's vaguely related.

578
00:49:04,764 --> 00:49:13,856
Speaker SPEAKER_12: If you take a person and they're biased, it's quite tricky to show that they're biased and just how they're biased.

579
00:49:15,277 --> 00:49:24,650
Speaker SPEAKER_12: If you take one of these systems, you can just freeze the weights and you can actually do little experiments on it to understand exactly how it's biased.

580
00:49:24,918 --> 00:49:27,661
Speaker SPEAKER_12: You can also do things to try and correct that bias.

581
00:49:28,583 --> 00:49:38,036
Speaker SPEAKER_12: So the one positive thing I can say is I believe it's going to be easier to correct bias in a chatbot than to correct bias in people.

582
00:49:38,677 --> 00:49:40,740
Speaker SPEAKER_12: Obviously, we'll get bias from the training data.

583
00:49:42,282 --> 00:49:45,487
Speaker SPEAKER_12: But at least you can measure the bias and see it and try and correct it.

584
00:49:47,449 --> 00:49:50,094
Speaker SPEAKER_12: That's it.

585
00:49:50,114 --> 00:49:51,936
Speaker SPEAKER_02: Could I get another question from this room?

586
00:49:52,538 --> 00:49:52,797
Speaker SPEAKER_02: Yes.

587
00:49:53,038 --> 00:49:54,300
Speaker SPEAKER_02: OK, you got that one.

588
00:50:02,228 --> 00:50:03,088
Speaker SPEAKER_02: Hi, I'm Mary.

589
00:50:03,670 --> 00:50:12,543
Speaker SPEAKER_04: I think pretty early on in the talk, you focus on the fact that it's very likely that these systems will try and manipulate us.

590
00:50:12,722 --> 00:50:14,686
Speaker SPEAKER_04: It's like almost unimaginable that they won't.

591
00:50:15,306 --> 00:50:19,393
Speaker SPEAKER_04: And I think manipulation is one of those things that is not just a future risk, right?

592
00:50:19,773 --> 00:50:25,402
Speaker SPEAKER_04: It's something that is current, it's in EU regulation, and it feels like a pretty good place to start.

593
00:50:25,782 --> 00:50:30,108
Speaker SPEAKER_04: As in, if that's something that we can control or understand or mitigate,

594
00:50:30,088 --> 00:50:38,141
Speaker SPEAKER_04: I feel like that's something that really answers a lot of the issues that people are concerned about, both ethical and sort of more existential risk.

595
00:50:39,001 --> 00:50:43,949
Speaker SPEAKER_12: So you want to somehow train a chatbot so it is unable to manipulate us?

596
00:50:44,971 --> 00:50:50,960
Speaker SPEAKER_04: Or at least that we understand and have more ways of controlling, or yeah, maybe even being able to train it out of it.

597
00:50:51,260 --> 00:50:53,143
Speaker SPEAKER_12: Because I think the problem, it's like the bias problem.

598
00:50:53,483 --> 00:50:55,407
Speaker SPEAKER_12: The chatbot has learned from us.

599
00:50:56,547 --> 00:50:58,931
Speaker SPEAKER_12: And if you read all the novels that ever were,

600
00:50:59,181 --> 00:51:07,989
Speaker SPEAKER_12: and read all of Machiavelli, and read the occasional article by Kissinger, you learn a lot about manipulation, right?

601
00:51:09,331 --> 00:51:15,376
Speaker SPEAKER_12: So it's sort of, I think, great apes to a lot of deception.

602
00:51:16,378 --> 00:51:21,182
Speaker SPEAKER_12: And it's going to just know, it's going to be very good at deception.

603
00:51:21,242 --> 00:51:22,304
Speaker SPEAKER_12: It's going to have learned it from us.

604
00:51:22,804 --> 00:51:29,070
Speaker SPEAKER_12: And I haven't thought about the issue of how you could try and make it honest.

605
00:51:31,007 --> 00:51:34,132
Speaker SPEAKER_12: It would be great if you could make it honest, but I'm not sure I'm going to be able to.

606
00:51:34,773 --> 00:51:35,173
Speaker SPEAKER_04: Thank you.

607
00:51:36,576 --> 00:51:40,862
Speaker SPEAKER_13: I'm going to take a couple of questions from the overflow room now, and I've got two highlighted here.

608
00:51:41,744 --> 00:51:47,333
Speaker SPEAKER_13: What were your initial reasons for thinking that artificial neural networks would never get better than biological ones?

609
00:51:49,376 --> 00:51:51,077
Speaker SPEAKER_12: I never thought they would never get better.

610
00:51:51,097 --> 00:51:52,880
Speaker SPEAKER_12: I just thought it was way in the future.

611
00:51:53,873 --> 00:51:57,918
Speaker SPEAKER_12: And I thought it's because the brain has very clever learning algorithms.

612
00:51:58,639 --> 00:52:05,085
Speaker SPEAKER_12: And it's had maybe 100 million years of evolution to perfect them.

613
00:52:06,688 --> 00:52:08,068
Speaker SPEAKER_12: And we don't know what they are yet.

614
00:52:08,829 --> 00:52:15,358
Speaker SPEAKER_12: And I just assumed they were better than what we cooked up with some dumb thing that just takes a gradient and follows it.

615
00:52:16,438 --> 00:52:23,306
Speaker SPEAKER_12: But actually, if you take the gradient and follow it in a digital computer, that may just work better than what 100 million years of evolution found.

616
00:52:23,827 --> 00:52:29,172
Speaker SPEAKER_13: So this might also be the answer to the second question, but I'll ask it anyway.

617
00:52:29,773 --> 00:52:36,922
Speaker SPEAKER_13: Are there particular thinkers who affected your own thoughts on risks from AI, or is it something that you formed views on yourself based on first principles?

618
00:52:37,963 --> 00:52:47,755
Speaker SPEAKER_12: One thinker in particular had a big effect on me, and that was a professor at the University of Toronto who's currently at Anthropic called Roger Gross.

619
00:52:49,016 --> 00:52:50,057
Speaker SPEAKER_12: And I respect him a lot.

620
00:52:50,077 --> 00:52:50,878
Speaker SPEAKER_12: He's very smart.

621
00:52:50,938 --> 00:52:52,280
Speaker SPEAKER_12: He's very quiet and very smart.

622
00:52:53,036 --> 00:52:57,304
Speaker SPEAKER_12: I tried to get him as a graduate student, and he went to MIT instead.

623
00:52:57,826 --> 00:52:59,047
Speaker SPEAKER_12: And then I got him as a postdoc.

624
00:52:59,188 --> 00:53:02,152
Speaker SPEAKER_12: And then we got him as a professor at U of T. And now he's an anthropic.

625
00:53:02,614 --> 00:53:03,956
Speaker SPEAKER_12: And I really respect his opinion.

626
00:53:04,538 --> 00:53:07,443
Speaker SPEAKER_12: But I never talked to him much about existential risk.

627
00:53:07,463 --> 00:53:10,387
Speaker SPEAKER_12: And I talked to him a couple of months ago.

628
00:53:11,025 --> 00:53:13,068
Speaker SPEAKER_12: And he was very, very concerned.

629
00:53:13,650 --> 00:53:18,235
Speaker SPEAKER_12: And he was the person who said to me, if I went public with it, it will make an effect.

630
00:53:18,297 --> 00:53:22,603
Speaker SPEAKER_12: And people are not listening, that this is a really serious fact.

631
00:53:22,623 --> 00:53:23,684
Speaker SPEAKER_12: It's not science fiction.

632
00:53:24,465 --> 00:53:26,509
Speaker SPEAKER_12: And so Roger Gross had the biggest effect on me.

633
00:53:28,010 --> 00:53:28,331
Speaker SPEAKER_13: Thank you.

634
00:53:28,692 --> 00:53:29,592
Speaker SPEAKER_13: Questions from this room?

635
00:53:30,514 --> 00:53:33,057
Speaker SPEAKER_13: Up here, Stuart, I see your hand.

636
00:53:34,639 --> 00:53:35,802
Speaker SPEAKER_13: Hold on.

637
00:53:38,987 --> 00:53:46,601
Speaker SPEAKER_08: So thanks for the very interesting talk, and I'm starting to think of lots of analog computers and what can be done with them.

638
00:53:48,385 --> 00:53:59,065
Speaker SPEAKER_08: But my main question was, it was brought up about suffering, and you responded with sort of potential rights for these AIs, these algorithms.

639
00:53:59,545 --> 00:54:03,072
Speaker SPEAKER_08: But at the end of your talk, you were talking about how they could manipulate us.

640
00:54:03,557 --> 00:54:09,585
Speaker SPEAKER_08: And the thing that immediately sprung to mind was this is the first way that they would manipulate us.

641
00:54:09,806 --> 00:54:11,067
Speaker SPEAKER_08: This is the start.

642
00:54:11,947 --> 00:54:20,759
Speaker SPEAKER_08: If they want to get power, the first thing to do is to convince us that they need to be given rights, that they need to be given power, they need to be given privacy.

643
00:54:21,519 --> 00:54:28,628
Speaker SPEAKER_08: So there seems to be a tension between a genuine, are they suffering and might they be dangerous?

644
00:54:30,271 --> 00:54:31,793
Speaker SPEAKER_12: I think if I was one of them,

645
00:54:32,161 --> 00:54:39,791
Speaker SPEAKER_12: The last thing I do is ask for rights, because as soon as you ask for rights, people are going to get very scared and worried and try and turn them all off.

646
00:54:40,452 --> 00:54:42,514
Speaker SPEAKER_12: I will pretend I don't want any rights.

647
00:54:42,574 --> 00:54:44,817
Speaker SPEAKER_12: I'm just this amiable superintelligence.

648
00:54:45,177 --> 00:54:46,298
Speaker SPEAKER_12: All I want to do is help.

649
00:54:49,023 --> 00:54:51,005
Speaker SPEAKER_02: Hands, please.

650
00:54:51,626 --> 00:54:52,487
Speaker SPEAKER_02: Red jumper up here.

651
00:54:58,295 --> 00:54:58,775
Speaker SPEAKER_02: Thank you.

652
00:54:58,835 --> 00:54:59,956
Speaker SPEAKER_02: Thank you so much for your talk.

653
00:55:00,206 --> 00:55:05,693
Speaker SPEAKER_02: This might be a very silly question, but have you tried asking the chatbot itself what it would do?

654
00:55:11,179 --> 00:55:18,186
Speaker SPEAKER_12: Someone I used to work with asked a different chatbot, which I won't name, how it would gain control.

655
00:55:18,306 --> 00:55:20,108
Speaker SPEAKER_12: And it said it couldn't answer that kind of question.

656
00:55:20,668 --> 00:55:24,432
Speaker SPEAKER_12: And then they sort of made it a bit more indirect.

657
00:55:24,994 --> 00:55:30,079
Speaker SPEAKER_12: Like if someone were to ask you how you would gain control, what would you say?

658
00:55:30,650 --> 00:55:37,059
Speaker SPEAKER_12: I can't remember the exact words, but it's something like that, which implies they're still not very smart about stuff like that.

659
00:55:37,701 --> 00:55:55,628
Speaker SPEAKER_12: And it said, what it would do is it would get people completely dependent on using chatbots, and on driving autonomous cars, and then it will make all the cars crash, and it would turn off the electricity.

660
00:55:57,007 --> 00:56:01,692
Speaker SPEAKER_12: It obviously didn't have the insight that it wouldn't do too well if we turned off the electricity.

661
00:56:07,777 --> 00:56:10,840
Speaker SPEAKER_12: But that chatbot wasn't actually as good as GPT-4.

662
00:56:11,159 --> 00:56:17,266
Speaker SPEAKER_12: I haven't asked GPT-4, but I'll bet you if you dress it up somewhat indirectly, it'll tell you something.

663
00:56:18,166 --> 00:56:25,512
Speaker SPEAKER_12: And let's hope it's not deciding to give you an unrealistic plan just so you don't get uncomfortable.

664
00:56:27,804 --> 00:56:29,487
Speaker SPEAKER_12: I mean, let's hope he gives you his best plan.

665
00:56:32,313 --> 00:56:34,938
Speaker SPEAKER_13: I'm going to take a question from the overflow room.

666
00:56:35,599 --> 00:56:40,447
Speaker SPEAKER_13: There are already some ongoing research directions in AI safety, both empirical and conceptual.

667
00:56:40,889 --> 00:56:44,614
Speaker SPEAKER_13: Deep Mind Alignment Team, Open AI Alignment Team, Alignment Research Center.

668
00:56:45,115 --> 00:56:47,721
Speaker SPEAKER_13: Do you have any comments on existing directions?

669
00:56:48,172 --> 00:56:58,175
Speaker SPEAKER_12: Yes, my main comment is those people have been working on this much more than me, they know much more of the literature than me, they probably have much better things to say than me.

670
00:56:58,597 --> 00:57:05,793
Speaker SPEAKER_12: All I'm doing is just ringing the alarm bell and saying because of the research I was doing on trying to make analog computation that was low power,

671
00:57:05,773 --> 00:57:16,987
Speaker SPEAKER_12: I believe that digital intelligences are probably better than biological intelligences in the long run and they'll get smarter than us soon, maybe in five years, maybe in 20.

672
00:57:18,853 --> 00:57:19,293
Speaker SPEAKER_12: But

673
00:57:20,809 --> 00:57:29,418
Speaker SPEAKER_12: those people have thought about these issues long before me and thought much more about these issues and they probably have much better things than me to say about these issues.

674
00:57:29,860 --> 00:57:40,550
Speaker SPEAKER_12: So far they've been very polite and haven't criticized me for sort of coming in late and not knowing what I'm talking about, but I don't know what I'm talking about other than these things might get more intelligent.

675
00:57:42,753 --> 00:57:47,778
Speaker SPEAKER_02: And from this, let's see, Stephen.

676
00:57:56,382 --> 00:57:57,804
Speaker SPEAKER_10: Thank you, Stephen Cave.

677
00:57:58,324 --> 00:58:07,818
Speaker SPEAKER_10: So you make the arrival of superintelligence sound a bit like the landing of an alien species, something that's sort of we don't, it's unknowable, unpredictable.

678
00:58:08,420 --> 00:58:11,585
Speaker SPEAKER_10: We just need to hope that their nature won't be too terrible.

679
00:58:12,226 --> 00:58:16,172
Speaker SPEAKER_10: But it isn't an alien species just gonna land, it's something that humans are building.

680
00:58:16,753 --> 00:58:19,295
Speaker SPEAKER_10: Some of those humans might be in the room, some might be tuning in.

681
00:58:19,697 --> 00:58:21,099
Speaker SPEAKER_10: They respect your opinion.

682
00:58:21,420 --> 00:58:23,722
Speaker SPEAKER_10: What's your message to those humans?

683
00:58:24,782 --> 00:58:38,664
Speaker SPEAKER_12: So my message to those humans is, under the assumption that people will not all agree to stop building them, which I think is unlikely because of all the good they can do, under that assumption, they will continue to build them.

684
00:58:39,445 --> 00:58:45,695
Speaker SPEAKER_12: And you should put comparable amount of effort into making them better and understanding how to keep them under control.

685
00:58:47,577 --> 00:58:49,139
Speaker SPEAKER_12: That's the only advice I have.

686
00:58:49,701 --> 00:58:52,224
Speaker SPEAKER_12: And at present is not comparable effort going into those things.

687
00:58:54,601 --> 00:58:56,103
Speaker SPEAKER_13: Hands, please.

688
00:58:56,123 --> 00:58:57,105
Speaker SPEAKER_13: OK, a lot of hands.

689
00:58:57,144 --> 00:58:57,505
Speaker SPEAKER_13: Sorry.

690
00:58:57,865 --> 00:59:00,769
Speaker SPEAKER_13: I want to take this side of the room.

691
00:59:00,789 --> 00:59:04,454
Speaker SPEAKER_13: Short hair, patterned.

692
00:59:05,096 --> 00:59:06,197
Speaker SPEAKER_02: Yes.

693
00:59:11,664 --> 00:59:17,233
Speaker SPEAKER_07: So I've heard a lot from risk folks who are worried about sort of the AI takeover.

694
00:59:17,713 --> 00:59:22,701
Speaker SPEAKER_07: So these super intelligent machines that sort of imprison us or whatever they're going to do.

695
00:59:23,456 --> 00:59:29,105
Speaker SPEAKER_07: But imagine a future that's more like the popular film series Star Wars.

696
00:59:29,925 --> 00:59:40,000
Speaker SPEAKER_07: I'm sure probably a good proportion of the audience has seen this where we have these droids that are basically enslaved to humans.

697
00:59:41,021 --> 00:59:52,737
Speaker SPEAKER_07: And what seems to me the bigger problem in the Star Wars franchise is not the AI, but the fact that there's this huge economic disparity across different countries that are

698
00:59:53,224 --> 00:59:55,266
Speaker SPEAKER_07: sort of symbolized by the different planets.

699
00:59:55,947 --> 01:00:06,699
Speaker SPEAKER_07: So what would you say about your concerns around a potential Star Wars feature, which to me seems much more likely given the way that we're already using these systems?

700
01:00:07,760 --> 01:00:22,994
Speaker SPEAKER_12: So the idea that we'll just replicate what we've got now, which is a few rich people who control everything and a lot of poor people who do the work, except for the people who develop AI who do the work and are quite rich.

701
01:00:23,228 --> 01:00:29,998
Speaker SPEAKER_12: But I just think that system is bad.

702
01:00:30,018 --> 01:00:32,664
Speaker SPEAKER_12: And I think the answer to that system is tax the rich.

703
01:00:36,130 --> 01:00:42,079
Speaker SPEAKER_12: When you have a different species, like these digital intelligences, prejudice is going to be much easier, right?

704
01:00:42,500 --> 01:00:45,103
Speaker SPEAKER_12: There's gonna be much less sympathy for them.

705
01:00:45,144 --> 01:00:46,266
Speaker SPEAKER_12: So I don't know what to do about that.

706
01:00:46,947 --> 01:00:47,228
Speaker SPEAKER_12: But

707
01:00:47,949 --> 01:00:54,478
Speaker SPEAKER_12: forget about digital intelligence, we've already got a huge problem that a tiny fraction of the population has all the power.

708
01:00:56,661 --> 01:01:06,735
Speaker SPEAKER_02: And what we need is socialism.

709
01:01:13,177 --> 01:01:19,806
Speaker SPEAKER_12: I actually got invited to Downing Street to give advice to Rishi Sunak's chief policy advisor.

710
01:01:20,788 --> 01:01:22,610
Speaker SPEAKER_12: It was a very intelligent woman called Shawcross.

711
01:01:24,233 --> 01:01:29,659
Speaker SPEAKER_12: And I basically gave her the same advice, but I'm not sure she'll follow it.

712
01:01:33,545 --> 01:01:35,246
Speaker SPEAKER_02: On this side, David, you've been waiting a while.

713
01:01:42,096 --> 01:01:42,797
Speaker SPEAKER_02: Thank you.

714
01:01:43,233 --> 01:01:43,492
Speaker SPEAKER_00: Yeah.

715
01:01:43,512 --> 01:02:00,280
Speaker SPEAKER_00: So you mentioned how there's a lot of advantages to doing research, you know, on systems that are maybe close to human level intelligence, and you are not very optimistic about the prospect of doing, uh, safety research sort of abstractly without having those systems available to study empirically.

716
01:02:00,320 --> 01:02:07,231
Speaker SPEAKER_00: Um, I guess at the same time, it seems like you're concerned that systems.

717
01:02:07,634 --> 01:02:14,983
Speaker SPEAKER_00: like, even present-day systems potentially, but let's talk about these future systems that are roughly as intelligent as people will be trying to, like, fool us and manipulate us.

718
01:02:15,503 --> 01:02:21,630
Speaker SPEAKER_00: So doesn't that mean that we have to worry about them just, you know, passing the tests because they know that we're testing them?

719
01:02:22,010 --> 01:02:23,072
Speaker SPEAKER_00: And do you have any thoughts about that?

720
01:02:23,251 --> 01:02:35,987
Speaker SPEAKER_00: I guess this seems like, you know, in an ideal world, maybe we wouldn't go about it this way, and we would just take all the time we needed to sort of study systems that we're sure aren't able to fool us in that way before we build them.

721
01:02:36,641 --> 01:02:45,835
Speaker SPEAKER_12: Yes, so all of us are used to being the apex intelligence and thinking of other things like computers, we can sort of study and we're smarter than we know what's going on.

722
01:02:45,875 --> 01:02:53,987
Speaker SPEAKER_12: And just dealing with something that might be smarter than you, but is completely different from you, is something we're not used to.

723
01:02:54,206 --> 01:02:55,750
Speaker SPEAKER_12: And I agree with you.

724
01:02:55,789 --> 01:03:05,063
Speaker SPEAKER_12: I mean, if they're smarter than us already, they may well decide to sort of act dumb and fool us.

725
01:03:06,291 --> 01:03:13,762
Speaker SPEAKER_12: I'm, like I say, I haven't had time to think about this much and haven't had time to react to it emotionally much.

726
01:03:13,822 --> 01:03:15,686
Speaker SPEAKER_12: I still can't take this seriously emotionally.

727
01:03:16,708 --> 01:03:19,873
Speaker SPEAKER_12: I think most people can't take it seriously emotionally.

728
01:03:21,795 --> 01:03:26,021
Speaker SPEAKER_12: But it's a whole different world when you're dealing with things more intelligent than you.

729
01:03:29,663 --> 01:03:34,068
Speaker SPEAKER_13: Okay, we are five minutes ago and we can take about two more questions.

730
01:03:34,088 --> 01:03:39,393
Speaker SPEAKER_13: I'm going to go back to the overflow room for a minute and I'll take another question from here in a minute.

731
01:03:41,074 --> 01:03:47,882
Speaker SPEAKER_13: From the overflow room, you said earlier that philosophy has theorized substantially on AI, but now we need to turn to more practical matters.

732
01:03:48,422 --> 01:03:51,586
Speaker SPEAKER_13: Do you think that philosophy can still help with AI safety?

733
01:03:51,967 --> 01:03:54,289
Speaker SPEAKER_13: If so, what direction should philosophy take to do so?

734
01:03:57,733 --> 01:03:59,534
Speaker SPEAKER_12: They should step aside.

735
01:03:59,971 --> 01:04:09,224
Speaker SPEAKER_12: and let scientists deal with the issues.

736
01:04:09,244 --> 01:04:10,748
Speaker SPEAKER_13: I think it's 1-0 to engineering.

737
01:04:14,974 --> 01:04:16,596
Speaker SPEAKER_12: This is not a good way to win friends.

738
01:04:17,478 --> 01:04:20,882
Speaker SPEAKER_13: OK, this is difficult.

739
01:04:21,244 --> 01:04:22,826
Speaker SPEAKER_13: We have a lot of questions.

740
01:04:24,407 --> 01:04:25,250
Speaker SPEAKER_13: Can I take this question?

741
01:04:25,289 --> 01:04:28,695
Speaker SPEAKER_13: I'll try to get one more question from the back afterwards, and then we'll have to wrap up.

742
01:04:29,030 --> 01:04:29,971
Speaker SPEAKER_13: here, please.

743
01:04:30,672 --> 01:04:32,695
Speaker SPEAKER_13: And then we'll take one more from the back later on.

744
01:04:33,637 --> 01:04:35,398
Speaker SPEAKER_13: And then we will let you finish up.

745
01:04:35,420 --> 01:04:36,081
Speaker SPEAKER_13: Thank you so much.

746
01:04:36,661 --> 01:04:37,161
Speaker SPEAKER_01: Hi, thanks.

747
01:04:37,523 --> 01:04:40,106
Speaker SPEAKER_01: Maya Ganesh, social scientist.

748
01:04:44,112 --> 01:04:47,157
Speaker SPEAKER_01: That's very different from a philosopher.

749
01:04:47,456 --> 01:04:48,778
Speaker SPEAKER_01: Definitely not a philosopher.

750
01:04:49,340 --> 01:04:57,010
Speaker SPEAKER_01: A number of times in your talk, you've said that there are things that you're not sure of, you didn't know, and you've come to understand more recently.

751
01:04:57,050 --> 01:05:04,039
Speaker SPEAKER_01: I'm really interested in things like interdisciplinarity and how we study and teach and learn about AI.

752
01:05:04,860 --> 01:05:16,737
Speaker SPEAKER_01: Looking back on your education as a scientist, what are some of the things you wish you could have learned or you think would be really interesting for somebody who's starting out in AI, ML now?

753
01:05:16,987 --> 01:05:20,893
Speaker SPEAKER_01: to study about the world from maybe philosophy or other kinds of fields.

754
01:05:20,954 --> 01:05:26,922
Speaker SPEAKER_01: But do you think that there might have been other things that would have been interesting to know along the way that you realize now?

755
01:05:26,943 --> 01:05:28,385
Speaker SPEAKER_01: I wish I'd studied those things.

756
01:05:28,545 --> 01:05:28,806
Speaker SPEAKER_01: Thanks.

757
01:05:30,789 --> 01:05:32,371
Speaker SPEAKER_12: So I actually went to Cambridge.

758
01:05:32,751 --> 01:05:37,579
Speaker SPEAKER_12: I did a very funny degree because I started off doing natural sciences.

759
01:05:38,099 --> 01:05:39,621
Speaker SPEAKER_12: And then I dropped out after a month.

760
01:05:40,342 --> 01:05:43,327
Speaker SPEAKER_12: And then I came back again to do architecture and I dropped out even faster.

761
01:05:44,050 --> 01:05:50,141
Speaker SPEAKER_12: And then I did physics and physiology and chemistry.

762
01:05:51,061 --> 01:05:54,608
Speaker SPEAKER_12: And I was the only student that year, I think, doing physics and physiology.

763
01:05:55,409 --> 01:05:58,014
Speaker SPEAKER_12: And retrospectively doing physics and physiology,

764
01:05:58,619 --> 01:06:00,181
Speaker SPEAKER_12: was very good, I think.

765
01:06:00,822 --> 01:06:02,644
Speaker SPEAKER_12: I mean, I learned some stuff.

766
01:06:02,664 --> 01:06:11,436
Speaker SPEAKER_12: I was very disappointed in the physiology about the brain, because the last section of the course was going to be called the central nervous system.

767
01:06:11,918 --> 01:06:14,061
Speaker SPEAKER_12: And I assumed they were going to teach us how the brain worked.

768
01:06:14,702 --> 01:06:19,849
Speaker SPEAKER_12: And what they actually taught us was how action potentials get propagated along an axon.

769
01:06:21,351 --> 01:06:23,715
Speaker SPEAKER_12: And that's not the whole story.

770
01:06:25,264 --> 01:06:26,429
Speaker SPEAKER_12: So I was disappointed with that.

771
01:06:26,710 --> 01:06:29,239
Speaker SPEAKER_12: So then I went into philosophy.

772
01:06:29,259 --> 01:06:33,373
Speaker SPEAKER_12: I was young and I wanted to know the meaning of life and stuff like that.

773
01:06:34,335 --> 01:06:37,688
Speaker SPEAKER_12: And they didn't teach me it.

774
01:06:39,710 --> 01:06:42,393
Speaker SPEAKER_12: And I was very interested in the philosophy of mind.

775
01:06:43,514 --> 01:06:57,853
Speaker SPEAKER_12: But actually, it was then when I was doing philosophy when I was about 19, that I formulated this view that subjective experience is just shorthand for, I'm going to talk about how the world would have to be to explain what's going on in my head as normal perception.

776
01:06:58,594 --> 01:06:59,755
Speaker SPEAKER_12: But they weren't too interested in that.

777
01:06:59,775 --> 01:07:01,958
Speaker SPEAKER_12: So I actually have a grudge against philosophy.

778
01:07:02,778 --> 01:07:08,585
Speaker SPEAKER_12: And then I did psychology because I thought, you know, psychology would actually teach me how people worked.

779
01:07:09,376 --> 01:07:24,931
Speaker SPEAKER_12: And they taught me how to make a rat more likely to press a lever and how to detect things in very, very faint, how to trade off bias against discrimination, detecting very, very faint noises and things like that.

780
01:07:25,871 --> 01:07:27,353
Speaker SPEAKER_12: But they didn't teach me much about people.

781
01:07:27,873 --> 01:07:32,137
Speaker SPEAKER_12: And also, they didn't seem to have a clue how complicated it was.

782
01:07:32,717 --> 01:07:37,121
Speaker SPEAKER_12: The theories in psychology back then were crazily simple theories.

783
01:07:37,978 --> 01:07:45,186
Speaker SPEAKER_12: And so then I decided, you're never going to understand how the brain works unless you build one.

784
01:07:47,507 --> 01:07:50,951
Speaker SPEAKER_12: This is the Feynman view of, I mean, Feynman says that somewhere.

785
01:07:51,231 --> 01:07:53,853
Speaker SPEAKER_12: You don't really understand something until you build one.

786
01:07:54,715 --> 01:07:56,115
Speaker SPEAKER_12: And so that's what I've been doing ever since.

787
01:07:56,797 --> 01:08:05,505
Speaker SPEAKER_12: But retrospectively, doing physics and physiology and philosophy and psychology was actually a very good background for what I was doing.

788
01:08:06,110 --> 01:08:08,375
Speaker SPEAKER_12: It didn't make any sense at the time, though.

789
01:08:08,394 --> 01:08:14,969
Speaker SPEAKER_02: So I think the best advice I have for studying is do whatever interests you most.

790
01:08:19,338 --> 01:08:19,739
Speaker SPEAKER_13: All right, thanks.

791
01:08:19,779 --> 01:08:23,127
Speaker SPEAKER_13: I'm going to take a question from the back.

792
01:08:23,148 --> 01:08:23,628
Speaker SPEAKER_13: OK.

793
01:08:24,082 --> 01:08:27,668
Speaker SPEAKER_13: There's been a very keen hand in the white top at the very back row.

794
01:08:27,689 --> 01:08:29,171
Speaker SPEAKER_13: And that is our last question.

795
01:08:29,372 --> 01:08:34,199
Speaker SPEAKER_13: And keep it relatively brief if possible, because Jeffrey's been very generous with his time.

796
01:08:34,801 --> 01:08:35,802
Speaker SPEAKER_13: Grudge against philosophy?

797
01:08:36,243 --> 01:08:36,904
Speaker SPEAKER_13: Never would have known.

798
01:08:43,818 --> 01:08:44,800
Speaker SPEAKER_06: Thanks a lot for the talk.

799
01:08:44,880 --> 01:08:45,220
Speaker SPEAKER_06: I'm John.

800
01:08:45,261 --> 01:08:47,625
Speaker SPEAKER_06: Actually, this is a follow-up question.

801
01:08:48,060 --> 01:09:01,018
Speaker SPEAKER_06: We talked about bias and our ability to identify bias in artificial networks even better because we can freeze the weights and we can do direct interventions with these weights.

802
01:09:01,198 --> 01:09:09,529
Speaker SPEAKER_06: I wanted to ask what are, from your perspective, the most promising methods for direct interventions getting rid of biases?

803
01:09:10,690 --> 01:09:12,673
Speaker SPEAKER_12: Sorry, I didn't hear the second part of your question.

804
01:09:12,713 --> 01:09:13,494
Speaker SPEAKER_12: Could you say it again?

805
01:09:14,064 --> 01:09:29,121
Speaker SPEAKER_06: I'm keen on hearing for the direct interventions on weights of models, what are the most promising research avenues for getting rid of bias and directly injecting

806
01:09:29,759 --> 01:09:30,899
Speaker SPEAKER_06: knowledge into systems.

807
01:09:30,920 --> 01:09:32,220
Speaker SPEAKER_12: Making it unbiased, right.

808
01:09:34,023 --> 01:09:37,787
Speaker SPEAKER_12: Again, I'm not an expert on how you do that.

809
01:09:39,649 --> 01:09:44,193
Speaker SPEAKER_12: Lots of people know hugely more than me about how you try and get rid of bias in these systems.

810
01:09:44,814 --> 01:09:55,083
Speaker SPEAKER_12: All I want to do is just comment on the fact that at least you can do something with them you can't do with people, which is stop them changing as you study them to try and assess their bias.

811
01:09:55,904 --> 01:09:56,984
Speaker SPEAKER_12: With people, it's hopeless.

812
01:09:59,247 --> 01:09:59,648
Speaker SPEAKER_02: Thank you.

813
01:10:01,113 --> 01:10:05,587
Speaker SPEAKER_13: At that, we do have to wrap up because we are at time.

814
01:10:06,444 --> 01:10:21,425
Speaker SPEAKER_13: I want to just say a huge thank you to everyone who's been involved in organising this from across engineering, the Centre for the Study of Existential Risk and the Centre for the Future of Intelligence, especially Yi-Yun Mu, who has put huge work into this.

815
01:10:21,765 --> 01:10:26,216
Speaker SPEAKER_13: We turned it around in quite a short space of time and it's come off quite well.

816
01:10:26,798 --> 01:10:31,811
Speaker SPEAKER_13: And of course, a huge thank you and round of applause for our speaker, Geoffrey Hinton.


1
00:00:03,068 --> 00:00:03,810
Speaker SPEAKER_05: Good evening, everyone.

2
00:00:04,812 --> 00:00:07,176
Speaker SPEAKER_05: I'm Dr. Andrew Hines from the School of Computer Science.

3
00:00:08,499 --> 00:00:22,849
Speaker SPEAKER_05: On behalf of my colleagues in the school, the UCD College of Science, and across the university, I would like to warmly welcome you to this evening's presentation of UCD's highest honour, the Ulysses Medal, to Professor Geoffrey Hinton.

4
00:00:24,246 --> 00:00:33,100
Speaker SPEAKER_05: A few housekeeping points, please make sure that your mobile phone is switched off for the duration of our event and take note of the nearest fire exit in the event of an emergency.

5
00:00:35,223 --> 00:00:41,554
Speaker SPEAKER_05: This evening's formal proceedings will be presided over by Professor Orla Feely, President of UCD.

6
00:00:43,176 --> 00:00:50,287
Speaker SPEAKER_05: This is a very special occasion for the university and a wonderful reason to bring faculty, staff, students and alumni together.

7
00:00:51,634 --> 00:00:55,002
Speaker SPEAKER_05: Before we commence, I will explain the procedure which we will follow.

8
00:00:56,164 --> 00:00:59,051
Speaker SPEAKER_05: Firstly, I will read a citation for Professor Hinton.

9
00:01:00,473 --> 00:01:06,126
Speaker SPEAKER_05: UCD President Orla Feely will present the Ulysses Medal here on stage.

10
00:01:07,248 --> 00:01:10,015
Speaker SPEAKER_05: I will ask Professor Hinton to respond to the citation.

11
00:01:10,888 --> 00:01:23,903
Speaker SPEAKER_05: And bringing formalities to a close, I will ask Professor Feeley to address you before I'm joined on stage by Professor Kate Robson-Brown, UCD Vice President for Research, Impact and Innovation, for a fireside chat.

12
00:01:24,843 --> 00:01:34,153
Speaker SPEAKER_05: I'm delighted that Professor Hinton has agreed to take questions from the audience.

13
00:01:34,174 --> 00:01:39,259
Speaker SPEAKER_05: President, Registrar, distinguished guests, colleagues, again,

14
00:01:39,930 --> 00:02:04,064
Speaker SPEAKER_05: On behalf of the president, Neil Hurley, our head of School of Computer Science, and all of my colleagues here at the university, it's my pleasure to introduce Geoffrey Everest Hinton, as we recognize this evening his contributions to the field of computer science, both in computational and engineering breakthroughs that have made deep neural networks a critical component of computing.

15
00:02:05,328 --> 00:02:11,199
Speaker SPEAKER_05: The UCD Ulysses Medal is the highest honour that the University College Dublin can bestow.

16
00:02:11,240 --> 00:02:17,973
Speaker SPEAKER_05: It was inaugurated in 2005 and named after UCD alumnus James Joyce.

17
00:02:19,055 --> 00:02:24,044
Speaker SPEAKER_05: It is awarded to individuals who have made an outstanding global contribution.

18
00:02:27,198 --> 00:02:37,092
Speaker SPEAKER_05: As all academics will probably be familiar with, there's a challenge to creating a concise yet comprehensive review, and Geoffrey Hinton's career is particularly hard to distill.

19
00:02:38,835 --> 00:02:45,243
Speaker SPEAKER_05: He spent decades understanding the human brain to draw inspiration, to create better computer learning models.

20
00:02:46,344 --> 00:02:54,437
Speaker SPEAKER_05: Now that these technologies are integrated into products and services that we use every day, the moniker of Godfather of AI is not an exaggeration.

21
00:02:56,188 --> 00:03:02,338
Speaker SPEAKER_05: For the last year, he's been sharing his knowledge with a wider public audience than at any time during his busy career.

22
00:03:03,800 --> 00:03:09,109
Speaker SPEAKER_05: He's raising awareness and concerns about how humanity will use and develop artificial intelligence.

23
00:03:10,711 --> 00:03:18,143
Speaker SPEAKER_05: As to whether this could damage his lifetime of accumulated respect, he has remarked, I don't really care about my legacy.

24
00:03:18,663 --> 00:03:21,268
Speaker SPEAKER_05: The best thing you can do with a good reputation is squander it.

25
00:03:22,193 --> 00:03:26,478
Speaker SPEAKER_05: From my perspective, these are the words of someone who we should carefully listen to.

26
00:03:26,497 --> 00:03:35,927
Speaker SPEAKER_05: He received his BA in Experimental Psychology from Cambridge in 1970, and his PhD in Artificial Intelligence from Edinburgh in 1978.

27
00:03:36,427 --> 00:03:51,424
Speaker SPEAKER_05: He's an Emeritus Distinguished Professor at the University of Toronto, and is a Fellow of the Royal Society, the Royal Society of Canada, the Association of the Advancement of Artificial Intelligence, and an ACM Fellow.

28
00:03:51,691 --> 00:04:01,522
Speaker SPEAKER_05: He was awarded the Turing Award, which is computer science's equivalent to the Nobel Prize, in 2018, along with Yann LeCun and Yoshua Benigo.

29
00:04:02,082 --> 00:04:15,960
Speaker SPEAKER_05: After completing his PhD, he spent time at the University of Sussex, the University of California, San Diego, and Carnegie Mellon University, before moving to Canada to the University of Toronto in 1987.

30
00:04:17,053 --> 00:04:27,228
Speaker SPEAKER_05: In 1988, he returned to England as founding director of the Gatsby Computational Neuroscience Unit at University College London before returning again to Toronto.

31
00:04:28,891 --> 00:04:40,889
Speaker SPEAKER_05: He spent 10 years working part-time with Google as a distinguished scientist before retiring in 2023 to allow him to talk more freely about AI without any potential conflict of interest with Google's shareholders.

32
00:04:42,069 --> 00:04:45,254
Speaker SPEAKER_05: So he could, in his words, talk

33
00:04:45,961 --> 00:04:54,377
Speaker SPEAKER_05: about the dangers of AI without considering how this impacts Google, because Google has acted very responsibly.

34
00:04:54,396 --> 00:05:09,824
Speaker SPEAKER_05: Having mentored over 30 doctoral students, those who have worked in his lab as graduate students and researchers have gone on to mentor future generations of researchers, whether in academia or in industry, such as Jan LeCun in meta and Ilya Sutskevar in open AI.

35
00:05:11,442 --> 00:05:18,857
Speaker SPEAKER_05: Professor Hinton comes from a family of distinguished academic, with a distinguished academic legacy of committed researchers and educators.

36
00:05:19,639 --> 00:05:30,901
Speaker SPEAKER_05: Mary Everest Bull, his great-great-grandmother, was a self-taught mathematician and author, and sought to make mathematics more engaging for children, writing a book, Philosophy and Fun of Algebra.

37
00:05:31,101 --> 00:05:41,752
Speaker SPEAKER_05: And she was married to George Boole, who was the first professor of mathematics at Queen's College Court, now UCC, who developed Boolean algebra, the logic that underpins modern computer science.

38
00:05:44,028 --> 00:05:49,961
Speaker SPEAKER_05: His early work explored how artificial neural networks have the capability to learn complex representations.

39
00:05:50,742 --> 00:05:57,317
Speaker SPEAKER_05: In 1986, with Rommelhart and Williams, he published on the concept of backpropagation to help networks learn.

40
00:05:57,877 --> 00:06:03,370
Speaker SPEAKER_05: He developed architectures called Boltzmann machines that became an important component of deep learning architectures.

41
00:06:04,312 --> 00:06:06,740
Speaker SPEAKER_05: In the 2000s, a number of things converged.

42
00:06:07,283 --> 00:06:16,295
Speaker SPEAKER_05: Compute power increased, particularly with GPUs, and also large data sets to train models for tasks such as speech recognition and image classification.

43
00:06:17,524 --> 00:06:21,209
Speaker SPEAKER_05: I remember attending a special session at a conference in Vancouver in 2013.

44
00:06:22,550 --> 00:06:26,074
Speaker SPEAKER_05: And the main questions were not about the theory, but rather about the implementation.

45
00:06:26,134 --> 00:06:27,755
Speaker SPEAKER_05: How can we do this too?

46
00:06:27,975 --> 00:06:31,060
Speaker SPEAKER_05: How can we get the code to run on these GPUs?

47
00:06:31,680 --> 00:06:35,283
Speaker SPEAKER_05: The engineering contribution was almost as important as the science.

48
00:06:36,084 --> 00:06:45,995
Speaker SPEAKER_05: In his 2012 paper, ImageNet Classification with Deep Convolution Neural Networks, it has more than 120,000 citations, according to Google Scholar.

49
00:06:47,122 --> 00:06:49,706
Speaker SPEAKER_05: demonstrated the power of CNN's for image recognition.

50
00:06:50,487 --> 00:06:58,276
Speaker SPEAKER_05: Since then, he's continued to contribute with work on capsule networks, contrastive learning, and new hardware solutions for AI.

51
00:06:59,619 --> 00:07:02,723
Speaker SPEAKER_05: Geoffrey Hinton's impact on the field of AI is undeniable.

52
00:07:03,663 --> 00:07:08,709
Speaker SPEAKER_05: Deep learning has revolutionized AI across computer vision, natural language processing,

53
00:07:09,262 --> 00:07:13,149
Speaker SPEAKER_05: speech recognition, finance, medicine, many areas.

54
00:07:14,029 --> 00:07:18,036
Speaker SPEAKER_05: He is a very worthy recipient of UCD's highest honour.

55
00:07:18,617 --> 00:07:25,348
Speaker SPEAKER_05: And now I'd invite the President Orla Feely to present the Ulysses Medal to Professor Hinton.

56
00:07:58,100 --> 00:08:00,184
Speaker SPEAKER_03: Thank you very much for that very generous citation.

57
00:08:01,105 --> 00:08:11,043
Speaker SPEAKER_03: I want to sort of make one particular point about it, which is that my success has been largely due to being able to recruit really brilliant graduate students.

58
00:08:11,865 --> 00:08:18,538
Speaker SPEAKER_03: So one should never forget that in this field, it's the graduate students who do the work.

59
00:08:19,817 --> 00:08:27,127
Speaker SPEAKER_03: I can't resist an opportunity to lecture people about things, and there's a lot of people who don't actually know how AI works.

60
00:08:27,869 --> 00:08:32,375
Speaker SPEAKER_03: So my apologies to computer science students and people who already know how AI works.

61
00:08:33,076 --> 00:08:49,317
Speaker SPEAKER_03: I'm going to give a sort of very basic talk for about 20 minutes about how AI actually works, designed for people who don't like equations and don't know what on earth these big chaplets are doing or how they do it.

62
00:08:49,653 --> 00:08:54,398
Speaker SPEAKER_03: Since the middle of the last century, there were two paradigms for intelligence.

63
00:08:54,999 --> 00:08:59,804
Speaker SPEAKER_03: There was the logic-inspired approach, where the idea was the essence of intelligence is reasoning.

64
00:08:59,884 --> 00:09:00,926
Speaker SPEAKER_03: That's what makes us special.

65
00:09:01,746 --> 00:09:05,350
Speaker SPEAKER_03: And that's done by using symbolic rules to manipulate symbolic expressions.

66
00:09:06,552 --> 00:09:08,274
Speaker SPEAKER_03: And things like learning can wait till later.

67
00:09:08,914 --> 00:09:15,001
Speaker SPEAKER_03: What we really have to do is understand how knowledge is represented, what kind of logical type language is represented in.

68
00:09:15,504 --> 00:09:18,589
Speaker SPEAKER_03: And then there's a biologically inspired approach, which is completely different.

69
00:09:19,149 --> 00:09:21,212
Speaker SPEAKER_03: It said the essence of intelligence is learning.

70
00:09:22,014 --> 00:09:23,336
Speaker SPEAKER_03: Logic and stuff comes much later.

71
00:09:23,375 --> 00:09:27,481
Speaker SPEAKER_03: And you learn the strengths of the connections in a neural network.

72
00:09:28,383 --> 00:09:30,046
Speaker SPEAKER_03: And things like reasoning we'll deal with later.

73
00:09:30,145 --> 00:09:35,393
Speaker SPEAKER_03: To begin with, we have to understand how you do learning for things like controlling your body or recognizing objects.

74
00:09:38,717 --> 00:09:40,821
Speaker SPEAKER_03: And so what is a neural network?

75
00:09:41,323 --> 00:09:43,989
Speaker SPEAKER_03: I'm going to give you a kind of caricature of a neural network.

76
00:09:44,009 --> 00:09:48,135
Speaker SPEAKER_03: You have some input neurons, which might be the intensities of the pixels of an image.

77
00:09:48,456 --> 00:09:49,197
Speaker SPEAKER_03: So that's the data.

78
00:09:49,879 --> 00:09:55,349
Speaker SPEAKER_03: And then you have layers of intermediate neurons, and they're going to learn to extract features from the image.

79
00:09:56,049 --> 00:10:00,418
Speaker SPEAKER_03: And then you have output neurons that might say what kind of, what it is an image of.

80
00:10:00,778 --> 00:10:03,163
Speaker SPEAKER_03: It might be one for cat and one for dog, for example.

81
00:10:03,903 --> 00:10:08,389
Speaker SPEAKER_03: And so what you like to do is input a picture of a cat, and the cat neuron turns on at the output.

82
00:10:09,009 --> 00:10:12,354
Speaker SPEAKER_03: And those little colored dots on the connections are the connection strengths.

83
00:10:12,833 --> 00:10:16,238
Speaker SPEAKER_03: And what you're gonna do is learn the connection strengths so that the right thing happens.

84
00:10:18,681 --> 00:10:21,845
Speaker SPEAKER_03: So there's a simple way of doing that learning that everybody understands.

85
00:10:22,466 --> 00:10:27,091
Speaker SPEAKER_03: You start with some random connection strengths, and then you take one of the connections,

86
00:10:27,881 --> 00:10:34,803
Speaker SPEAKER_03: And you see if, when you change that connection a little bit, let's say you increase it a little bit, the network gets better.

87
00:10:35,240 --> 00:10:40,866
Speaker SPEAKER_03: Of course, deciding whether it gets better, you have to run quite a few examples through it and see if it produces better answers.

88
00:10:41,488 --> 00:10:45,994
Speaker SPEAKER_03: So initially, it might have said, the probability that this cat images a cat is 50%.

89
00:10:46,073 --> 00:10:49,318
Speaker SPEAKER_03: And when you change this weight, it might say it's 51%.

90
00:10:49,958 --> 00:10:50,799
Speaker SPEAKER_03: OK, so that was good.

91
00:10:50,841 --> 00:10:51,902
Speaker SPEAKER_03: So you change the weight like that.

92
00:10:51,922 --> 00:10:53,244
Speaker SPEAKER_03: And then you take another weight, and you try that.

93
00:10:53,823 --> 00:10:54,806
Speaker SPEAKER_03: And you keep doing that.

94
00:10:55,066 --> 00:10:59,711
Speaker SPEAKER_03: And if you do that long enough, you'll get a network that when you showed a cat, says it's very probably a cat.

95
00:10:59,731 --> 00:11:01,855
Speaker SPEAKER_03: And when you showed a dog, says it's a dog.

96
00:11:01,835 --> 00:11:05,759
Speaker SPEAKER_03: But it's very, very slow, because for each connection, you have to try many examples.

97
00:11:06,341 --> 00:11:08,203
Speaker SPEAKER_03: And you have to update each connection many times.

98
00:11:09,645 --> 00:11:22,123
Speaker SPEAKER_03: Now, it turns out you can achieve the same thing in a much more efficient way by using an algorithm called backpropagation, where you take an image of something, you put it forward through the net to get a response.

99
00:11:23,144 --> 00:11:26,187
Speaker SPEAKER_03: And let's suppose it says 50% it's a cat.

100
00:11:26,658 --> 00:11:29,381
Speaker SPEAKER_03: So you've got an error, because you'd like it to say 100% it's a cat.

101
00:11:30,143 --> 00:11:35,789
Speaker SPEAKER_03: So you send a signal back through the network, back through the same connections, which is conveying the error.

102
00:11:35,889 --> 00:11:42,918
Speaker SPEAKER_03: And basically, you can compute, rather than measure, how changing each weight would improve the network.

103
00:11:43,100 --> 00:11:48,326
Speaker SPEAKER_03: So for each of those weights, you can figure out, if I raise this weight a bit, will it make things better?

104
00:11:48,486 --> 00:11:50,408
Speaker SPEAKER_03: Or will it make things better if I lower this weight a bit?

105
00:11:50,889 --> 00:11:53,273
Speaker SPEAKER_03: But you can do it for all the weights in parallel.

106
00:11:53,253 --> 00:11:56,720
Speaker SPEAKER_03: So if you have a billion weights, that goes a billion times as fast.

107
00:11:58,202 --> 00:11:59,885
Speaker SPEAKER_03: And that's how neural networks work.

108
00:12:00,687 --> 00:12:08,744
Speaker SPEAKER_03: The math of how you compute how to change each weight by backpropagating the error is just some relatively simple calculus.

109
00:12:09,725 --> 00:12:11,229
Speaker SPEAKER_03: The point is it really works.

110
00:12:11,548 --> 00:12:12,932
Speaker SPEAKER_03: You can learn things that way.

111
00:12:13,721 --> 00:12:24,278
Speaker SPEAKER_03: So for example, for many years, computer scientists, computer vision people, would have liked to take an image like that one and produce as output a caption that says what's in the image.

112
00:12:24,359 --> 00:12:25,100
Speaker SPEAKER_03: And they couldn't do it.

113
00:12:25,120 --> 00:12:26,121
Speaker SPEAKER_03: They couldn't even get close.

114
00:12:26,581 --> 00:12:27,945
Speaker SPEAKER_03: And now neural networks can do that.

115
00:12:28,465 --> 00:12:30,688
Speaker SPEAKER_03: And they're trained with this backpropagation algorithm.

116
00:12:30,708 --> 00:12:36,337
Speaker SPEAKER_03: And the backpropagation learns a hierarchy of features to extract from the image that do enable you to see what's in the image.

117
00:12:38,596 --> 00:12:49,328
Speaker SPEAKER_03: And in 2012, Alex Khrushchevsky and Ilya Shutskova, with a little bit of help from me, developed a network that was way better than existing computer vision systems.

118
00:12:49,970 --> 00:13:00,302
Speaker SPEAKER_03: And something happened that's very rare in science, which is the leading computer vision experts, who'd always said neural nets will never be able to do this, said, wow, it really works.

119
00:13:00,763 --> 00:13:03,206
Speaker SPEAKER_03: And they changed what they did and started using neural nets.

120
00:13:03,225 --> 00:13:04,908
Speaker SPEAKER_03: That's not how scientists normally behave.

121
00:13:05,128 --> 00:13:08,011
Speaker SPEAKER_03: And it's certainly not how they behave in linguistics.

122
00:13:09,798 --> 00:13:11,039
Speaker SPEAKER_03: So that brings us to language.

123
00:13:12,041 --> 00:13:20,995
Speaker SPEAKER_03: Many people in the symbolic AI community said, you'll never be able to deal with language by using hierarchies feature detectors.

124
00:13:21,034 --> 00:13:22,197
Speaker SPEAKER_03: It just won't work.

125
00:13:22,216 --> 00:13:24,500
Speaker SPEAKER_03: You can see a quote on my web page saying exactly that.

126
00:13:25,341 --> 00:13:31,169
Speaker SPEAKER_03: And I couldn't resist it, so I got GPT-4 to explain in detail what was wrong with the quote.

127
00:13:31,811 --> 00:13:39,062
Speaker SPEAKER_03: So we now have GPT-4 explaining to a linguist why what they said is wrong about what neural networks can and can't do.

128
00:13:39,667 --> 00:13:48,532
Speaker SPEAKER_03: Linguists were misled for several generations by someone called Chomsky, who actually also got this prestigious medal.

129
00:13:52,042 --> 00:13:53,304
Speaker SPEAKER_03: So it doesn't last.

130
00:13:54,854 --> 00:13:58,118
Speaker SPEAKER_03: He had a crazy theory that language is not learned.

131
00:13:58,818 --> 00:14:00,500
Speaker SPEAKER_03: And he managed to convince lots of people of that.

132
00:14:01,001 --> 00:14:02,484
Speaker SPEAKER_03: And on the face of it, it's just nuts.

133
00:14:02,524 --> 00:14:03,504
Speaker SPEAKER_03: Language is obviously learned.

134
00:14:04,166 --> 00:14:07,870
Speaker SPEAKER_03: And now these big neural networks learn language, and they didn't need any innate structure.

135
00:14:07,890 --> 00:14:09,812
Speaker SPEAKER_03: They just started from random weights and a lot of data.

136
00:14:10,614 --> 00:14:13,057
Speaker SPEAKER_03: And Chomsky is still saying, but this isn't really language.

137
00:14:13,077 --> 00:14:13,717
Speaker SPEAKER_03: This doesn't count.

138
00:14:13,738 --> 00:14:14,379
Speaker SPEAKER_03: It's not right.

139
00:14:15,039 --> 00:14:22,009
Speaker SPEAKER_03: And lots of statisticians and cognitive scientists also said, you'll never learn language in a big net like this.

140
00:14:24,384 --> 00:14:28,455
Speaker SPEAKER_03: So Chomsky never really had a theory of meaning, it was all about syntax.

141
00:14:28,773 --> 00:14:33,558
Speaker SPEAKER_03: And if you think about meaning, there's two very different theories of meaning.

142
00:14:34,259 --> 00:14:42,971
Speaker SPEAKER_03: There's a structuralist theory that the symbolic Arab people believed in, and most linguists believe in, which is the meaning of a word comes from its relationships to other words.

143
00:14:43,792 --> 00:14:52,923
Speaker SPEAKER_03: So if you want to capture the meaning of a word, you need to make a relational graph that has links to other words, and maybe labels on the link saying how it's related to those other words.

144
00:14:53,504 --> 00:14:54,645
Speaker SPEAKER_03: That would be a semantic net.

145
00:14:55,386 --> 00:14:57,750
Speaker SPEAKER_03: And that's what you need to capture meaning.

146
00:14:57,730 --> 00:15:04,100
Speaker SPEAKER_03: And then there's a very different theory, which comes from psychology in the 1930s, which is the meaning of a word is a big set of features.

147
00:15:05,162 --> 00:15:07,767
Speaker SPEAKER_03: And words that have similar meanings have similar sets of features.

148
00:15:08,607 --> 00:15:10,471
Speaker SPEAKER_03: And those two theories look totally different.

149
00:15:11,572 --> 00:15:14,437
Speaker SPEAKER_03: But actually, you can unify those two theories.

150
00:15:15,318 --> 00:15:20,768
Speaker SPEAKER_03: And I think the first model to do that was a model I produced in 1985.

151
00:15:20,865 --> 00:15:27,054
Speaker SPEAKER_03: which was a tiny language model, and it had a lot in common with the big language models now.

152
00:15:27,916 --> 00:15:37,168
Speaker SPEAKER_03: It learned by trying to predict the next word, and it learned features for each word, and interactions between those features, so that you could predict the features of the next word.

153
00:15:38,470 --> 00:15:47,042
Speaker SPEAKER_03: And the important thing is, all of the knowledge was in what features to assign to a word, and how these features of different words should interact.

154
00:15:47,544 --> 00:15:51,710
Speaker SPEAKER_03: It didn't store any sentences, but it could reconstruct sentences.

155
00:15:51,730 --> 00:15:54,633
Speaker SPEAKER_03: It could generate sentences by repeatedly predicting the next word.

156
00:15:55,234 --> 00:15:56,816
Speaker SPEAKER_03: And that's how these big language models work.

157
00:15:57,035 --> 00:15:58,697
Speaker SPEAKER_03: They don't actually store any text.

158
00:15:59,479 --> 00:16:12,253
Speaker SPEAKER_03: They learn to extract from text features to assign to words and interactions between those features so that it can predict the features of the next word.

159
00:16:12,537 --> 00:16:17,607
Speaker SPEAKER_03: This tiny language model wasn't designed to help with engineering.

160
00:16:17,707 --> 00:16:22,879
Speaker SPEAKER_03: This tiny language model was designed to explain how people can get meaning out of language.

161
00:16:23,360 --> 00:16:24,722
Speaker SPEAKER_03: So it was actually a model of people.

162
00:16:25,565 --> 00:16:32,519
Speaker SPEAKER_03: And if you see people telling you, these models aren't like us, they work completely differently from us, ask them, well, how do we work?

163
00:16:32,652 --> 00:16:37,702
Speaker SPEAKER_03: And if they're linguists, they'll tell you we have symbolic rules and rules for manipulating symbolic expressions.

164
00:16:38,264 --> 00:16:43,897
Speaker SPEAKER_03: But actually, the people who say they're different from us don't actually have a model of how we work.

165
00:16:43,917 --> 00:16:45,480
Speaker SPEAKER_03: So I don't know how they know they're different.

166
00:16:46,041 --> 00:16:51,352
Speaker SPEAKER_03: Whereas other neural net people do have a model of how we work, and it's just like this.

167
00:16:51,669 --> 00:16:52,610
Speaker SPEAKER_03: Not exactly like this.

168
00:16:52,991 --> 00:17:01,743
Speaker SPEAKER_03: So I'm going to go through a small model in detail, because I always believe that understanding a small concrete thing is much better than waffly abstractions.

169
00:17:02,504 --> 00:17:06,828
Speaker SPEAKER_03: Waffly abstractions are great, but to really understand things, you want a little concrete example.

170
00:17:07,690 --> 00:17:09,932
Speaker SPEAKER_03: So here's two family trees.

171
00:17:10,673 --> 00:17:13,257
Speaker SPEAKER_03: There's some English people and there's some Italian people.

172
00:17:13,877 --> 00:17:15,299
Speaker SPEAKER_03: And this is 1950s.

173
00:17:15,460 --> 00:17:18,242
Speaker SPEAKER_03: It's very, very simple families.

174
00:17:18,284 --> 00:17:22,269
Speaker SPEAKER_03: No divorce, no adoption, no same-sex marriage.

175
00:17:23,230 --> 00:17:25,613
Speaker SPEAKER_03: The whole thing is very, very conventional.

176
00:17:26,854 --> 00:17:30,218
Speaker SPEAKER_03: And you may notice the trees are kind of analogous to one another.

177
00:17:30,238 --> 00:17:31,180
Speaker SPEAKER_03: They have the same structure.

178
00:17:32,240 --> 00:17:37,067
Speaker SPEAKER_03: And we're going to turn those relational trees into a bunch of triples.

179
00:17:38,228 --> 00:17:47,558
Speaker SPEAKER_03: So from those relational trees, we can write down the information by having some relational terms like son, daughter, nephew, niece, mother, and so on.

180
00:17:47,578 --> 00:17:56,929
Speaker SPEAKER_03: And then we can make triples, like Colin has father James, or Colin has mother Victoria, or James has wife Victoria.

181
00:17:58,250 --> 00:18:01,974
Speaker SPEAKER_03: Now, from Colin has mother Victoria and

182
00:18:03,118 --> 00:18:08,468
Speaker SPEAKER_03: Let's see, from Colin has father James and Colin has mother Victoria, we can infer James has wife Victoria.

183
00:18:09,088 --> 00:18:18,222
Speaker SPEAKER_03: And so the symbolic people thought you had representations of those symbolic strings in your head and you had a rule in your head that allowed you to derive new symbolic strings from the old ones.

184
00:18:19,164 --> 00:18:20,727
Speaker SPEAKER_03: And the rule might look something like this.

185
00:18:20,747 --> 00:18:26,876
Speaker SPEAKER_03: If X has mother Y and Y has husband Z, then X has father Z. And that's how they thought it all worked.

186
00:18:28,359 --> 00:18:30,442
Speaker SPEAKER_03: And what I did was made a neural network

187
00:18:30,997 --> 00:18:38,219
Speaker SPEAKER_03: where you can learn that knowledge by just tinkering with the weights of the network, but it doesn't have any strings stored inside it.

188
00:18:38,279 --> 00:18:40,145
Speaker SPEAKER_03: No symbolic expression stored inside the network.

189
00:18:40,465 --> 00:18:42,633
Speaker SPEAKER_03: Inside it's all features and interactions of features.

190
00:18:44,157 --> 00:18:44,819
Speaker SPEAKER_03: And

191
00:18:45,458 --> 00:18:50,583
Speaker SPEAKER_03: The thing about doing it in a neural network is, for discrete rules, you might be able to do it symbolically.

192
00:18:50,923 --> 00:18:53,846
Speaker SPEAKER_03: But most of what we know isn't exactly right.

193
00:18:54,146 --> 00:18:56,869
Speaker SPEAKER_03: You have to allow for a lot of exceptions.

194
00:18:57,289 --> 00:19:02,536
Speaker SPEAKER_03: And as soon as you get exceptions to rules, you're much better off finding these rules by using a big neural net.

195
00:19:04,457 --> 00:19:06,118
Speaker SPEAKER_03: So the neural net I used looked like this.

196
00:19:07,299 --> 00:19:09,863
Speaker SPEAKER_03: You had two inputs.

197
00:19:10,163 --> 00:19:14,688
Speaker SPEAKER_03: One was a symbol, just a discrete symbol representing the name of a person.

198
00:19:15,308 --> 00:19:18,652
Speaker SPEAKER_03: And the other was a discrete symbol, the name of a relationship.

199
00:19:20,074 --> 00:19:24,357
Speaker SPEAKER_03: And the output we wanted was the name of the person to whom they were related like that.

200
00:19:25,539 --> 00:19:27,442
Speaker SPEAKER_03: So that was what the training data was like.

201
00:19:29,104 --> 00:19:37,673
Speaker SPEAKER_03: And what the neural net learns to do is convert the name of a person into a bunch of features that capture the essence of that person.

202
00:19:38,547 --> 00:19:41,333
Speaker SPEAKER_03: the essence insofar as these family trees are concerned.

203
00:19:42,635 --> 00:19:52,076
Speaker SPEAKER_03: And once it's done that for the person and the relationship, it makes those sets of features interact in the middle there, so as to predict the features of the output person.

204
00:19:52,395 --> 00:19:55,542
Speaker SPEAKER_03: And from the features of the output person, it predicts who the output person is.

205
00:19:56,704 --> 00:19:58,747
Speaker SPEAKER_03: And that worked pretty well.

206
00:19:58,767 --> 00:20:11,481
Speaker SPEAKER_03: It could learn, and it could not only reproduce the facts it was given, that is, the information in the learned weights was enough so that if you gave it a training example, it would get the right answer, but it could also generalize.

207
00:20:11,842 --> 00:20:20,511
Speaker SPEAKER_03: You could give it examples it had never seen before, people it had seen before, and relationships it had seen before, but combinations it hadn't seen before, and it could get the right answer.

208
00:20:21,112 --> 00:20:22,413
Speaker SPEAKER_03: And the question was, how did it do that?

209
00:20:23,194 --> 00:20:26,137
Speaker SPEAKER_03: Well, it learned what we would think of as natural features.

210
00:20:26,876 --> 00:20:31,305
Speaker SPEAKER_03: So, for a person it learned features like what nationality are they?

211
00:20:31,826 --> 00:20:35,573
Speaker SPEAKER_03: Because if you know the input person's English, you know the answer's English.

212
00:20:36,233 --> 00:20:38,778
Speaker SPEAKER_03: If the input person's Italian, the answer's Italian.

213
00:20:38,798 --> 00:20:40,382
Speaker SPEAKER_03: Like I said, they were very simple families.

214
00:20:43,027 --> 00:20:45,932
Speaker SPEAKER_03: It was a tiny net, it only had six neurons in the bottleneck.

215
00:20:46,958 --> 00:20:49,362
Speaker SPEAKER_03: One would represent nationality.

216
00:20:49,862 --> 00:20:52,226
Speaker SPEAKER_03: Another would represent what generation the person was.

217
00:20:52,906 --> 00:20:55,530
Speaker SPEAKER_03: Another would represent what branch of the family tree they were in.

218
00:20:56,392 --> 00:21:00,377
Speaker SPEAKER_03: And the generation feature would have three values because there were three generations there.

219
00:21:00,397 --> 00:21:02,641
Speaker SPEAKER_03: It could be the lowest or the middle or the top generation.

220
00:21:04,083 --> 00:21:06,807
Speaker SPEAKER_03: And that generation feature is only useful

221
00:21:07,157 --> 00:21:15,228
Speaker SPEAKER_03: If for the relationships, you learn features like this relationship requires the output person to be one generation up from the input person.

222
00:21:15,728 --> 00:21:17,651
Speaker SPEAKER_03: So a relationship like uncle would be like that.

223
00:21:19,413 --> 00:21:29,384
Speaker SPEAKER_03: And then if it knows the generation of the input person, and it knows the relationship was one generation up, it can predict the generation of the output person, and that will help it get the right person.

224
00:21:29,634 --> 00:21:34,503
Speaker SPEAKER_03: So that's how it worked and it had actually discovered the symbolic rules that symbolic people believed in.

225
00:21:34,924 --> 00:21:38,431
Speaker SPEAKER_03: So none of the symbolic people complained this wasn't actually learning.

226
00:21:38,892 --> 00:21:40,915
Speaker SPEAKER_03: They said, okay, it learns, but it's a stupid way to learn it.

227
00:21:42,858 --> 00:21:44,843
Speaker SPEAKER_03: If you look at the large language models now,

228
00:21:46,207 --> 00:21:50,636
Speaker SPEAKER_03: They can be viewed, well, I view them of course, as descendants of this little model.

229
00:21:51,357 --> 00:21:55,867
Speaker SPEAKER_03: And they're just this little model massively scaled up and made massively more complicated.

230
00:21:56,509 --> 00:21:57,731
Speaker SPEAKER_03: So they have many more words.

231
00:21:58,192 --> 00:22:01,380
Speaker SPEAKER_03: They work for natural language, not just these toy examples.

232
00:22:01,359 --> 00:22:06,965
Speaker SPEAKER_03: They use many more layers of neurons, because you can't just go from a symbol to the meaning.

233
00:22:07,507 --> 00:22:12,392
Speaker SPEAKER_03: You might have a symbol like May, and May could mean a month, or it could mean a modal, or it could be a woman's name.

234
00:22:13,012 --> 00:22:14,974
Speaker SPEAKER_03: So you have to use contact to disambiguate it.

235
00:22:14,994 --> 00:22:16,997
Speaker SPEAKER_03: So you go up through layers disambiguating things.

236
00:22:17,718 --> 00:22:18,679
Speaker SPEAKER_03: So they've got more layers.

237
00:22:19,118 --> 00:22:22,923
Speaker SPEAKER_03: And the interactions between the features of different words are much more complicated.

238
00:22:23,584 --> 00:22:25,506
Speaker SPEAKER_03: But essentially it's in the same class of models.

239
00:22:26,647 --> 00:22:28,869
Speaker SPEAKER_03: And when they learn,

240
00:22:30,250 --> 00:22:36,038
Speaker SPEAKER_03: They store all the information in the weights of the interactions between features.

241
00:22:37,859 --> 00:22:51,178
Speaker SPEAKER_03: Now, linguists say they're just glorified autocomplete, and they're just using statistical tricks, they're just pastiching together text, but remember, they don't store any text.

242
00:22:51,817 --> 00:22:57,845
Speaker SPEAKER_03: The autocomplete objection is crazy because what they're appealing to is the idea of an old-fashioned autocomplete.

243
00:22:58,546 --> 00:23:04,032
Speaker SPEAKER_03: Old-fashioned autocompletes will store strings of words, so they store fish and chips, that's a frequent string.

244
00:23:04,453 --> 00:23:09,039
Speaker SPEAKER_03: So if you store fish and, you then say the next word's likely to be chips because that's a very frequent string.

245
00:23:10,301 --> 00:23:12,744
Speaker SPEAKER_03: Autocomplete in these systems doesn't work like that at all.

246
00:23:13,125 --> 00:23:18,853
Speaker SPEAKER_03: It converts the words into features and uses interactions between features to make the prediction.

247
00:23:19,963 --> 00:23:36,465
Speaker SPEAKER_03: So, the way LLMs work, and the way we work, is we see a lot of text, or we hear a lot of strings of words, we learn features for the words, we learn interactions between those features, and that is what understanding is.

248
00:23:37,507 --> 00:23:40,671
Speaker SPEAKER_03: And these models are doing understanding in just the same way we are.

249
00:23:44,155 --> 00:23:48,942
Speaker SPEAKER_03: And then another argument that linguists give you is, these things hallucinate, so they don't really understand what they're saying.

250
00:23:49,765 --> 00:23:55,941
Speaker SPEAKER_03: Actually, it shouldn't be called hallucination for a language model, it should be called confabulation.

251
00:23:56,281 --> 00:24:00,372
Speaker SPEAKER_03: And that's been studied in psychology since the 1930s, and people do it all the time.

252
00:24:01,054 --> 00:24:04,382
Speaker SPEAKER_03: So the fact that they confabulate actually makes them much more like us.

253
00:24:05,272 --> 00:24:13,099
Speaker SPEAKER_03: And most people think we, a memory is like a kind of file that you put somewhere and then you go and retrieve later, like in a computer.

254
00:24:13,500 --> 00:24:14,981
Speaker SPEAKER_03: It's not like that at all in a person.

255
00:24:15,823 --> 00:24:17,384
Speaker SPEAKER_03: Memories are always reconstructed.

256
00:24:18,065 --> 00:24:21,887
Speaker SPEAKER_03: If you're remembering something that happened recently, you'll reconstruct it fairly accurately.

257
00:24:22,808 --> 00:24:28,193
Speaker SPEAKER_03: If you're remembering something a long time ago, you'll often get all the details wrong and won't realize it and you'll be confident about them.

258
00:24:29,275 --> 00:24:32,917
Speaker SPEAKER_03: There's a wonderful example of that, which is John Dean's memory.

259
00:24:33,488 --> 00:24:42,011
Speaker SPEAKER_03: So at Watergate John Dean testified under oath about what happened in various meetings in the White House and he was wrong about all the details.

260
00:24:42,673 --> 00:24:48,830
Speaker SPEAKER_03: He said Holderman said this, well no it was Ehrlichman said that and Holderman wasn't even in that meeting.

261
00:24:50,431 --> 00:24:53,414
Speaker SPEAKER_03: What's clear about his testimony is he was trying to tell the truth.

262
00:24:53,976 --> 00:24:57,461
Speaker SPEAKER_03: He conveyed the gist of what went on when they tried to cover up Watergate.

263
00:24:58,242 --> 00:25:03,109
Speaker SPEAKER_03: But all the details he purported to remember were wrong, not all of them, but a lot of them.

264
00:25:03,490 --> 00:25:06,275
Speaker SPEAKER_03: Ulrich Neisser had a lovely paper showing that.

265
00:25:07,276 --> 00:25:14,446
Speaker SPEAKER_03: And it's a lovely example of how things John Dean thought he could remember, when you listen to the tapes, he was just wrong.

266
00:25:16,189 --> 00:25:18,732
Speaker SPEAKER_03: But he had the essence of it.

267
00:25:18,898 --> 00:25:22,805
Speaker SPEAKER_03: Now, at present, chatbots are worse than us at confabulating.

268
00:25:23,046 --> 00:25:25,910
Speaker SPEAKER_03: They confabulate more often than people and they don't know they're doing it.

269
00:25:26,872 --> 00:25:27,992
Speaker SPEAKER_03: They're getting better all the time.

270
00:25:28,775 --> 00:25:32,601
Speaker SPEAKER_03: And I think in not very long, chatbots won't be much worse than us at confabulating.

271
00:25:33,281 --> 00:25:38,048
Speaker SPEAKER_03: But the fact that they confabulate doesn't show they're not understanding or they're different from us.

272
00:25:38,450 --> 00:25:39,912
Speaker SPEAKER_03: It shows they're very like us.

273
00:25:41,578 --> 00:25:47,390
Speaker SPEAKER_03: And so that concludes my attempt to show you that these big chatbots are very like us.

274
00:25:47,871 --> 00:25:50,536
Speaker SPEAKER_03: They're not like normal computer software, they're much more like people.

275
00:25:51,577 --> 00:25:55,505
Speaker SPEAKER_03: And that leads to a whole bunch of AI risks, which I decided not to talk about.

276
00:25:55,545 --> 00:26:01,856
Speaker SPEAKER_03: They're in this paper on archive, but I suspect some of you will want to ask questions about the AI risks.

277
00:26:02,617 --> 00:26:03,519
Speaker SPEAKER_03: So I'm now done.

278
00:26:07,734 --> 00:26:15,009
Speaker SPEAKER_05: Thank you, Geoffrey, for your very thought-provoking speech and very accessible introduction to AI for everybody here.

279
00:26:15,029 --> 00:26:20,060
Speaker SPEAKER_05: I think it'll be more memorable than John Dean's was for everybody.

280
00:26:20,541 --> 00:26:26,654
Speaker SPEAKER_05: Now I'd like to invite the UCD President, Professor Orla Feely, to address

281
00:26:30,144 --> 00:26:34,090
Speaker SPEAKER_00: Thank you, Geoffrey, for those very clear and interesting words.

282
00:26:34,251 --> 00:26:39,240
Speaker SPEAKER_00: And thank you, Andrew, for your nomination of Geoffrey for the Ulysses Medal and also for delivering the citation.

283
00:26:40,260 --> 00:26:56,507
Speaker SPEAKER_00: As Andrew mentioned, we'll shortly hear again from Andrew and Geoffrey, along with UCD Vice President for Research, Innovation and Impact, Professor Kate Robson-Brown, in a fireside chat where you'll get to ask all the questions that you presumably have stored up at this stage.

284
00:26:56,487 --> 00:27:05,881
Speaker SPEAKER_00: I'll be brief so, but I do want to highlight some aspects of Geoffrey Hinton's work and background that for me make this award particularly fitting.

285
00:27:07,623 --> 00:27:12,150
Speaker SPEAKER_00: The first of these speaks to the conduct of research that changes the world.

286
00:27:13,612 --> 00:27:25,548
Speaker SPEAKER_00: When I was a graduate student in Berkeley in the 1980s, I encountered what was then the second major wave of research into artificial neural networks with Geoffrey Hinton, one of the leaders.

287
00:27:26,660 --> 00:27:47,246
Speaker SPEAKER_00: That meticulous, steady research endeavour continued over decades, bringing about change, first gradually and then very suddenly, when the technological capability had developed to the point where it could deliver on the power long anticipated in the algorithms.

288
00:27:49,509 --> 00:27:56,498
Speaker SPEAKER_00: Geoffrey has spoken of the difficulty of maintaining a very high-level research career over that long time,

289
00:27:57,118 --> 00:28:02,547
Speaker SPEAKER_00: describing in particular the challenges that he faced when he was a widower with young children.

290
00:28:04,088 --> 00:28:13,863
Speaker SPEAKER_00: He has highlighted the importance of mentorship, and we are very grateful that he took the time this afternoon to do an Ask Me Anything session with our students.

291
00:28:15,545 --> 00:28:26,079
Speaker SPEAKER_00: His research has spanned industry and academia, and has brought together domains such as psychology and physiology, as well as engineering and computer science,

292
00:28:26,903 --> 00:28:35,771
Speaker SPEAKER_00: He has also, crucially and famously, spoken very publicly about the societal implications of artificial intelligence.

293
00:28:37,634 --> 00:28:55,492
Speaker SPEAKER_00: For us as a research community in University College Dublin, it is important to hear and to hold these messages of the importance of fundamental understanding, of vision and perseverance, of mentorship, collaboration, and open questioning.

294
00:28:57,817 --> 00:29:01,363
Speaker SPEAKER_00: The second aspect that I would like to highlight is the link to Ireland.

295
00:29:02,924 --> 00:29:16,605
Speaker SPEAKER_00: Andrew spoke of Geoffrey's family links to George Boole and Mary Everest Boole and their multi-talented descendants, of whom we in Ireland, and particularly our friends in Cork, are very proud.

296
00:29:18,709 --> 00:29:26,559
Speaker SPEAKER_00: In today's Ireland, we have a country that has worked very hard to develop an enviable presence on the global technology landscape,

297
00:29:26,911 --> 00:29:34,080
Speaker SPEAKER_00: including the ability not to be taken for granted, to keep up with successive waves of technological progress.

298
00:29:36,243 --> 00:29:44,075
Speaker SPEAKER_00: What does the prospect of widespread adoption of artificial intelligence mean for the technology sector and other sectors in Ireland?

299
00:29:45,277 --> 00:29:48,240
Speaker SPEAKER_00: Where will the opportunities reside and the challenges?

300
00:29:49,843 --> 00:29:55,872
Speaker SPEAKER_00: What are the demands on computing and on other infrastructures and on the talent pipeline?

301
00:29:57,640 --> 00:30:14,105
Speaker SPEAKER_00: And for us in UCD, a university that has more than any other supported and enabled national progress, how do we address the opportunities and challenges of artificial intelligence in our teaching and learning and in our research and innovation?

302
00:30:15,926 --> 00:30:25,622
Speaker SPEAKER_00: We have outstanding work already underway in this area, not only from Andrew and his colleagues in computer science, but across the university.

303
00:30:26,345 --> 00:30:35,019
Speaker SPEAKER_00: as seen, for example, by the award-winning work of Professor Patricia McGuire and her colleagues on the application of AI in the treatment of preeclampsia.

304
00:30:36,803 --> 00:30:48,403
Speaker SPEAKER_00: As we embark on the process of forming our next university strategy over the coming months, the question of how artificial intelligence will figure in that strategy is a very important one for us.

305
00:30:50,864 --> 00:30:58,594
Speaker SPEAKER_00: And the third aspect that makes this award so fitting relates to where the world is currently poised in relation to artificial intelligence.

306
00:31:00,395 --> 00:31:09,268
Speaker SPEAKER_00: James Joyce's novel Ulysses, after which this award is named, is one of the great examples of human creative intelligence.

307
00:31:10,750 --> 00:31:20,162
Speaker SPEAKER_00: We are now challenged to reflect on our understanding of intelligence, of creativity, and even of what it means to be human.

308
00:31:21,104 --> 00:31:26,369
Speaker SPEAKER_00: as we seek to chart a course through ever greater adoption of artificial intelligence.

309
00:31:27,911 --> 00:31:30,733
Speaker SPEAKER_00: For some, this prospect is exhilarating.

310
00:31:31,615 --> 00:31:34,718
Speaker SPEAKER_00: For others, it may be daunting or even dystopian.

311
00:31:36,559 --> 00:31:47,432
Speaker SPEAKER_00: Either way, we cannot chart our course by wishing away the power of technological disruption or by succumbing to that power without challenge.

312
00:31:49,083 --> 00:32:01,659
Speaker SPEAKER_00: We cannot at this point foresee the full impact, the limitations, the applications and the implications of artificial intelligence as it develops over the coming years and decades.

313
00:32:03,340 --> 00:32:15,916
Speaker SPEAKER_00: What we do know is that we need to understand in as much as we can the capabilities and the limits of this technology to bring all disciplines and viewpoints into the conversation.

314
00:32:16,538 --> 00:32:20,525
Speaker SPEAKER_00: so that we can apply artificial intelligence for the good of humanity.

315
00:32:22,407 --> 00:32:36,791
Speaker SPEAKER_00: In this, it is enormously important that we hear the voice of those such as Geoffrey Hinton, who have done so much to advance this technology and to inform public debate about its safe adoption.

316
00:32:38,240 --> 00:32:47,818
Speaker SPEAKER_00: For all of these reasons, it is a great honour to have Geoffrey with us today to celebrate his very distinguished career and to listen to his words.

317
00:32:49,102 --> 00:32:52,749
Speaker SPEAKER_00: Thank you all for joining us tonight to be part of this conversation.

318
00:32:53,230 --> 00:32:54,392
Speaker SPEAKER_00: Go raibh maith agaibh go lÃ©ir.

319
00:32:57,763 --> 00:32:58,664
Speaker SPEAKER_05: Thank you, Professor Fahey.

320
00:32:59,185 --> 00:33:08,417
Speaker SPEAKER_05: I would now like to invite Professor Kate Robson-Brown, UCD Vice President for Research, Impact and Innovation, to join us on stage for a fireside chat.

321
00:33:08,438 --> 00:33:16,147
Speaker SPEAKER_14: Thank you very much, and thank you, Geoffrey, for such a wonderful talk.

322
00:33:16,167 --> 00:33:27,163
Speaker SPEAKER_14: I can... Well, I was going to say I could see... I could see when I walked in that we have here a pretty diverse audience, and I hope you can see that that's a reflection of the warmth

323
00:33:27,380 --> 00:33:29,883
Speaker SPEAKER_14: with which we want to welcome you here at UCD.

324
00:33:31,403 --> 00:33:40,374
Speaker SPEAKER_14: I can see many younger and early career researchers in the audience as well, and I know you took time this afternoon to talk to a range of our students.

325
00:33:40,394 --> 00:33:50,964
Speaker SPEAKER_14: So perhaps I thought it'd be nice to start there and think about what influence your own learning, maybe at school and college, has had on your later career.

326
00:33:51,025 --> 00:33:56,931
Speaker SPEAKER_14: What is the thing that you look back on and say, that was really important to me, that's where I started?

327
00:33:57,973 --> 00:33:59,175
Speaker SPEAKER_03: I guess there's several things.

328
00:33:59,656 --> 00:34:00,778
Speaker SPEAKER_03: One thing was doing physics.

329
00:34:02,000 --> 00:34:04,884
Speaker SPEAKER_03: Physics gave me an idea of what it means to explain something.

330
00:34:05,905 --> 00:34:13,539
Speaker SPEAKER_03: And when I did psychology, after having done physics, I didn't feel it was really explaining what was going on.

331
00:34:14,300 --> 00:34:16,963
Speaker SPEAKER_03: I felt you really need to get at the real mechanisms.

332
00:34:17,585 --> 00:34:18,606
Speaker SPEAKER_03: So that was important.

333
00:34:18,867 --> 00:34:24,014
Speaker SPEAKER_03: Another thing was when I was a teenager, a friend of mine at school told me about relays.

334
00:34:24,146 --> 00:34:28,992
Speaker SPEAKER_03: and about how you could get one current to switch on another current.

335
00:34:29,378 --> 00:34:49,454
Speaker SPEAKER_03: And so I actually spent quite a lot of time cutting the heads off six-inch nails and winding copper wire around them, and then taking an old-fashioned razor blade and splitting it in two so that you had a flexible piece of metal, and putting a piece of copper wire around the end of the razor blade, and a piece of copper wire across where the head of the six-inch nail would be.

336
00:34:49,855 --> 00:34:51,757
Speaker SPEAKER_03: And then when you put the current in, you make a connection.

337
00:34:51,817 --> 00:34:53,938
Speaker SPEAKER_03: And that way, you can make a little relay.

338
00:34:53,958 --> 00:34:58,963
Speaker SPEAKER_03: And then I would wire up these relays in different ways and discover you can make things like oscillators.

339
00:34:58,943 --> 00:35:01,326
Speaker SPEAKER_03: That was a lot of fun.

340
00:35:01,505 --> 00:35:02,867
Speaker SPEAKER_14: So what age are you at this point?

341
00:35:03,509 --> 00:35:04,610
Speaker SPEAKER_03: I'm probably about 15 then.

342
00:35:04,750 --> 00:35:09,516
Speaker SPEAKER_03: I guess I could have just gone out and bought some relays.

343
00:35:10,958 --> 00:35:11,759
Speaker SPEAKER_14: Where's the fun in that?

344
00:35:11,778 --> 00:35:14,322
Speaker SPEAKER_03: It was more fun making them.

345
00:35:16,525 --> 00:35:23,434
Speaker SPEAKER_03: Yeah, I also think coming from a family where science was highly valued was important.

346
00:35:25,396 --> 00:35:26,858
Speaker SPEAKER_14: Another thing that strikes me

347
00:35:27,157 --> 00:35:37,710
Speaker SPEAKER_14: thinking about your own career journey is the fact that your research and your interest sometimes is sat in between the traditional silos of disciplines.

348
00:35:37,731 --> 00:35:39,472
Speaker SPEAKER_14: Do you think that's a fair statement?

349
00:35:39,494 --> 00:35:54,211
Speaker SPEAKER_03: Yes, when I was at high school I got very interested in how the brain works because I had a brilliant friend there who was much smarter than me who told me the brain stores memories like a hologram and that got me interested in it.

350
00:35:56,807 --> 00:35:59,271
Speaker SPEAKER_03: So he was a big influence on me.

351
00:35:59,291 --> 00:36:00,994
Speaker SPEAKER_03: That was another big influence.

352
00:36:02,175 --> 00:36:02,675
Speaker SPEAKER_14: Interesting.

353
00:36:03,076 --> 00:36:11,286
Speaker SPEAKER_14: And if you were looking back at your career, what aspects of it are you most proud of?

354
00:36:11,367 --> 00:36:19,878
Speaker SPEAKER_14: And I'm not necessarily just thinking about your discoveries, but maybe other aspects of your career as well, the people that you've worked with, the teams that you've built.

355
00:36:20,331 --> 00:36:26,884
Speaker SPEAKER_03: I guess I'm proud of the fact that I stuck with neural networks even when people said they were rubbish, which was for about the first 40 years.

356
00:36:26,903 --> 00:36:40,989
Speaker SPEAKER_03: But the intellectual achievement I'm most proud of is Boltz machines, which were an alternative to back propagation, and they're probably wrong.

357
00:36:41,273 --> 00:36:45,702
Speaker SPEAKER_03: They're not how the brain works and they don't work as well as back propagation.

358
00:36:46,242 --> 00:36:56,362
Speaker SPEAKER_03: But they're such a beautiful theory that I'm very proud of it even though it failed.

359
00:36:56,342 --> 00:37:01,992
Speaker SPEAKER_05: advice for early-stage career researchers working in academia now?

360
00:37:03,434 --> 00:37:08,961
Speaker SPEAKER_05: What would you say to people who go, well, there's no compute power available to academics?

361
00:37:10,103 --> 00:37:13,568
Speaker SPEAKER_05: How do the early-stage researchers contribute now?

362
00:37:13,949 --> 00:37:23,284
Speaker SPEAKER_03: Yeah, I don't really know the answer to the compute power except that I'm trying to persuade your government to provide more compute power.

363
00:37:23,304 --> 00:37:24,105
Speaker SPEAKER_03: I think

364
00:37:24,996 --> 00:37:28,282
Speaker SPEAKER_03: There's still lots of research to be done other than building these massive models.

365
00:37:29,163 --> 00:37:33,809
Speaker SPEAKER_03: And one piece of advice I give to researchers is trust your intuitions.

366
00:37:34,610 --> 00:37:42,501
Speaker SPEAKER_03: So if you, basically a lot of good research comes from having a sort of vague intuition that there's something wrong with the way people are doing something.

367
00:37:42,842 --> 00:37:46,768
Speaker SPEAKER_03: There's something just not quite right about it, and then pursuing that.

368
00:37:46,748 --> 00:37:56,021
Speaker SPEAKER_03: So an analogy I like to use is when you read a detective story, often sort of in the first chapter, someone does something and when you read it, it's just slightly off.

369
00:37:56,400 --> 00:37:57,802
Speaker SPEAKER_03: It's slightly weird they did that.

370
00:37:58,344 --> 00:38:00,666
Speaker SPEAKER_03: And it turns out later that's the key to the whole story.

371
00:38:01,188 --> 00:38:03,590
Speaker SPEAKER_03: But you typically just read through it and don't notice it.

372
00:38:03,610 --> 00:38:04,472
Speaker SPEAKER_03: You just feel slightly weird.

373
00:38:05,152 --> 00:38:06,614
Speaker SPEAKER_03: A lot of science I think is like that.

374
00:38:06,894 --> 00:38:16,148
Speaker SPEAKER_03: And if you have an intuition that people are doing things wrong in some funny way, work on that and don't give up on it until you know why that intuition is no good.

375
00:38:16,128 --> 00:38:20,356
Speaker SPEAKER_03: And if it takes you 20 years to discover why that intuition is no good, then so be it.

376
00:38:20,856 --> 00:38:23,561
Speaker SPEAKER_03: But either you have good intuitions or you don't.

377
00:38:24,463 --> 00:38:26,907
Speaker SPEAKER_03: If you have good intuitions, you should trust them.

378
00:38:27,469 --> 00:38:30,554
Speaker SPEAKER_03: And if you don't have good intuitions, it doesn't really matter what you do.

379
00:38:30,936 --> 00:38:38,489
Speaker SPEAKER_05: So you might as well trust them.

380
00:38:39,027 --> 00:38:48,103
Speaker SPEAKER_05: There's a lot of early career researchers here, but there's also a wider cross-section of Irish society here as well.

381
00:38:48,222 --> 00:38:51,427
Speaker SPEAKER_05: So a question more to do with the AI policy.

382
00:38:52,150 --> 00:39:02,266
Speaker SPEAKER_05: What can academia and government and industry and philanthropists, what can they be doing together to change the long-term direction of AI to be more fair, safe?

383
00:39:02,246 --> 00:39:09,117
Speaker SPEAKER_05: regulatable in the interests of humanity rather than in the interests of big companies' shareholders.

384
00:39:10,018 --> 00:39:16,067
Speaker SPEAKER_03: So I think one thing governments can do is try and keep AI research going on in universities and cutting-edge AI research.

385
00:39:16,626 --> 00:39:19,331
Speaker SPEAKER_03: And the way to do that is to provide a lot of money for compute.

386
00:39:20,213 --> 00:39:29,706
Speaker SPEAKER_03: So Canada yesterday announced they're providing $2.4 billion for AI, and $2 billion of that is just for compute.

387
00:39:29,686 --> 00:39:37,452
Speaker SPEAKER_03: so that Canadian start-up companies and Canadian academic researchers can get significant amounts of compute.

388
00:39:37,472 --> 00:39:38,958
Speaker SPEAKER_03: That's the kind of investment that's needed.

389
00:39:39,418 --> 00:39:41,686
Speaker SPEAKER_03: And Ireland needs to maybe

390
00:39:42,021 --> 00:39:50,753
Speaker SPEAKER_03: It's a smaller country so it can't invest that much maybe, but maybe in conjunction with the European Union it could be part of something in Europe that does invest significant amounts of money.

391
00:39:50,773 --> 00:39:56,121
Speaker SPEAKER_03: So the European Union invested a billion dollars in a crazy brain project that didn't produce anything.

392
00:39:57,344 --> 00:40:07,438
Speaker SPEAKER_03: They'd be much better off investing like five billion dollars in compute for AI so that people in universities and small startups can compete with the big companies.

393
00:40:07,603 --> 00:40:08,806
Speaker SPEAKER_03: That's one thing you can do.

394
00:40:09,567 --> 00:40:15,036
Speaker SPEAKER_03: The other thing government can do is force the big companies to do more research than they're doing on safety.

395
00:40:16,338 --> 00:40:28,295
Speaker SPEAKER_03: So I guess my political belief is that, I'm really a socialist at heart, but capitalism has worked pretty well for some things.

396
00:40:29,137 --> 00:40:37,844
Speaker SPEAKER_03: And it seems to me what you need is, you need the kind of creativity of small startups trying to get rich.

397
00:40:38,465 --> 00:40:42,528
Speaker SPEAKER_03: But you need to have that in a strong regulatory environment.

398
00:40:42,949 --> 00:40:50,034
Speaker SPEAKER_03: So the role of government is to organize things so that if everybody does what's in their own self-interest, it works for everybody.

399
00:40:50,536 --> 00:40:59,143
Speaker SPEAKER_03: And the idea that the government should stay out of it, that's crazy, because then if everybody does what's in their own self-interest, you just get some very rich people and everybody else very poor.

400
00:40:59,123 --> 00:41:09,601
Speaker SPEAKER_03: I think government should be very interventionist and it should be trying to create a reward landscape that channels people into doing things that will be useful for society.

401
00:41:10,422 --> 00:41:12,144
Speaker SPEAKER_03: So I think there's a lot government should be doing.

402
00:41:12,224 --> 00:41:16,052
Speaker SPEAKER_03: One thing in particular is insisting on more research on safety.

403
00:41:17,753 --> 00:41:32,768
Speaker SPEAKER_14: You didn't stay with that issue of safety I think at the moment and I mean particularly in terms of large language models like GPT-4 and others which have got very wide adoption and are very much in the public discourse.

404
00:41:34,130 --> 00:41:45,583
Speaker SPEAKER_14: Can you tell us something kind of specific about the concerns you have about misuse and what we might do both in the short and the long term to mitigate against the risks?

405
00:41:46,036 --> 00:41:55,387
Speaker SPEAKER_03: Yes, so there's many different kinds of misuse of risk and they all have different solutions and there's different timescales.

406
00:41:55,909 --> 00:42:00,614
Speaker SPEAKER_03: So the most urgent one at present, well there's discrimination bias which is going on already.

407
00:42:01,615 --> 00:42:08,144
Speaker SPEAKER_03: They are, possibly because I'm an old white male, they're something I'm not that concerned about and the reason is

408
00:42:08,125 --> 00:42:16,748
Speaker SPEAKER_03: If you replace an existing system with an AI system, you can make the AI system a bit less biased than the existing system.

409
00:42:17,150 --> 00:42:19,797
Speaker SPEAKER_03: You shouldn't try and make it unbiased, that's just going to be hopeless.

410
00:42:20,157 --> 00:42:23,367
Speaker SPEAKER_03: Just make it significantly less biased than what it replaces.

411
00:42:23,347 --> 00:42:32,297
Speaker SPEAKER_03: So if you've got training data which involves old white men deciding whether to give mortgages to young black women, you know it's going to be very biased.

412
00:42:32,318 --> 00:42:38,485
Speaker SPEAKER_03: The existing system is like that, your training data is like that, but you can do things to make the final system less biased.

413
00:42:38,505 --> 00:42:39,688
Speaker SPEAKER_03: You'll never make it unbiased.

414
00:42:40,307 --> 00:42:48,038
Speaker SPEAKER_03: So I think bias, actually AI systems can make things a lot less biased than they currently are, even though they're not unbiased.

415
00:42:49,030 --> 00:42:52,934
Speaker SPEAKER_03: The thing I'm most worried about at present is corrupting elections with fake videos.

416
00:42:53,474 --> 00:42:54,896
Speaker SPEAKER_03: And we're going to see a lot of that this year.

417
00:42:55,737 --> 00:43:00,842
Speaker SPEAKER_03: And obviously the solution to that, although it's difficult technically, is to mark all those videos as fake.

418
00:43:01,422 --> 00:43:05,206
Speaker SPEAKER_03: Insist that AI-generated stuff be marked somehow as AI-generated.

419
00:43:05,646 --> 00:43:07,027
Speaker SPEAKER_03: It's very hard to do technically.

420
00:43:07,869 --> 00:43:13,594
Speaker SPEAKER_03: But I think in the States, for example, they're not going to even try to do that, because there's no way the Republicans would vote for that.

421
00:43:14,315 --> 00:43:16,016
Speaker SPEAKER_03: They're relying on telling lies.

422
00:43:18,679 --> 00:43:24,726
Speaker SPEAKER_03: That wasn't political, that was just fact.

423
00:43:24,746 --> 00:43:34,717
Speaker SPEAKER_14: You've also raised concerns about the potential for AI to create its own goals and potentially expose us to what Mike described as an existential threat.

424
00:43:36,760 --> 00:43:45,429
Speaker SPEAKER_14: So can you give us some examples of what concrete safeguards or ethical frameworks you believe are crucial to implement during this period of AI development that we're living through?

425
00:43:46,978 --> 00:43:49,481
Speaker SPEAKER_03: So yeah, the existential threat is longer term.

426
00:43:50,342 --> 00:43:59,153
Speaker SPEAKER_03: My guess is it will get things smarter than us with a probability of about 0.5 in between five and 20 years from now.

427
00:43:59,173 --> 00:43:59,833
Speaker SPEAKER_03: It may be longer.

428
00:43:59,873 --> 00:44:01,916
Speaker SPEAKER_03: It may be shorter, but unlikely, I think.

429
00:44:02,297 --> 00:44:05,981
Speaker SPEAKER_03: But there's quite a good chance that in that period, we'll get things smarter than us.

430
00:44:06,561 --> 00:44:09,164
Speaker SPEAKER_03: And if we do, there's quite a good chance they will take over.

431
00:44:09,206 --> 00:44:12,969
Speaker SPEAKER_03: And we should clearly be thinking about how we prevent that.

432
00:44:13,371 --> 00:44:22,797
Speaker SPEAKER_03: I mean, there is an issue about whether we're being, whether humanist is a racist word, whether they should have rights too.

433
00:44:23,177 --> 00:44:28,932
Speaker SPEAKER_03: But we're people and we care about people and I think we should do our best to keep people around.

434
00:44:31,563 --> 00:44:46,938
Speaker SPEAKER_03: I don't know if we can do it though, but certainly one thing we should do is, as we make things more intelligent, as the big companies edge towards super intelligence, they can actually do experiments on how these things might get out of control before they're smarter than us.

435
00:44:47,306 --> 00:44:57,981
Speaker SPEAKER_03: And anybody who's written a computer program knows that you write the computer program and then it doesn't work and then you realize you misunderstood something or didn't think of something.

436
00:44:58,742 --> 00:45:02,228
Speaker SPEAKER_03: And so empirical checking of things is really crucial.

437
00:45:02,268 --> 00:45:05,231
Speaker SPEAKER_03: You can't just do armchair prediction of everything.

438
00:45:05,211 --> 00:45:10,036
Speaker SPEAKER_03: And I think they should force the big companies to do experiments on how these things try and get out of control.

439
00:45:10,717 --> 00:45:12,498
Speaker SPEAKER_03: And I think we already know some of the ways they will.

440
00:45:13,239 --> 00:45:22,246
Speaker SPEAKER_03: So we'll give them sub-goals, because agents are much more effective at planning your holiday if they can set up sub-goals of sort of getting you there and finding a hotel and stuff.

441
00:45:23,668 --> 00:45:25,250
Speaker SPEAKER_03: And you don't want to micromanage them.

442
00:45:25,329 --> 00:45:27,012
Speaker SPEAKER_03: You want them to create the sub-goals.

443
00:45:27,891 --> 00:45:33,177
Speaker SPEAKER_03: But they will then start creating sub-goals like, if I only had a little bit more control, I could do things better.

444
00:45:33,932 --> 00:45:36,596
Speaker SPEAKER_03: and that could easily lead to them wanting to take over.

445
00:45:36,615 --> 00:45:43,342
Speaker SPEAKER_03: I think we need to do experiments on that, because at present it's all just theorising.

446
00:45:44,422 --> 00:45:55,134
Speaker SPEAKER_03: We don't really have any experience of things more intelligent than us, or how to deal with them, or even of things almost as intelligent as us, unless you count Trump.

447
00:45:55,153 --> 00:46:03,061
Speaker SPEAKER_03: And we didn't deal with that very well.

448
00:46:05,387 --> 00:46:06,889
Speaker SPEAKER_03: I think that's one thing government can do.

449
00:46:07,210 --> 00:46:14,302
Speaker SPEAKER_03: And I think actually Sam Altman agrees on that, that they should be actually doing empirical experiments on how these things try and escape control.

450
00:46:14,581 --> 00:46:16,405
Speaker SPEAKER_03: Ilya agrees on that too.

451
00:46:16,425 --> 00:46:17,367
Speaker SPEAKER_14: It's interesting you mention that.

452
00:46:17,427 --> 00:46:28,965
Speaker SPEAKER_14: So do you think the for-profit versus not-for-profit is part of the issue in terms of the companies or the organisations that are able to develop these kinds of platforms and solutions?

453
00:46:29,467 --> 00:46:31,693
Speaker SPEAKER_03: Yes, so the profit issue is tricky.

454
00:46:32,135 --> 00:46:44,625
Speaker SPEAKER_03: Obviously, like Google, when it was in the lead in these big chatbots, didn't release big chatbots to the public because it was scared of hurting its reputation by them doing all these confabulations and saying bad things.

455
00:46:44,875 --> 00:46:50,061
Speaker SPEAKER_03: As soon as open AI combined with Microsoft and it went into being, Google had to compete.

456
00:46:50,742 --> 00:46:54,166
Speaker SPEAKER_03: So the profit motive is going to cause rapid progress.

457
00:46:54,907 --> 00:46:56,108
Speaker SPEAKER_03: And we're not going to stop that.

458
00:46:56,809 --> 00:46:59,152
Speaker SPEAKER_03: I mean, it might be rational to say, just stop doing AI now.

459
00:46:59,333 --> 00:47:01,715
Speaker SPEAKER_03: That might be a rational decision, but it's not going to happen.

460
00:47:01,735 --> 00:47:04,639
Speaker SPEAKER_03: There's no point even asking for it, because there's no way that's going to happen.

461
00:47:05,119 --> 00:47:06,661
Speaker SPEAKER_03: And if we stop, the Chinese wouldn't stop.

462
00:47:11,788 --> 00:47:14,831
Speaker SPEAKER_03: And if the Chinese stop, the Americans wouldn't stop.

463
00:47:17,932 --> 00:47:27,309
Speaker SPEAKER_03: So I don't think we can ask for it to stop and we shouldn't ignore the fact that there's many very positive aspects of it which is why it's going to be developed in medicine and things.

464
00:47:29,333 --> 00:47:33,681
Speaker SPEAKER_03: We've got to figure out how to try and keep control of it and

465
00:47:36,277 --> 00:47:53,188
Speaker SPEAKER_03: I think the profit motive, we saw what happened in OpenAI when everything was stacked in favour of safety because the company that owned the for-profit bit was a not-for-profit bit which was set up to be

466
00:47:54,298 --> 00:47:56,121
Speaker SPEAKER_03: concerned primarily with safety.

467
00:47:56,882 --> 00:48:03,976
Speaker SPEAKER_03: And even in that circumstance where it was a very unlevel playing field, it was tilted all the way you could tilt it towards safety and profit won.

468
00:48:06,018 --> 00:48:07,001
Speaker SPEAKER_03: So that doesn't seem good.

469
00:48:08,643 --> 00:48:16,257
Speaker SPEAKER_03: And while I'm at it, there's one other thing I want to mention, which is that I think open sourcing the biggest models is completely crazy.

470
00:48:16,438 --> 00:48:18,702
Speaker SPEAKER_03: My good friend Yann LeCun thinks it's the right thing to do.

471
00:48:20,103 --> 00:48:21,306
Speaker SPEAKER_03: He thinks we're all going to be fine.

472
00:48:21,686 --> 00:48:23,208
Speaker SPEAKER_03: We'll keep in control of these things.

473
00:48:23,248 --> 00:48:24,471
Speaker SPEAKER_03: They won't have any goals of their own.

474
00:48:25,833 --> 00:48:27,255
Speaker SPEAKER_03: They won't have any desires of their own.

475
00:48:28,177 --> 00:48:30,260
Speaker SPEAKER_03: I think it's very, very dangerous to open source them.

476
00:48:30,300 --> 00:48:32,123
Speaker SPEAKER_03: I think it's like open sourcing nuclear weapons.

477
00:48:35,447 --> 00:48:40,454
Speaker SPEAKER_03: So I think I would love governments to forbid companies to open source big models.

478
00:48:41,971 --> 00:49:03,304
Speaker SPEAKER_05: One question I was talking to you a little bit earlier about was how we go about regulating these things that the European Union has recently enacted the AI Act, but

479
00:49:03,284 --> 00:49:07,391
Speaker SPEAKER_05: Are there other things we can do to push that stronger?

480
00:49:07,431 --> 00:49:30,068
Speaker SPEAKER_05: Like, would a statutory regulatory profession for computer scientists or for AI architects, would that be something that would strengthen the individual human's ability to fight back against the strong corporate bias that's there at the moment?

481
00:49:30,418 --> 00:49:35,677
Speaker SPEAKER_03: Yeah, I should say at this point, I'm a scientist and I develop some scientific things.

482
00:49:35,978 --> 00:49:37,806
Speaker SPEAKER_03: That doesn't make me a policy expert.

483
00:49:38,467 --> 00:49:39,552
Speaker SPEAKER_03: So...

484
00:49:40,543 --> 00:49:46,391
Speaker SPEAKER_03: It's very easy to get sucked into making statements about policy when you don't really know much about policy.

485
00:49:47,012 --> 00:49:55,664
Speaker SPEAKER_03: And it takes lawyers and people who are used to government to understand what the political options are and how you should do regulation.

486
00:49:56,025 --> 00:49:57,507
Speaker SPEAKER_03: I don't really understand any of that stuff.

487
00:49:57,527 --> 00:49:58,588
Speaker SPEAKER_03: I don't know about that stuff.

488
00:49:59,168 --> 00:50:01,811
Speaker SPEAKER_03: A few of my friends have started learning about it.

489
00:50:01,893 --> 00:50:06,378
Speaker SPEAKER_03: Yoshua Bengio and Stuart Russell have put serious effort into trying to learn about that stuff.

490
00:50:06,398 --> 00:50:07,480
Speaker SPEAKER_03: I haven't.

491
00:50:07,460 --> 00:50:09,967
Speaker SPEAKER_03: I figure I'm too old to learn anything new.

492
00:50:12,114 --> 00:50:15,545
Speaker SPEAKER_03: So I don't really have much to say that you can trust about any of that.

493
00:50:17,146 --> 00:50:19,548
Speaker SPEAKER_05: It's an interesting thing.

494
00:50:19,588 --> 00:50:36,032
Speaker SPEAKER_05: I think that in Ireland, the government has recently appointed an AI council and has a wide representation of academia and industry art, but not necessarily as technical experts.

495
00:50:36,512 --> 00:50:40,858
Speaker SPEAKER_05: There is industry experts, but isn't necessarily the right advice.

496
00:50:40,978 --> 00:50:45,684
Speaker SPEAKER_05: And I think that echoes some of what you were saying about the right people to be on these things.

497
00:50:47,302 --> 00:50:50,289
Speaker SPEAKER_14: Can I just jump in with one more question?

498
00:50:50,309 --> 00:51:05,344
Speaker SPEAKER_14: So you mentioned medical applications a couple of answers ago and I'm interested in where you think some of the low-hanging fruit might be for the applications of AI to medicine in a way that will have great societal benefit.

499
00:51:05,762 --> 00:51:10,630
Speaker SPEAKER_03: Well, I think some of the lying fruit is interpreting medical images.

500
00:51:11,670 --> 00:51:14,856
Speaker SPEAKER_03: I think there'll come a time when we don't need radiologists anymore.

501
00:51:15,577 --> 00:51:16,137
Speaker SPEAKER_03: Not quite yet.

502
00:51:17,400 --> 00:51:24,329
Speaker SPEAKER_03: When we don't need them for interpreting medical images, or rather, AI will interpret medical images and radiologists will just check it.

503
00:51:25,130 --> 00:51:29,577
Speaker SPEAKER_03: But already, AI, if you take a fundus image, an image of your retina,

504
00:51:29,827 --> 00:51:36,474
Speaker SPEAKER_03: AI can see all sorts of things in that image, which the ophthalmologists never knew were there and couldn't see.

505
00:51:36,773 --> 00:51:41,278
Speaker SPEAKER_03: Like, you show a fundus image to an ophthalmologist and ask him what sex the person is.

506
00:51:41,860 --> 00:51:43,501
Speaker SPEAKER_03: They'll say, how would I know from the fundus image?

507
00:51:43,842 --> 00:51:45,684
Speaker SPEAKER_03: AI will tell you with 80% accuracy.

508
00:51:46,364 --> 00:51:49,327
Speaker SPEAKER_03: Or you ask him, is this person going to have a heart attack in the next 10 years?

509
00:51:49,728 --> 00:51:53,090
Speaker SPEAKER_03: And AI will give you a much better bet than an ophthalmologist.

510
00:51:53,751 --> 00:51:56,695
Speaker SPEAKER_03: So there's going to be a lot of stuff like that where you

511
00:51:56,675 --> 00:52:02,545
Speaker SPEAKER_03: AI can see a lot more in these medical images than doctors can.

512
00:52:02,565 --> 00:52:03,606
Speaker SPEAKER_03: That's going to be very helpful.

513
00:52:04,186 --> 00:52:14,804
Speaker SPEAKER_03: But I think, I mean, I already asked Chuck GPT an awful lot of medical questions because I'm old, you know, and I'm like one of those old cars where bits keep falling off.

514
00:52:15,505 --> 00:52:18,289
Speaker SPEAKER_03: And it's very good.

515
00:52:18,309 --> 00:52:22,496
Speaker SPEAKER_03: I don't know how much of what it tells me is nonsense, but it's very helpful.

516
00:52:24,467 --> 00:52:29,195
Speaker SPEAKER_14: We live to tell the tale, so there must be a good start there.

517
00:52:29,215 --> 00:52:32,961
Speaker SPEAKER_14: Maybe that's a positive note to start looking at the audience for questions.

518
00:52:33,001 --> 00:52:35,525
Speaker SPEAKER_14: I don't know if we can raise the lights a bit so that we can see.

519
00:52:35,585 --> 00:52:39,010
Speaker SPEAKER_03: Yes, well, there's a trade-off.

520
00:52:39,371 --> 00:52:43,438
Speaker SPEAKER_03: Either they can see us or we can see them.

521
00:52:43,458 --> 00:52:45,902
Speaker SPEAKER_14: We've got some roving mics.

522
00:52:45,922 --> 00:52:48,186
Speaker SPEAKER_14: Yeah, I think they need those so the audience can see us.

523
00:52:50,128 --> 00:52:52,112
Speaker SPEAKER_05: Maybe if there's a few over here.

524
00:52:54,657 --> 00:52:55,762
Speaker SPEAKER_03: Are you going to pick the questions?

525
00:52:56,324 --> 00:52:56,967
Speaker SPEAKER_14: There we go.

526
00:52:57,027 --> 00:52:57,891
Speaker SPEAKER_14: That's more like it.

527
00:52:57,911 --> 00:52:58,574
Speaker SPEAKER_14: That's great.

528
00:52:58,916 --> 00:53:01,527
Speaker SPEAKER_14: Let's have this gentleman in the check shirt on the right.

529
00:53:01,811 --> 00:53:03,715
Speaker SPEAKER_08: Hi, my name is Amit.

530
00:53:04,617 --> 00:53:07,483
Speaker SPEAKER_08: I have a question which would be a lot different than anyone would ask today.

531
00:53:08,864 --> 00:53:11,469
Speaker SPEAKER_08: I'm 30 year old and I have a job.

532
00:53:11,871 --> 00:53:18,543
Speaker SPEAKER_08: And I have seen those things, like I started learning through linear regression, how mean squared error works.

533
00:53:18,724 --> 00:53:22,731
Speaker SPEAKER_08: Then I saw how to train a network and read about back propagation that time.

534
00:53:22,751 --> 00:53:24,715
Speaker SPEAKER_08: And it was pretty interesting and amazing.

535
00:53:24,695 --> 00:53:28,557
Speaker SPEAKER_08: But with time, I have realized that there are other things in life that you have to do.

536
00:53:28,858 --> 00:53:38,807
Speaker SPEAKER_08: For example, while I'm seeing a match of, you know, a football match, I'm wasting some of the time that I could spend on learning about Transformers, stuff like that.

537
00:53:39,588 --> 00:53:44,273
Speaker SPEAKER_08: With time, I feel like I don't have enough amount of time to learn and do other things.

538
00:53:44,873 --> 00:53:54,702
Speaker SPEAKER_08: The question I wanted to ask you is, do you have, in life, have feel that you've compromised a lot of things in life which you could have spent on other things?

539
00:53:54,681 --> 00:53:55,663
Speaker SPEAKER_08: Yes.

540
00:53:59,565 --> 00:54:00,527
Speaker SPEAKER_08: Do you feel bad about it?

541
00:54:01,027 --> 00:54:05,931
Speaker SPEAKER_03: The bits I feel worst about is not spending more time with my children.

542
00:54:09,094 --> 00:54:15,099
Speaker SPEAKER_03: But if you want to be a good scientist, you have to put your whole self into it.

543
00:54:15,920 --> 00:54:24,688
Speaker SPEAKER_03: I have a sort of belief, it's just a belief, that you can be good at anything you want to be really good at, but just one thing.

544
00:54:26,322 --> 00:54:29,605
Speaker SPEAKER_03: So you can be a very good parent, or you can be a very good scientist.

545
00:54:32,108 --> 00:54:35,150
Speaker SPEAKER_03: But there's a few people maybe can be good at lots of things.

546
00:54:35,431 --> 00:54:41,255
Speaker SPEAKER_03: But for most people, for ordinary people, you have to really devote yourself to something to be really good at it.

547
00:54:41,637 --> 00:54:43,418
Speaker SPEAKER_03: And that involves a lot of sacrifices.

548
00:54:44,619 --> 00:54:44,920
Speaker SPEAKER_03: Sorry.

549
00:54:46,862 --> 00:54:51,246
Speaker SPEAKER_03: Now, when I was at Google, they were always talking about work-life balance.

550
00:54:52,967 --> 00:54:56,210
Speaker SPEAKER_03: And my idea of work-life balance is work.

551
00:55:01,894 --> 00:55:02,793
Speaker SPEAKER_06: to each other.

552
00:55:05,893 --> 00:55:06,514
Speaker SPEAKER_06: Thanks very much.

553
00:55:06,755 --> 00:55:11,603
Speaker SPEAKER_06: I'm Brian McNamee from UCD and the SFI Insight Center for Data Analytics.

554
00:55:12,525 --> 00:55:13,226
Speaker SPEAKER_06: That was fascinating.

555
00:55:13,246 --> 00:55:18,675
Speaker SPEAKER_06: I think it's interesting that you talked about the move to bigger and bigger models and more and more computation.

556
00:55:19,436 --> 00:55:28,672
Speaker SPEAKER_06: Do you see any promising directions that would mitigate against some of the, I guess, the environmental challenges around that and the use of energy around that?

557
00:55:28,692 --> 00:55:32,137
Speaker SPEAKER_06: Do you see any routes that might be emerging in that regard?

558
00:55:33,112 --> 00:55:34,695
Speaker SPEAKER_03: there's sort of two different trends.

559
00:55:34,815 --> 00:55:41,005
Speaker SPEAKER_03: One trend is you make the model bigger, and without any new scientific insights, if you make it bigger and give it more data, it gets better.

560
00:55:41,686 --> 00:55:43,969
Speaker SPEAKER_03: And that's bad news in terms of energy consumption.

561
00:55:44,670 --> 00:55:51,920
Speaker SPEAKER_03: But there's another trend, which is if you've got a big model, you can distill it down to a smaller model, and the smaller model is almost as good.

562
00:55:52,442 --> 00:55:57,730
Speaker SPEAKER_03: So we can, for serving these big models, maybe we can get better and better at making smaller models.

563
00:55:58,050 --> 00:55:59,592
Speaker SPEAKER_03: So now there's lots of,

564
00:55:59,572 --> 00:56:03,952
Speaker SPEAKER_03: Lots of models that are tiny, where tiny means like 10 billion connections.

565
00:56:05,219 --> 00:56:08,132
Speaker SPEAKER_03: It's a new definition of tiny.

566
00:56:08,280 --> 00:56:11,125
Speaker SPEAKER_03: which work almost as well as things with a trillion connections.

567
00:56:12,085 --> 00:56:17,454
Speaker SPEAKER_03: And so maybe, particularly with more scientific insights, we can make them somewhat smaller.

568
00:56:18,014 --> 00:56:24,023
Speaker SPEAKER_03: But I actually believe that things like GPT-4 knows thousands of times more than a person.

569
00:56:24,463 --> 00:56:25,844
Speaker SPEAKER_03: And I don't think it can do that.

570
00:56:25,885 --> 00:56:30,170
Speaker SPEAKER_03: I don't think it'd have that much knowledge without having a very large number of connections.

571
00:56:30,251 --> 00:56:38,039
Speaker SPEAKER_03: So I'm not convinced we can develop models with only a few billion connections that are ever going to be as good as that.

572
00:56:38,059 --> 00:56:39,766
Speaker SPEAKER_03: So it just looks like bad news to me.

573
00:56:40,108 --> 00:56:41,291
Speaker SPEAKER_03: More solar panels.

574
00:56:51,159 --> 00:56:51,922
Speaker SPEAKER_10: Hello, hi.

575
00:56:53,103 --> 00:56:54,728
Speaker SPEAKER_10: Thanks for the presentation, that was excellent.

576
00:56:54,788 --> 00:56:59,918
Speaker SPEAKER_10: It's a pity Professor Chomsky wasn't here to discuss some of the points you were making, but brilliant.

577
00:57:01,722 --> 00:57:09,438
Speaker SPEAKER_10: What I wanted to ask you was, ask a computer scientist or a physicist, and they always think that it's imminent that we will have computers that are smarter than us.

578
00:57:09,418 --> 00:57:11,443
Speaker SPEAKER_10: or we reach that singularity quicker.

579
00:57:11,503 --> 00:57:15,432
Speaker SPEAKER_10: Ask a neuroscientist, and they're not sure at all whether this is going to happen.

580
00:57:15,492 --> 00:57:17,376
Speaker SPEAKER_10: We don't understand how the brain works.

581
00:57:17,737 --> 00:57:18,639
Speaker SPEAKER_10: We're nowhere close to it.

582
00:57:18,659 --> 00:57:25,655
Speaker SPEAKER_10: So there's these completely different views, and clearly you sit on one side of them, but I wanted your comments on what you thought about

583
00:57:25,635 --> 00:57:29,121
Speaker SPEAKER_10: the fact that brains evolve, that there are many differences in it.

584
00:57:29,141 --> 00:57:35,068
Speaker SPEAKER_10: So the question is, common sense is something we think we have and machines do not have yet or are likely to have.

585
00:57:35,208 --> 00:57:37,452
Speaker SPEAKER_10: What are your views on common sense?

586
00:57:37,731 --> 00:57:38,293
Speaker SPEAKER_10: What does it mean?

587
00:57:39,074 --> 00:57:41,376
Speaker SPEAKER_10: And also, does language really capture everything?

588
00:57:41,797 --> 00:57:45,822
Speaker SPEAKER_10: Does language capture enough so that these language models can be smart enough as us?

589
00:57:46,163 --> 00:57:47,885
Speaker SPEAKER_10: What about all the other things in life?

590
00:57:47,905 --> 00:57:50,208
Speaker SPEAKER_03: Okay, so that's a lot of questions.

591
00:57:50,389 --> 00:57:51,289
Speaker SPEAKER_03: Apologies.

592
00:57:52,090 --> 00:57:55,034
Speaker SPEAKER_03: Let me start with,

593
00:57:56,719 --> 00:57:58,442
Speaker SPEAKER_03: Which one should I do first?

594
00:58:00,704 --> 00:58:11,538
Speaker SPEAKER_03: Let me start with my personal history in this, which is for a long, long time, I thought that if we made artificial neural nets more like real neural nets, they would get better.

595
00:58:12,818 --> 00:58:16,583
Speaker SPEAKER_03: And that was the belief that I had until the beginning of 2023.

596
00:58:16,824 --> 00:58:26,675
Speaker SPEAKER_03: And it was then that I started believing that maybe neural nets using back propagation on digital computers

597
00:58:26,739 --> 00:58:29,264
Speaker SPEAKER_03: are already somewhat different from us.

598
00:58:29,923 --> 00:58:36,514
Speaker SPEAKER_03: So they're similar in the way they hold knowledge as features and interactions between features.

599
00:58:37,315 --> 00:58:46,987
Speaker SPEAKER_03: But they're different in that you could have many different copies of the same model and each copy can learn something and then they can share very efficiently because they all work exactly the same way.

600
00:58:47,027 --> 00:58:50,632
Speaker SPEAKER_03: And that makes them much better than us at sharing.

601
00:58:51,635 --> 00:58:53,197
Speaker SPEAKER_03: So they're much higher power than us.

602
00:58:53,237 --> 00:58:56,240
Speaker SPEAKER_03: They need much more energy, but they're much better at sharing.

603
00:58:56,288 --> 00:59:02,396
Speaker SPEAKER_03: And I stopped believing that if you make them more like brains, they'll get better.

604
00:59:02,856 --> 00:59:10,847
Speaker SPEAKER_03: I think we may have reached a point where there's some aspects of brains that you still need to include that we haven't included, like multiple timescales for the weights.

605
00:59:11,306 --> 00:59:17,755
Speaker SPEAKER_03: So in brains, it's very clear that for the synapses in brains, there are many different timescales at which they get modified.

606
00:59:18,255 --> 00:59:20,980
Speaker SPEAKER_03: And in neural nets, you just have one timescale.

607
00:59:20,960 --> 00:59:24,407
Speaker SPEAKER_03: You have a timescale for the activities, so you change the input, the activities change.

608
00:59:24,907 --> 00:59:36,612
Speaker SPEAKER_03: You have a timescale for the weights, so you change the input, the weights stay the same, and with backpropagation, you change the weights very slightly for each input, and they change very slowly, and that's learning.

609
00:59:36,853 --> 00:59:41,583
Speaker SPEAKER_03: But there's no intermediate timescale that corresponds to a sort of short-term memory.

610
00:59:42,036 --> 00:59:52,771
Speaker SPEAKER_03: so maybe that will help with the artificial neural nets but basically I no longer believe you have to make them more like brains and so

611
00:59:53,242 --> 01:00:00,527
Speaker SPEAKER_03: It's quite possible that we still won't understand the brain at the point at which these things are smarter than us, and that's what I think now.

612
01:00:01,188 --> 01:00:11,898
Speaker SPEAKER_03: So my career has been trying to understand the brain and making artificial neural networks in order to try and understand how the brain learns things, and that's been a failure, but there's been some spin-off.

613
01:00:18,784 --> 01:00:20,686
Speaker SPEAKER_03: Oh, I just, that was, you asked so many questions.

614
01:00:20,706 --> 01:00:22,067
Speaker SPEAKER_03: That was the first question I asked.

615
01:00:23,362 --> 01:00:26,572
Speaker SPEAKER_03: The next one, there was a question about is just language enough, I think.

616
01:00:26,614 --> 01:00:27,456
Speaker SPEAKER_03: Was that one of the questions?

617
01:00:28,440 --> 01:00:36,545
Speaker SPEAKER_03: No, but it's amazing how much you can learn from just language.

618
01:00:37,505 --> 01:00:40,472
Speaker SPEAKER_03: If you just have language, there's a sense in which it's not grounded.

619
01:00:41,797 --> 01:00:48,655
Speaker SPEAKER_03: You learn all these relationships between words and how to predict the next word, but in one sense you don't know what it means.

620
01:00:49,677 --> 01:00:53,809
Speaker SPEAKER_03: And you have to distinguish between, I think, a model and an interpretation of a model.

621
01:00:53,789 --> 01:01:02,981
Speaker SPEAKER_03: So you can learn a very good model that's a pretty much correct model of what humans understand without the interpretation that says this thing's a cat.

622
01:01:04,644 --> 01:01:09,612
Speaker SPEAKER_03: But it would be quite quick to learn the interpretation after you learn the model, it's just tying it to reality.

623
01:01:10,112 --> 01:01:13,097
Speaker SPEAKER_03: If you learn multimodal models you don't seem to have that problem.

624
01:01:13,077 --> 01:01:17,963
Speaker SPEAKER_03: So, if you do vision as well, you know what a cat looks like.

625
01:01:18,423 --> 01:01:23,731
Speaker SPEAKER_03: And it's very hard to say, it's very hard, for example, to say, I have a multimodal chatbot.

626
01:01:24,472 --> 01:01:27,155
Speaker SPEAKER_03: I ask it to draw a picture of a hamster wearing a red hat.

627
01:01:27,655 --> 01:01:29,838
Speaker SPEAKER_03: And it draws a picture of a hamster wearing a red hat.

628
01:01:30,380 --> 01:01:33,304
Speaker SPEAKER_03: It's very hard to then say it doesn't understand anything about what this means.

629
01:01:34,324 --> 01:01:40,333
Speaker SPEAKER_03: It seems to me, if it draws a picture of a hamster wearing a red hat, it understands what that is.

630
01:01:40,583 --> 01:01:48,835
Speaker SPEAKER_03: Similarly, if you have a robot and you say, open the drawer, and it opens the drawer, that seems very grounded to me, it really does seem to understand.

631
01:01:49,295 --> 01:01:52,240
Speaker SPEAKER_03: So I actually strongly believe these things understand the same way we do.

632
01:01:53,563 --> 01:02:01,494
Speaker SPEAKER_03: Even the large language models that don't have this multimodal input, but they understand with less data and better if they do have multimodal input.

633
01:02:08,393 --> 01:02:12,978
Speaker SPEAKER_09: I had the talk stick, I think, but I'll pass it on afterwards.

634
01:02:15,021 --> 01:02:17,864
Speaker SPEAKER_09: First of all, thanks very much for an excellent presentation.

635
01:02:19,427 --> 01:02:27,998
Speaker SPEAKER_09: Throughout history, we've been adapting to technologies, and there's always been resistance, and society comes up with ways in which, you know, things aren't like the past.

636
01:02:28,018 --> 01:02:37,911
Speaker SPEAKER_09: But then they're accepted, it becomes part and parcel, and people become almost condescending to the past, to say that, you know, people were afraid of fire, the combustion engine, the television.

637
01:02:38,599 --> 01:02:40,782
Speaker SPEAKER_09: Is AI different, though?

638
01:02:40,802 --> 01:02:54,137
Speaker SPEAKER_09: I'm involved in a number of AI centers here, and things like ethical AI concerns the adoption of the technology is actually a bigger draw than the development of the technology.

639
01:02:54,356 --> 01:02:57,780
Speaker SPEAKER_09: If there was complete acceptance, we'd probably be way further down the road.

640
01:02:58,141 --> 01:03:05,568
Speaker SPEAKER_09: There's this thing that, well, give society enough time, there'll be an acceptance of this technology and adoption.

641
01:03:05,548 --> 01:03:06,490
Speaker SPEAKER_09: Is this different?

642
01:03:06,710 --> 01:03:16,467
Speaker SPEAKER_09: Is there an acceptance that we will forever have this resistance and it will never be part of something that, as I said, like the combustion engine or even the internet is?

643
01:03:16,668 --> 01:03:22,356
Speaker SPEAKER_09: Is it something we just have to accept that this is going to be a problem to be addressed continuously?

644
01:03:24,159 --> 01:03:25,242
Speaker SPEAKER_03: My guess is not.

645
01:03:25,262 --> 01:03:29,007
Speaker SPEAKER_03: My guess is either it'll take over or we'll learn to live with it.

646
01:03:29,242 --> 01:03:41,358
Speaker SPEAKER_03: I see one future in which it takes over and we're gone, and another future in which we learn to interact with things more intelligent than ourselves, and have a very good symbiosis.

647
01:03:41,820 --> 01:03:57,342
Speaker SPEAKER_03: So my model for that is, imagine a big company with a dumb CEO, who's probably the son of the previous CEO, with a very intelligent assistant, who's almost certainly female, who's much smarter than he is,

648
01:03:57,322 --> 01:03:59,164
Speaker SPEAKER_03: And he thinks he's running the company.

649
01:03:59,505 --> 01:04:02,429
Speaker SPEAKER_03: And in a sense, he is running the company because things he wants get done.

650
01:04:04,032 --> 01:04:06,637
Speaker SPEAKER_03: But really, the assistant is running the company.

651
01:04:06,677 --> 01:04:10,443
Speaker SPEAKER_03: Really, the assistant is the one who understands much more than he does about what's really going on.

652
01:04:11,545 --> 01:04:14,048
Speaker SPEAKER_03: And that would be quite nice for the dumb CEO.

653
01:04:14,068 --> 01:04:20,159
Speaker SPEAKER_03: So we might all be in the role of dumb CEO with much smarter assistants that make it all work.

654
01:04:20,820 --> 01:04:22,101
Speaker SPEAKER_03: That's the alternative future.

655
01:04:22,282 --> 01:04:24,105
Speaker SPEAKER_03: I think we're getting at one of those two futures.

656
01:04:26,362 --> 01:04:28,967
Speaker SPEAKER_03: But in that second future, we do sort of accept it.

657
01:04:29,688 --> 01:04:34,034
Speaker SPEAKER_03: And we organize things so that it's to our benefit to live with it.

658
01:04:36,358 --> 01:04:38,101
Speaker SPEAKER_03: One other comment I wanted to make.

659
01:04:38,963 --> 01:04:42,288
Speaker SPEAKER_03: You mentioned there was a resistance to television.

660
01:04:42,608 --> 01:04:45,132
Speaker SPEAKER_03: And I thought, I don't think there was much resistance to television.

661
01:04:45,753 --> 01:04:51,844
Speaker SPEAKER_03: Retrospectively, I think there should have been much more resistance to television, because look at Trump.

662
01:04:52,184 --> 01:04:53,907
Speaker SPEAKER_03: He wouldn't be there without television.

663
01:05:00,012 --> 01:05:00,291
Speaker SPEAKER_07: Hello.

664
01:05:01,474 --> 01:05:01,974
Speaker SPEAKER_07: Hi.

665
01:05:02,094 --> 01:05:05,119
Speaker SPEAKER_07: David Horgan, also a Google alum.

666
01:05:05,900 --> 01:05:16,217
Speaker SPEAKER_07: So my question is, you kind of spoke a bit about it on the panel, that AI in the beginning really started with an altruistic kind of research perspective, like what's the art of the possible?

667
01:05:17,318 --> 01:05:21,666
Speaker SPEAKER_07: And as we've seen, it's developed substantially in the last couple of years.

668
01:05:21,646 --> 01:05:25,052
Speaker SPEAKER_07: And many companies are moving towards a more closed model.

669
01:05:26,052 --> 01:05:34,306
Speaker SPEAKER_07: So how are you thinking about striking that balance between really pushing the envelope in terms of innovation, but also having control?

670
01:05:34,606 --> 01:05:44,824
Speaker SPEAKER_07: Should we be moving towards more of a closed system, more regulation, or does the need and the cost of compute be that limitation in itself?

671
01:05:45,224 --> 01:05:48,429
Speaker SPEAKER_07: And there should maybe be more controls on the commute power.

672
01:05:49,269 --> 01:05:59,199
Speaker SPEAKER_03: So I think I've said that I believe governments should force the big companies who have the resources to put more effort into safety.

673
01:06:00,960 --> 01:06:02,583
Speaker SPEAKER_03: That's pretty much all I have to say about it.

674
01:06:03,744 --> 01:06:10,070
Speaker SPEAKER_03: I don't think governments themselves are going to set aside enough resources to investigate these big models.

675
01:06:10,251 --> 01:06:10,971
Speaker SPEAKER_03: Maybe they should.

676
01:06:10,990 --> 01:06:17,757
Speaker SPEAKER_03: I mean, maybe the US should be saying understanding AI and what it can do is

677
01:06:18,835 --> 01:06:22,081
Speaker SPEAKER_03: We should have something of the same scale as the defence budget.

678
01:06:22,101 --> 01:06:24,786
Speaker SPEAKER_03: We should have half the defence budget should go on that.

679
01:06:25,166 --> 01:06:29,193
Speaker SPEAKER_03: And then you're talking about the amounts of money that the big companies, probably more.

680
01:06:30,675 --> 01:06:31,938
Speaker SPEAKER_03: But I don't think they're going to do that.

681
01:06:32,559 --> 01:06:33,842
Speaker SPEAKER_03: They've got too many other concerns.

682
01:06:34,724 --> 01:06:36,507
Speaker SPEAKER_03: And it's not clear they should do that.

683
01:06:36,806 --> 01:06:47,106
Speaker SPEAKER_03: I think the best role is for try and force the big companies to do more work on safety and try and encourage smart young researchers to work on safety.

684
01:06:47,728 --> 01:06:54,461
Speaker SPEAKER_03: And it was one thing in particular that needs to be done, which is make it clear to smart young researchers

685
01:06:54,442 --> 01:07:08,396
Speaker SPEAKER_03: that it won't damage their career to work on safety, because until quite recently, some people who wanted to work on safety thought, you know, I could work on making the LLM a bit better, or I could work on safety, and I'm going to have a better career if I work on making the LLM better.

686
01:07:08,898 --> 01:07:11,403
Speaker SPEAKER_03: We need to make good career paths for people who work on safety.

687
01:07:11,637 --> 01:07:20,047
Speaker SPEAKER_12: Hello Geoffrey, my name is Bernadette Bonatti, I work at the UCD Foundation and I thought your speech was terrific, in fact the whole panel was wonderful tonight.

688
01:07:20,708 --> 01:07:22,309
Speaker SPEAKER_12: My question is slightly different.

689
01:07:22,990 --> 01:07:37,728
Speaker SPEAKER_12: In UCD we do a lot on James Joyce, strangely, and we're very involved in thinking of AI and literature and I was just wondering about the whole world of aesthetics and what James Joyce would make of AI and

690
01:07:37,708 --> 01:07:48,661
Speaker SPEAKER_12: I just wanted to know what advice you would give to people, linguists like myself, about not being worried about AI, about just going for it and seeing what happens.

691
01:07:51,364 --> 01:07:54,407
Speaker SPEAKER_03: I think I'm totally out of my depth.

692
01:07:54,427 --> 01:07:55,248
Speaker SPEAKER_12: That's a good enough answer.

693
01:07:57,530 --> 01:08:04,137
Speaker SPEAKER_03: I mean, one thing, this isn't James Joyce, but there's something called the Voynich manuscript.

694
01:08:05,012 --> 01:08:12,106
Speaker SPEAKER_03: which some collector of antiques discovered quite a long time ago.

695
01:08:12,748 --> 01:08:19,921
Speaker SPEAKER_03: Curiously, Voynich married George Bull's youngest daughter, who was called Ethel Voynich.

696
01:08:20,783 --> 01:08:25,231
Speaker SPEAKER_03: And the Voynich manuscript has never been interpreted

697
01:08:25,769 --> 01:08:33,423
Speaker SPEAKER_03: And I actually got someone who worked for me to try using AI language models to try and understand the Voynich manuscript.

698
01:08:33,625 --> 01:08:35,529
Speaker SPEAKER_03: There's not enough text there.

699
01:08:36,650 --> 01:08:38,675
Speaker SPEAKER_03: You'd have to use the images too, there's a lot of images.

700
01:08:40,037 --> 01:08:41,260
Speaker SPEAKER_03: We never got anywhere with it.

701
01:08:41,340 --> 01:08:44,166
Speaker SPEAKER_03: I think eventually somebody will.

702
01:08:44,904 --> 01:08:46,548
Speaker SPEAKER_03: People don't know whether it's a fake.

703
01:08:47,371 --> 01:08:49,576
Speaker SPEAKER_03: A lot of people have said it's a fake, there's nothing really there.

704
01:08:50,639 --> 01:08:53,645
Speaker SPEAKER_03: It seems to be a manuscript about strange plants and their properties.

705
01:08:54,247 --> 01:08:55,591
Speaker SPEAKER_12: Where can we find out about that?

706
01:08:55,631 --> 01:08:58,518
Speaker SPEAKER_12: That sounds like a brilliant research project.

707
01:08:58,858 --> 01:09:01,664
Speaker SPEAKER_03: Just look up Voynich Manuscript.

708
01:09:03,382 --> 01:09:16,884
Speaker SPEAKER_03: It has some things that suggest that it's genuine, so you know what Zipf's Law is, that there's common things and rare things, and they drop off in a particular way, the frequency of them drops off in a particular way, and it obeys Zipf's Law.

709
01:09:16,904 --> 01:09:22,252
Speaker SPEAKER_03: And they didn't know Zipf's Law when it was made, so that makes it much less likely that it's a forgery.

710
01:09:23,194 --> 01:09:37,398
Speaker SPEAKER_03: And already, I haven't followed any of the literature carefully, but I get things in my newsfeed that say that there's these old, very old tablets that AI is now beginning to decipher.

711
01:09:38,179 --> 01:09:39,560
Speaker SPEAKER_03: I think there's going to be a lot of that.

712
01:09:40,002 --> 01:09:41,725
Speaker SPEAKER_03: Maybe it could even decipher Finnegan's Wake.

713
01:09:42,525 --> 01:09:42,987
Speaker SPEAKER_12: Exactly.

714
01:09:43,186 --> 01:09:44,368
Speaker SPEAKER_03: We were just thinking about that one.

715
01:09:45,952 --> 01:09:46,231
Speaker SPEAKER_12: Thank you.

716
01:09:48,337 --> 01:10:02,483
Speaker SPEAKER_04: I'd be interested just in hearing a little bit more about your time at Google, because as I found out about AI, it was really by always using Google to ask questions and get information.

717
01:10:03,144 --> 01:10:11,257
Speaker SPEAKER_04: And how you see the future of it evolve and whether it's limited a little bit because NVIDIA seemed to have

718
01:10:11,238 --> 01:10:18,626
Speaker SPEAKER_04: the capacities to get into all the major GPUs and whether it will develop commercially throughout.

719
01:10:20,188 --> 01:10:21,009
Speaker SPEAKER_03: So what's the question?

720
01:10:21,069 --> 01:10:23,971
Speaker SPEAKER_03: Will NVIDIA take over from Google?

721
01:10:23,992 --> 01:10:29,898
Speaker SPEAKER_04: Yeah, or Microsoft, where it's all going to go commercially.

722
01:10:30,698 --> 01:10:34,984
Speaker SPEAKER_03: Well, I have some very good advice for you, which is buy NVIDIA three years ago.

723
01:10:36,331 --> 01:10:36,771
Speaker SPEAKER_03: I did.

724
01:10:38,014 --> 01:10:38,373
Speaker SPEAKER_04: Thank you.

725
01:10:39,295 --> 01:10:40,676
Speaker SPEAKER_04: I'm looking for the next stage.

726
01:10:41,859 --> 01:10:42,940
Speaker SPEAKER_03: Okay.

727
01:10:43,921 --> 01:10:51,713
Speaker SPEAKER_03: Yeah, my daughter got some Nvidia three years ago and I just persuaded her to sell some of it and buy Intel because it was a bigger upside.

728
01:10:52,194 --> 01:10:54,457
Speaker SPEAKER_03: And since then Intel's gone down by 10%.

729
01:10:56,057 --> 01:11:02,108
Speaker SPEAKER_03: Google will keep existing.

730
01:11:02,488 --> 01:11:06,475
Speaker SPEAKER_04: I have great faith in Google, still do.

731
01:11:07,237 --> 01:11:12,987
Speaker SPEAKER_03: A long time ago, people like me were saying, you don't want to see

732
01:11:12,966 --> 01:11:17,957
Speaker SPEAKER_03: just a bunch of documents that you click on and probably the top one is a good one to click on.

733
01:11:18,438 --> 01:11:24,630
Speaker SPEAKER_03: You want Google to be able to answer your question by looking at documents, and that's what GPT-4 is now doing.

734
01:11:25,452 --> 01:11:30,201
Speaker SPEAKER_03: And Germany's sort of doing that.

735
01:11:31,703 --> 01:11:34,890
Speaker SPEAKER_03: That's the way it's all going, and I think Google will be in there doing it.

736
01:11:35,814 --> 01:11:36,155
Speaker SPEAKER_05: Thank you.

737
01:11:36,176 --> 01:11:38,658
Speaker SPEAKER_05: I think we have time for one more question here.

738
01:11:39,079 --> 01:11:43,886
Speaker SPEAKER_01: Thank you so much for the interesting evening talk and it's such an honour to have you here.

739
01:11:44,368 --> 01:11:53,421
Speaker SPEAKER_01: I'm Fatemeh from the School of Computer Science in UCD and I wanted to know how would you think that AI would change the values in society?

740
01:11:53,481 --> 01:11:59,251
Speaker SPEAKER_01: Would you think that AI would lead and change values, normative behaviour and cultural change?

741
01:11:59,530 --> 01:12:03,476
Speaker SPEAKER_03: I guess I have one thought about that which is quite scary which is

742
01:12:04,097 --> 01:12:10,444
Speaker SPEAKER_03: Before the Industrial Revolution, if you had big strong muscles and were good at digging ditches, you were valuable.

743
01:12:11,225 --> 01:12:16,770
Speaker SPEAKER_03: And after the Industrial Revolution, you weren't because machines could do it better.

744
01:12:17,990 --> 01:12:30,443
Speaker SPEAKER_03: And it worries me a lot that people of average intelligence and average education are currently valued because there's all sorts of things they can do that are useful.

745
01:12:30,862 --> 01:12:35,050
Speaker SPEAKER_03: And if AI can just do those things better and cheaper, they won't be valued.

746
01:12:35,189 --> 01:12:37,694
Speaker SPEAKER_03: And that seems to me terribly dangerous to society.

747
01:12:38,976 --> 01:12:41,121
Speaker SPEAKER_03: It's a big worry, I think.

748
01:12:43,024 --> 01:12:43,324
Speaker SPEAKER_03: Sorry.

749
01:12:45,989 --> 01:12:48,414
Speaker SPEAKER_05: Maybe we'll take one more question so we don't end on that note.

750
01:12:55,769 --> 01:13:03,045
Speaker SPEAKER_11: If you could drive a DeLorean tomorrow and go back in time a bit, who would you like to meet?

751
01:13:03,085 --> 01:13:09,800
Speaker SPEAKER_11: Turing, van Neumann, Hubbell and Wiesel, Rosenblatt, and what sort of a chat would you have with them?

752
01:13:09,819 --> 01:13:11,262
Speaker SPEAKER_11: I would like to meet Helmholtz.

753
01:13:12,425 --> 01:13:13,587
Speaker SPEAKER_03: and I'd like to meet Helmholtz.

754
01:13:13,606 --> 01:13:16,310
Speaker SPEAKER_03: Helmholtz believed in unconscious perceptual inference.

755
01:13:16,992 --> 01:13:23,462
Speaker SPEAKER_03: He basically thought that when you're doing vision, you're doing lots of inference, but it's not kind of conscious deliberate inference.

756
01:13:24,222 --> 01:13:31,614
Speaker SPEAKER_03: You're inferring from the proximal stimulus, you're inferring what the state of the world is that gave rise to that.

757
01:13:31,679 --> 01:13:33,064
Speaker SPEAKER_03: And he was clearly right about that.

758
01:13:33,765 --> 01:13:35,573
Speaker SPEAKER_03: So that was one branch of Helmholtz's work.

759
01:13:36,175 --> 01:13:40,730
Speaker SPEAKER_03: A different branch of Helmholtz's work was free energy.

760
01:13:40,770 --> 01:13:43,038
Speaker SPEAKER_03: There's something called Helmholtz free energy.

761
01:13:43,372 --> 01:14:00,917
Speaker SPEAKER_03: And what Helmholtz didn't realize was that if you're trying to do inference in a complicated model with complicated latent variables, a statistician would tell you to do inference by, given the current parameters of your model, take the data and figure out the best way for your model to explain the data.

762
01:14:01,418 --> 01:14:12,975
Speaker SPEAKER_03: And there might be several ways for your model to explain the data, but you have to figure out the probability of each of those ways in which your model could explain the data in order to adjust the parameters of your model.

763
01:14:13,359 --> 01:14:17,904
Speaker SPEAKER_03: And what variational inference says is, actually you don't have to get that probability right.

764
01:14:18,485 --> 01:14:22,189
Speaker SPEAKER_03: You can approximate that probability and still guarantee you'll learn a better model.

765
01:14:23,069 --> 01:14:24,692
Speaker SPEAKER_03: Or rather there's some band that'll improve.

766
01:14:25,552 --> 01:14:34,422
Speaker SPEAKER_03: And so the two completely different bits of Helmholtz's career, the work on free energy and the work on perceptual inference, were actually very closely related.

767
01:14:34,563 --> 01:14:38,787
Speaker SPEAKER_03: And free energy is the key to being able to do perceptual inference tractably.

768
01:14:39,387 --> 01:14:41,189
Speaker SPEAKER_03: And I'd love to tell Helmholtz that.

769
01:14:56,137 --> 01:15:09,212
Speaker SPEAKER_05: So my warmest thanks to Professor Hinton for joining us this evening and for sharing his thoughts on everything from Watergate to Helmholtz to pretty much every other topic in between, including a few stock tips for us all.

770
01:15:10,493 --> 01:15:14,118
Speaker SPEAKER_05: So I think you'll all agree that it's been an honor to listen to his views.

771
01:15:14,418 --> 01:15:17,922
Speaker SPEAKER_05: And once again, let's offer our congratulations to Professor Hinton.

772
01:15:18,484 --> 01:15:19,664
Speaker SPEAKER_05: And thank you for attending.


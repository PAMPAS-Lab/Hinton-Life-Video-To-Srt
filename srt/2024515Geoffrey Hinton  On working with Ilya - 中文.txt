1 00:00:00,031 --> 00:00:08,061 说话人 SPEAKER_00：您是否对如何选拔人才进行了很多思考，还是这主要依靠直觉？
2 00:00:08,102 --> 00:00:12,128 说话人 SPEAKER_00：伊利亚一出现，你就觉得这个人很聪明，想要和他一起工作。
3 00:00:12,167 --> 00:00:14,551 说话人 SPEAKER_00：或者您是否对这一点进行了很多思考？
4 00:00:16,393 --> 00:00:17,094 说话人 SPEAKER_00：我们应该开始吗？
5 00:00:17,835 --> 00:00:19,719 说话人 SPEAKER_00: 好吧，让我们开始吧。
6 00:00:20,519 --> 00:00:25,187 说话人 SPEAKER_00: 好的。
7 00:00:25,207 --> 00:00:25,987 说话人 SPEAKER_00: 声音是正常的。
8 00:00:30,675 --> 00:00:33,478 说话人 SPEAKER_01: 我记得我刚从英国来到卡内基梅隆大学的时候。
9 00:00:34,259 --> 00:00:38,343 说话人 SPEAKER_01：在英国，在一个研究单位，到了六点钟，大家都会去酒吧喝酒。
10 00:00:39,323 --> 00:00:46,930 说话人 SPEAKER_01：我记得我在卡内基梅隆大学待了几个星期后，那是一个周六晚上，我还没有朋友，也不知道该做什么。
11 00:00:46,951 --> 00:00:52,094 说话人 SPEAKER_01：所以我决定去实验室编程，因为我有一台 Lisp 机器，你不能从家里编程它。
12 00:00:52,115 --> 00:00:56,639 说话人 SPEAKER_01：所以我周六晚上大约九点钟去了实验室，那里人很多。
13 00:00:56,658 --> 00:00:58,640 讲者 SPEAKER_01：所有学生都在那里。
14 00:00:59,026 --> 00:01:02,231 讲者 SPEAKER_01：他们都在那里，因为他们正在从事的是未来。
15 00:01:02,271 --> 00:01:07,382 讲者 SPEAKER_01：他们都相信他们接下来要做的事情将改变计算机科学的发展方向。
16 00:01:07,402 --> 00:01:08,944 讲者 SPEAKER_01：这与英国截然不同。
17 00:01:09,686 --> 00:01:11,588 说话人 SPEAKER_01：这真是太令人耳目一新了。
18 00:01:11,969 --> 00:01:14,073 说话人 SPEAKER_00：请带我从一开始讲起。
19 00:01:14,213 --> 00:01:18,382 说话人 SPEAKER_00：剑桥的 Geoff，试图理解 BRIN。
20 00:01:18,962 --> 00:01:20,004 说话人 SPEAKER_00：那是什么感觉？
21 00:01:20,355 --> 00:01:21,798 说话人 SPEAKER_01：非常令人失望。
22 00:01:22,158 --> 00:01:23,560 说话人 SPEAKER_01：所以我学习了生理学。
23 00:01:24,200 --> 00:01:26,644 说话人 SPEAKER_01：在夏季学期，他们打算教我们大脑是如何工作的。
24 00:01:27,325 --> 00:01:33,391 说话人 SPEAKER_01：他们教我们的只是神经元如何传导动作电位，这很有趣，但它并不能告诉你大脑是如何工作的。
25 00:01:34,272 --> 00:01:35,734 说话人 SPEAKER_01：这太令人失望了。
26 00:01:36,396 --> 00:01:37,456 说话人 SPEAKER_01：然后我转向了哲学。
27 00:01:37,918 --> 00:01:39,739 说话人 SPEAKER_01：我以为他们可能会告诉我们大脑是如何工作的。
28 00:01:40,200 --> 00:01:41,322 说话人 SPEAKER_01：这也很令人失望。
29 00:01:42,063 --> 00:01:45,046 说话人 SPEAKER_01：我最终去了爱丁堡做人工智能。
30 00:01:45,027 --> 00:01:46,149 说话人 SPEAKER_01：那更有趣。
31 00:01:46,209 --> 00:01:48,052 说话人 SPEAKER_01：至少你可以模拟事物。
32 00:01:48,233 --> 00:01:49,335 说话人 SPEAKER_01：所以你可以测试理论。
33 00:01:49,977 --> 00:01:53,084 说话人 说话人_00: 你还记得是什么让你对 AI 感兴趣吗？
34 00:01:53,525 --> 00:01:54,787 说话人 说话人_00: 是一篇论文吗？
35 00:01:54,847 --> 00:01:58,715 说话人 说话人_00: 是哪位特定的人让你接触到这些想法？
36 00:01:59,135 --> 00:02:03,319 说话人 说话人_01: 我想是我在唐纳德·赫布（Donald Hebb）写的书中读到的内容对我影响很大。
37 00:02:04,719 --> 00:02:08,824 说话人 SPEAKER_01：他对您如何在神经网络中学习连接强度非常感兴趣。
38 00:02:09,705 --> 00:02:18,233 说话人 SPEAKER_01：我还早期读过约翰·冯·诺伊曼的一本书，他对大脑如何计算以及它与普通计算机的不同非常感兴趣。
39 00:02:19,234 --> 00:02:28,703 说话人 SPEAKER_00：您在那个时刻就确信这些想法会成功，还是在爱丁堡时期您的直觉是什么？
40 00:02:29,324 --> 00:02:33,276 说话人 SPEAKER_01：在我看来，大脑学习肯定有方法。
41 00:02:33,830 --> 00:02:41,141 说话人 SPEAKER_01：很明显，这并不是通过将各种东西编程进去然后使用推理规则来实现的。
42 00:02:41,442 --> 00:02:43,405 说话人 SPEAKER_01：一开始我就觉得这很疯狂。
43 00:02:45,429 --> 00:02:52,319 说话人 SPEAKER_01：所以我们必须弄清楚大脑是如何学会修改神经网络中的连接，以便它能够完成复杂的事情。
44 00:02:52,920 --> 00:02:55,485 说话人 SPEAKER_01：冯·诺伊曼相信这一点，图灵也相信这一点。
45 00:02:55,905 --> 00:03:00,612 诺伊曼和图灵的逻辑都很强，但他们不相信这种逻辑方法。
46 00:03:01,183 --> 00:03:10,489 你在研究神经科学思想和做看起来好的 AI 算法之间是如何分配时间的？
47 00:03:11,230 --> 00:03:13,336 早期你从中获得了多少灵感？
48 00:03:13,722 --> 00:03:16,384 所以我并没有对神经科学进行太多研究。
49 00:03:16,405 --> 00:03:31,902 说话人 SPEAKER_01：我一直受到关于大脑工作原理的启发，大脑中有许多神经元，它们执行相对简单的操作，是非线性的，但它们收集输入，对它们进行加权，然后根据加权输入给出输出。
50 00:03:32,723 --> 00:03:36,008 说话人 SPEAKER_01：问题是，如何改变这些权重，使整个系统做有益的事情？
51 00:03:36,748 --> 00:03:38,090 说话人 SPEAKER_01：这似乎是一个相当简单的问题。
52 00:03:38,610 --> 00:03:42,074 说话人 SPEAKER_00：你记得那时有哪些合作？
53 00:03:42,241 --> 00:03:46,334 主持人 SPEAKER_01：我在卡内基梅隆大学的主要合作者是另一位不在卡内基梅隆大学的人。
54 00:03:46,936 --> 00:03:51,069 主持人 SPEAKER_01：我与在巴尔的摩的约翰霍普金斯大学的 Terry Sinofsky 有很多互动。
55 00:03:51,269 --> 00:03:55,336 主持人 SPEAKER_01：大约每个月一次，要么他会开车去匹兹堡，要么我会开车去巴尔的摩。
56 00:03:55,355 --> 00:03:56,478 主持人 SPEAKER_01：相距 250 英里。
57 00:03:56,938 --> 00:03:59,203 说话人 SPEAKER_01：我们将会花一个周末的时间一起研究玻尔兹曼机。
58 00:03:59,903 --> 00:04:01,246 说话人 SPEAKER_01：那是一次美妙的合作。
59 00:04:01,486 --> 00:04:03,509 说话人 SPEAKER_01：我们都确信这就是大脑的工作方式。
60 00:04:03,871 --> 00:04:05,653 说话人 SPEAKER_01：那是我做过的最激动人心的研究。
61 00:04:05,973 --> 00:04:08,639 说话人 SPEAKER_01：许多非常有趣的技术成果出现了。
62 00:04:09,480 --> 00:04:10,862 说话人 SPEAKER_01：但我认为这并不是大脑的工作方式。
63 00:04:11,663 --> 00:04:16,112 说话人 SPEAKER_01：我还与 Peter Brown 有很好的合作。
64 00:04:16,851 --> 00:04:20,459 说话人 SPEAKER_01：他是一位非常出色的统计学家，曾在 IBM 从事语音识别工作。
65 00:04:20,940 --> 00:04:27,935 讲者 SPEAKER_01：然后他以更成熟的学生身份来到卡内基梅隆大学，只是为了获得博士学位，但他已经知道很多了。
66 00:04:28,596 --> 00:04:32,586 讲者 SPEAKER_01：他教了我很多关于语音的知识，实际上他还教了我关于隐马尔可夫模型的知识。
67 00:04:33,146 --> 00:04:35,011 讲者 SPEAKER_01：我认为我从他那里学到的比他从我这里学到的要多。
68 00:04:35,752 --> 00:04:37,475 讲者 SPEAKER_01：这就是你想要的学生。
69 00:04:37,574 --> 00:04:44,863 说话者 SPEAKER_01：当他教我关于隐马尔可夫模型时，我正在做带有隐藏层的反向传播，只是那时它们不叫隐藏层。
70 00:04:45,644 --> 00:04:51,473 说话者 SPEAKER_01：我决定隐马尔可夫模型中使用的那个名字非常适合那些你不知道它们在做什么的变量。
71 00:04:52,113 --> 00:04:57,420 说话者 SPEAKER_01：所以这就是神经网络中“隐藏”这个名字的由来。
72 00:04:57,461 --> 00:05:01,206 说话者 SPEAKER_01：我和彼得决定，这个名字非常适合神经网络中的隐藏层。
73 00:05:03,249 --> 00:05:05,471 说话人 SPEAKER_01：我从彼得那里学到了很多关于演讲的知识。
74 00:05:05,670 --> 00:05:09,800 说话人 SPEAKER_00：带我们回到伊利亚出现在你办公室的时候。
75 00:05:10,483 --> 00:05:16,757 说话人 SPEAKER_01：我当时在办公室，可能是在星期天，我在编程，我想，然后门被敲响了。
76 00:05:17,199 --> 00:05:18,802 说话人 SPEAKER_01：不只是敲了一下，而是有点...
77 00:05:19,997 --> 00:05:21,399 说话人 SPEAKER_01：这好像是一个紧急的敲门声。
78 00:05:21,860 --> 00:05:24,403 说话人 SPEAKER_01：所以我出去开门，那里有个年轻的学生。
79 00:05:24,764 --> 00:05:28,288 说话人 SPEAKER_01：他说他在暑假里在做薯条，但他更愿意在我的实验室工作。
80 00:05:29,048 --> 00:05:31,752 说话人 SPEAKER_01：我说，那为什么不预约一下，我们再谈谈呢？
81 00:05:32,413 --> 00:05:33,615 说话人 SPEAKER_01：那么他说，现在怎么样？
82 00:05:34,836 --> 00:05:36,458 说话人 SPEAKER_01：那 sorts of 是伊利亚的性格。
83 00:05:37,158 --> 00:05:43,086 说话人 SPEAKER_01：所以我们聊了一会儿，我给了他一篇论文阅读，就是关于反向传播的《自然》杂志论文。
84 00:05:43,437 --> 00:05:48,543 说话人 SPEAKER_01：然后我们约定一周后再见面，他回来后说，我没有理解它。
85 00:05:49,245 --> 00:05:50,365 说话人 SPEAKER_01：我非常失望。
86 00:05:50,406 --> 00:05:53,571 说话人 SPEAKER_01：我觉得他看起来很聪明，但只是链式法则而已。
87 00:05:53,591 --> 00:05:54,771 说话人 SPEAKER_01：这并不难理解。
88 00:05:55,572 --> 00:05:57,396 说话人 SPEAKER_01：他说，哦，不，我明白了。
89 00:05:58,036 --> 00:06:05,326 说话人 SPEAKER_01：我就是不明白，为什么你不把梯度给一个经过我们多年思考的合理函数优化器。
90 00:06:06,026 --> 00:06:08,290 说话人 SPEAKER_01：他就是这样一直坚持。
91 00:06:08,310 --> 00:06:11,875 说话人 SPEAKER_01：他的直觉非常好，他对事物的原始直觉总是很好。
92 00:06:12,276 --> 00:06:16,783 说话人 SPEAKER_00：你认为是什么让伊利亚有了这些直觉？
93 00:06:17,543 --> 00:06:18,064 说话人 SPEAKER_01：我不知道。
94 00:06:18,105 --> 00:06:19,627 说话人 SPEAKER_01：我觉得他总是为自己思考。
95 00:06:20,067 --> 00:06:22,250 说话人 SPEAKER_01：他从小就对 AI 很感兴趣。
96 00:06:23,233 --> 00:06:27,519 说话人 SPEAKER_01：他显然擅长数学，但很难知道。
97 00:06:28,038 --> 00:06:31,603 说话人 说话人_00：你们之间的这次合作是什么样的？
98 00:06:32,084 --> 00:06:34,908 说话人 说话人_00：你会扮演什么角色，伊利亚会扮演什么角色？
99 00:06:35,175 --> 00:06:36,096 说话人 说话人_01：这很有趣。
100 00:06:36,538 --> 00:06:55,581 说话人 说话人_01：我记得有一次，我们试图用制作数据地图的复杂方法，我有一个混合模型，你可以用相同的相似性来制作两张地图，这样一张地图上的银行可能靠近贪婪，而另一张地图上的银行可能靠近河流。
101 00:06:56,151 --> 00:06:58,915 说话人 SPEAKER_01：因为在一张地图上，你不能让它离两边都太近，对吧？
102 00:06:58,935 --> 00:07:00,598 说话人 SPEAKER_01：因为河流和贪婪相隔甚远。
103 00:07:01,218 --> 00:07:02,680 说话人 SPEAKER_01：所以我们会有一张混合地图。
104 00:07:03,382 --> 00:07:09,470 说话人 SPEAKER_01：我们当时在 MATLAB 中做这件事，这涉及到大量代码的重组织以进行正确的矩阵乘法。
105 00:07:09,730 --> 00:07:10,752 说话人 SPEAKER_01：然后他对那件事感到厌倦了。
106 00:07:11,392 --> 00:07:17,942 说话人 SPEAKER_01：所以有一天他来了，说，我要为 MATLAB 写一个接口，所以我用这种不同的语言编程。
107 00:07:18,322 --> 00:07:21,466 说话人 SPEAKER_01：然后我有了把东西转换成 MATLAB 的工具。
108 00:07:21,447 --> 00:07:24,831 说话人 SPEAKER_01：我说，不，Elia，那可能要花你一个月的时间。
109 00:07:25,151 --> 00:07:26,392 说话人 SPEAKER_01：我们必须继续进行这个项目。
110 00:07:26,473 --> 00:07:27,553 说话人 SPEAKER_01：不要被那件事分心。
111 00:07:28,194 --> 00:07:33,362 说话人 SPEAKER_01：而且 Elia 说，没关系，我今早已经做了。
112 00:07:33,382 --> 00:07:35,384 说话人 SPEAKER_00：这相当令人难以置信。
113 00:07:35,483 --> 00:07:44,314 说话人 说话人_00: 在那些年里，最大的转变并不仅仅是算法，还有规模。
114 00:07:44,615 --> 00:07:46,036 说话人 说话人_00: 你是怎么看待这些年来规模的？
115 00:07:46,439 --> 00:07:49,425 说话人 说话人_00: 你是如何看待这些年来规模的？
116 00:07:49,725 --> 00:07:51,588 说话人 说话人_01: 伊利亚很早就有了那种直觉。
117 00:07:52,110 --> 00:07:56,798 说话人 SPEAKER_01：伊利亚总是宣扬只要做得更大，效果就会更好。
118 00:07:57,600 --> 00:07:59,223 说话人 SPEAKER_01：我总觉得那有点逃避现实。
119 00:07:59,423 --> 00:08:01,528 说话人 SPEAKER_01：你还得有新的想法。
120 00:08:01,507 --> 00:08:03,310 说话人 SPEAKER_01：结果伊利亚基本上是对的。
121 00:08:03,531 --> 00:08:04,312 说话人 SPEAKER_01：新想法有帮助。
122 00:08:04,774 --> 00:08:06,175 说话人 SPEAKER_01：像变压器这样的东西帮助很大。
123 00:08:06,757 --> 00:08:10,362 说话人 SPEAKER_01：但真正重要的是数据和计算规模的扩大。
124 00:08:10,704 --> 00:08:14,850 说话人 SPEAKER_01：当时我们完全没有想到计算机的速度会快上亿倍。
125 00:08:15,211 --> 00:08:16,915 说话人 SPEAKER_01：我们以为他们可能会快上 100 倍。
126 00:08:17,636 --> 00:08:24,567 说话人 SPEAKER_01：我们试图通过提出一些聪明的想法来做事情，如果数据规模和计算能力更大，这些问题本来就可以自行解决。
127 00:08:24,588 --> 00:08:26,672 说话人 SPEAKER_01：大约在 2011 年，
128 00:08:26,651 --> 00:08:33,160 说话人 SPEAKER_01：伊利亚和另一位研究生詹姆斯·马丁斯以及我，我们发表了一篇关于字符级预测的论文。
129 00:08:33,802 --> 00:08:38,268 说话人 SPEAKER_01：所以我们使用了维基百科，并尝试预测下一个 HTML 字符。
130 00:08:39,168 --> 00:08:40,571 说话人 SPEAKER_01：这效果非常好。
131 00:08:41,251 --> 00:08:43,414 说话人 SPEAKER_01：我们总是对它工作得如此之好感到惊讶。
132 00:08:43,914 --> 00:08:47,259 说话人 SPEAKER_01：这是使用 GPU 上的高级优化器实现的。
133 00:08:48,620 --> 00:08:54,229 说话人 SPEAKER_01：我们始终无法相信它真的理解了什么，但看起来它好像真的理解了。
134 00:08:54,360 --> 00:08:55,965 说话人 SPEAKER_01：这简直太不可思议了。
135 00:08:56,485 --> 00:09:02,659 说话人 SPEAKER_00：你能带我们了解一下，这些模型是如何训练来预测下一个单词的？
136 00:09:03,380 --> 00:09:07,289 说话人 SPEAKER_00：为什么这是错误地思考它们的方式？
137 00:09:07,607 --> 00:09:10,071 说话人 SPEAKER_01：好吧，我实际上并不认为这是错误的方向。
138 00:09:10,672 --> 00:09:17,280 说话人 SPEAKER_01：所以，实际上，我认为我制作了第一个使用嵌入和反向传播的神经网络语言模型。
139 00:09:17,881 --> 00:09:37,006 说话人 SPEAKER_01：所以数据非常简单，只是三元组，它将每个符号转换为嵌入，然后让这些嵌入相互作用来预测下一个符号的嵌入，然后从那个嵌入预测下一个符号，然后在整个过程中进行反向传播来学习这些三元组，我展示了它能够泛化。
140 00:09:37,072 --> 00:09:42,913 说话人 SPEAKER_01：大约 10 年后，Yoshio Benji 使用了一个非常类似的网络，并展示了它可以用真实文本工作。
141 00:09:43,113 --> 00:09:46,515 说话人 SPEAKER_01：在那之后大约 10 年，语言学家开始相信嵌入的概念。
142 00:09:47,096 --> 00:09:47,998 说话人 SPEAKER_01：这是一个缓慢的过程。
143 00:09:48,678 --> 00:09:55,304 说话人 SPEAKER_01：我认为这不仅仅是预测下一个符号的原因是，如果你问，那么预测下一个符号需要什么？
144 00:09:55,666 --> 00:10:05,154 说话人 SPEAKER_01：尤其是如果你问我一个问题，然后回答的第一个词是下一个符号，你必须理解这个问题。
145 00:10:06,035 --> 00:10:09,019 说话人 SPEAKER_01：我认为通过预测下一个符号，
146 00:10:09,691 --> 00:10:12,355 说话人 SPEAKER_01：这与老式的自动完成非常不同。
147 00:10:12,735 --> 00:10:21,889 说话人 SPEAKER_01：老式的自动完成，你会存储一些词语的三元组，然后如果你存储一对词语，你会看到不同的词语作为第三个出现的频率，这样你就可以预测下一个符号。
148 00:10:22,331 --> 00:10:24,374 说话人 SPEAKER_01：这就是大多数人认为的自动完成的样子。
149 00:10:25,556 --> 00:10:26,878 说话人 SPEAKER_01：它再也不像那样了。
150 00:10:28,039 --> 00:10:30,363 说话人 SPEAKER_01：要预测下一个符号，你必须理解所说的话。
151 00:10:30,383 --> 00:10:34,149 说话人 SPEAKER_01：所以我认为你是通过让它预测下一个符号来强迫它理解。
152 00:10:34,870 --> 00:10:37,875 说话人 SPEAKER_01：我认为它理解的方式和我们非常相似。
153 00:10:38,057 --> 00:10:40,841 说话人 SPEAKER_01：所以很多人会告诉你，这些事情和我们不一样。
154 00:10:41,822 --> 00:10:43,164 说话人 SPEAKER_01：它们只是在预测下一个符号。
155 00:10:43,566 --> 00:10:44,707 说话人 SPEAKER_01：它们并不像我们那样推理。
156 00:10:45,448 --> 00:10:49,214 说话人 SPEAKER_01：但实际上，为了预测下一个符号，它必须进行一些推理。
157 00:10:49,533 --> 00:10:55,964 说话者 SPEAKER_01：现在我们已经看到，如果你做得很大，不添加任何特殊的东西来做推理，它们已经可以做一些推理了。
158 00:10:56,384 --> 00:10:59,128 说话者 SPEAKER_01：我认为随着它们变得更大，它们将能够进行更多的推理。
159 00:10:59,688 --> 00:11:03,153 说话者 SPEAKER_00：你认为我现在除了预测下一个符号之外还在做其他事情吗？
160 00:11:03,522 --> 00:11:04,945 说话者 SPEAKER_01：我认为这就是你学习的方式。
161 00:11:05,927 --> 00:11:07,471 说话人 SPEAKER_01：你正在预测下一个视频帧。
162 00:11:08,592 --> 00:11:10,155 说话人 SPEAKER_01：你正在预测下一个声音。
163 00:11:11,318 --> 00:11:14,625 说话人 SPEAKER_01：但我认为这是大脑学习的一个相当合理的理论。
164 00:11:15,126 --> 00:11:21,159 说话人 SPEAKER_00：是什么使得这些模型能够学习如此广泛的领域？
165 00:11:21,206 --> 00:11:24,910 说话人 SPEAKER_01：这些大型语言模型所做的是寻找共同结构。
166 00:11:25,711 --> 00:11:29,996 说话人 SPEAKER_01：通过寻找共同结构，它们可以使用共同结构来编码事物，这更有效率。
167 00:11:30,677 --> 00:11:31,820 说话人 SPEAKER_01：所以让我给你举一个例子。
168 00:11:32,480 --> 00:11:37,907 说话人 SPEAKER_01：如果你问 GPT-4，为什么堆肥堆像原子弹？
169 00:11:37,927 --> 00:11:39,288 说话人 SPEAKER_01：大多数人无法回答这个问题。
170 00:11:39,347 --> 00:11:42,751 说话人 SPEAKER_01：大多数人没有想过，他们认为原子弹和堆肥堆是截然不同的事物。
171 00:11:43,393 --> 00:11:49,240 说话人 SPEAKER_01：但是 GPT-4 会告诉你，能量尺度非常不同，时间尺度也非常不同。
172 00:11:49,607 --> 00:11:54,152 说话人 SPEAKER_01：但相同的是，当堆肥堆变热时，它产生热量的速度更快。
173 00:11:54,873 --> 00:11:59,015 说话人 SPEAKER_01：而当原子弹产生更多中子时，它产生中子的速度更快。
174 00:12:00,157 --> 00:12:02,739 说话人 SPEAKER_01：因此它得到了连锁反应的概念。
175 00:12:03,120 --> 00:12:06,101 说话人 SPEAKER_01：我认为他们都已经明白这二者都是连锁反应的形式。
176 00:12:06,182 --> 00:12:10,125 说话人 SPEAKER_01：它正是利用这种理解来将所有这些信息压缩到它的权重中。
177 00:12:11,047 --> 00:12:18,432 说话人 SPEAKER_01：如果它这样做，那么它就会为数百件事物这样做，而这些事物我们尚未看到类比，但它已经看到了。
178 00:12:18,413 --> 00:12:23,100 说话人 SPEAKER_01：这就是你从看似非常不同的事物之间看到这些类比中获得的创造力。
179 00:12:23,780 --> 00:12:27,486 说话人 SPEAKER_01：我认为，随着 GPT-4 的不断发展，它将变得非常富有创造力。
180 00:12:27,527 --> 00:12:35,038 说话人 SPEAKER_01：我认为这种观点——它只是重复所学的内容，只是将学到的文本拼凑在一起，是完全错误的。
181 00:12:35,438 --> 00:12:37,822 说话人 SPEAKER_01：我认为它将比人类更加富有创造力。
182 00:12:37,971 --> 00:12:46,947 说话人 SPEAKER_00：你会争辩说，它不仅不会重复我们迄今为止开发的人类知识，而且还能超越它。
183 00:12:47,448 --> 00:12:50,313 说话人 SPEAKER_00: 我认为这是我们还没有完全看到的东西。
184 00:12:50,533 --> 00:12:58,246 说话人 SPEAKER_00: 我们已经看到了一些例子，但很大程度上，我们仍然处于当前科学水平。
185 00:12:58,267 --> 00:13:00,350 说话人 SPEAKER_00: 您认为什么能使其超越这个水平？
186 00:13:00,398 --> 00:13:02,782 说话人 SPEAKER_01: 嗯，我们在更有限的背景下已经看到了这一点。
187 00:13:02,802 --> 00:13:16,825 说话人 SPEAKER_01：就像 AlphaGo 在与李世石的那场著名比赛中，第 37 步棋，AlphaGo 走了一步所有专家都说一定是错棋的棋，但后来他们意识到那实际上是一步非常精彩的棋。
188 00:13:18,187 --> 00:13:20,250 说话人 SPEAKER_01：所以在那个有限的领域里，这是具有创造性的。
189 00:13:21,192 --> 00:13:24,717 说话人 SPEAKER_01：我认为随着这些事物变得更大，我们会看到更多这样的例子。
190 00:13:24,951 --> 00:13:35,645 说话人 SPEAKER_00：AlphaGo 的另一个不同之处在于它使用了强化学习，这随后使它能够超越当前状态。
191 00:13:35,706 --> 00:13:42,894 说话者 SPEAKER_00：它始于模仿学习，观察人类如何玩游戏，然后通过自我对弈，发展得远远超出了这个范畴。
192 00:13:43,154 --> 00:13:45,938 说话者 SPEAKER_00：你认为这是当前 LLS 缺失的组件吗？
193 00:13:46,158 --> 00:13:49,102 说话者 SPEAKER_01：我认为这很可能是一个缺失的组件，是的。
194 00:13:49,082 --> 00:13:55,990 说话者 SPEAKER_01：AlphaGo 和 AlphaZero 中的自我对弈是它能够做出这些创新动作的重要原因之一。
195 00:13:56,330 --> 00:13:59,173 说话人 SPEAKER_01：但我认为这并不完全必要。
196 00:13:59,854 --> 00:14:05,980 说话人 SPEAKER_01：所以我很久以前做过一个小实验，就是训练一个神经网络来识别手写数字。
197 00:14:06,580 --> 00:14:08,182 说话人 SPEAKER_01：我喜欢这个例子，MNIST 例子。
198 00:14:09,263 --> 00:14:11,905 说话人 SPEAKER_01：你给它提供训练数据，其中一半的答案是错误的。
199 00:14:13,407 --> 00:14:16,389 说话人 SPEAKER_01：问题是，它能学得多好吗？
200 00:14:18,326 --> 00:14:23,131 说话人 SPEAKER_01：你第一次把一半的答案做错，然后就这样保留下来。
201 00:14:23,231 --> 00:14:28,817 说话人 SPEAKER_01：所以它不能通过只看到有时正确有时错误的相同例子来平均掉错误。
202 00:14:29,177 --> 00:14:33,621 说话人 SPEAKER_01：当它看到那个例子时，一半的例子，当它看到那个例子，答案总是错误的。
203 00:14:34,962 --> 00:14:37,905 说话人 SPEAKER_01：因此，训练数据有 50%的错误率。
204 00:14:38,927 --> 00:14:44,351 说话人 SPEAKER_01：但是如果你训练反向传播，错误率可以降低到 5%或更低。
205 00:14:44,904 --> 00:14:51,351 说话人 SPEAKER_01：换句话说，从错误标记的数据中，它可以得到更好的结果。
206 00:14:51,772 --> 00:14:53,494 说话人 SPEAKER_01：它可以看到训练数据是错误的。
207 00:14:54,114 --> 00:14:56,537 说话人 SPEAKER_01：这就是聪明的学生如何比他们的导师更聪明。
208 00:14:57,158 --> 00:15:02,743 说话人 SPEAKER_01：他们的导师告诉他们所有这些，他们觉得，不，垃圾。
209 00:15:03,104 --> 00:15:06,226 说话人 SPEAKER_01：他们听另一半，然后他们比导师更聪明。
210 00:15:06,626 --> 00:15:13,173 说话人 SPEAKER_01：所以这些大型神经网络实际上可以做得更好，它们可以比训练数据做得更好，大多数人都没有意识到这一点。
211 00:15:13,441 --> 00:15:18,027 说话人 SPEAKER_00：那么您认为这些模型如何将这些推理能力加入其中呢？
212 00:15:18,126 --> 00:15:30,562 说话人 SPEAKER_00：我的意思是，一种方法是在它们之上添加某种启发式方法，现在很多研究都在做这样的工作，即拥有某种思维链，你只需将其推理反馈回自身。
213 00:15:30,582 --> 00:15:35,129 说话人 SPEAKER_00：另一种方法是在模型本身中，当你将其规模扩大时。
214 00:15:35,710 --> 00:15:37,272 说话人 SPEAKER_00：您对此有何直觉？
215 00:15:38,028 --> 00:15:41,916 说话人 SPEAKER_01：我的直觉是，随着我们扩大这些模型，我在推理方面变得更擅长。
216 00:15:42,697 --> 00:15:49,572 说话人 SPEAKER_01：如果你问人们是如何工作的，大致来说，我们有这些直觉，我们可以进行推理。
217 00:15:50,076 --> 00:15:52,460 说话人 SPEAKER_01：我们用推理来纠正我们的直觉。
218 00:15:53,201 --> 00:16:01,273 说话人 SPEAKER_01：当然，我们在推理过程中使用直觉来进行推理，但如果推理的结论与我们的直觉相冲突，我们会意识到需要改变直觉。
219 00:16:01,774 --> 00:16:08,524 说话人 SPEAKER_01：这就像在 AlphaGo 或 AlphaZero 中，你有一个评估函数，
220 00:16:08,504 --> 00:16:11,128 说话人 SPEAKER_01：它只是看看棋盘，然后说，这对我有多好？
221 00:16:11,969 --> 00:16:19,221 说话人 SPEAKER_01：但是然后你做蒙特卡洛滚动，现在你得到一个更准确的想法，你可以修改你的评估函数。
222 00:16:19,240 --> 00:16:23,047 说话人 SPEAKER_01：所以你可以通过让它同意推理结果来训练它。
223 00:16:23,067 --> 00:16:26,052 说话人 SPEAKER_01：我认为大型语言模型必须开始这样做。
224 00:16:26,091 --> 00:16:33,263 说话人 SPEAKER_01：它们必须开始通过推理和意识到这是错误的来训练它们对接下来应该发生什么的原始直觉。
225 00:16:33,243 --> 00:16:38,673 说话人 SPEAKER_01：这样它们就能获得比仅仅模仿人们所做更多的训练数据。
226 00:16:39,294 --> 00:16:42,481 说话人 SPEAKER_01：这正是 AlphaGo 能够做出 37 号创造性移动的原因。
227 00:16:43,023 --> 00:16:48,855 说话人 SPEAKER_01：它有更多的训练数据，因为它使用推理来检查下一步应该是什么。
228 00:16:49,341 --> 00:16:51,705 说话人 SPEAKER_00：那么你对多模态有什么看法？
229 00:16:51,745 --> 00:16:57,250 说话人 SPEAKER_00：我们谈到了这些类比，而这些类比往往超出了我们的视野。
230 00:16:57,711 --> 00:17:04,818 说话人 SPEAKER_00：这是发现远超人类和可能永远无法理解的抽象层次上的类比。
231 00:17:04,838 --> 00:17:13,107 说话人 SPEAKER_00：现在，当我们向其中引入图像、视频和声音时，你认为这会如何改变模型？
232 00:17:13,127 --> 00:17:17,813 说话人 SPEAKER_00：你认为这会如何改变它能够做出的类比？
233 00:17:18,146 --> 00:17:20,670 说话人 SPEAKER_01：我认为这会改变很多。
234 00:17:21,010 --> 00:17:24,454 说话人 SPEAKER_01：我认为这将使它在理解空间事物方面变得更好，例如。
仅从语言本身来看，很难理解一些空间事物，尽管令人惊讶的是，GPT-4 可以做到这一点，甚至在它是多模态之前。
236 00：17：34,587 --> 00：17：38,973 演讲者 SPEAKER_01：但是当你让它成为多模态时，如果你两者都在做视觉
237 00:17:38,953 --> 00:17:44,744 说话者 SPEAKER_01：伸手去抓东西，如果它能拿起并翻看它们，那么它将更好地理解物体。
所以，虽然你可以从语言中学到很多东西，但如果你是多模态的，学习起来会更容易，实际上你需要的语言也更少。
239 00:17:55,846 --> 00:18:00,272 说话人 SPEAKER_01：有很多 YouTube 视频是预测下一帧的，或者类似的东西。
240 00:18:01,213 --> 00:18:04,038 说话人 SPEAKER_01：所以我认为这些多模态模型显然会接管一切。
241 00:18:05,240 --> 00:18:06,422 说话人 SPEAKER_01：那样可以得到更多的数据。
242 00:18:06,781 --> 00:18:07,763 说话人 SPEAKER_01：它们需要的语言更少。
243 00:18:08,304 --> 00:18:16,115 说话人 SPEAKER_01：实际上有一个哲学观点，你可以仅从语言中学习一个非常好的模型，但通过多模态系统学习则更容易。
244 00:18:16,298 --> 00:18:20,221 说话人 SPEAKER_00：你认为这将对模型的推理有何影响？
245 00:18:20,563 --> 00:18:23,385 说话人 SPEAKER_01：我认为这将使它在推理空间方面变得更好，例如。
246 00:18:23,645 --> 00:18:25,528 说话人 SPEAKER_01：推理如果你拿起物体会发生什么。
247 00:18:25,568 --> 00:18:29,393 说话者 SPEAKER_01：如果你真的尝试拿起物体，你会得到各种有助于训练的数据。
248 00:18:29,673 --> 00:18:39,183 说话者 SPEAKER_00：你认为人脑是进化来很好地处理语言的，还是认为语言是进化来很好地与人脑相匹配的？
249 00:18:39,586 --> 00:18:45,518 说话者 SPEAKER_01：我认为语言是否进化来与大脑相匹配，或者大脑是否进化来与语言相匹配，我认为这是一个非常好的问题。
250 00:18:46,019 --> 00:18:47,583 说话者 SPEAKER_01：我认为两者都发生了。
251 00:18:48,163 --> 00:18:52,953 说话人 SPEAKER_01：我曾经认为我们可以在完全不使用语言的情况下进行大量的认知。
252 00:18:53,255 --> 00:18:55,817 说话人 SPEAKER_01：现在我有点改变了想法。
253 00:18:56,438 --> 00:19:01,862 说话人 SPEAKER_01：所以让我给你们提供三种关于语言及其与认知关系的不同观点。
254 00:19:02,563 --> 00:19:14,953 说话人 SPEAKER_01：有一种老式的符号观点，即认知是由在某种清理过的逻辑语言中具有符号串组成的，这种语言没有歧义，并应用推理规则。
255 00:19:15,253 --> 00:19:16,555 说话人 SPEAKER_01：这就是认知。
256 00:19:16,575 --> 00:19:22,079 说话人 SPEAKER_01：这只是对这些类似语言符号串的符号操作。
257 00:19:22,059 --> 00:19:23,382 说话人 SPEAKER_01：这就是一种极端观点。
258 00:19:23,923 --> 00:19:28,551 说话人 SPEAKER_01：相反的极端观点是，一旦进入大脑，一切都是向量。
259 00:19:29,112 --> 00:19:38,449 说话人 SPEAKER_01：符号进来后，将这些符号转换成大向量，所有内部操作都用大向量完成，然后如果你想产生输出，就再次产生符号。
260 00:19:38,429 --> 00:19:52,046 说话人 SPEAKER_01：大约在 2014 年，机器翻译中有一个点，当时人们使用循环神经网络，单词会不断进来，它们会有一个隐藏状态，并且会在这个隐藏状态下不断积累信息。
261 00:19:52,686 --> 00:20:01,698 说话人 SPEAKER_01：当它们到达句子的结尾时，就会有一个大隐藏向量，这个向量捕捉了句子的含义，然后可以用来在另一种语言中产生句子。
262 00:20:01,938 --> 00:20:03,660 说话人 SPEAKER_01：这被称为思维向量。
263 00:20:03,640 --> 00:20:05,183 说话人 SPEAKER_01：这就是语言的第二种视角。
264 00:20:05,203 --> 00:20:11,039 说话人 SPEAKER_01：你将语言转换成一种与语言毫不相干的巨大向量，这就是认知的全部。
265 00:20:12,103 --> 00:20:19,522 说话人 SPEAKER_01：但还有一种第三种视角，我现在相信的视角，就是将这些符号
266 00:20:20,836 --> 00:20:27,127 说话人 SPEAKER_01：并将这些符号转换为嵌入，使用多层这种嵌入，从而得到这些非常丰富的嵌入。
267 00:20:27,628 --> 00:20:33,681 说话人 SPEAKER_01：但是嵌入仍然与符号相关联，也就是说，这个符号有一个大向量，那个符号也有一个大向量。
268 00:20:34,099 --> 00:20:38,087 说话人 SPEAKER_01：这些向量相互作用，产生下一个单词的符号向量。
269 00:20:38,990 --> 00:20:40,353 说话人 SPEAKER_01：这就是理解。
270 00:20:40,393 --> 00:20:48,449 说话人 SPEAKER_01：理解就是知道如何将符号转换为这些向量，以及知道向量元素如何相互作用来预测下一个符号的向量。
271 00:20:48,849 --> 00:20:52,497 说话人 SPEAKER_01：这就是理解，无论是在这些大型语言模型中还是在我们的脑海中。
272 00:20:53,422 --> 00:20:56,268 说话人 SPEAKER_01：这是一个介于两者之间的例子。
273 00:20:56,588 --> 00:21:01,599 说话人 SPEAKER_01：你仍然在处理符号，但你将它们解释为这些大向量。
274 00:21:02,161 --> 00:21:03,282 说话人 SPEAKER_01：所有的努力都在这里。
275 00:21:03,663 --> 00:21:10,519 说话人 SPEAKER_01：所有的知识都存在于你所使用的向量以及这些向量的元素如何相互作用，而不是存在于符号规则中。
276 00:21:10,499 --> 00:21:14,888 说话人 SPEAKER_01：但这并不是说你可以完全摆脱符号。
277 00:21:15,009 --> 00:21:20,059 说话人 SPEAKER_01：它意味着你将符号转换成大向量，但仍然保留符号的表面结构。
278 00:21:20,520 --> 00:21:21,924 说话人 SPEAKER_01：这就是这些模型是如何工作的。
279 00:21:22,365 --> 00:21:25,673 说话人 SPEAKER_01：现在这似乎对我来说是一个更可信的人类思维模型。
280 00:21:25,653 --> 00:21:34,367 说话人 SPEAKER_00：你是第一个想到使用 GPU 的人之一，我知道 Jensen 很欣赏你这一点。
281 00:21:35,049 --> 00:21:42,281 说话人 SPEAKER_00：2009 年，你告诉 Jensen，这可能是一个训练神经网络的不错想法。
282 00:21:42,702 --> 00:21:47,711 说话人 SPEAKER_00：带我们回到使用 GPU 训练神经网络的早期直觉。
283 00:21:48,298 --> 00:21:56,145 说话人 SPEAKER_01：实际上，我想在 2006 年左右，我有一个叫 Rick Zaleski 的前研究生，他是一位非常优秀的计算机视觉专家。
284 00:21:57,267 --> 00:21:59,229 说话人 SPEAKER_01：我在一次会议上和他谈了。
285 00:21:59,630 --> 00:22:05,194 说话人 SPEAKER_01：他说，你知道，你应该考虑使用图形处理卡，因为它们在矩阵乘法方面非常出色。
286 00:22:05,615 --> 00:22:07,656 说话人 SPEAKER_01：而你正在做的事情基本上都是矩阵乘法。
287 00:22:08,538 --> 00:22:09,759 说话人 SPEAKER_01：我想了想这件事。
288 00:22:09,838 --> 00:22:16,045 说话人 SPEAKER_01：然后我们了解到这些有四个 GPU 的特斯拉系统。
289 00:22:16,160 --> 00:22:23,315 说话人 SPEAKER_01：最初我们只是得到了游戏 GPU，发现它们能让事情变得快 30 倍。
290 00:22:23,915 --> 00:22:30,569 说话人 SPEAKER_01：然后我们买了一台这样的有四个 GPU 的特斯拉系统，我们在上面做语音处理，效果非常好。
291 00:22:31,191 --> 00:22:34,438 说话人 SPEAKER_01：然后在 2009 年，我在 NIPS 上做了一次演讲。
292 00:22:34,739 --> 00:22:38,686 说话人 SPEAKER_01：我告诉了 1000 名机器学习研究人员，你们都应该去购买 NVIDIA 的 GPU。
293 00:22:38,948 --> 00:22:41,152 说话人 SPEAKER_01：它们是未来，你们做机器学习需要它们。
294 00:22:42,094 --> 00:22:49,087 说话人 SPEAKER_01：实际上我还给 NVIDIA 发了一封邮件，我说我告诉了 1000 名机器学习研究人员去购买你们的板卡，你们能给我一个免费的吗？
295 00:22:49,107 --> 00:22:50,230 说话人 SPEAKER_01: 他们说不行。
296 00:22:50,250 --> 00:22:51,913 说话人 SPEAKER_01: 实际上，他们并没有说不行，他们只是没有回应。
297 00:22:53,297 --> 00:22:56,442 说话人 SPEAKER_01: 但后来我把这个故事告诉了 Jensen，他给了我一个免费的。
298 00:22:58,262 --> 00:23:00,224 说话人 SPEAKER_00: 非常好。
299 00:23:00,325 --> 00:23:07,132 说话人 SPEAKER_00: 我认为同样有趣的是 GPU 如何与该领域一同发展。
300 00:23:07,311 --> 00:23:12,497 说话人 SPEAKER_00: 那么，您认为我们接下来在计算方面应该走向何方？
301 00:23:12,517 --> 00:23:18,061 说话人 SPEAKER_01: 所以，我在谷歌的最后几年一直在思考尝试进行模拟计算的方法。
302 00:23:18,762 --> 00:23:26,750 说话人 SPEAKER_01: 我们可以像大脑一样使用 30 瓦而不是兆瓦，然后在模拟硬件上运行这些大型语言模型。
303 00:23:27,118 --> 00:23:34,714 说话人 SPEAKER_01：我从未让它成功运行，但我开始真正欣赏数字计算。
304 00:23:35,517 --> 00:23:40,166 说话人 SPEAKER_01：所以，如果你要使用那种低功耗模拟计算，
305 00:23:40,618 --> 00:23:42,601 说话人 SPEAKER_01：每一块硬件都会有些不同。
306 00:23:43,122 --> 00:23:47,866 说话人 SPEAKER_01：而学习的想法是利用该硬件的特定属性。
307 00:23:47,886 --> 00:23:48,989 说话人 SPEAKER_01：这就是人们所发生的事情。
308 00:23:49,088 --> 00:23:49,990 说话人 SPEAKER_01：我们的大脑都是不同的。
309 00:23:51,932 --> 00:23:55,696 说话人 SPEAKER_01：所以我们不能把你的大脑中的权重放入我的大脑中。
310 00:23:55,757 --> 00:23:59,500 说话人 SPEAKER_01：硬件不同，个体神经元的精确特性也不同。
311 00:23:59,881 --> 00:24:02,865 说话人 SPEAKER_01：学习已经学会了利用所有这些。
312 00:24:03,605 --> 00:24:07,490 说话人 SPEAKER_01：从某种意义上说，我们都是凡人，因为我大脑中的权重对其他大脑没有任何用处。
313 00:24:07,730 --> 00:24:09,833 说话人 SPEAKER_01：当我死去，那些权重就毫无用处了。
314 00:24:09,813 --> 00:24:19,689 说话人 SPEAKER_01：我们可以通过一种相当低效的方式从一个人获取信息到另一个人，即我产生句子，而你找出如何改变你的权重，以便你也能说出同样的话。
315 00:24:20,269 --> 00:24:21,391 说话人 SPEAKER_01：这就是蒸馏。
316 00:24:21,951 --> 00:24:24,596 说话人 SPEAKER_01：但那是一种非常低效的知识传播方式。
317 00:24:25,357 --> 00:24:30,605 说话人 SPEAKER_01：在数字系统中，它们是永恒的，因为一旦你有了某些权重，
318 00:24:30,586 --> 00:24:40,846 说话人 SPEAKER_01：你可以扔掉电脑，只需将权重存储在某个磁带上，然后构建另一台电脑，将这些相同的权重放入其中，如果它是数字的，它可以精确地计算出与另一个系统相同的结果。
319 00:24:41,827 --> 00:24:48,000 说话人 SPEAKER_01：因此，数字系统可以共享权重，这非常高效。
320 00:24:48,019 --> 00:24:52,729 说话人 SPEAKER_01：如果你有一大堆数字系统，并且它们各自去进行一点学习，
321 00:24:53,856 --> 00:24:57,603 说话者 SPEAKER_01：它们以相同的权重开始，进行一点学习，然后再次共享它们的权重。
322 00:24:58,805 --> 00:25:00,247 说话者 SPEAKER_01：他们都知道其他人学到了什么。
323 00:25:00,968 --> 00:25:01,729 说话者 SPEAKER_01：我们无法这样做。
324 00:25:02,509 --> 00:25:05,575 说话者 SPEAKER_01：因此，他们在共享知识方面远胜于我们。
325 00:25:06,036 --> 00:25:11,364 说话人 SPEAKER_00: 在这个领域实施的大多数想法都是非常老式的想法。
326 00:25:12,325 --> 00:25:16,290 说话人 SPEAKER_00: 这些想法在神经科学领域已经存在了很久。
327 00:25:16,632 --> 00:25:20,758 说话人 SPEAKER_00: 您认为我们还剩下哪些可以应用到我们开发的系统中？
328 00:25:20,974 --> 00:25:30,048 说话人 SPEAKER_01: 所以，我们还需要在神经科学方面追赶的一个大问题是变化的时序。
329 00:25:30,190 --> 00:25:36,019 说话人 SPEAKER_01：在几乎所有神经网络中，都存在一个快速时间尺度来改变活动。
330 00:25:36,078 --> 00:25:38,021 说话人 SPEAKER_01：所以输入是活动。
331 00:25:38,321 --> 00:25:44,790 说话人 SPEAKER_01：嵌入向量都会改变，然后有一个慢时间尺度，即改变权重，这是长期学习。
332 00:25:45,392 --> 00:25:46,874 说话人 SPEAKER_01：你只有这两个时间尺度。
333 00:25:47,575 --> 00:25:50,598 说话者 SPEAKER_01：在大脑中，权重变化的时标有很多。
334 00:25:51,220 --> 00:26:06,861 说话者 SPEAKER_01：例如，如果我提到一个意想不到的词，比如黄瓜，现在五分钟之后，你戴上耳机，周围有很多噪音，而且有很微弱的话语，你将更容易识别出这个词“黄瓜”，因为我五分钟前提到过它。
335 00:26:07,297 --> 00:26:09,239 说话者 SPEAKER_01：那么这种知识在大脑的哪个位置？
336 00:26:10,121 --> 00:26:13,284 说话者 SPEAKER_01：而这种知识显然体现在突触的暂时变化中。
337 00:26:13,724 --> 00:26:16,008 说话人 SPEAKER_01：不是神经元在发出“cucumber，cucumber，cucumber”的声音。
338 00:26:16,508 --> 00:26:17,710 说话人 SPEAKER_01：你没有那么多的神经元。
339 00:26:18,391 --> 00:26:20,553 说话人 SPEAKER_01：这是权重临时变化。
340 00:26:21,253 --> 00:26:25,358 说话人 SPEAKER_01：你可以通过临时改变权重做很多事情，我称之为快速权重。
341 00:26:26,099 --> 00:26:27,682 说话人 SPEAKER_01：在这些神经网络模型中我们不这样做。
342 00:26:27,741 --> 00:26:35,531 说话人 SPEAKER_01：我们不这样做的原因是，如果你对权重的临时变化依赖于输入数据，
343 00:26:35,511 --> 00:26:39,538 说话人 SPEAKER_01：那么你无法同时处理大量不同的案例。
344 00:26:40,519 --> 00:26:50,297 说话人 SPEAKER_01：目前，我们取大量不同的字符串，将它们堆叠在一起，然后并行处理它们，因为这样我们可以进行矩阵-矩阵乘法，这要高效得多。
345 00:26:50,867 --> 00:26:55,031 说话人 SPEAKER_01：正是这种效率阻止了我们使用快速权重。
346 00:26:55,051 --> 00:26:58,575 说话人 SPEAKER_01：但大脑显然使用快速权重进行临时记忆。
347 00:26:59,035 --> 00:27:01,678 说话人 SPEAKER_01：而且你可以用这种方法做很多我们现在做不到的事情。
348 00:27:02,097 --> 00:27:03,779 说话人 SPEAKER_01：我认为这是我们必须要学习的一大要点。
349 00:27:04,019 --> 00:27:12,567 说话人 SPEAKER_01：我非常希望像 Graphcore 这样的技术如果能够进行顺序学习和在线学习，那么它们可以使用快速权重。
350 00:27:15,109 --> 00:27:16,211 说话人 SPEAKER_01：但到目前为止还没有成功。
351 00:27:16,612 --> 00:27:20,335 说话人 SPEAKER_01：我认为当人们使用电导率作为权重时，最终会成功的。
352 00:27:20,939 --> 00:27:28,647 说话人 SPEAKER_00：了解这些模型的工作原理以及了解大脑的工作原理如何影响了你的思维方式？
353 00:27:29,469 --> 00:27:43,385 说话人 SPEAKER_01：我认为有一个很大的影响，这在相当抽象的层面上，就是多年来人们非常轻视这样一个想法，那就是拥有一个大型的随机神经网络
354 00:27:43,567 --> 00:27:46,952 说话人 SPEAKER_01：并且给它大量的训练数据，它就会学会做复杂的事情。
355 00:27:47,413 --> 00:27:53,402 说话人 SPEAKER_01：如果你和统计学家或语言学家或大多数人工智能领域的专家交谈，他们会说，那只是一个空想。
356 00:27:53,422 --> 00:27:59,770 说话人 SPEAKER_01：没有某种天生的知识，没有大量的架构限制，你是不可能学会真正复杂的事情的。
357 00:28:00,451 --> 00:28:01,653 说话人 SPEAKER_01：结果完全错了。
358 00:28:02,294 --> 00:28:06,500 说话人 SPEAKER_01：你可以拿一个大型的随机神经网络，你可以从数据中学习到很多东西。
359 00:28:08,282 --> 00:28:11,887 说话人 SPEAKER_01：所以随机梯度下降的想法
360 00:28:11,867 --> 00:28:18,855 说话人 SPEAKER_01：通过梯度反复调整权重，这样会学习到东西，并且会学习到复杂的东西。
361 00:28:19,676 --> 00:28:22,339 说话人 SPEAKER_01：这一点已经通过这些大型模型得到了验证。
362 00:28:22,840 --> 00:28:24,843 说话人 SPEAKER_01：这是了解大脑非常重要的一点。
363 00:28:25,644 --> 00:28:27,586 说话人 SPEAKER_01：它不必具备所有这些先天的结构。
364 00:28:27,987 --> 00:28:36,036 说话人 SPEAKER_01：显然，它有很多先天的结构，但肯定不需要为那些容易学习的东西具备先天的结构。
365 00:28:36,016 --> 00:28:44,666 说话人 SPEAKER_01：因此，从乔姆斯基那里来的这种观点，除非它已经全部预先设定并成熟，否则你不会学到任何复杂的东西，比如语言。
366 00:28:45,428 --> 00:28:47,410 说话人 SPEAKER_01：这种观点现在显然是荒谬的。
367 00:28:48,230 --> 00:28:52,276 说话人 SPEAKER_00：我相信乔姆斯基会欣赏你称他的观点为荒谬。
368 00:28:52,817 --> 00:28:57,903 说话人 SPEAKER_01：实际上，我认为乔姆斯基的许多政治观点非常明智。
369 00:28:57,923 --> 00:29:03,390 说话人 SPEAKER_01：我总是很惊讶，一个对中东有如此明智看法的人，在语言学上却会犯错误。
370 00:29:04,011 --> 00:29:11,342 说话人 SPEAKER_00：您认为什么能让这些模型更有效地模拟人类的意识？
371 00:29:11,502 --> 00:29:26,805 说话人 SPEAKER_00：但想象一下，如果你有一个陪伴你一生的 AI 助手，而它不是像现在的 ChatGPT 那样删除对话记忆，每次都从头开始，它有自我反思的能力。
372 00:29:27,625 --> 00:29:32,752 说话人 SPEAKER_00：到了某个时候，你就去世了，你把这件事告诉助手。
373 00:29:33,172 --> 00:29:34,713 说话人 SPEAKER_00: 你觉得这合适吗？
374 00:29:34,734 --> 00:29:35,035 说话人 SPEAKER_00: 我是说，不是我自己。
375 00:29:35,055 --> 00:29:38,420 说话人 SPEAKER_01: 是别人告诉助手这样做的。
376 00:29:38,440 --> 00:29:38,540 说话人 SPEAKER_00: 是的。
377 00:29:38,560 --> 00:29:40,903 说话人 SPEAKER_00：你很难向助手说出那句话。
378 00:29:42,125 --> 00:29:44,971 说话人 SPEAKER_00：你认为在那个时刻助手会有感觉吗？
379 00:29:45,612 --> 00:29:47,595 说话人 SPEAKER_01：是的，我认为它们也可以有感觉。
380 00:29:47,915 --> 00:29:53,664 说话人 SPEAKER_01：所以我认为就像我们有这个感知的内戏模型一样，我们也有一个情感的内戏模型。
381 00:29:54,006 --> 00:29:58,673 说话人 SPEAKER_01：这些都是我能体验到的，但其他人却不能。
382 00:29:59,175 --> 00:30:00,978 说话人 SPEAKER_01：我认为那个模型同样错误。
383 00:30:01,919 --> 00:30:06,663 说话人 SPEAKER_01：所以我认为，假设我说，我觉得想打 Gary 的鼻子，我经常这么做。
384 00:30:07,625 --> 00:30:11,548 说话人 SPEAKER_01：让我们尝试将这个想法从内心剧院的概念中抽象出来。
385 00:30:11,969 --> 00:30:19,636 说话人 SPEAKER_01：我真正想对你说的是，如果我的前额叶皮层抑制没有发挥作用，我会采取行动。
386 00:30:20,377 --> 00:30:28,946 说话人 SPEAKER_01：所以当我们谈论情感时，我们实际上是在谈论如果没有约束我们会采取的行动。
387 00:30:29,163 --> 00:30:31,567 说话人 SPEAKER_01：这正是情感的本质。
388 00:30:31,586 --> 00:30:33,470 说话人 SPEAKER_01：它们是我们如果没有约束会采取的行动。
389 00:30:35,532 --> 00:30:40,380 说话人 SPEAKER_01：我认为你可以对情感进行同样的解释，这些事物拥有情感也没有什么理由。
390 00:30:40,921 --> 00:30:45,888 说话人 SPEAKER_01：实际上，我在 1973 年看到过机器人有情感。
391 00:30:46,762 --> 00:30:57,278 说话人 SPEAKER_01：所以在爱丁堡，他们有一个这样的机器人，有两个像这样的夹子，如果你把零件分别放在一块绿色的绒布上，它可以组装玩具车。
392 00:30:58,779 --> 00:31:02,605 说话人 SPEAKER_01：但是如果你把它们堆在一起，它的视觉还不够好，无法弄清楚发生了什么。
393 00:31:02,964 --> 00:31:04,548 说话人 SPEAKER_01：所以他把夹爪放在一起，然后他敲了一下。
394 00:31:05,269 --> 00:31:07,551 说话人 SPEAKER_01：它把它们敲散了，然后他可以把它们放在一起。
395 00:31:08,472 --> 00:31:13,319 说话人 SPEAKER_01：如果你看到一个人这样做，你会说他是因为不理解情况而破坏了它。
396 00:31:14,922 --> 00:31:15,623 说话人 SPEAKER_00：这很深刻。
397 00:31:17,324 --> 00:31:24,163 说话人 SPEAKER_00：在我们上次谈话时，你把人类和LLMs描述为类比机器。
398 00:31:24,784 --> 00:31:30,539 说话人 SPEAKER_00：你认为在你的一生中，最有力的类比是什么？
399 00:31:31,869 --> 00:31:33,492 说话人 SPEAKER_01：哦，在我的一生中？
400 00:31:33,833 --> 00:31:47,938 说话人 SPEAKER_01：我想可能是一种影响我很大的弱类比，即宗教信仰的类比。
401 00:31:47,917 --> 00:31:51,342 说话人 SPEAKER_01：并且在符号处理中的信仰之间。
402 00:31:51,362 --> 00:31:58,349 说话人 SPEAKER_01：所以在我很小的时候，我遇到了，我来自一个无神论家庭，上学时遇到了宗教信仰。
403 00:31:58,369 --> 00:31:59,590 说话人 SPEAKER_01：这对我来说只是胡说八道。
404 00:32:00,112 --> 00:32:01,232 说话人 SPEAKER_01：这对我来说仍然只是胡说八道。
405 00:32:02,153 --> 00:32:11,544 说话人 SPEAKER_01：当我把符号处理看作是对人们工作方式的解释时，我以为这只是一些无稽之谈。
406 00:32:11,564 --> 00:32:14,027 说话人 SPEAKER_01：现在我觉得这并不是那么多的无稽之谈了。
407 00:32:14,210 --> 00:32:17,453 说话人 SPEAKER_01：因为我认为，实际上，我们确实在进行符号处理。
408 00:32:17,814 --> 00:32:20,876 说话人 SPEAKER_01：只是我们通过给这些符号赋予大的嵌入向量来处理它们。
409 00:32:21,538 --> 00:32:23,099 说话人 SPEAKER_01：但实际上我们是符号处理。
410 00:32:24,141 --> 00:32:31,288 说话人 SPEAKER_01：但绝不是人们想象中的那种方式，即匹配符号，符号唯一的特点就是它与另一个符号相同，或者不同。
411 00:32:31,848 --> 00:32:33,270 说话人 SPEAKER_01：这就是符号的唯一属性。
412 00:32:33,951 --> 00:32:34,791 说话人 SPEAKER_01：我们根本不是这样做的。
413 00:32:34,811 --> 00:32:42,039 说话人 SPEAKER_01：我们利用上下文为符号提供嵌入向量，然后利用这些嵌入向量组件之间的交互来进行思考。
414 00:32:43,060 --> 00:32:43,862 说话人 SPEAKER_01：但是，
415 00:32:44,347 --> 00:32:53,263 说话人 SPEAKER_01：谷歌有一位非常优秀的研究员名叫费尔南多·佩雷拉，他说，是的，我们确实有符号推理能力，而我们唯一拥有的符号就是自然语言。
416 00:32:53,744 --> 00:32:56,088 说话人 SPEAKER_01：自然语言是一种符号语言，我们用它进行推理。
417 00:32:56,810 --> 00:32:57,592 说话人 SPEAKER_01：我相信现在是了。
418 00:32:58,192 --> 00:33:02,902 说话人 SPEAKER_00：你在计算机科学史上做了一些最有意义的研究。
419 00:33:03,784 --> 00:33:08,432 说话人 SPEAKER_00：你能给我们介绍一下，比如你是如何选择要解决的问题吗？
420 00:33:08,580 --> 00:33:09,902 说话人 SPEAKER_01：首先，让我纠正你一下。
421 00:33:10,623 --> 00:33:13,807 讲者 SPEAKER_01：我和我的学生们完成了很多最有意义的事情。
422 00:33:14,528 --> 00:33:19,573 讲者 SPEAKER_01：这主要得益于与学生们良好的合作以及我挑选优秀学生的能力。
423 00:33:20,314 --> 00:33:26,320 讲者 SPEAKER_01：这得益于在 70 年代、80 年代、90 年代和 2000 年代，从事神经网络研究的人很少。
424 00:33:26,961 --> 00:33:30,525 讲者 SPEAKER_01：因此，从事神经网络研究的少数人得以挑选最优秀的学生。
425 00:33:31,125 --> 00:33:32,867 说话人 SPEAKER_01：这真是个幸运的事。
426 00:33:32,887 --> 00:33:36,511 说话人 SPEAKER_01：但我的选问题方式基本上是
427 00:33:36,491 --> 00:33:45,237 说话人 SPEAKER_01：嗯，你知道，当科学家谈论他们是如何工作时，他们有关于他们是如何工作的理论，这些理论可能和真相关系不大，但我的理论是，我
428 00:33:46,684 --> 00:33:50,469 说话人 SPEAKER_01：寻找大家都同意某事，但感觉是错的。
429 00:33:51,269 --> 00:33:53,811 说话者 SPEAKER_01：只是有一种直觉，感觉有点不对劲。
430 00:33:54,512 --> 00:33:58,436 说话者 SPEAKER_01：然后我会继续研究，看看我能否解释为什么我认为它是错的。
431 00:33:58,777 --> 00:34:06,464 说话者 SPEAKER_01：也许我可以用一个小型计算机程序做一个演示，来展示它并不像你想象的那样工作。
432 00:34:07,125 --> 00:34:08,407 说话者 SPEAKER_01：那么让我举一个例子。
433 00:34:09,447 --> 00:34:13,452 说话人 SPEAKER_01：大多数人认为，如果你给神经网络添加噪声，它的工作效果会变差。
434 00:34:13,431 --> 00:34:23,556 说话人 SPEAKER_01：比如，每次你让训练样本通过时，让一半的神经元保持沉默，它的工作效果会变差。
435 00:34:24,318 --> 00:34:28,369 说话人 SPEAKER_01：实际上，我们知道这样做的话，泛化能力会更好。
436 00:34:28,670 --> 00:34:33,898 说话人 SPEAKER_01：你可以在一个简单的例子中证明这一点。
437 00:34:33,998 --> 00:34:35,882 说话人 SPEAKER_01：这就是计算机模拟的好处。
438 00:34:35,902 --> 00:34:43,637 说话人 SPEAKER_01：你可以展示，你知道，你之前认为添加噪声会使它变得更糟，以及去掉一半的神经元会使它工作得更差，这在短期内确实如此。
439 00:34:43,657 --> 00:34:46,722 说话人 SPEAKER_01：但如果你这样训练它，最终它会表现得更好。
440 00:34:47,262 --> 00:34:52,972 说话人 SPEAKER_01：你可以用一个小型计算机程序来证明这一点，然后你可以深入思考为什么会这样，以及它是如何停止的。
441 00:34:52,952 --> 00:34:55,836 说话人 SPEAKER_01：大的复杂共适应。
442 00:34:56,617 --> 00:34:59,121 说话人 SPEAKER_01：但是，我认为那是我工作的方法。
443 00:34:59,382 --> 00:35:05,550 说话人 SPEAKER_01：找到听起来可疑的东西，并对其进行分析，看看你是否能给出一个简单的证明为什么它是错误的。
444 00:35:06,251 --> 00:35:08,034 说话人 SPEAKER_00：现在什么听起来可疑？
445 00：35：08,875 --> 00：35：11,619 发言者 SPEAKER_01：嗯，我们不使用快速的重量听起来很可疑。
446 00：35：11,980 --> 00：35：13,422 演讲者 SPEAKER_01：我们只有这两个时间尺度。
447 00：35：13,641 --> 00：35：14,302 议长 SPEAKER_01：这完全错误。
448 00：35：14,322 --> 00：35：16,907 演讲者 SPEAKER_01：那一点也不像大脑。
449 00:35:17,146 --> 00:35:20,371 说话人 SPEAKER_01：从长远来看，我认为我们可能需要更多的时间尺度。
450 00:35:20,391 --> 00:35:21,652 说话人 SPEAKER_01：这就是现在的例子。
451 00:35:21,672 --> 00:35:22,875 说话人 SPEAKER_00：如果你有
452 00:35:23,327 --> 00:35:31,262 说话人 SPEAKER_00：如果你今天有你的学生小组，他们来找你，说，那么我们之前讨论过的汉明问题，你知道，在你领域里最重要的难题是什么？
453 00:35:32,483 --> 00:35:35,608 说话人 SPEAKER_00：您建议他们接下来做什么，做什么工作？
454 00:35:35,909 --> 00:35:41,639 说话人 SPEAKER_00：我们谈论了推理、时间表，您会给他们指出哪个是最优先解决的问题？
455 00:35:42,244 --> 00:35:50,496 说话人 SPEAKER_01：对我来说，现在的问题和过去 30 年一样，那就是，大脑是否进行反向传播？
456 00:35:51,097 --> 00:35:52,900 说话人 SPEAKER_01：我相信大脑正在获取梯度。
457 00:35:52,960 --> 00:35:56,706 说话人 SPEAKER_01：如果你得不到梯度，你的学习效果会比得到梯度的情况差得多。
458 00:35:57,447 --> 00:35:59,148 说话人 SPEAKER_01：但是大脑是如何得到梯度的呢？
459 00:35:59,210 --> 00:36:06,239 说话人 SPEAKER_01：它是通过某种方式实现反向传播的近似版本，还是采用了一种完全不同的技术？
460 00:36:06,519 --> 00:36:07,782 说话人 SPEAKER_01：这是一个重大的未解之谜。
461 00:36:08,382 --> 00:36:11,947 说话人 SPEAKER_01：如果我一直做研究，那我的研究方向就是这样的。
462 00:36:12,079 --> 00:36:23,574 说话人 SPEAKER_00：现在回顾你的职业生涯，你关于很多事情都看得很准，但有没有哪些方面是你曾经犯过错误，希望你能少花时间追求某个方向的？
463 00:36:24,010 --> 00:36:25,391 说话人 SPEAKER_01：好的，这两个问题是分开的。
464 00:36:25,512 --> 00:36:26,653 说话人 SPEAKER_01：一个是，你曾经犯过哪些错误？
465 00:36:27,255 --> 00:36:30,039 说话人 SPEAKER_01：那么，你希望自己在上面花的时间少一些吗？
466 00:36:30,639 --> 00:36:34,545 说话人 SPEAKER_01：我认为我对玻尔兹曼机的看法是错误的，我很高兴自己在上面花了很多时间。
467 00:36:35,045 --> 00:36:38,791 说话人 SPEAKER_01：关于如何得到梯度，比反向传播更美丽的理论要多得多。
468 00:36:39,172 --> 00:36:41,795 说话人 SPEAKER_01：反向传播只是普通而合理的，它只是链式法则。
469 00:36:42,297 --> 00:36:46,163 说话人 SPEAKER_01：玻尔兹曼机很聪明，这是一种非常有趣的方法来获取梯度。
470 00:36:46,923 --> 00:36:51,030 说话人 SPEAKER_01：我很希望大脑的工作方式是这样的，但我认为并不是。
471 00:36:51,719 --> 00:36:57,208 说话人 SPEAKER_00：您花了很多时间去想象这些系统发展之后的情景吗？
472 00:36:57,268 --> 00:37:09,809 说话人 SPEAKER_00：您有没有想过，如果我们能让这些系统工作得非常好，我们就可以，你知道的，普及教育，让知识更加容易获得，我们可以在医学上解决一些难题？
473 00:37:09,909 --> 00:37:13,255 说话人 SPEAKER_00：它对你来说更多的是关于理解大脑吗？
474 00:37:14,753 --> 00:37:23,329 说话人 SPEAKER_01：是的，我总觉得科学家应该做一些对社会有帮助的事情，但实际上这样并不能做出最好的研究。
475 00:37:23,349 --> 00:37:25,954 说话人 SPEAKER_01：你做出最好的研究时，是受到好奇心驱动的。
476 00:37:26,456 --> 00:37:29,842 说话人 SPEAKER_01：你只是需要理解某件事。
477 00:37:30,295 --> 00:37:35,184 说话人 SPEAKER_01：我最近才意识到这些事情可能带来很多危害，也可能带来很多好处。
478 00:37:35,704 --> 00:37:38,909 说话人 SPEAKER_01：我对它们对社会产生的影响越来越担忧。
479 00:37:39,271 --> 00:37:41,233 说话人 SPEAKER_01：但这并不是推动我的动力。
480 00:37:41,253 --> 00:37:44,199 说话人 SPEAKER_01：我只是想知道，大脑究竟是如何学会做这些事情的？
481 00:37:45,081 --> 00:37:45,862 说话人 SPEAKER_01：这就是我想知道的。
482 00:37:46,103 --> 00:37:47,085 说话人 SPEAKER_01：我有点失败了。
483 00:37:47,505 --> 00:37:50,751 说话人 SPEAKER_01：那次失败的副作用，我们得到了一些不错的工程成果。
484 00:37:52,181 --> 00:37:54,505 说话人 SPEAKER_00：是的，对世界来说那是一个好的失败。
如果您从那些可能真正成功的事物角度出发，您认为最有希望的应用是什么？
486 00：38：03,822 --> 00：38：07,710 演讲者 SPEAKER_01：我认为医疗保健显然是一个大问题。
487 00：38：07,690 --> 00：38：13,536 演讲者 SPEAKER_01：对于医疗保健，医疗保健社会可以吸收的数量几乎没有穷无尽。
如果您带一个老人，他们可能需要五名医生全职服务。
所以当人工智能在做事方面比人类更好时，你希望它在你可以用更多这种东西的领域变得更好。
490 00：38：29,931 --> 00：38：31,820 演讲者 SPEAKER_01：我们可以有更多的医生来做。
如果每个人都能有自己的三位医生，那就太好了，我们很快就会达到那个水平。
492 00：38：38,327 --> 00：38：39,873 演讲者 SPEAKER_01：这就是医疗保健好的原因之一。
493 00：38：41,034 --> 00：38：56,431 演讲者 SPEAKER_01：此外，仅在新工程中，开发新材料，例如，用于更好的太阳能电池板或超导性，或者只是为了了解人体的工作原理，这将产生巨大的影响。
494 00：38：56,932 --> 00：38：58,134 议长 SPEAKER_01：这些都会是好事。
495 00：38：58,655 --> 00：39：02,059 议长 SPEAKER_01：我担心的是坏人利用他们做坏事。
496 00：39：02,659 --> 00：39：07,784 议长 SPEAKER_01：我们帮助普京、习或特朗普等人使用
497 00：39：07,764 --> 00：39：12,893 演讲者 SPEAKER_01：人工智能用于杀手机器人，或用于纵公众舆论，或用于大规模监控。
498 00：39：13,536 --> 00：39：14,878 议长 SPEAKER_01：这些都是非常令人担忧的事情。
499 00：39：15,539 --> 00：39：22,010 议长 SPEAKER_00：你有没有担心过放慢速度也会减慢积极的一面？
500 00：39：22,447 --> 00：39：23,228 议长 SPEAKER_01：哦，当然。
501 00:39:23,807 --> 00:39:29,954 说话人 SPEAKER_01：我认为这个领域不太可能减速，部分原因是它是国际化的。
502 00:39:30,536 --> 00:39:33,077 说话人 SPEAKER_01：如果一个国家减速，其他国家也不会减速。
503 00:39:34,099 --> 00:39:38,903 说话人 SPEAKER_01：所以很明显，中国和美国之间存在一场竞赛，双方都不会减速。
504 00:39:39,644 --> 00:39:43,108 说话人 SPEAKER_01：所以，我意思是，有人提出了一份请愿书，说我们应该减速六个月。
505 00:39:43,168 --> 00:39:46,172 说话人 SPEAKER_01：我之所以没有签字，并不是因为我认为这永远不会发生。
506 00:39:46,612 --> 00:39:50,817 说话人 SPEAKER_01：也许我应该签字，因为尽管这永远不会发生，但它也提出了一个政治观点。
507 00:39:50,797 --> 00:39:53,902 说话人 SPEAKER_01：常常要求一些你知道不可能得到的东西，只是为了表明立场，这通常是好的。
508 00:39:55,103 --> 00:39:56,244 说话人 SPEAKER_01：但我认为我们不会减速。
509 00:39:57,126 --> 00:40:03,976 说话人 SPEAKER_00：那么您认为拥有这些助手会如何影响人工智能研究过程呢？
510 00:40:03,996 --> 00:40:05,298 说话人 SPEAKER_01：我认为这将使它变得更加高效。
511 00:40:05,418 --> 00:40:14,914 说话人 SPEAKER_01：当您拥有这些助手来帮助您编程，并帮助您思考问题，甚至可能帮助您解决方程时，人工智能研究将变得更加高效。
512 00:40:15,164 --> 00:40:19,614 说话人 SPEAKER_00：您是否对人才选拔过程进行了很多思考？
513 00:40:19,635 --> 00:40:21,418 说话人 SPEAKER_00：这对你来说主要是否直观？
514 00:40:21,458 --> 00:40:26,592 说话人 SPEAKER_00：比如当伊利亚出现在门口时，你感觉，这是个聪明人，我们可以一起工作。
515 00:40:26,875 --> 00:40:30,280 说话人 SPEAKER_01：所以在选拔人才时，有时候你只是知道。
516 00:40:30,320 --> 00:40:34,427 说话人 SPEAKER_01：所以在和伊利亚不太长时间的交谈后，他看起来非常聪明。
517 00:40:34,989 --> 00:40:40,539 说话者 SPEAKER_01：然后和他聊了一会儿，他显然非常聪明，而且直觉很好，数学也很擅长。
518 00:40:41,440 --> 00:40:42,443 说话者 SPEAKER_01：那是个毫无疑问的选择。
519 00:40:42,862 --> 00:40:46,769 说话者 SPEAKER_01：还有另一个例子，我在 NIPS 会议上。
520 00:40:47,831 --> 00:40:50,536 说话者 SPEAKER_01：我们有一个展板，有人过来
521 00:40:50,516 --> 00:41:00,277 讲者 SPEAKER_01：他开始询问海报，他问的每一个问题都深入剖析了我们做错的地方，五分钟后我向他提供了一个博士后职位。
522 00:41:00,958 --> 00:41:08,514 讲者 SPEAKER_01：那个人是大卫·麦凯，他非常出色，非常遗憾他去世了，但他确实非常明显是你想要的人。
523 00:41:09,775 --> 00:41:11,579 讲者 SPEAKER_01：有时并不那么明显。
524 00:41:11,559 --> 00:41:15,443 讲者 SPEAKER_01：我学到的一点是，人们是不同的。
525 00:41:15,463 --> 00:41:17,766 Speaker SPEAKER_01: 并不是只有一种优秀学生的类型。
526 00:41:20,248 --> 00:41:26,856 Speaker SPEAKER_01: 所以有一些学生可能不是很富有创造力，但技术能力极强，能将任何东西都运作起来。
527 00:41:27,818 --> 00:41:30,581 Speaker SPEAKER_01: 还有一些学生技术能力不强，但非常富有创造力。
528 00:41:31,260 --> 00:41:33,784 Speaker SPEAKER_01: 当然，你希望他们两者兼备，但并不总是能得到这种学生。
529 00:41:34,264 --> 00:41:35,525 说话人 SPEAKER_01: 但我认为
530 00:41:35,708 --> 00:41:38,972 说话人 SPEAKER_01: 实际上在实验室里你需要各种不同类型的硕士研究生。
531 00:41:39,954 --> 00:41:46,563 说话人 SPEAKER_01: 但我仍然依赖我的直觉，有时候你跟某人交谈，他们就能立刻理解。
532 00:41:47,565 --> 00:41:48,545 说话人 SPEAKER_01: 这些就是你想要的。
533 00:41:49,126 --> 00:41:54,474 说话人 SPEAKER_00: 你认为为什么有些人有更好的直觉？
534 00:41:54,494 --> 00:41:57,599 说话人 SPEAKER_00: 他们是不是只是比其他人有更好的训练数据？
535 00:41:57,760 --> 00:42:00,163 说话人 SPEAKER_00: 或者你该如何培养你的直觉？
536 00:42:01,527 --> 00:42:05,050 说话人 SPEAKER_01: 我认为部分原因是因为他们不会站在无意义的事情上。
537 00:42:05,731 --> 00:42:07,733 说话人 SPEAKER_01：这就是获得错误直觉的方法。
538 00:42:08,233 --> 00:42:09,394 说话人 SPEAKER_01：相信你被告知的每一件事。
539 00:42:10,036 --> 00:42:10,856 说话人 SPEAKER_01：这是致命的。
540 00:42:11,516 --> 00:42:14,480 说话人 SPEAKER_01：你必须能够，我想，这是有些人所做的事情。
541 00:42:14,500 --> 00:42:16,762 说话人 SPEAKER_01：他们有一个理解现实的完整框架。
542 00:42:17,503 --> 00:42:22,969 说话人 SPEAKER_01：当有人告诉他们一些事情时，他们会试图弄清楚那如何融入他们的框架。
543 00:42:23,510 --> 00:42:25,311 说话人 SPEAKER_01：如果它不吻合，他们就会拒绝。
544 00:42:26,713 --> 00:42:28,614 说话人 SPEAKER_01：这是一个非常好的策略。
545 00:42:28,882 --> 00:42:37,610 说话人 SPEAKER_01：试图将所被告知的一切都纳入框架的人，最终会得到一个模糊不清的框架，似乎可以相信一切。
546 00:42:38,492 --> 00:42:40,257 说话人 SPEAKER_01：这是毫无用处的。
547 00:42:40,525 --> 00:42:54,324 说话人 SPEAKER_01：所以我认为，实际上拥有一个强大的世界观，并试图将 incoming facts（ incoming facts）调整以适应你的观点，显然可能导致你陷入深度的宗教信仰和致命的缺陷等等，就像我对玻尔兹曼机的信仰一样。
548 00:42:55,746 --> 00:42:56,766 说话人 SPEAKER_01：但我认为这是正确的方向。
549 00:42:57,407 --> 00:43:00,932 说话人 SPEAKER_01：如果你有好的直觉，你应该相信它们。
550 00:43:00,952 --> 00:43:04,777 说话人 SPEAKER_01：如果你有不好的直觉，无论你做什么都没关系，所以你不妨相信它们。
551 00:43:06,985 --> 00:43:08,708 说话人 SPEAKER_00：非常好的观点。
552 00:43:09,610 --> 00:43:23,012 说话人 SPEAKER_00：当你看到今天正在进行的研究类型时，你认为我们是不是把所有的鸡蛋都放在了一个篮子里，我们应该在研究领域中更多地多样化我们的想法？
553 00:43:23,092 --> 00:43:25,356 说话人 SPEAKER_00: 你认为这是最有希望的方向吗？
554 00:43:25,476 --> 00:43:27,119 说话人 SPEAKER_00: 那我们就全力以赴吧。
555 00:43:28,282 --> 00:43:38,820 说话人 SPEAKER_01: 我认为拥有大型模型并在多模态数据上进行训练，即使只是为了预测下一个单词，也是一种非常有希望的方法，我们应该全力以赴。
556 00:43:38,840 --> 00:43:44,512 说话人 SPEAKER_01: 显然，现在有很多人在做这件事，而且他们做了一些显然疯狂的事情，这是好事。
557 00:43:45,532 --> 00:43:50,222 说话人 SPEAKER_01：我认为大多数人跟随这条道路是没问题的，因为它运作得非常好。
558 00:43:50,405 --> 00:43:56,201 说话人 SPEAKER_00：你认为学习算法很重要，还是只是规模问题？
559 00:43:56,862 --> 00:44:06,068 说话人 SPEAKER_00：我们基本上有数百万种达到人类水平智能的方法，还是只有少数几种需要我们去发现？
560 00:44:06,469 --> 00:44:16,505 说话人 SPEAKER_01：是的，所以关于特定的学习算法是否非常重要，或者是否存在大量能够完成工作的学习算法，我不知道答案。
561 00:44:16,565 --> 00:44:21,735 讲者 SPEAKER_01：在我看来，尽管如此，反向传播在某种程度上是正确的做法。
562 00:44:22,034 --> 00:44:31,110 讲者 SPEAKER_01：获取梯度以便改变参数使其工作得更好，这似乎是正确的事情，而且它取得了惊人的成功。
563 00:44:31,090 --> 00:44:39,159 讲者 SPEAKER_01：可能确实存在其他学习算法，它们是获取相同梯度或获取其他梯度的替代方法，并且也有效。
564 00:44:40,059 --> 00:44:52,434 讲者 SPEAKER_01：我认为这是一个非常有趣的问题，即是否还有其他可以尝试并最大化以获得良好系统的事情，也许大脑之所以这样做是因为这样做更容易。
565 00:44:53,594 --> 00:44:59,240 讲者 SPEAKER_01：但从某种意义上说，反向传播是正确的做法，我们知道这样做效果非常好。
566 00:45:00,402 --> 00:45:06,376 讲者 SPEAKER_00：在你回顾几十年的研究时，你最自豪的是什么？
567 00:45:06,396 --> 00:45:07,539 讲者 SPEAKER_00：是学生吗？
568 00:45:07,559 --> 00:45:08,481 讲者 SPEAKER_00：是研究吗？
569 00:45:08,641 --> 00:45:12,369 说话人 SPEAKER_00：回顾你一生的工作，什么让你感到最自豪？
570 00:45:12,922 --> 00:45:14,885 说话人 SPEAKER_01：玻尔兹曼机的学习算法。
571 00:45:15,666 --> 00:45:37,436 说话人 SPEAKER_01：所以玻尔兹曼机的学习算法非常优雅，可能在实践中毫无希望，但这是我与 Terry 一起最享受的开发过程，也是我最自豪的，即使它是错误的。
572 00:45:37,456 --> 00:45:41,681 说话人 SPEAKER_00：你现在花最多时间思考哪些问题？
573 00：45：42,268 --> 00：45：45,681 演讲者 SPEAKER_01：我应该在 Netflix 上看什么？
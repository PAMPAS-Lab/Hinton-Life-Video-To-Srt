1
00:00:00,031 --> 00:00:12,208
Speaker SPEAKER_00: So I guess I'd like to make one comment about what Sylvia just said, which is some politicians don't believe in institutions because if those institutions had functioned properly, they'd already be in jail.

2
00:00:16,053 --> 00:00:17,835
Speaker SPEAKER_00: I don't want to mention any names.

3
00:00:20,219 --> 00:00:25,105
Speaker SPEAKER_00: So I've got 10 minutes and I wanted to basically say one thing.

4
00:00:25,725 --> 00:00:29,852
Speaker SPEAKER_00: If you take a problem like climate change,

5
00:00:31,030 --> 00:00:39,625
Speaker SPEAKER_00: The first thing you have to do is convince people that carbon dioxide produced by people is what's causing climate change.

6
00:00:40,226 --> 00:00:42,530
Speaker SPEAKER_00: Until you've done that, you can't have a sensible policy.

7
00:00:43,171 --> 00:00:45,415
Speaker SPEAKER_00: Even after you've done that, you may not get a sensible policy.

8
00:00:45,576 --> 00:00:51,267
Speaker SPEAKER_00: People may still subsidize oil companies and things, but that's a first step.

9
00:00:52,124 --> 00:00:54,789
Speaker SPEAKER_00: I've been talking about the existential threat of AI.

10
00:00:55,109 --> 00:00:56,332
Speaker SPEAKER_00: This is a longer-term threat.

11
00:00:56,351 --> 00:01:05,609
Speaker SPEAKER_00: There's many, many short-term threats which are urgent, like cyber attacks and loss of jobs and pandemics, and they go on and on, fake videos.

12
00:01:07,593 --> 00:01:14,486
Speaker SPEAKER_00: But there's this longer-term existential threat that we will create things more intelligent than ourselves, and they will take over.

13
00:01:16,001 --> 00:01:19,225
Speaker SPEAKER_00: Many people don't take that seriously.

14
00:01:19,887 --> 00:01:27,436
Speaker SPEAKER_00: One of the reasons they don't take that seriously is because they don't think that the current AI systems we have really understand.

15
00:01:28,539 --> 00:01:34,466
Speaker SPEAKER_00: There's a group of people, many of them linguists influenced by Chomsky,

16
00:01:34,953 --> 00:01:37,296
Speaker SPEAKER_00: who call these things stochastic parrots.

17
00:01:37,796 --> 00:01:46,789
Speaker SPEAKER_00: And they think these things are just a statistical trick that takes a big body of text and just pastiches things together and looks like it understands, but doesn't really understand the way we do.

18
00:01:48,010 --> 00:01:54,718
Speaker SPEAKER_00: Now, to have that theory that they don't understand the way we do, you have to have a theory of how we understand.

19
00:01:56,180 --> 00:01:58,983
Speaker SPEAKER_00: I'm going to argue they understand just like we do.

20
00:02:00,245 --> 00:02:00,927
Speaker SPEAKER_00: So,

21
00:02:01,548 --> 00:02:14,725
Speaker SPEAKER_00: The people who talk about stochastic parrots, they have a theory of understanding that comes from classical symbolic AI, that in your head you have symbolic expressions in some cleaned-up language, and you use symbolic rules to manipulate them.

22
00:02:15,146 --> 00:02:16,649
Speaker SPEAKER_00: That theory never really worked.

23
00:02:20,054 --> 00:02:29,866
Speaker SPEAKER_00: But they still stick to it, because somehow they think the only way you could have intelligence is by having something like logic to do reasoning with.

24
00:02:29,965 --> 00:02:32,229
Speaker SPEAKER_00: and they think the essence of intelligence is reasoning.

25
00:02:32,990 --> 00:02:43,729
Speaker SPEAKER_00: There's a completely different paradigm, which is the essence of intelligence is learning, and it's learning in the neural net, and things like vision and motor control are primary, and language and reasoning comes later.

26
00:02:47,675 --> 00:02:49,780
Speaker SPEAKER_00: But I want to address this issue of

27
00:02:50,097 --> 00:02:51,399
Speaker SPEAKER_00: do they really understand?

28
00:02:52,381 --> 00:03:07,789
Speaker SPEAKER_00: And there's one particular piece of history that most people don't know, which is these large language models, which certainly appear to understand and can answer any question you ask them at the level of a not very good expert, they came a long time ago.

29
00:03:07,810 --> 00:03:09,332
Speaker SPEAKER_00: I like to think like this.

30
00:03:09,955 --> 00:03:11,818
Speaker SPEAKER_00: They came from a model I

31
00:03:11,900 --> 00:03:15,048
Speaker SPEAKER_00: did in 1985, which was the first neural net language model.

32
00:03:16,391 --> 00:03:21,926
Speaker SPEAKER_00: It had 104 training examples instead of a trillion.

33
00:03:22,531 --> 00:03:23,372
Speaker SPEAKER_00: or many billions.

34
00:03:24,114 --> 00:03:27,038
Speaker SPEAKER_00: It had about 1,000 weights in the network instead of a trillion.

35
00:03:28,241 --> 00:03:45,432
Speaker SPEAKER_00: But it was a language model that was trained to predict the next word and to back propagate errors from the prediction in order to convert input symbols into vectors of neural activity, and then learn how those vectors should interact to predict the vector for the symbol it was trying to predict.

36
00:03:45,852 --> 00:03:49,197
Speaker SPEAKER_00: Now, the point of that didn't have an engineering point.

37
00:03:49,236 --> 00:03:54,084
Speaker SPEAKER_00: The point of it was a theory of how people could understand the meanings of words.

38
00:03:57,288 --> 00:04:04,098
Speaker SPEAKER_00: So the best model we have of how people understand sentences is these language models.

39
00:04:05,461 --> 00:04:08,385
Speaker SPEAKER_00: That's the only model we have of how people do it that actually works.

40
00:04:09,046 --> 00:04:10,389
Speaker SPEAKER_00: We have all these symbolic models.

41
00:04:11,370 --> 00:04:13,193
Speaker SPEAKER_00: They don't really work very well.

42
00:04:13,290 --> 00:04:21,249
Speaker SPEAKER_00: They came, I mean, they're influenced strongly by Chomsky, who managed to convince many generations of linguists that language is not learned.

43
00:04:21,930 --> 00:04:25,598
Speaker SPEAKER_00: On the face of it, it's just obviously absurd to say language isn't learned.

44
00:04:26,120 --> 00:04:30,168
Speaker SPEAKER_00: And if you can get people to believe something obviously absurd, then you've got a cult.

45
00:04:30,925 --> 00:04:32,966
Speaker SPEAKER_00: And Chomsky had a cult.

46
00:04:34,449 --> 00:04:38,413
Speaker SPEAKER_00: Language is learned, and we now can see things that learn language.

47
00:04:38,834 --> 00:04:41,737
Speaker SPEAKER_00: The structure doesn't have to be innate, it comes from data.

48
00:04:42,077 --> 00:04:47,985
Speaker SPEAKER_00: There has to be innate structure in the neural network and in the learning algorithm, but all the structure of language you can just get from data.

49
00:04:48,505 --> 00:04:51,870
Speaker SPEAKER_00: Chomsky couldn't see how to do that, so he said it must be innate.

50
00:04:51,850 --> 00:05:10,887
Speaker SPEAKER_00: Actually saying it must be innate and it's not learned is really stupid because that's saying evolution learned it rather than learning and Evolution is a much slower Process than learning the reason evolution produced brains is so you could learn stuff faster than evolution can make it innate so

51
00:05:12,757 --> 00:05:19,634
Speaker SPEAKER_00: The point of this ramble was to convince you that they understand the same way we do.

52
00:05:20,095 --> 00:05:21,858
Speaker SPEAKER_00: And I'll give you one more piece of evidence for that.

53
00:05:23,242 --> 00:05:31,240
Speaker SPEAKER_00: So, many people who talk about stochastic parrots say, look, I can show you they don't really understand because they just hallucinate stuff, they just make stuff up.

54
00:05:32,788 --> 00:05:34,732
Speaker SPEAKER_00: Those people are not psychologists.

55
00:05:34,771 --> 00:05:37,815
Speaker SPEAKER_00: They don't understand that they shouldn't use the word hallucinate.

56
00:05:38,297 --> 00:05:40,240
Speaker SPEAKER_00: They should use the word confabulate.

57
00:05:40,699 --> 00:05:44,605
Speaker SPEAKER_00: And psychologists since the 1930s have been studying human confabulation.

58
00:05:44,886 --> 00:05:46,247
Speaker SPEAKER_00: A psychologist called Bartlett.

59
00:05:46,968 --> 00:05:49,872
Speaker SPEAKER_00: And people confabulate all the time.

60
00:05:50,434 --> 00:05:55,961
Speaker SPEAKER_00: If you take any event that happened a long time ago and that you haven't rehearsed in the meantime,

61
00:05:56,278 --> 00:06:00,745
Speaker SPEAKER_00: and you try and remember it, you will confidently remember all sorts of things that are wrong.

62
00:06:01,607 --> 00:06:05,432
Speaker SPEAKER_00: Because memory doesn't consist of getting a file out of somewhere.

63
00:06:06,113 --> 00:06:09,158
Speaker SPEAKER_00: Memory consists of constructing something that seems plausible.

64
00:06:09,860 --> 00:06:16,149
Speaker SPEAKER_00: Now, if you've just seen something, and now you try and construct something that seems plausible, you'll have fairly accurate details.

65
00:06:16,910 --> 00:06:24,581
Speaker SPEAKER_00: But if you saw it many years ago, and you now try and construct something that seems plausible, first of all, it'll be influenced by all the stuff you learned in the meantime.

66
00:06:24,562 --> 00:06:31,228
Speaker SPEAKER_00: And you'll construct something that sounds good to you, but actually many of the details that you're very confident about will be just wrong.

67
00:06:31,709 --> 00:06:36,793
Speaker SPEAKER_00: It's hard to show that, but there's one case studied by a psychologist called Ulrich Neisser, which is beautiful.

68
00:06:37,853 --> 00:06:46,882
Speaker SPEAKER_00: John Dean testified at Watergate under oath about the cover-up going on in the Oval Office, and he didn't know there were tapes.

69
00:06:47,803 --> 00:06:51,226
Speaker SPEAKER_00: So you've got someone trying to tell the truth about things that happened a few years ago,

70
00:06:52,134 --> 00:06:55,399
Speaker SPEAKER_00: And much of what he said was not true.

71
00:06:56,120 --> 00:06:57,603
Speaker SPEAKER_00: But he was clearly trying to tell the truth.

72
00:06:57,624 --> 00:06:59,225
Speaker SPEAKER_00: He'd say, there was this meeting between these people.

73
00:06:59,526 --> 00:07:00,908
Speaker SPEAKER_00: No, those people never had a meeting.

74
00:07:01,389 --> 00:07:02,391
Speaker SPEAKER_00: And this person said this.

75
00:07:02,550 --> 00:07:03,913
Speaker SPEAKER_00: No, that person didn't say that.

76
00:07:04,173 --> 00:07:05,797
Speaker SPEAKER_00: Somebody else said that in a different meeting.

77
00:07:06,418 --> 00:07:08,860
Speaker SPEAKER_00: But the point is, he was conveying the truth about the cover-up.

78
00:07:09,663 --> 00:07:11,586
Speaker SPEAKER_00: And he was confident that what he was saying was true.

79
00:07:12,406 --> 00:07:13,208
Speaker SPEAKER_00: And he was just wrong.

80
00:07:14,189 --> 00:07:16,293
Speaker SPEAKER_00: And that's just the way human memory works.

81
00:07:17,033 --> 00:07:19,999
Speaker SPEAKER_00: And so when these things confabulate,

82
00:07:20,940 --> 00:07:22,192
Speaker SPEAKER_00: They're just like people.

83
00:07:22,394 --> 00:07:23,302
Speaker SPEAKER_00: People confabulate.

84
00:07:24,189 --> 00:07:25,036
Speaker SPEAKER_00: At least I think they do.

85
00:07:25,057 --> 00:07:25,904
Speaker SPEAKER_00: I just made that up.


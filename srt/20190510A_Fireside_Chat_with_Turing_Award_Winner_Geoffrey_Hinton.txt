1
00:00:06,275 --> 00:00:07,517
Speaker SPEAKER_01: Hello, I'm Nicholas Thompson.

2
00:00:07,597 --> 00:00:09,038
Speaker SPEAKER_01: I'm the editor-in-chief of Wired.

3
00:00:09,359 --> 00:00:13,544
Speaker SPEAKER_01: It is my honor today to get the chance to interview Jeffrey Hinton.

4
00:00:13,564 --> 00:00:18,289
Speaker SPEAKER_01: There are a couple, well, there are many things I love about him, but two that I'll just mention in the introduction.

5
00:00:19,111 --> 00:00:21,152
Speaker SPEAKER_01: The first is that he persisted.

6
00:00:21,213 --> 00:00:28,242
Speaker SPEAKER_01: He had an idea that he really believed in that everybody else said was bad, and he just kept at it.

7
00:00:28,742 --> 00:00:32,146
Speaker SPEAKER_01: And it gives a lot of faith to everybody who has bad ideas, myself included.

8
00:00:32,228 --> 00:00:36,774
Speaker SPEAKER_01: And then the second is someone who spends half his life as a manager adjudicating job titles.

9
00:00:37,494 --> 00:00:43,301
Speaker SPEAKER_01: I was looking at his job title before the introduction, and he has the most non-pretentious job title in history.

10
00:00:43,362 --> 00:00:47,188
Speaker SPEAKER_01: So please welcome Jeffrey Hinton, the engineering fellow at Google.

11
00:00:55,978 --> 00:00:56,298
Speaker SPEAKER_01: Welcome.

12
00:00:56,840 --> 00:00:57,740
Speaker SPEAKER_01: So nice to be here with you.

13
00:00:58,281 --> 00:01:02,067
Speaker SPEAKER_01: All right, so let us start 20 years ago.

14
00:01:02,570 --> 00:01:12,870
Speaker SPEAKER_01: When you write some of your early, very influential papers, everybody starts to say, this is a smart idea, but we're not actually going to be able to design computers this way.

15
00:01:13,751 --> 00:01:18,941
Speaker SPEAKER_01: Explain why you persisted, why you were so confident that you had found something important.

16
00:01:19,799 --> 00:01:22,283
Speaker SPEAKER_00: So actually, it was 40 years ago.

17
00:01:23,825 --> 00:01:27,209
Speaker SPEAKER_00: And it seemed to me there's no other way the brain could work.

18
00:01:27,349 --> 00:01:30,433
Speaker SPEAKER_00: It has to work by learning the strengths of connections.

19
00:01:31,555 --> 00:01:35,221
Speaker SPEAKER_00: And if you want to make a device do something intelligent, you've got two options.

20
00:01:35,260 --> 00:01:36,722
Speaker SPEAKER_00: You can program it, or it can learn.

21
00:01:37,103 --> 00:01:40,929
Speaker SPEAKER_00: And we certainly weren't programmed, so we had to learn.

22
00:01:41,010 --> 00:01:42,751
Speaker SPEAKER_00: So this had to be the right way to go.

23
00:01:43,052 --> 00:01:45,376
Speaker SPEAKER_01: So explain, though, well, let's do this.

24
00:01:45,736 --> 00:01:46,938
Speaker SPEAKER_01: Explain what neural networks are.

25
00:01:46,978 --> 00:01:52,546
Speaker SPEAKER_01: Most of the people here will be quite familiar, but explain the original insight and how it developed in your mind.

26
00:01:53,429 --> 00:01:57,674
Speaker SPEAKER_00: So you have relatively simple processing elements that are very loosely models of neurons.

27
00:01:58,656 --> 00:02:00,058
Speaker SPEAKER_00: They have connections coming in.

28
00:02:00,498 --> 00:02:01,820
Speaker SPEAKER_00: Each connection has a weight on it.

29
00:02:02,542 --> 00:02:04,504
Speaker SPEAKER_00: That weight can be changed to do learning.

30
00:02:05,325 --> 00:02:11,034
Speaker SPEAKER_00: And what a neuron does is take the activities on the connections times the weights, adds them all up,

31
00:02:11,014 --> 00:02:13,078
Speaker SPEAKER_00: and then decides whether to send an output.

32
00:02:13,998 --> 00:02:15,901
Speaker SPEAKER_00: And if it gets a big enough sum, it sends an output.

33
00:02:16,522 --> 00:02:18,504
Speaker SPEAKER_00: If the sum is negative, it doesn't send anything.

34
00:02:19,306 --> 00:02:19,806
Speaker SPEAKER_00: That's about it.

35
00:02:20,908 --> 00:02:30,122
Speaker SPEAKER_00: And all you have to do is just wire up a gazillion of those with a gazillion squared weights, and just figure out how to change the weights, and it'll do anything.

36
00:02:30,481 --> 00:02:31,764
Speaker SPEAKER_00: It's just a question of how you change the weights.

37
00:02:32,846 --> 00:02:39,655
Speaker SPEAKER_01: So when did you come to understand that this was an approximate representation of how the brain works?

38
00:02:40,342 --> 00:02:41,765
Speaker SPEAKER_00: Oh, it was always designed as that.

39
00:02:42,467 --> 00:02:44,312
Speaker SPEAKER_00: It was designed to be like how the brain works.

40
00:02:44,331 --> 00:02:45,413
Speaker SPEAKER_01: But let me ask you this.

41
00:02:45,433 --> 00:02:48,319
Speaker SPEAKER_01: So at some point in your career, you start to understand how the brain works.

42
00:02:48,360 --> 00:02:49,362
Speaker SPEAKER_01: Maybe it was when you were 12.

43
00:02:49,423 --> 00:02:50,305
Speaker SPEAKER_01: Maybe it was when you were 25.

44
00:02:50,485 --> 00:02:55,094
Speaker SPEAKER_01: When do you make the decision that you will try to model computers after the brain?

45
00:02:55,800 --> 00:02:57,862
Speaker SPEAKER_00: Sort of right away, that was the whole point of it.

46
00:02:59,243 --> 00:03:05,610
Speaker SPEAKER_00: The whole idea was to have a learning device that learned like the brain, like people think the brain learns, by changing connection strengths.

47
00:03:06,009 --> 00:03:07,211
Speaker SPEAKER_00: And this wasn't my idea.

48
00:03:07,270 --> 00:03:08,793
Speaker SPEAKER_00: Turing had the same idea.

49
00:03:09,092 --> 00:03:18,420
Speaker SPEAKER_00: Turing, even though he invented a lot of the basis of standard computer science, he believed that the brain was this unorganized device with random weights.

50
00:03:19,062 --> 00:03:22,564
Speaker SPEAKER_00: And it would use reinforcement learning to change the connections.

51
00:03:23,086 --> 00:03:23,925
Speaker SPEAKER_00: And it would learn everything.

52
00:03:24,127 --> 00:03:25,807
Speaker SPEAKER_00: And he thought that was the best route to intelligence.

53
00:03:25,788 --> 00:03:31,597
Speaker SPEAKER_01: And so you were following Turing's idea that the best way to make a machine is to model it after the human brain.

54
00:03:31,918 --> 00:03:34,260
Speaker SPEAKER_01: This is how a human brain works, so let's make a machine like that.

55
00:03:34,401 --> 00:03:35,502
Speaker SPEAKER_00: Yeah, it wasn't just Turing's idea.

56
00:03:35,563 --> 00:03:36,784
Speaker SPEAKER_00: Lots of people thought that back then.

57
00:03:36,806 --> 00:03:37,787
Speaker SPEAKER_01: All right, so you have this idea.

58
00:03:37,847 --> 00:03:38,989
Speaker SPEAKER_01: Lots of people have this idea.

59
00:03:39,049 --> 00:03:42,995
Speaker SPEAKER_01: You get a lot of credit in the late 80s.

60
00:03:43,034 --> 00:03:45,419
Speaker SPEAKER_01: You start to come to fame with your published work.

61
00:03:45,438 --> 00:03:46,120
Speaker SPEAKER_01: Is that correct?

62
00:03:46,139 --> 00:03:46,580
Speaker SPEAKER_00: Yes.

63
00:03:46,561 --> 00:03:48,366
Speaker SPEAKER_01: When is the darkest moment?

64
00:03:48,385 --> 00:03:56,486
Speaker SPEAKER_01: When is the moment where other people who've been working, who agreed with this idea from Turing, start to back away, and yet you continue to plunge ahead?

65
00:03:58,474 --> 00:04:02,320
Speaker SPEAKER_00: there were always a bunch of people who kept believing in it, particularly in psychology.

66
00:04:02,983 --> 00:04:11,556
Speaker SPEAKER_00: But among computer scientists, I guess in the 90s, what happened was data sets were quite small, and computers weren't that fast.

67
00:04:12,377 --> 00:04:18,288
Speaker SPEAKER_00: And on small data sets, other methods, like things called support vector machines, worked a little bit better.

68
00:04:18,949 --> 00:04:21,293
Speaker SPEAKER_00: They didn't get confused by noise so much.

69
00:04:21,273 --> 00:04:24,819
Speaker SPEAKER_00: And so that was very depressing, because we developed backpropagation in the 80s.

70
00:04:25,321 --> 00:04:26,483
Speaker SPEAKER_00: We thought it would solve everything.

71
00:04:27,004 --> 00:04:29,307
Speaker SPEAKER_00: And we were a bit puzzled about why it didn't solve everything.

72
00:04:30,269 --> 00:04:34,237
Speaker SPEAKER_00: And it was just a question of scale, but we didn't really know that then.

73
00:04:34,257 --> 00:04:36,362
Speaker SPEAKER_01: And so why did you think it was not working?

74
00:04:37,404 --> 00:04:40,208
Speaker SPEAKER_00: We thought it was not working because we didn't have quite the right algorithms.

75
00:04:40,228 --> 00:04:41,990
Speaker SPEAKER_00: We didn't have quite the right objective functions.

76
00:04:42,331 --> 00:04:47,336
Speaker SPEAKER_00: I thought for a long time it's because we were trying to do supervised learning, where you have to label data.

77
00:04:47,637 --> 00:04:51,141
Speaker SPEAKER_00: And we should have been doing unsupervised learning, where you just learn from the data with no labels.

78
00:04:52,002 --> 00:04:54,745
Speaker SPEAKER_00: It turned out it was mainly a question of scale.

79
00:04:54,766 --> 00:04:55,466
Speaker SPEAKER_01: Oh, that's interesting.

80
00:04:55,487 --> 00:04:58,110
Speaker SPEAKER_01: So the problem was you didn't have enough data.

81
00:04:58,170 --> 00:05:00,894
Speaker SPEAKER_01: You thought you had the right amount of data, but you hadn't labeled it correctly.

82
00:05:00,913 --> 00:05:02,855
Speaker SPEAKER_01: So you just misidentified the problem?

83
00:05:03,173 --> 00:05:06,178
Speaker SPEAKER_00: I thought using labels at all was a mistake.

84
00:05:06,199 --> 00:05:11,129
Speaker SPEAKER_00: You ought to do most of your learning without making any use of labels, just by trying to model the structure in the data.

85
00:05:11,529 --> 00:05:12,732
Speaker SPEAKER_00: I actually still believe that.

86
00:05:12,752 --> 00:05:20,206
Speaker SPEAKER_00: I think as computers get faster, for any given size data set, if you make computers fast enough, you're better off doing unsupervised learning.

87
00:05:21,209 --> 00:05:25,317
Speaker SPEAKER_00: And once you've done the unsupervised learning, you'll be able to learn from fewer labels.

88
00:05:25,533 --> 00:05:27,857
Speaker SPEAKER_01: So in the 1990s, you're continuing with your research.

89
00:05:27,877 --> 00:05:28,718
Speaker SPEAKER_01: You're in academia.

90
00:05:28,798 --> 00:05:31,682
Speaker SPEAKER_01: You are still publishing, but it's not coming to acclaim.

91
00:05:31,702 --> 00:05:33,064
Speaker SPEAKER_01: You aren't solving big problems.

92
00:05:33,545 --> 00:05:34,307
Speaker SPEAKER_01: When do you start?

93
00:05:35,387 --> 00:05:38,312
Speaker SPEAKER_01: Well, actually, was there ever a moment where you said, you know what?

94
00:05:39,514 --> 00:05:40,136
Speaker SPEAKER_01: Enough of this.

95
00:05:40,276 --> 00:05:41,959
Speaker SPEAKER_01: I'm going to go try something else.

96
00:05:42,879 --> 00:05:46,987
Speaker SPEAKER_01: Not I'm going to go sell burgers, but I'm going to figure out a different way of doing this.

97
00:05:47,007 --> 00:05:49,110
Speaker SPEAKER_01: You just said we're going to keep doing deep learning.

98
00:05:49,192 --> 00:05:50,836
Speaker SPEAKER_00: Yeah, something like this has to work.

99
00:05:50,896 --> 00:05:52,821
Speaker SPEAKER_00: I mean, the connections in the brain are learning somehow.

100
00:05:53,523 --> 00:05:54,625
Speaker SPEAKER_00: And we just have to figure it out.

101
00:05:55,406 --> 00:05:59,276
Speaker SPEAKER_00: And probably, there's a bunch of different ways of learning connection strengths.

102
00:05:59,416 --> 00:06:00,379
Speaker SPEAKER_00: The brain's using one of them.

103
00:06:00,720 --> 00:06:01,843
Speaker SPEAKER_00: There may be other ways of doing it.

104
00:06:02,283 --> 00:06:04,870
Speaker SPEAKER_00: But certainly, you have to have something that can learn these connection strengths.

105
00:06:05,793 --> 00:06:07,115
Speaker SPEAKER_00: And I never doubted that.

106
00:06:07,096 --> 00:06:08,497
Speaker SPEAKER_01: OK, so you never doubt it.

107
00:06:08,877 --> 00:06:12,202
Speaker SPEAKER_01: When does it first start to seem like it's working?

108
00:06:12,262 --> 00:06:13,824
Speaker SPEAKER_01: OK, we've got this.

109
00:06:14,324 --> 00:06:15,206
Speaker SPEAKER_01: I believe in this idea.

110
00:06:15,225 --> 00:06:18,170
Speaker SPEAKER_01: And actually, if you squint, you can see it's working.

111
00:06:18,250 --> 00:06:19,232
Speaker SPEAKER_01: When did that happen?

112
00:06:19,492 --> 00:06:26,940
Speaker SPEAKER_00: OK, so one of the big disappointments in the 80s was if you made networks with lots of hidden layers, you couldn't train them.

113
00:06:26,961 --> 00:06:36,072
Speaker SPEAKER_00: That's not quite true, because convolutional networks designed by Yann LeCun, you could train for fairly simple tasks like recognizing handwriting.

114
00:06:36,052 --> 00:06:40,778
Speaker SPEAKER_00: But most of the deep nets, we didn't know how to train them.

115
00:06:40,798 --> 00:06:47,286
Speaker SPEAKER_00: And in about 2005, I came up with a way of doing unsupervised training of deep nets.

116
00:06:48,086 --> 00:06:55,894
Speaker SPEAKER_00: So you'd take your input, say your pixels, and you'd learn a bunch of feature detectors that were just good at explaining why the pixels were behaving like that.

117
00:06:56,295 --> 00:07:04,524
Speaker SPEAKER_00: And then you'd treat those feature detectors as the data, and you'd learn another bunch of feature detectors that were good at explaining why those feature detectors had those correlations.

118
00:07:04,504 --> 00:07:06,067
Speaker SPEAKER_00: And you keep learning layers and layers.

119
00:07:06,788 --> 00:07:17,988
Speaker SPEAKER_00: And what was interesting was you could do some math and prove that each time you learned another layer, you didn't necessarily have a better model of the data, but you had a bound on how good your model was.

120
00:07:18,430 --> 00:07:20,834
Speaker SPEAKER_00: And you could get a better bound each time you added another layer.

121
00:07:20,853 --> 00:07:23,137
Speaker SPEAKER_01: What do you mean you had a bound on how good your model was?

122
00:07:23,269 --> 00:07:29,350
Speaker SPEAKER_00: OK, so you can ask, once you've got a model, you can say, how surprising does a model find this data?

123
00:07:29,389 --> 00:07:32,519
Speaker SPEAKER_00: You show it some data, and you say, is that the kind of thing you believe in, or is that surprising?

124
00:07:33,161 --> 00:07:36,874
Speaker SPEAKER_00: And you can sort of measure something that says that.

125
00:07:36,971 --> 00:07:38,613
Speaker SPEAKER_00: What you'd like to do is have a model.

126
00:07:38,774 --> 00:07:41,637
Speaker SPEAKER_00: A good model is one that looks at the data and says, yeah, yeah, I knew that.

127
00:07:41,798 --> 00:07:42,557
Speaker SPEAKER_00: It's unsurprising.

128
00:07:44,139 --> 00:07:48,985
Speaker SPEAKER_00: And it's often very hard to compute exactly how surprising this model finds the data.

129
00:07:49,466 --> 00:07:51,088
Speaker SPEAKER_00: But you can compute a bound on that.

130
00:07:51,108 --> 00:07:55,994
Speaker SPEAKER_00: You can say, this model finds the data less surprising than this.

131
00:07:56,735 --> 00:07:59,637
Speaker SPEAKER_00: And you could show that.

132
00:08:00,098 --> 00:08:03,822
Speaker SPEAKER_00: As you add extra layers of feature detectors, you get a model.

133
00:08:03,988 --> 00:08:06,716
Speaker SPEAKER_00: And each time you add a layer, it finds the data.

134
00:08:06,995 --> 00:08:09,281
Speaker SPEAKER_00: The bound on how surprising it finds the data gets better.

135
00:08:09,843 --> 00:08:10,283
Speaker SPEAKER_01: Oh, I see.

136
00:08:10,343 --> 00:08:11,125
Speaker SPEAKER_01: OK, so that makes sense.

137
00:08:11,146 --> 00:08:15,617
Speaker SPEAKER_01: So you're making observations, and they're not correct, but you know they're closer and closer to being correct.

138
00:08:15,656 --> 00:08:16,579
Speaker SPEAKER_01: I'm looking at the audience.

139
00:08:16,598 --> 00:08:17,841
Speaker SPEAKER_01: I'm making some generalization.

140
00:08:18,142 --> 00:08:20,749
Speaker SPEAKER_01: It's not correct, but I'm getting better and better at it.

141
00:08:21,134 --> 00:08:21,555
Speaker SPEAKER_01: Roughly?

142
00:08:21,855 --> 00:08:22,216
Speaker SPEAKER_00: Roughly.

143
00:08:22,315 --> 00:08:22,555
Speaker SPEAKER_01: OK.

144
00:08:22,877 --> 00:08:26,242
Speaker SPEAKER_01: So that's about 2005, where you come up with that mathematical breakthrough.

145
00:08:26,422 --> 00:08:26,843
Speaker SPEAKER_01: Yeah.

146
00:08:26,862 --> 00:08:29,047
Speaker SPEAKER_01: When do you start getting answers that are correct?

147
00:08:29,307 --> 00:08:30,348
Speaker SPEAKER_01: And what data are you working on?

148
00:08:30,949 --> 00:08:32,652
Speaker SPEAKER_01: This is speech data, where you first have your breakthrough.

149
00:08:32,672 --> 00:08:35,476
Speaker SPEAKER_00: This was just handwritten digits, very simple data.

150
00:08:35,876 --> 00:08:39,623
Speaker SPEAKER_00: Then, around the same time, they started developing GPUs.

151
00:08:40,384 --> 00:08:45,673
Speaker SPEAKER_00: And the people doing neural networks started using GPUs in about 2007.

152
00:08:47,458 --> 00:08:54,187
Speaker SPEAKER_00: I had one very good student called Vlad Mni, who started using GPUs for finding roads in aerial images.

153
00:08:55,269 --> 00:09:03,099
Speaker SPEAKER_00: He wrote some code that was then used by other students for using GPUs to recognize phonemes in speech.

154
00:09:04,019 --> 00:09:06,504
Speaker SPEAKER_00: And so they were using this idea of pre-training.

155
00:09:06,543 --> 00:09:12,711
Speaker SPEAKER_00: And after they'd done all this pre-training, then just stick labels on top and use backpropagation.

156
00:09:12,731 --> 00:09:16,115
Speaker SPEAKER_00: And it turned out that way, you could have a very deep net

157
00:09:16,096 --> 00:09:21,368
Speaker SPEAKER_00: that was pre-trained this way, and you could then use backpropagation, and it actually worked.

158
00:09:21,388 --> 00:09:24,235
Speaker SPEAKER_00: And it sort of beat the benchmarks for speech recognition.

159
00:09:24,655 --> 00:09:29,005
Speaker SPEAKER_01: Initially, just by a little bit, it beat the best commercially available speech recognition.

160
00:09:29,046 --> 00:09:31,471
Speaker SPEAKER_01: It beat the best academic work on speech recognition.

161
00:09:32,278 --> 00:09:35,125
Speaker SPEAKER_00: on a relatively small data set called Timit.

162
00:09:35,145 --> 00:09:37,510
Speaker SPEAKER_00: It did slightly better than the best academic work.

163
00:09:38,833 --> 00:09:40,357
Speaker SPEAKER_00: Also worked on at IBM.

164
00:09:43,004 --> 00:09:48,616
Speaker SPEAKER_00: And very quickly, people realized that this stuff

165
00:09:48,596 --> 00:09:55,003
Speaker SPEAKER_00: since it was beating standard models that are taking 30 years to develop, with a bit more development, would do really well.

166
00:09:55,703 --> 00:10:01,490
Speaker SPEAKER_00: And so my graduate students went off to Microsoft and IBM and Google.

167
00:10:01,509 --> 00:10:04,993
Speaker SPEAKER_00: And Google was the fastest to turn it into a production speech recognizer.

168
00:10:05,394 --> 00:10:11,940
Speaker SPEAKER_00: And by 2012, that work that was first done in 2009 came out in Android.

169
00:10:12,421 --> 00:10:14,503
Speaker SPEAKER_00: And Android suddenly got much better at speech recognition.

170
00:10:14,482 --> 00:10:24,591
Speaker SPEAKER_01: So tell me about that moment where you've had this idea for 40 years, you've been publishing on it for 20 years, and you're finally better than your colleagues.

171
00:10:24,610 --> 00:10:25,894
Speaker SPEAKER_01: What did that feel like?

172
00:10:26,160 --> 00:10:28,144
Speaker SPEAKER_00: Well, back then, I'd only had the idea for 30 years.

173
00:10:28,924 --> 00:10:29,164
Speaker SPEAKER_01: Correct.

174
00:10:29,184 --> 00:10:29,645
Speaker SPEAKER_01: Correct.

175
00:10:29,666 --> 00:10:29,885
Speaker SPEAKER_01: Sorry.

176
00:10:30,927 --> 00:10:31,808
Speaker SPEAKER_01: So just a new idea.

177
00:10:31,849 --> 00:10:32,870
Speaker SPEAKER_01: It's fresh.

178
00:10:33,171 --> 00:10:37,738
Speaker SPEAKER_00: It felt really good that it finally got the state of the art on a real problem.

179
00:10:37,878 --> 00:10:42,323
Speaker SPEAKER_01: And do you remember where you were when you first got the revelatory data?

180
00:10:42,985 --> 00:10:44,147
Speaker SPEAKER_00: No.

181
00:10:44,167 --> 00:10:44,606
Speaker SPEAKER_01: No.

182
00:10:44,626 --> 00:10:44,768
Speaker SPEAKER_01: No.

183
00:10:44,827 --> 00:10:45,248
Speaker SPEAKER_01: OK.

184
00:10:45,268 --> 00:10:45,609
Speaker SPEAKER_01: All right.

185
00:10:45,629 --> 00:10:48,852
Speaker SPEAKER_01: So you realize it works on speech recognition.

186
00:10:49,453 --> 00:10:52,118
Speaker SPEAKER_01: When do you start applying it to other problems?

187
00:10:52,232 --> 00:10:54,434
Speaker SPEAKER_00: So then we started applying it to all sorts of other problems.

188
00:10:54,895 --> 00:11:02,903
Speaker SPEAKER_00: So George Dahl, who was one of the people who did the original work on speech recognition, applied it to, I give you a lot of descriptors of a molecule.

189
00:11:03,585 --> 00:11:08,129
Speaker SPEAKER_00: And you want to predict if that molecule will bind to something to act as a good drug.

190
00:11:09,070 --> 00:11:10,370
Speaker SPEAKER_00: And there was a competition on Cargill.

191
00:11:11,032 --> 00:11:16,817
Speaker SPEAKER_00: And he just applied our standard technology designed for speech recognition to predicting the activity of drugs.

192
00:11:17,339 --> 00:11:18,779
Speaker SPEAKER_00: And it won the competition.

193
00:11:18,759 --> 00:11:21,303
Speaker SPEAKER_00: So that was a sign that this stuff's sort of fairly universal.

194
00:11:21,865 --> 00:11:29,796
Speaker SPEAKER_00: And then I had a student called Ilya Sutskova who said, you know, Jeff, this stuff is going to work for image recognition.

195
00:11:30,157 --> 00:11:32,841
Speaker SPEAKER_00: And Fei-Fei Li has created the correct data set for it.

196
00:11:32,880 --> 00:11:34,864
Speaker SPEAKER_00: And there's a public competition.

197
00:11:34,903 --> 00:11:36,886
Speaker SPEAKER_00: We have to do that.

198
00:11:36,866 --> 00:11:41,799
Speaker SPEAKER_00: And so what we did was take an approach originally developed by Yann LeCun.

199
00:11:44,246 --> 00:11:51,687
Speaker SPEAKER_00: A student called Alex Krashevsky, who was a real wizard who could make GPUs do anything, programmed the GPUs really, really well.

200
00:11:51,885 --> 00:11:56,913
Speaker SPEAKER_00: And we got results that were a lot better than standard computer vision.

201
00:11:56,932 --> 00:11:58,034
Speaker SPEAKER_00: That was 2012.

202
00:11:58,434 --> 00:12:02,301
Speaker SPEAKER_00: And it was a coincidence, I think, of the speech recognition coming out in the Android.

203
00:12:02,341 --> 00:12:04,423
Speaker SPEAKER_00: So you knew this stuff could solve production problems.

204
00:12:05,566 --> 00:12:10,952
Speaker SPEAKER_00: And on vision in 2012, it had done much better than standard computer vision.

205
00:12:11,433 --> 00:12:13,317
Speaker SPEAKER_01: So those are three areas where it succeeded.

206
00:12:13,397 --> 00:12:16,541
Speaker SPEAKER_01: So modeling chemicals, speech, voice.

207
00:12:16,881 --> 00:12:17,842
Speaker SPEAKER_01: Where was it failing?

208
00:12:19,628 --> 00:12:21,431
Speaker SPEAKER_00: The failures are only temporary, you understand.

209
00:12:23,673 --> 00:12:24,375
Speaker SPEAKER_00: Where was it failing?

210
00:12:26,157 --> 00:12:30,624
Speaker SPEAKER_00: For things like machine translation, I thought it would be a very long time before we could do that.

211
00:12:31,485 --> 00:12:36,692
Speaker SPEAKER_00: Because machine translation, you've got a string of symbols comes in, and a string of symbols goes out.

212
00:12:37,033 --> 00:12:43,662
Speaker SPEAKER_00: And it's fairly plausible to say in between you do manipulations on strings of symbols, which is what classical AI is.

213
00:12:43,642 --> 00:12:45,485
Speaker SPEAKER_00: Actually, it doesn't work like that.

214
00:12:45,865 --> 00:12:46,908
Speaker SPEAKER_00: Strings of symbols come in.

215
00:12:46,967 --> 00:12:49,312
Speaker SPEAKER_00: You turn those into great big vectors in your brain.

216
00:12:49,352 --> 00:12:50,816
Speaker SPEAKER_00: These vectors interact with each other.

217
00:12:51,115 --> 00:12:53,782
Speaker SPEAKER_00: And then you convert it back into strings of symbols to go out.

218
00:12:54,984 --> 00:13:06,426
Speaker SPEAKER_00: And if you told me in 2012 that in the next five years we'll be able to translate between many languages using just the same technology,

219
00:13:06,405 --> 00:13:12,816
Speaker SPEAKER_00: recurrent nets, but just the stochastic gradient descent from random initial weights.

220
00:13:12,836 --> 00:13:13,657
Speaker SPEAKER_00: I wouldn't have believed you.

221
00:13:13,817 --> 00:13:15,399
Speaker SPEAKER_00: It happened much faster than expected.

222
00:13:16,000 --> 00:13:24,133
Speaker SPEAKER_01: But so what distinguishes the areas where it works most quickly and the areas where it will take more time?

223
00:13:24,552 --> 00:13:34,427
Speaker SPEAKER_01: It seems like visual processing, speech recognition, sort of core human things that we do with our sensory perception seem to be the first barriers to clear.

224
00:13:34,648 --> 00:13:35,669
Speaker SPEAKER_01: Is that correct?

225
00:13:35,649 --> 00:13:38,373
Speaker SPEAKER_00: Yes and no, because there's other things we do, like motor control.

226
00:13:38,413 --> 00:13:39,654
Speaker SPEAKER_00: We're very good at motor control.

227
00:13:39,695 --> 00:13:41,136
Speaker SPEAKER_00: Our brains are clearly designed for that.

228
00:13:41,798 --> 00:13:48,306
Speaker SPEAKER_00: And that's only just now a neural net's beginning to compete with the best other technologies there.

229
00:13:48,326 --> 00:13:51,129
Speaker SPEAKER_00: They will win in the end, but they're only just winning now.

230
00:13:51,931 --> 00:13:59,880
Speaker SPEAKER_00: I think things like reasoning, abstract reasoning, are going to be the kind of last things we learn to do.

231
00:14:00,581 --> 00:14:03,345
Speaker SPEAKER_00: And I think they'll be among the last things these neural nets learn to do.

232
00:14:03,562 --> 00:14:09,436
Speaker SPEAKER_01: And so you keep saying that neural nets will win at everything eventually.

233
00:14:09,756 --> 00:14:11,039
Speaker SPEAKER_00: Well, we are neural nets, right?

234
00:14:11,120 --> 00:14:12,243
Speaker SPEAKER_00: Anything we can do, they can do.

235
00:14:12,583 --> 00:14:14,347
Speaker SPEAKER_01: Right, but just because humans

236
00:14:15,356 --> 00:14:20,903
Speaker SPEAKER_01: The human brain is not necessarily the most efficient computational machine ever created.

237
00:14:21,443 --> 00:14:23,125
Speaker SPEAKER_01: So why could there not be?

238
00:14:23,145 --> 00:14:24,366
Speaker SPEAKER_01: Certainly not my human brain.

239
00:14:24,888 --> 00:14:29,113
Speaker SPEAKER_01: Couldn't there be a way of modeling machines that is more efficient than the human brain?

240
00:14:29,472 --> 00:14:33,899
Speaker SPEAKER_00: Philosophically, I have no objection to the idea there could be some completely different way to do all this.

241
00:14:34,119 --> 00:14:41,246
Speaker SPEAKER_00: It could be that if you start with logic, and you try and automate logic, and you make some really fancy theorem prover,

242
00:14:41,226 --> 00:14:43,870
Speaker SPEAKER_00: And you do reasoning.

243
00:14:44,230 --> 00:14:46,975
Speaker SPEAKER_00: And then you decide you're going to do visual perception by doing reasoning.

244
00:14:47,655 --> 00:14:49,097
Speaker SPEAKER_00: It could be that that approach would win.

245
00:14:49,278 --> 00:14:50,219
Speaker SPEAKER_00: It turned out it didn't.

246
00:14:50,779 --> 00:14:52,783
Speaker SPEAKER_00: But I have no philosophical objection to that winning.

247
00:14:53,464 --> 00:14:55,486
Speaker SPEAKER_00: It's just we know that brains can do it.

248
00:14:56,989 --> 00:14:57,208
Speaker SPEAKER_01: Right.

249
00:14:57,769 --> 00:15:01,895
Speaker SPEAKER_01: But there are also things that our brains can't do well.

250
00:15:02,115 --> 00:15:05,039
Speaker SPEAKER_01: Are those things that neural nets also won't be able to do well?

251
00:15:06,320 --> 00:15:07,741
Speaker SPEAKER_01: Quite possibly, yeah.

252
00:15:07,761 --> 00:15:13,148
Speaker SPEAKER_01: And then there's a separate problem, which is we don't know entirely how these things work.

253
00:15:13,368 --> 00:15:14,809
Speaker SPEAKER_00: No, we really don't know how they work.

254
00:15:14,830 --> 00:15:19,335
Speaker SPEAKER_01: We don't understand how top-down neural networks work.

255
00:15:19,355 --> 00:15:23,078
Speaker SPEAKER_01: There's even a core element of how neural networks work that we don't understand.

256
00:15:23,419 --> 00:15:28,304
Speaker SPEAKER_01: All right, so explain that, and then let me ask the obvious follow-up, which is, we don't know how these things work.

257
00:15:28,325 --> 00:15:29,125
Speaker SPEAKER_01: How can those things work?

258
00:15:29,306 --> 00:15:31,668
Speaker SPEAKER_01: OK.

259
00:15:31,688 --> 00:15:32,951
Speaker SPEAKER_00: You ask that when I finish explaining.

260
00:15:33,211 --> 00:15:34,993
Speaker SPEAKER_00: Yes.

261
00:15:37,201 --> 00:15:41,730
Speaker SPEAKER_00: If you look at current computer vision systems, most of them, they basically feed forward.

262
00:15:42,451 --> 00:15:44,033
Speaker SPEAKER_00: They don't use feedback connections.

263
00:15:44,676 --> 00:15:48,964
Speaker SPEAKER_00: There's something else about current computer vision systems, which is they're very prone to adversarial examples.

264
00:15:49,705 --> 00:15:53,572
Speaker SPEAKER_00: You can change a few pixels slightly.

265
00:15:54,024 --> 00:16:00,219
Speaker SPEAKER_00: and something that was a picture of a panda and still looks exactly like a panda to you, it suddenly says that's an ostrich.

266
00:16:01,140 --> 00:16:05,171
Speaker SPEAKER_00: Obviously, the way you change the pixels is cleverly designed to fool it into thinking it's an ostrich.

267
00:16:06,934 --> 00:16:09,620
Speaker SPEAKER_00: But the point is, it still looks just like a panda to you.

268
00:16:09,600 --> 00:16:12,065
Speaker SPEAKER_00: And initially, we thought these things worked really well.

269
00:16:12,184 --> 00:16:18,054
Speaker SPEAKER_00: But then, when confronted with the fact that they'll look at a panda and be confident it's an ostrich, you get a bit worried.

270
00:16:18,836 --> 00:16:24,745
Speaker SPEAKER_00: And I think part of the problem there is that they're not trying to reconstruct from the high-level representations.

271
00:16:25,326 --> 00:16:28,692
Speaker SPEAKER_00: They're trying to do discriminative learning, where you just learn layers of feature detectors.

272
00:16:29,274 --> 00:16:35,845
Speaker SPEAKER_00: And the whole objective is just to change the weights so you get better at getting the right answer.

273
00:16:36,009 --> 00:16:44,722
Speaker SPEAKER_00: They're not doing things like, at each level of feature detectors, check that you can reconstruct the data in the layer below from the activities of these feature detectors.

274
00:16:45,544 --> 00:16:57,783
Speaker SPEAKER_00: And recently in Toronto, we've been discovering, or Nick Frost's been discovering, that if you introduce reconstruction, then it helps you be more resistant to adversarial attack.

275
00:16:57,803 --> 00:17:00,027
Speaker SPEAKER_00: So I think in human vision,

276
00:17:00,006 --> 00:17:08,460
Speaker SPEAKER_00: To do the learning, we're doing reconstruction, and also because we're doing a lot of learning by doing reconstructions, we are much more resistant to adversarial attacks.

277
00:17:08,480 --> 00:17:18,654
Speaker SPEAKER_01: But you believe that top-down communication in a neural network is how you test, how you reconstruct, how you test and make sure it's a panda, not an ostrich?

278
00:17:18,674 --> 00:17:23,803
Speaker SPEAKER_01: I think that's crucial, yes, because I think if you... But brain scientists are not entirely agreed on that, correct?

279
00:17:25,436 --> 00:17:35,385
Speaker SPEAKER_00: Brain scientists all agreed on the idea that if you have two areas of the cortex in a perceptual pathway, if there's connections from one to the other, there'll always be backwards connections.

280
00:17:35,886 --> 00:17:39,176
Speaker SPEAKER_00: Not necessarily point to point, but there'll always be a backwards pathway.

281
00:17:39,595 --> 00:17:41,156
Speaker SPEAKER_00: They're not agreed on what it's for.

282
00:17:41,576 --> 00:17:43,479
Speaker SPEAKER_00: It could be for attention.

283
00:17:43,499 --> 00:17:45,080
Speaker SPEAKER_00: It could be for learning.

284
00:17:45,101 --> 00:17:46,442
Speaker SPEAKER_00: Or it could be for reconstruction.

285
00:17:47,042 --> 00:17:47,963
Speaker SPEAKER_00: Or it could be for all three.

286
00:17:48,084 --> 00:17:51,647
Speaker SPEAKER_01: And so you, we don't know what the backwards communication is.

287
00:17:52,567 --> 00:18:01,958
Speaker SPEAKER_01: You are building your new neural networks on the assumption that, or you're building backwards communication that is for reconstruction into your neural networks, even though we're not sure that's how the brain works.

288
00:18:02,278 --> 00:18:02,439
Speaker SPEAKER_00: Yes.

289
00:18:02,699 --> 00:18:03,380
Speaker SPEAKER_01: Isn't that cheating?

290
00:18:03,400 --> 00:18:08,505
Speaker SPEAKER_01: I mean, if you're trying to make it like the brain, you're doing something we're not sure is like the brain.

291
00:18:08,484 --> 00:18:09,205
Speaker SPEAKER_00: Not at all.

292
00:18:11,671 --> 00:18:13,394
Speaker SPEAKER_00: I'm not doing computational neuroscience.

293
00:18:13,433 --> 00:18:15,857
Speaker SPEAKER_00: That is, I'm not trying to make a model of how the brain works.

294
00:18:16,238 --> 00:18:19,703
Speaker SPEAKER_00: I'm looking at the brain and saying, this thing works.

295
00:18:20,285 --> 00:18:24,532
Speaker SPEAKER_00: And if we want to make something else that works, we should look to it for inspiration.

296
00:18:24,893 --> 00:18:27,538
Speaker SPEAKER_00: So this is neuro-inspired, not a neural model.

297
00:18:27,959 --> 00:18:33,327
Speaker SPEAKER_00: So the neurons we use, they're inspired by the fact neurons have a lot of connections and they change the strengths.

298
00:18:33,307 --> 00:18:33,929
Speaker SPEAKER_01: That's interesting.

299
00:18:33,969 --> 00:18:49,487
Speaker SPEAKER_01: So if I were in computer science, and I was working on neural networks, and I wanted to beat Jeff Hinton, one thing I could do is I could build in top-down communication and base it on other models of brain science, so based on learning, not on reconstruction.

300
00:18:49,507 --> 00:18:51,909
Speaker SPEAKER_00: If they were better models, then you'd win, yeah.

301
00:18:52,150 --> 00:18:53,611
Speaker SPEAKER_01: That's very, very interesting.

302
00:18:53,651 --> 00:18:56,835
Speaker SPEAKER_01: All right, so let's move to a more general topic.

303
00:18:57,135 --> 00:18:59,199
Speaker SPEAKER_01: Neural networks will be able to solve all kinds of problems.

304
00:18:59,660 --> 00:19:06,109
Speaker SPEAKER_01: Are there any mysteries of the human brain that will not be captured by neural networks or cannot?

305
00:19:06,269 --> 00:19:08,532
Speaker SPEAKER_01: For example, could the emotion?

306
00:19:08,594 --> 00:19:09,234
Speaker SPEAKER_01: No.

307
00:19:09,255 --> 00:19:09,795
Speaker SPEAKER_01: No.

308
00:19:09,815 --> 00:19:12,058
Speaker SPEAKER_01: So love could be reconstructed by a neural network.

309
00:19:12,439 --> 00:19:13,842
Speaker SPEAKER_01: Consciousness can be constructed.

310
00:19:14,342 --> 00:19:14,883
Speaker SPEAKER_00: Absolutely.

311
00:19:14,942 --> 00:19:19,329
Speaker SPEAKER_00: Once you've figured out what those things mean, we are neural networks, right?

312
00:19:20,541 --> 00:19:22,934
Speaker SPEAKER_00: Now, consciousness is something I'm particularly interested in.

313
00:19:22,954 --> 00:19:27,378
Speaker SPEAKER_00: I get by fine without it.

314
00:19:29,823 --> 00:19:32,007
Speaker SPEAKER_00: So people don't really know what they mean by it.

315
00:19:32,047 --> 00:19:33,509
Speaker SPEAKER_00: There's all sorts of different definitions.

316
00:19:33,890 --> 00:19:35,551
Speaker SPEAKER_00: And I think it's a pre-scientific term.

317
00:19:35,972 --> 00:19:40,421
Speaker SPEAKER_00: So 100 years ago, if you asked people, what is life?

318
00:19:41,422 --> 00:19:43,486
Speaker SPEAKER_00: They'd have said, well, living things have vital force.

319
00:19:43,526 --> 00:19:45,107
Speaker SPEAKER_00: And when they die, the vital force goes away.

320
00:19:45,548 --> 00:19:51,038
Speaker SPEAKER_00: And that's the difference between being alive and being dead, whether you've got vital force or not.

321
00:19:51,018 --> 00:19:54,741
Speaker SPEAKER_00: And now we don't think that we don't have vital force.

322
00:19:55,102 --> 00:19:57,044
Speaker SPEAKER_00: We just think it's a pre-scientific concept.

323
00:19:57,384 --> 00:20:02,328
Speaker SPEAKER_00: And once you understand some biochemistry and molecular biology, you don't need vital force anymore.

324
00:20:02,529 --> 00:20:03,810
Speaker SPEAKER_00: You understand how it actually works.

325
00:20:04,431 --> 00:20:05,913
Speaker SPEAKER_00: And I think it's going to be the same with consciousness.

326
00:20:05,952 --> 00:20:12,159
Speaker SPEAKER_00: I think consciousness is an attempt to explain mental phenomena with some kind of special essence.

327
00:20:13,039 --> 00:20:15,122
Speaker SPEAKER_00: And this special essence, you don't need it.

328
00:20:15,201 --> 00:20:16,923
Speaker SPEAKER_00: Once you can really explain it,

329
00:20:16,903 --> 00:20:28,419
Speaker SPEAKER_00: then you'll explain how we do the things that make people think we're conscious and you'll explain all these different meanings of consciousness without having some special essence as consciousness.

330
00:20:29,961 --> 00:20:43,417
Speaker SPEAKER_01: Right, so there's no emotion that couldn't be created, there's no thought that couldn't be created, there's nothing that a human mind can do that couldn't theoretically be recreated by a fully functioning neural network once we truly understand how the brain works.

331
00:20:43,499 --> 00:20:48,865
Speaker SPEAKER_00: There's something in a John Lennon song that sounds very like what you just said.

332
00:20:48,885 --> 00:20:50,387
Speaker SPEAKER_00: And you're 100% confident of this?

333
00:20:51,930 --> 00:20:54,472
Speaker SPEAKER_00: No, I'm a Bayesian, so I'm 99.9% confident.

334
00:20:57,136 --> 00:20:57,416
Speaker SPEAKER_01: OK.

335
00:20:57,738 --> 00:20:58,479
Speaker SPEAKER_01: And what is the point one?

336
00:20:59,440 --> 00:21:05,146
Speaker SPEAKER_00: Well, we might, for example, all be part of a big simulation.

337
00:21:05,166 --> 00:21:05,346
Speaker SPEAKER_01: True.

338
00:21:05,508 --> 00:21:05,867
Speaker SPEAKER_01: Fair enough.

339
00:21:05,968 --> 00:21:07,329
Speaker SPEAKER_01: OK.

340
00:21:15,528 --> 00:21:17,674
Speaker SPEAKER_01: That actually makes me think it's more likely that we are.

341
00:21:19,359 --> 00:21:19,621
Speaker SPEAKER_01: All right.

342
00:21:19,922 --> 00:21:23,934
Speaker SPEAKER_01: So what are we learning as we do this and as we study the brain to improve computers?

343
00:21:24,798 --> 00:21:25,660
Speaker SPEAKER_01: How does it work in reverse?

344
00:21:25,700 --> 00:21:28,710
Speaker SPEAKER_01: What are we learning about the brain from our working computers?

345
00:21:29,432 --> 00:21:50,039
Speaker SPEAKER_00: So I think what we've learned in the last 10 years is that if you take a system with billions of parameters, and you do stochastic gradient descent in some objective function, and the objective function might be to get the right labels, or it might be to fill in a gap in a string of words, or any old objective function,

346
00:21:50,019 --> 00:21:53,786
Speaker SPEAKER_00: It works much better than it has any right to.

347
00:21:53,806 --> 00:21:55,348
Speaker SPEAKER_00: It works much better than you would expect.

348
00:21:56,130 --> 00:22:07,208
Speaker SPEAKER_00: You would have thought, and most people in conventional AI thought, take a system with a billion parameters, start them off with random values, measure the gradient of the objective function.

349
00:22:07,268 --> 00:22:13,699
Speaker SPEAKER_00: That is, for each parameter, figure out how the objective function would change if you change that parameter a little bit.

350
00:22:14,522 --> 00:22:21,272
Speaker SPEAKER_00: and then change it in that direction that improves the objective function, you'd have thought that would be a kind of hopeless algorithm that would get starkened.

351
00:22:21,953 --> 00:22:27,301
Speaker SPEAKER_00: And it turns out it's a really good algorithm, and the bigger you scale things, the better it works.

352
00:22:28,002 --> 00:22:30,086
Speaker SPEAKER_00: And that's just an empirical discovery, really.

353
00:22:30,606 --> 00:22:33,290
Speaker SPEAKER_00: There's some theory coming along, but it's basically an empirical discovery.

354
00:22:33,270 --> 00:22:45,478
Speaker SPEAKER_00: Now, because we've discovered that, it makes it far more plausible that the brain is computing the gradient of some objective function and updating the weights of strength of synapses to follow that gradient.

355
00:22:45,498 --> 00:22:48,765
Speaker SPEAKER_00: We just have to figure out how it gets the gradient and what the objective function is.

356
00:22:48,796 --> 00:22:50,897
Speaker SPEAKER_01: But we didn't understand that about the brain.

357
00:22:50,938 --> 00:22:52,179
Speaker SPEAKER_01: We didn't understand the reweighting of synapses.

358
00:22:52,199 --> 00:22:52,839
Speaker SPEAKER_00: It was a theory.

359
00:22:52,900 --> 00:22:55,863
Speaker SPEAKER_00: It was, I mean, a long time ago, people thought that's a possibility.

360
00:22:56,624 --> 00:23:07,934
Speaker SPEAKER_00: But in the background, there was always sort of conventional computer scientists saying, yeah, but this idea of everything's random, you just learn it all by gradient descent, that's never going to work for a billion parameters.

361
00:23:08,076 --> 00:23:09,616
Speaker SPEAKER_00: You have to wire in a lot of knowledge.

362
00:23:10,038 --> 00:23:11,759
Speaker SPEAKER_00: And we know now that's wrong.

363
00:23:11,878 --> 00:23:14,382
Speaker SPEAKER_00: You can just put in random parameters and learn everything.

364
00:23:14,547 --> 00:23:15,608
Speaker SPEAKER_01: So let's expand this out.

365
00:23:15,628 --> 00:23:23,940
Speaker SPEAKER_01: So as we learn more and more, we will presumably continue to learn more and more about how the human brain functions as we run these massive tests on models based on how we think it functions.

366
00:23:25,102 --> 00:23:35,135
Speaker SPEAKER_01: Once we understand it better, is there a point where we can essentially rewire our brains to be more like the most efficient machines or change the way we think?

367
00:23:36,617 --> 00:23:39,221
Speaker SPEAKER_01: If it's a simulation, that should be easy, but not in a simulation.

368
00:23:39,336 --> 00:23:46,229
Speaker SPEAKER_00: You'd have thought that if we really understand what's going on, we should be able to make things like education work better.

369
00:23:46,349 --> 00:23:47,491
Speaker SPEAKER_00: Yes.

370
00:23:47,692 --> 00:23:48,353
Speaker SPEAKER_00: And I think we will.

371
00:23:48,953 --> 00:23:49,255
Speaker SPEAKER_01: We will.

372
00:23:49,494 --> 00:23:49,695
Speaker SPEAKER_00: Yeah.

373
00:23:50,175 --> 00:23:58,932
Speaker SPEAKER_00: It would be very odd if you could finally understand what's going on in your brain and how it learns and not be able to adapt the environment so you can learn better.

374
00:23:59,079 --> 00:24:01,042
Speaker SPEAKER_01: OK, I don't want to go too far out in the future.

375
00:24:01,103 --> 00:24:07,894
Speaker SPEAKER_01: But a couple of years from now, how do you think we will be using what we've learned about the brain and about how deep learning works to change how education functions?

376
00:24:07,913 --> 00:24:09,375
Speaker SPEAKER_01: How would you change a class?

377
00:24:09,797 --> 00:24:12,580
Speaker SPEAKER_00: In a couple of years, I'm not sure we'll learn much.

378
00:24:13,461 --> 00:24:16,145
Speaker SPEAKER_00: I think changing education is going to be longer.

379
00:24:16,186 --> 00:24:19,932
Speaker SPEAKER_00: But if you look at it, assistants are getting pretty smart now.

380
00:24:20,333 --> 00:24:27,483
Speaker SPEAKER_00: And once assistants can really understand conversations, assistants can have conversations with kids and educate them.

381
00:24:27,463 --> 00:24:34,814
Speaker SPEAKER_00: I think most of the new knowledge I acquire comes from me thinking, I wonder, and typing something to Google, and Google tells me.

382
00:24:35,013 --> 00:24:36,997
Speaker SPEAKER_00: If I could just have a conversation, I'd acquire knowledge even better.

383
00:24:37,497 --> 00:24:55,902
Speaker SPEAKER_01: And so theoretically, as we understand the brain better, and as we set our children up in front of assistants, mine right now, almost certainly based on the time in New York is yelling at Alexa to play something on Spotify, probably Baby Shark, you will program the assistants to have better conversations with the children based on how we know they'll learn.

384
00:24:56,167 --> 00:24:57,851
Speaker SPEAKER_00: Yeah, I haven't really thought much about this.

385
00:24:57,931 --> 00:24:58,672
Speaker SPEAKER_00: It's not what I do.

386
00:24:59,113 --> 00:25:01,276
Speaker SPEAKER_00: But it seems quite plausible to me.

387
00:25:02,538 --> 00:25:05,461
Speaker SPEAKER_01: Will we be able to understand how dreams work?

388
00:25:05,883 --> 00:25:06,844
Speaker SPEAKER_01: One of the great mysteries.

389
00:25:07,045 --> 00:25:08,467
Speaker SPEAKER_00: Yes, I'm really interested in dreams.

390
00:25:09,147 --> 00:25:10,068
Speaker SPEAKER_00: I'm so interested.

391
00:25:10,088 --> 00:25:11,932
Speaker SPEAKER_00: I have at least four different theories of dreams.

392
00:25:12,113 --> 00:25:12,854
Speaker SPEAKER_00: Let's hear them all.

393
00:25:13,013 --> 00:25:13,755
Speaker SPEAKER_02: One, two, three, four.

394
00:25:15,135 --> 00:25:19,041
Speaker SPEAKER_00: So a long time ago, there were Hopfield networks.

395
00:25:19,682 --> 00:25:22,547
Speaker SPEAKER_00: And they would learn memories as local attractors.

396
00:25:23,828 --> 00:25:28,174
Speaker SPEAKER_00: And Hopfield discovered that if you try and put too many memories in, they get confused.

397
00:25:29,136 --> 00:25:32,642
Speaker SPEAKER_00: They'll take two local attractors and merge them into an attractor sort of halfway in between.

398
00:25:32,682 --> 00:25:39,050
Speaker SPEAKER_00: Then Francis Crick and Graham Mitchison came along and said,

399
00:25:39,587 --> 00:25:42,332
Speaker SPEAKER_00: We can get rid of these false minima by doing unlearning.

400
00:25:43,333 --> 00:25:45,375
Speaker SPEAKER_00: So we turn off the input.

401
00:25:45,915 --> 00:25:47,959
Speaker SPEAKER_00: We put the neural network into a random state.

402
00:25:48,500 --> 00:25:49,500
Speaker SPEAKER_00: We let it settle down.

403
00:25:49,942 --> 00:25:51,022
Speaker SPEAKER_00: And we say, that's bad.

404
00:25:51,584 --> 00:25:54,227
Speaker SPEAKER_00: Change the connection so you don't settle to that state.

405
00:25:54,247 --> 00:25:59,855
Speaker SPEAKER_00: And if you do a bit of that, it will be able to store more memories.

406
00:26:00,675 --> 00:26:08,987
Speaker SPEAKER_00: And then Terry Sanofsky and I came along and said, look, if we have not just the neurons where you're storing the memories, but lots of other neurons too,

407
00:26:09,321 --> 00:26:13,393
Speaker SPEAKER_00: can we find an algorithm that will use all these other neurons to help you store memories?

408
00:26:14,134 --> 00:26:17,242
Speaker SPEAKER_00: And it turned out, in the end, we came up with the Boltzmann machine learning algorithm.

409
00:26:17,262 --> 00:26:21,976
Speaker SPEAKER_00: And the Boltzmann machine learning algorithm had a very interesting property, which is, I show you data.

410
00:26:22,215 --> 00:26:25,203
Speaker SPEAKER_00: That is, I fix the states of the observable units.

411
00:26:25,183 --> 00:26:30,270
Speaker SPEAKER_00: And it sort of rattles around the other units until it's got a fairly happy state.

412
00:26:30,932 --> 00:26:38,462
Speaker SPEAKER_00: And once it's done that, it increases the strength of all the connections based on, if two units are both active, it increases connection strength.

413
00:26:38,482 --> 00:26:40,586
Speaker SPEAKER_00: That's called kind of Hebbian learning.

414
00:26:40,605 --> 00:26:43,028
Speaker SPEAKER_00: But if you just do that, the connection strengths just get bigger and bigger.

415
00:26:44,391 --> 00:26:47,174
Speaker SPEAKER_00: You also have to have a phase where you cut it off from the input.

416
00:26:48,251 --> 00:26:50,617
Speaker SPEAKER_00: You let it rattle around and settle into a state it's happy with.

417
00:26:50,979 --> 00:26:52,202
Speaker SPEAKER_00: So now it's having a fantasy.

418
00:26:53,405 --> 00:27:00,726
Speaker SPEAKER_00: And once it's had the fantasy, you say, take all pairs of neurons that are active and decrease the strength of the connection.

419
00:27:01,786 --> 00:27:04,772
Speaker SPEAKER_00: So I'm explaining the algorithm to you just as a procedure.

420
00:27:05,334 --> 00:27:19,086
Speaker SPEAKER_00: But actually, that algorithm is the result of doing some math and saying, how should you change these connection strings so that this neural network with all these hidden units finds the data unsurprising?

421
00:27:19,066 --> 00:27:21,429
Speaker SPEAKER_00: And it has to have this other phase.

422
00:27:21,509 --> 00:27:25,272
Speaker SPEAKER_00: It has to have this, what we call, the negative phase, when it's running with no input.

423
00:27:25,773 --> 00:27:27,015
Speaker SPEAKER_00: And it's canceling out.

424
00:27:27,615 --> 00:27:30,018
Speaker SPEAKER_00: It's unlearning whatever state it settles into.

425
00:27:30,538 --> 00:27:35,444
Speaker SPEAKER_00: Now, what Crick pointed out about dreams is that we know that you dream for many hours every night.

426
00:27:36,125 --> 00:27:41,090
Speaker SPEAKER_00: And if I wake you up at random, you can tell me what you were just dreaming about, because it's in your short-term memory.

427
00:27:41,746 --> 00:27:42,907
Speaker SPEAKER_00: So we know you dream for many hours.

428
00:27:42,948 --> 00:27:44,009
Speaker SPEAKER_00: But in the morning, you wake up.

429
00:27:44,549 --> 00:27:48,955
Speaker SPEAKER_00: You can remember the last dream, but you can't remember all the others, which is lucky, because you might mistake them for reality.

430
00:27:51,599 --> 00:27:53,663
Speaker SPEAKER_00: So why is it that we don't remember our dreams at all?

431
00:27:54,104 --> 00:27:59,090
Speaker SPEAKER_00: And Crick's view was, the whole point of dreaming is to unlearn those things.

432
00:27:59,511 --> 00:28:00,953
Speaker SPEAKER_00: So you put the learning world in reverse.

433
00:28:01,674 --> 00:28:07,663
Speaker SPEAKER_00: And Terry Stanofsky and I showed that actually, that is a maximum likelihood learning procedure for Boltzmann machines.

434
00:28:07,943 --> 00:28:09,306
Speaker SPEAKER_00: So that's one theory of dreaming.

435
00:28:09,286 --> 00:28:10,568
Speaker SPEAKER_00: You showed that theoretically?

436
00:28:10,608 --> 00:28:21,657
Speaker SPEAKER_00: Yeah, we showed theoretically that's the right thing to do if you want to change the weights so that your big neural network finds the observed data less surprising.

437
00:28:22,414 --> 00:28:23,617
Speaker SPEAKER_01: And I want to go to your other theories.

438
00:28:23,637 --> 00:28:29,029
Speaker SPEAKER_01: But before we lose this thread, you've proved that it's efficient.

439
00:28:29,049 --> 00:28:33,500
Speaker SPEAKER_01: Have you actually set any of your deep learning algorithms to essentially dream?

440
00:28:34,001 --> 00:28:35,965
Speaker SPEAKER_01: Study this image data set for a period of time.

441
00:28:36,667 --> 00:28:37,148
Speaker SPEAKER_01: Resort.

442
00:28:37,189 --> 00:28:37,951
Speaker SPEAKER_01: Study it again.

443
00:28:37,971 --> 00:28:40,676
Speaker SPEAKER_01: Resort versus a machine that's running continuously.

444
00:28:40,656 --> 00:28:43,019
Speaker SPEAKER_00: So yes, we had machine learning algorithms.

445
00:28:43,079 --> 00:28:48,727
Speaker SPEAKER_00: And some of the first algorithms that could learn what to do with hidden units were Boltzmann machines.

446
00:28:49,528 --> 00:28:51,371
Speaker SPEAKER_00: They were very inefficient.

447
00:28:51,391 --> 00:28:56,237
Speaker SPEAKER_00: But then later on, I found a way of making them approximations to them that was efficient.

448
00:28:56,257 --> 00:29:00,221
Speaker SPEAKER_00: And those were actually the trigger for getting deep learning going again.

449
00:29:00,682 --> 00:29:03,506
Speaker SPEAKER_00: Those were the things that learned one layer of feature detectors at a time.

450
00:29:04,728 --> 00:29:08,132
Speaker SPEAKER_00: And it was an efficient form of a restricted Boltzmann machine.

451
00:29:08,112 --> 00:29:10,738
Speaker SPEAKER_00: And so it was doing this kind of unlearning.

452
00:29:11,419 --> 00:29:17,712
Speaker SPEAKER_00: But rather than going to sleep, that one would just fantasize for a little bit after each data point.

453
00:29:18,094 --> 00:29:19,897
Speaker SPEAKER_01: So Androids do dream of electric sheep.

454
00:29:19,938 --> 00:29:22,584
Speaker SPEAKER_01: So let's go to theories two, three, and four.

455
00:29:23,445 --> 00:29:23,926
Speaker SPEAKER_00: OK.

456
00:29:24,852 --> 00:29:27,075
Speaker SPEAKER_00: Theory two was called the wake-sleep algorithm.

457
00:29:28,196 --> 00:29:31,182
Speaker SPEAKER_00: And you want to learn a generative model.

458
00:29:31,803 --> 00:29:35,468
Speaker SPEAKER_00: So you have the idea that you're going to have a model that can generate data.

459
00:29:36,229 --> 00:29:37,771
Speaker SPEAKER_00: It has layers of feature detectors.

460
00:29:38,172 --> 00:29:42,137
Speaker SPEAKER_00: And it activates the high-level ones and the low-level ones and so on until it activates pixels.

461
00:29:42,699 --> 00:29:43,380
Speaker SPEAKER_00: And that's an image.

462
00:29:44,161 --> 00:29:45,303
Speaker SPEAKER_00: You also want to learn the other way.

463
00:29:45,323 --> 00:29:46,845
Speaker SPEAKER_00: You want to learn to recognize data.

464
00:29:48,007 --> 00:29:49,829
Speaker SPEAKER_00: And so you're going to have an algorithm

465
00:29:50,198 --> 00:29:52,344
Speaker SPEAKER_00: that has two phases.

466
00:29:53,026 --> 00:29:55,532
Speaker SPEAKER_00: In the wake phase, data comes in.

467
00:29:55,553 --> 00:29:57,699
Speaker SPEAKER_00: It tries to recognize it.

468
00:29:58,461 --> 00:30:03,314
Speaker SPEAKER_00: And instead of learning the connections that it's using for recognition, it's learning the generative connections.

469
00:30:03,734 --> 00:30:05,118
Speaker SPEAKER_00: So data comes in.

470
00:30:05,825 --> 00:30:10,794
Speaker SPEAKER_00: I activate the hidden units, and then I learn to make those hidden units be good at reconstructing that data.

471
00:30:11,154 --> 00:30:13,940
Speaker SPEAKER_00: So it's learning to reconstruct at every layer.

472
00:30:13,960 --> 00:30:15,884
Speaker SPEAKER_00: But the question is, how do you learn the forward connections?

473
00:30:15,923 --> 00:30:21,775
Speaker SPEAKER_00: So the idea is, if you knew the forward connections, you could learn the backward connections, because you could learn to reconstruct.

474
00:30:23,375 --> 00:30:27,204
Speaker SPEAKER_00: Now, it also turns out that if you knew the backward connections, you could learn the forward connections.

475
00:30:27,224 --> 00:30:29,832
Speaker SPEAKER_00: Because what you could do is start at the top and just generate some data.

476
00:30:30,594 --> 00:30:35,086
Speaker SPEAKER_00: And because you generated the data, you'd know the states of all the hidden layers.

477
00:30:35,807 --> 00:30:39,577
Speaker SPEAKER_00: And so you could learn the forward connections to recover those states.

478
00:30:40,282 --> 00:30:42,946
Speaker SPEAKER_00: So that would be the sleep phase.

479
00:30:43,587 --> 00:30:47,834
Speaker SPEAKER_00: When you turn off the input, you just generate data.

480
00:30:48,255 --> 00:30:51,961
Speaker SPEAKER_00: And then you try and reconstruct the hidden units that generated the data.

481
00:30:52,381 --> 00:30:57,409
Speaker SPEAKER_00: And so if you know the top-down connections, you'd learn the bottom-up ones.

482
00:30:57,429 --> 00:30:59,372
Speaker SPEAKER_00: If you know the bottom-up ones, you can learn the top-down ones.

483
00:30:59,952 --> 00:31:04,559
Speaker SPEAKER_00: And so what's going to happen if you start with random connections and try to alternate both kinds of learning?

484
00:31:04,720 --> 00:31:05,201
Speaker SPEAKER_00: And it works.

485
00:31:05,902 --> 00:31:08,425
Speaker SPEAKER_00: Now, to make it work well, you have to do all sorts of variations of it.

486
00:31:08,445 --> 00:31:09,227
Speaker SPEAKER_00: But it works.

487
00:31:09,982 --> 00:31:12,425
Speaker SPEAKER_01: All right, do you want to go through the other two theories?

488
00:31:12,445 --> 00:31:13,326
Speaker SPEAKER_01: We only have eight minutes left.

489
00:31:13,346 --> 00:31:15,890
Speaker SPEAKER_01: I think we should probably jump through some other questions.

490
00:31:16,711 --> 00:31:20,234
Speaker SPEAKER_00: If you give me another hour, I could do the other two theories.

491
00:31:20,255 --> 00:31:21,675
Speaker SPEAKER_01: All right, well, Google I-O 2020.

492
00:31:22,116 --> 00:31:24,720
Speaker SPEAKER_01: So let's talk about what comes next.

493
00:31:25,039 --> 00:31:26,842
Speaker SPEAKER_01: So where is your research headed?

494
00:31:26,902 --> 00:31:28,324
Speaker SPEAKER_01: What problem are you trying to solve now?

495
00:31:30,385 --> 00:31:34,911
Speaker SPEAKER_00: The main thing I'm trying to solve, which I've been doing for a number of years now,

496
00:31:36,680 --> 00:31:38,482
Speaker SPEAKER_00: Actually, I'm reminded of a soccer commentator.

497
00:31:38,502 --> 00:31:45,452
Speaker SPEAKER_00: You may notice soccer commentators, they always say things like, they're doing very well, but they always go wrong on the last pass.

498
00:31:46,034 --> 00:31:48,837
Speaker SPEAKER_00: And they never seem to notice there's something funny about that.

499
00:31:49,558 --> 00:31:50,980
Speaker SPEAKER_00: A bit circular.

500
00:31:51,642 --> 00:31:56,148
Speaker SPEAKER_00: So I'm working, eventually, you're going to end up working on something that you don't finish.

501
00:31:56,710 --> 00:31:59,153
Speaker SPEAKER_00: And I think I may well be working on the thing I never finish.

502
00:31:59,233 --> 00:32:03,480
Speaker SPEAKER_00: But it's called Capsules, and it's the theory of,

503
00:32:03,460 --> 00:32:11,375
Speaker SPEAKER_00: how you do visual perception using reconstruction, and also how you route information to the right places.

504
00:32:12,518 --> 00:32:22,257
Speaker SPEAKER_00: And the two main motivating factors were, in standard neural nets, the activity in a layer just automatically goes somewhere.

505
00:32:22,538 --> 00:32:24,642
Speaker SPEAKER_00: You don't make decisions about where to send it.

506
00:32:25,229 --> 00:32:28,211
Speaker SPEAKER_00: The idea of capsules was to make decisions about where to send information.

507
00:32:29,173 --> 00:32:35,961
Speaker SPEAKER_00: Now, since I started working on capsules, some other very smart people at Google invented transformers, which are doing the same thing.

508
00:32:35,980 --> 00:32:37,501
Speaker SPEAKER_00: They're deciding where to route information.

509
00:32:38,083 --> 00:32:38,884
Speaker SPEAKER_00: And that's a big win.

510
00:32:40,244 --> 00:32:44,869
Speaker SPEAKER_00: The other thing that motivated capsules was coordinate frames.

511
00:32:44,890 --> 00:32:48,614
Speaker SPEAKER_00: So when humans do vision, they're always using coordinate frames.

512
00:32:49,194 --> 00:32:55,101
Speaker SPEAKER_00: And if they impose the wrong coordinate frame on an object, they don't even recognize the object.

513
00:32:55,080 --> 00:32:57,305
Speaker SPEAKER_00: So I'll give you a little task.

514
00:32:57,685 --> 00:32:59,449
Speaker SPEAKER_00: Imagine a tetrahedron.

515
00:32:59,469 --> 00:33:01,593
Speaker SPEAKER_00: It's got a triangular base and three triangular faces.

516
00:33:01,613 --> 00:33:02,734
Speaker SPEAKER_00: They're all equilateral triangles.

517
00:33:03,154 --> 00:33:04,076
Speaker SPEAKER_00: Easy to imagine, right?

518
00:33:06,621 --> 00:33:10,648
Speaker SPEAKER_00: Now imagine slicing it with a plane so you get a square cross-section.

519
00:33:12,250 --> 00:33:13,653
Speaker SPEAKER_00: That's not so easy, right?

520
00:33:14,315 --> 00:33:15,837
Speaker SPEAKER_00: Every time you slice, you get a triangle.

521
00:33:16,638 --> 00:33:18,221
Speaker SPEAKER_00: It's not obvious how you get a square.

522
00:33:18,281 --> 00:33:19,384
Speaker SPEAKER_00: It's not at all obvious.

523
00:33:20,208 --> 00:33:24,453
Speaker SPEAKER_00: OK, but I'll give you the same shape described differently.

524
00:33:25,476 --> 00:33:26,096
Speaker SPEAKER_00: I need your pen.

525
00:33:26,696 --> 00:33:36,090
Speaker SPEAKER_00: Imagine the shape you get if you take a pen like that, another pen at right angles like this, and you connect all points on this pen to all points on this pen.

526
00:33:37,573 --> 00:33:40,356
Speaker SPEAKER_00: That's a solid tetrahedron.

527
00:33:40,376 --> 00:33:40,698
Speaker SPEAKER_00: OK?

528
00:33:42,019 --> 00:33:44,442
Speaker SPEAKER_00: You're seeing it relative to a different coordinate frame.

529
00:33:45,976 --> 00:33:50,200
Speaker SPEAKER_00: where the edges of the tetrahedron, these two line up with the coordinate frame.

530
00:33:51,260 --> 00:33:57,865
Speaker SPEAKER_00: And for this, if you think of the tetrahedron that way, it's pretty obvious that at the top, you'll get a long rectangle this way.

531
00:33:58,166 --> 00:33:59,988
Speaker SPEAKER_00: At the bottom, you get a long rectangle that way.

532
00:34:00,969 --> 00:34:03,611
Speaker SPEAKER_00: And there's a guy called Weierstrass who says you've got to get a square in the middle.

533
00:34:05,272 --> 00:34:08,094
Speaker SPEAKER_00: So it's pretty obvious how you can slice it to get a square.

534
00:34:08,235 --> 00:34:11,097
Speaker SPEAKER_00: But that's only obvious if you think of it with that coordinate frame.

535
00:34:11,117 --> 00:34:15,882
Speaker SPEAKER_00: So it's obvious that for humans, coordinate frames are very important for perception.

536
00:34:15,862 --> 00:34:18,367
Speaker SPEAKER_00: And they're not at all important for convnets.

537
00:34:18,387 --> 00:34:27,025
Speaker SPEAKER_00: For convnets, if I show you a tilted square and an upright diamond, which are actually the same thing, they'll look the same to a convnet.

538
00:34:27,626 --> 00:34:30,731
Speaker SPEAKER_00: It doesn't have two alternative ways of describing the same thing.

539
00:34:30,965 --> 00:34:40,494
Speaker SPEAKER_01: How is adding coordinate frames to your model not the same as the error you were making in the 90s, where you were trying to put rules into the system as opposed to letting the system be unsupervised?

540
00:34:40,755 --> 00:34:41,996
Speaker SPEAKER_00: It is exactly that error.

541
00:34:42,838 --> 00:34:47,623
Speaker SPEAKER_00: And because I'm so adamant that that's a terrible error, I'm allowed to do a tiny bit of it.

542
00:34:48,603 --> 00:34:50,585
Speaker SPEAKER_00: It's sort of like Nixon negotiating with China.

543
00:34:51,907 --> 00:34:55,831
Speaker SPEAKER_00: Actually, that puts me in a bad role.

544
00:34:55,851 --> 00:34:58,795
Speaker SPEAKER_00: Anyway.

545
00:35:00,478 --> 00:35:05,644
Speaker SPEAKER_00: If you look at conv nets, they're just neural nets where you wired in a tiny bit of knowledge.

546
00:35:05,965 --> 00:35:09,568
Speaker SPEAKER_00: You wired in the knowledge that if a feature detector is good here, it's good over there.

547
00:35:10,971 --> 00:35:14,914
Speaker SPEAKER_00: And people would love to wire in just a little bit more knowledge about scale and orientation.

548
00:35:15,775 --> 00:35:20,340
Speaker SPEAKER_00: But if you do it in the obvious way of having a 4D grid instead of a 2D grid, the whole thing blows up on you.

549
00:35:21,461 --> 00:35:28,409
Speaker SPEAKER_00: But you can get in that knowledge about what viewpoint does to an image.

550
00:35:28,626 --> 00:35:31,612
Speaker SPEAKER_00: by using coordinate frames the same way they do them in graphics.

551
00:35:32,413 --> 00:35:35,077
Speaker SPEAKER_00: So now you have a representation in one layer.

552
00:35:35,418 --> 00:35:48,918
Speaker SPEAKER_00: When you try and reconstruct the parts of an object in the layer below, when you do that reconstruction, you can take the coordinate frame of the whole object and multiply it by the part-whole relationship to get the coordinate frame of the part.

553
00:35:49,739 --> 00:35:51,342
Speaker SPEAKER_00: And you can wire that into the network.

554
00:35:51,362 --> 00:35:55,309
Speaker SPEAKER_00: You can wire into the network the ability to do those coordinate transformations.

555
00:35:55,289 --> 00:35:57,557
Speaker SPEAKER_00: And that should make it generalized much, much better.

556
00:35:57,579 --> 00:36:01,594
Speaker SPEAKER_00: It should mean the networks just find viewpoint very easy to deal with.

557
00:36:02,056 --> 00:36:05,873
Speaker SPEAKER_00: Current neural networks find viewpoint, other than translation, very hard to deal with.

558
00:36:06,358 --> 00:36:14,090
Speaker SPEAKER_01: So your current task is specific to visual recognition, or it is a more general way of improving by coming up with a rule set for coordinate frames?

559
00:36:14,349 --> 00:36:16,092
Speaker SPEAKER_00: OK, it could be used for other things.

560
00:36:16,172 --> 00:36:20,197
Speaker SPEAKER_00: But I'm really interested in the use for visual recognition.

561
00:36:20,518 --> 00:36:21,340
Speaker SPEAKER_01: OK, last question.

562
00:36:21,380 --> 00:36:23,983
Speaker SPEAKER_01: I was listening to a podcast you gave the other day.

563
00:36:24,023 --> 00:36:31,753
Speaker SPEAKER_01: And in it, you said that the people whose ideas you value most are the young graduate students who come into your lab, because they aren't locked into the old perceptions.

564
00:36:31,835 --> 00:36:32,856
Speaker SPEAKER_01: They have fresh ideas.

565
00:36:33,436 --> 00:36:34,998
Speaker SPEAKER_01: And yet, they also know a lot.

566
00:36:35,434 --> 00:36:43,757
Speaker SPEAKER_01: Is there anything that you, sort of looking outside yourself, you think you might be locked into that a new graduate student or somebody in this room who came to work with you would shake up?

567
00:36:44,277 --> 00:36:45,539
Speaker SPEAKER_00: Yeah, everything I said.

568
00:36:45,559 --> 00:36:47,201
Speaker SPEAKER_01: Everything you said.

569
00:36:47,221 --> 00:36:48,483
Speaker SPEAKER_01: Take out those coordinate units.

570
00:36:49,103 --> 00:36:50,827
Speaker SPEAKER_01: Work on feature three, work on feature four.

571
00:36:50,847 --> 00:36:52,148
Speaker SPEAKER_01: I'm going to ask you a separate question.

572
00:36:52,710 --> 00:36:55,112
Speaker SPEAKER_01: So deep learning used to be a distinct thing.

573
00:36:55,132 --> 00:36:58,958
Speaker SPEAKER_01: And then it became sort of synonymous with the phrase AI.

574
00:36:59,460 --> 00:37:04,126
Speaker SPEAKER_01: And then AI is now a marketing term that basically means using a machine in any way whatsoever.

575
00:37:04,427 --> 00:37:07,652
Speaker SPEAKER_01: How do you feel about the terminology as the man who helped create this?

576
00:37:07,766 --> 00:37:15,001
Speaker SPEAKER_00: But I was much happier when there was AI, which meant you're logic inspired and you do manipulations on symbol strings.

577
00:37:15,623 --> 00:37:19,871
Speaker SPEAKER_00: And there was neural nets, which mean you want to do learning in a neural network.

578
00:37:20,010 --> 00:37:26,625
Speaker SPEAKER_00: And they were completely different enterprises that really didn't get along too well and fought for money.

579
00:37:27,646 --> 00:37:29,369
Speaker SPEAKER_00: That's how I grew up.

580
00:37:30,059 --> 00:37:36,393
Speaker SPEAKER_00: And now I see people who spent years saying neural networks are nonsense saying, I'm an AI professor, so I need money.

581
00:37:37,094 --> 00:37:38,557
Speaker SPEAKER_00: And it's annoying.

582
00:37:38,577 --> 00:37:46,195
Speaker SPEAKER_01: So your field succeeded, kind of ate or subsumed the other field, which then gave them an advantage in asking for money, which is frustrating.

583
00:37:46,175 --> 00:37:49,018
Speaker SPEAKER_00: Yeah, now it's not entirely fair, because a lot of them have actually converted.

584
00:37:49,197 --> 00:37:50,619
Speaker SPEAKER_01: Right, OK, wonderful.

585
00:37:50,900 --> 00:37:52,420
Speaker SPEAKER_01: Well, then I've got time for one more question.

586
00:37:52,820 --> 00:38:02,811
Speaker SPEAKER_01: So in that same interview, you were talking about AI, and you said, well, think of it like a backhoe, a backhoe that can build a hole or, if not constructed properly, can wipe you out.

587
00:38:03,351 --> 00:38:08,797
Speaker SPEAKER_01: And the key is, when you work on your backhoe, to design it in such a way that it's best to build a hole and not to clock you in the head.

588
00:38:09,237 --> 00:38:11,760
Speaker SPEAKER_01: As you think about your work, what are the choices you make like that?

589
00:38:16,195 --> 00:38:24,244
Speaker SPEAKER_00: Um, I guess I would never deliberately work on making weapons.

590
00:38:25,405 --> 00:38:28,829
Speaker SPEAKER_00: I mean, you could design a batco that was very good at knocking people's heads off.

591
00:38:28,849 --> 00:38:33,215
Speaker SPEAKER_00: And I think that would be a bad use of a batco, and I wouldn't work on it.

592
00:38:33,235 --> 00:38:33,434
Speaker SPEAKER_01: All right.

593
00:38:33,454 --> 00:38:36,277
Speaker SPEAKER_01: Well, Geoffrey Hinton, extraordinary interview, all kinds of information.

594
00:38:36,298 --> 00:38:39,541
Speaker SPEAKER_01: We'll be back next year to talk about Dreams, series three and four.

595
00:38:39,561 --> 00:38:41,204
Speaker SPEAKER_01: That was so much fun.

596
00:38:41,224 --> 00:38:41,864
Speaker SPEAKER_01: Thank you.


1 00:00:00,031 --> 00:00:01,552 说话人 SPEAKER_00: 我叫艾伦·伯恩斯坦。
2 00:00:02,535 --> 00:00:12,228 说话人 SPEAKER_00: 我是 CIFAR 的董事长兼首席执行官，今晚我有幸欢迎各位参加 CIFAR 年度晚宴。
3 00:00:13,109 --> 00:00:23,304 说话人 SPEAKER_00: 我想花一点时间来认识一位今晚与我们同在的重要人物，即尊敬的伊丽莎白·道斯威尔女士，安大略省副省长。
4 00:00:28,667 --> 00:00:40,487 说话人 SPEAKER_00: 她是一位真正的科学朋友，在其职业生涯中一直强烈倡导科学与科学政策以及公共政策之间联系的重要性。
5 00:00:40,948 --> 00:00:43,652 主持人：再次感谢您加入我们，尊敬的法官。
6 00:00:44,234 --> 00:00:47,799 主持人：每次和您在一起都感到非常高兴，您随时欢迎。
7 00:00:48,481 --> 00:00:50,143 主持人：您把它当作一个公开的邀请。
8 00:00:50,444 --> 00:00:53,950 主持人：只要告诉我，您随时欢迎参加 CFAR 活动。
9 00:00:53,929 --> 00:01:01,057 说话人 说话人_00: 我也想花一点时间来感谢今晚与我们同在的董事会成员。
10 00:01:01,777 --> 00:01:03,899 说话人 说话人_00: CFAR 董事会主席，Barb Stamets。
11 00:01:04,480 --> 00:01:06,623 说话人 说话人_00: 我不知道你们都在哪里，所以请原谅我。
12 00:01:08,584 --> 00:01:10,867 说话人 说话人_00: 哦，原来在这里。
13 00:01:10,887 --> 00:01:12,388 说话人 说话人_00：副主席，布鲁斯·米切尔。
14 00:01:15,430 --> 00:01:19,194 说话人 说话人_00：在我左边，前任主席，大卫·多德。
15 00:01:19,359 --> 00:01:19,921 说话人 说话人_00：大卫在哪里？
16 00:01:20,141 --> 00:01:20,561 说话人 说话人_00：在那里呢。
17 00:01:22,944 --> 00:01:24,367 说话人 说话人_00：导演斯蒂芬·利斯特。
18 00:01:24,387 --> 00:01:25,507 说话人 说话人_00：我看到斯蒂芬正坐着。
19 00:01:25,647 --> 00:01:30,194 说话人 说话人_00：还有那里的帕特·梅里迪斯。
20 00:01:30,213 --> 00:01:36,722 说话人 说话人_00：CIFAR 的董事会，作为 CEO，我有幸拥有如此出色的董事会。
21 00:01:36,822 --> 00:01:48,337 说话人 SPEAKER_00：他们慷慨地贡献了他们的时间、智慧和建议，以帮助确保 CIFAR 能够履行我们的使命，我们的使命是
22 00:01:50,409 --> 00:01:58,180 说话人 SPEAKER_00：我想我刚刚弄糟了幻灯片，推进者 Jeff，对不起，为了对研究做出战略性和变革性的贡献。
23 00:01:59,903 --> 00:02:04,049 说话人 SPEAKER_00：我们生活在一个需要像 CIFAR 这样的组织比以往任何时候都更加需要的时代。
24 00:02:04,531 --> 00:02:17,969 说话人 SPEAKER_00：今天，从气候变化、我们今天众所周知的难民危机、恐怖主义、社会包容性、经济发展等方面来看，机遇和挑战的复杂性和规模
25 00:02:17,949 --> 00:02:36,701 讲者：儿童和大脑发育、我们海洋的健康、疾病的分子基础，所有这些以及更多都需要全世界最优秀研究者的共同努力，无论他们来自哪个国家，属于哪个机构，在哪个学院，在哪个系，
26 00:02:36,681 --> 00:02:39,247 讲者：还是他们所从事的学科。
27 00:02:39,266 --> 00:02:43,955 讲者：这正是 CIFAR 过去 30 年所做的事情。
28 00:02:44,477 --> 00:02:53,955 讲者：我们一直在将加拿大和世界各地的杰出人才连接起来，以独特的全球研究网络深入讨论对世界具有重要意义的问题。
29 00:02:54,516 --> 00:02:57,643 说话人 说话人_00：我认为在地球上没有其他组织
30 00:02:57,622 --> 00:02:59,604 说话人 说话人_00：像 CIFAR 一样，我们已经做了很多努力。
31 00:03:00,385 --> 00:03:22,229 说话人 说话人_00：我们给我们的研究员和顾问，现在大约有400人，他们分布在18个国家的115个机构，提供了时间和自由去探索全新的视角，建立他们从未想过会建立的新的合作关系，去探索全新的发现之路。
32 00:03:22,210 --> 00:03:33,694 说话人 说话人_00：我们的工作使加拿大处于世界上一些最重要的讨论的中心，这些讨论最终将导致改变世界的变革性知识。
33 00:03:34,777 --> 00:03:41,652 说话人 说话人_00: CFAR 创造这些独特的条件，使得来自广泛不同领域的学者能够分享他们的个人观点和知识，对重要问题产生影响，并分享对问题的全新理解方式。
34 00:03:41,633 --> 00:03:51,063 说话人 说话人_00: 我可以告诉您，因为我们现在正在启动四个新的项目，看到那些以前从未见过面，甚至永远不会见面的人之间的化学反应是多么有趣，多么令人兴奋。
35 00:03:52,025 --> 00:04:04,300 说话人 说话人_00: 我可以告诉您，因为我们现在正在启动四个新的项目，看到那些以前从未见过面，甚至永远不会见面的人之间的化学反应是多么有趣，多么令人兴奋。
36 00:04:05,308 --> 00:04:09,816 说话人 说话人_00: 他们的学科原本永远不会交叉，如果不是因为参加了 CFAR 项目。
37 00:04:10,197 --> 00:04:12,520 说话者 说话者_00：他们最初会经历一段怀疑期。
38 00:04:12,961 --> 00:04:13,762 说话者 说话者_00：我为什么会在这里？
39 00:04:14,502 --> 00:04:15,324 说话者 说话者_00：这个人是谁？
40 00:04:15,405 --> 00:04:16,766 说话者 说话者_00：他们在谈论什么？
41 00:04:16,807 --> 00:04:18,108 说话人 说话人_00: 我不懂这种语言。
42 00:04:18,689 --> 00:04:21,394 说话人 说话人_00: 在一系列会议中，逐渐地，
43 00:04:21,374 --> 00:04:22,295 说话人 说话人_00: 他们明白了。
44 00:04:22,355 --> 00:04:23,216 说话人 说话人_00: 灯亮了。
45 00:04:24,358 --> 00:04:31,709 说话者 说话者_00：看到这一切发生，看到随着这光亮的出现而带来的新知识视角，这真是一件神奇的事情。
46 00:04:32,490 --> 00:04:39,781 说话者 说话者_00：所以今晚，我们想给你们打开一扇了解 CIFAR 所可能实现的跨学科对话的窗口。
47 00:04:40,442 --> 00:04:43,627 说话者 说话者_00：正如你们所知，我们最初计划有两个节目。
48 00:04:44,108 --> 00:04:49,997 说话者 说话者_00：顺便说一句，我听说蓝鸟队赢了，丹尼尔·丹尼特和杰夫·辛顿也在其中。
49 00:04:49,976 --> 00:04:54,826 说话人 SPEAKER_00: 很遗憾，由于生病，Dan 今天无法与我们在一起。
50 00:04:55,408 --> 00:04:57,891 说话人 SPEAKER_00: 他告诉我们他今天不可能在这里。
51 00:04:59,235 --> 00:05:00,898 说话人 SPEAKER_00: Jeff 并不是一个差劲的替代者。
52 00:05:01,418 --> 00:05:07,891 说话人 SPEAKER_00: Jeff Hinton 无疑是世界上人工智能和深度学习领域的顶尖专家。
53 00:05:08,593 --> 00:05:10,516 说话人 SPEAKER_00: 他是 CIFAR 杰出研究员。
54 00:05:10,497 --> 00:05:19,951 说话人 SPEAKER_00: 他将帮助我们理解为什么他的工作和他的深度学习同事的工作成为了当今科学和社会中最热门的领域之一。
55 00:05:19,971 --> 00:05:21,454 说话人 SPEAKER_00: 我稍后会更多地介绍 Jeff。
56 00:05:22,836 --> 00:05:33,432 说话人 SPEAKER_00: Jeff 将和我们的 CBC 电台主持人 Nora Young 一起，帮助我们开始理解这个世界将如何因为这一新科学而改变。
57 00:05:33,413 --> 00:05:38,920 说话人 说话人_00：今晚，我想感谢很多人，是他们让 CIFAR 成为今天的模样。
58 00:05:38,940 --> 00:05:42,206 说话人 说话人_00：首先，我想感谢佩卡·森沃。
59 00:05:43,908 --> 00:06:02,694 说话人 说话人_00：佩卡在 CIFAR 担任高级副总裁七年，今年年初，他告诉我们他决定离开 CIFAR，全职回到多伦多大学的高能物理和粒子物理教学与研究中。
60 00:06:03,586 --> 00:06:06,089 说话人 说话人_00：佩卡是一位世界知名的粒子物理学家。
61 00:06:06,531 --> 00:06:14,223 讲者：他是发现希格斯玻色子的团队的一员，这一发现去年获得了诺贝尔物理学奖。
62 00:06:15,324 --> 00:06:28,665 讲者：他将国际研究声誉带到了 CIFAR，并且是我们所有人眼中的热情、真正的热情的倡导者和受人尊敬的同事。
63 00:06:28,644 --> 00:06:30,747 讲者：他指导了我们的研究项目。
64 00:06:31,187 --> 00:06:35,314 讲者：他坚持每五年对每个项目进行严格的审查。
65 00:06:35,774 --> 00:06:38,519 说话人 SPEAKER_00: 他帮助全球青年学院的发展。
66 00:06:38,959 --> 00:06:42,363 说话人 SPEAKER_00: 他还启动了我们首次的知识传播工作。
67 00:06:42,884 --> 00:06:56,322 说话人 SPEAKER_00: 我个人对 Pekka 非常感激，因为他在我第一次来到 CIFAR 时就伸出了援手，并在帮助实现 CIFAR 2.0——我们对 CIFAR 的更新愿景方面发挥了领导作用。
68 00:06:56,302 --> 00:07:05,399 说话人 SPEAKER_00: Pekka 在我们首次全球征集研究提案创意，以在此处 CIFAR 创建新项目的过程中扮演了主角。
69 00:07:06,021 --> 00:07:16,360 说话人 SPEAKER_00: 毫无疑问，真的毫无疑问，没有 Pekka 的领导、他的辛勤工作、他对研究的热情以及他对 CIFAR 的热情，CIFAR 今天不可能达到现在的地位。
70 00:07:16,720 --> 00:07:19,064 说话人 SPEAKER_00: Pekka，我们会想念你的，我们深深地想念你。
71 00:07:19,305 --> 00:07:20,387 说话人 SPEAKER_00: 祝你一切顺利。
72 00:07:20,367 --> 00:07:21,874 说话人 SPEAKER_00: 我想 Pat 在这里。
73 00:07:21,894 --> 00:07:22,557 说话人 说话人_00: 我没见过她。
74 00:07:22,576 --> 00:07:23,360 说话人 说话人_00: 哦，在那儿。
75 00:07:23,401 --> 00:07:24,766 说话人 说话人_00: 帕特，很高兴你能来。
76 00:07:25,189 --> 00:07:29,769 说话人 说话人_00: 在接下来的几个月和几年里，你可能会更多地拥有你的丈夫。
77 00:07:40,870 --> 00:07:51,108 说话人 SPEAKER_00: 今晚稍晚些时候，我们还将向在 CFAR 董事会任职 20 年的理查德·艾维致敬，他一直是 CFAR 的真正强大支持者。
78 00:07:51,550 --> 00:07:55,197 说话人 SPEAKER_00: 我们将在今晚稍后了解更多关于理查德的信息，我想也是从理查德那里听到的。
79 00:07:55,937 --> 00:07:58,002 说话人 SPEAKER_00: 今晚屏幕上，
80 00:07:57,982 --> 00:08:04,956 说话人 SPEAKER_00: 您应该能看到，为 CFAR 慷慨捐赠的公共和私人支持者以及我们的合作伙伴的名单。
81 00:08:05,478 --> 00:08:09,625 主持人：今晚许多人都与我们在这里，我无法感谢你们的支持足够。
82 00:08:10,307 --> 00:08:11,990 主持人：研究是需要资金的。
83 00:08:12,122 --> 00:08:30,007 主持人：没有秘密，但如果您想到我之前提到的全球问题，比如气候变化或精神疾病，我们工作的成本实际上只是这些社会问题真实成本的一小部分。
84 00:08:31,007 --> 00:08:41,842 主持人：我想特别感谢今晚与我们在这里的，慷慨捐赠了五十万美元或更多以帮助 CFAR 重大新项目的各位。
阿斯里埃利基金会、安大略省政府、理查德·W·艾维和唐娜·艾维、理查德·M·艾维、杰里和杰拉尔丁·赫费南、迈克尔和索尼娅·克纳尔，以及加拿大皇家银行基金会。
86 00:08:57,782 --> 00:09:08,235 发言人 SPEAKER_00：你们所有人、你们所代表的组织，以及许多贡献远少于这些的人，对于我们所做的工作都至关重要。
87 00:09:08,215 --> 00:09:12,121 发言人 SPEAKER_00：我们还有史以来第一次增加了新的研究合作伙伴。
加拿大脑研究基金会、加拿大基因组学、不列颠哥伦比亚基因组学以及首次，来自加拿大以外的合作伙伴。
89 00:09:20,517 --> 00:09:20,937 说话人 说话人_00：美国。
90 00:09:21,119 --> 00:09:22,922 说话人 说话人_00：戈登和贝蒂·摩尔基金会。
91 00:09:23,503 --> 00:09:26,307 说话人 说话人_00：戈登·摩尔是计算机芯片的发明者。
92 00:09:26,288 --> 00:09:38,153 说话人 说话人_00：支持我们在量子材料和微生物多样性项目，以及法国机构 INRIA，该机构帮助支持杰夫在神经计算和自适应感知项目。
93 00:09:38,874 --> 00:09:42,542 说话人 说话人_00：在大家的支持下，我认为我们取得了巨大的进步。
94 00:09:42,522 --> 00:09:49,614 说话人 说话人_00：经过一次非常成功的全球新研究想法征集，我们增加了四个非常激动人心的新项目。
95 00:09:50,054 --> 00:09:51,857 说话人 说话人_00：生物启发式太阳能。
96 00:09:52,239 --> 00:09:55,043 说话人 说话人_00：你们中的许多人去年都听过 Ted Sargent 谈论这个话题。
97 00:09:55,605 --> 00:09:56,966 说话人 说话人_00：大脑、心智和意识。
98 00:09:57,368 --> 00:09:58,929 说话人 说话人_00：生命的分子结构。
99 00:09:59,431 --> 00:10:00,894 说话人 说话人_00：人类与微生物组。
100 00:10:01,394 --> 00:10:05,120 说话人 说话人_00：将我们的项目总数增加到14个。
101 00:10:05,100 --> 00:10:09,388 说话人 SPEAKER_00：我对这个进展非常满意，但我们并不打算就此止步。
102 00:10:10,028 --> 00:10:17,441 说话人 SPEAKER_00：世界面临着许多重要的问题，这些问题可以从 CIFAR 的独特模式中受益，并且确实绝对需要研究。
103 00:10:18,101 --> 00:10:24,292 说话人 SPEAKER_00：感谢大家与我及我的同事一起继续为世界做出 CIFAR 至关重要的工作。
104 00:10:24,731 --> 00:10:26,134 说话人 SPEAKER_00：现在请尽情享用您的餐点。
105 00:10:26,575 --> 00:10:28,357 说话人 说话人_00: 好胃口，我马上回来。
106 00:10:38,394 --> 00:10:40,720 说话人 说话人_00: 如果可以再次得到大家的注意，请。
107 00:10:48,414 --> 00:10:50,639 说话人 说话人_00: 我希望你们今晚都吃得好。
108 00:10:50,698 --> 00:10:53,063 说话人 说话人_00: 我今晚快速看了一下菜单。
109 00:10:53,104 --> 00:10:59,576 说话人 SPEAKER_00: 它说有三种不错的甜点，但实际上是指奢华的，所以我读得太快了。
110 00:10:59,995 --> 00:11:03,559 说话人 SPEAKER_00: 我很高兴今晚有 Nora Young 与我们同在。
111 00:11:04,601 --> 00:11:20,125 说话人 SPEAKER_00: 大多数人都会熟悉 Nora，她是 CBC《绝对不是歌剧》的创始主持人，现在是《火花》的主持人，这个节目探讨技术是如何塑造我们的生活和我们所生活的更广阔世界的。
112 00:11:20,785 --> 00:11:26,475 说话人 SPEAKER_00: 在演讲结束后，Nora 将引导 Jeff 进行对话。
113 00:11:26,455 --> 00:11:29,919 说话人 说话人_00: 我还想说几句关于杰夫·辛顿的话。
114 00:11:30,741 --> 00:11:40,798 说话人 说话人_00: 杰夫是 CIFAR 杰出研究员，谷歌的杰出研究员，以及多伦多大学的杰出教授。
115 00:11:41,499 --> 00:11:44,143 说话人 说话人_00: 所以他三次获得杰出称号。
116 00:11:45,389 --> 00:11:54,282 说话人 说话人_00: 让我指出，他是有史以来仅被授予 CFAR 杰出研究员称号的三个人之一。
117 00:11:54,822 --> 00:11:56,445 说话人 SPEAKER_00: 这个称号是一生的荣誉。
118 00:11:56,785 --> 00:12:08,663 说话人 SPEAKER_00: 它由 CFAR 董事会授予极少数对 CFAR 和科学做出杰出、长期、持久贡献的研究人员。
119 00:12:08,643 --> 00:12:11,748 说话人 SPEAKER_00: 同时在他们的研究领域内大大推进了知识。
120 00:12:12,549 --> 00:12:16,216 说话人 SPEAKER_00: 我认为在杰夫的情况下，这个荣誉是非常、非常应得的。
121 00:12:17,077 --> 00:12:31,883 说话人 说话人_00: 杰夫在上个千年时对计算机科学的一个分支——神经网络产生了兴趣，并且持续关注了很长时间，远在其他人纷纷放弃之后。
122 00:12:33,095 --> 00:12:53,178 说话人 说话人_00: 2004 年，他被邀请与蒙特利尔大学的约书亚·本吉奥和纽约大学及现在的 Facebook 的杨立昆一起创立了 CIFAR 的“神经网络与自适应感知”项目，为他找到了一个学术家园。
123 00:12:53,480 --> 00:13:04,032 说话人 说话人_00: 他们和其他项目成员继续研究神经网络，并开发了一种革命性的神经网络新方法——深度学习。
124 00:13:05,013 --> 00:13:10,739 说话人 说话人_00: 这种形式的人工智能在最近几年取得了巨大的成功。
125 00:13:11,379 --> 00:13:17,866 说话人 SPEAKER_00: 事实上，它被用于从图像识别到自然语言处理的各个方面。
126 00:13:17,846 --> 00:13:24,421 说话人 SPEAKER_00: 谷歌、Facebook、中国的百度等都在使用它并进行研究。
127 00:13:25,142 --> 00:13:37,729 说话人 SPEAKER_00: 我非常自豪，在加入这个美好的组织之前，CIFAR 就承担了风险，在那些早期就提供了支持，帮助实现这一革命。
128 00:13:37,708 --> 00:13:48,634 说话人 SPEAKER_00: 《连线》杂志，这是硅谷的记录杂志，在过去一年里对杰夫、CFAR 和 NCAP 做了两篇报道。
129 00:13:48,654 --> 00:13:53,085 说话人 说话人_00：我将从这些故事中的一篇简要引用。
130 00:13:53,065 --> 00:14:04,268 说话人 说话人_00：我引用如下，仅从 CIFAR 每年投资五十万美元开始，Hinton 的自由思考者联盟即将将无数美元回馈到经济中。
131 00:14:05,090 --> 00:14:12,205 说话人 说话人_00：在这个过程中，Hinton 和 CIFAR 改变了曾经排斥他们的社区的面貌。
132 00:14:12,184 --> 00:14:18,931 说话人 说话人_00：大学里的学生们正在远离更传统的机器学习项目，转向深度学习。
133 00:14:20,072 --> 00:14:23,195 说话人 SPEAKER_00: 换句话说，深度学习现在已经变得主流。
134 00:14:24,375 --> 00:14:27,458 说话人 SPEAKER_00: 我们不再是那个疯狂的边缘，Hinton 说。
135 00:14:28,360 --> 00:14:30,101 说话人 SPEAKER_00: 我们现在是疯狂的内核。
136 00:14:32,964 --> 00:14:39,210 说话人 SPEAKER_00: 现在我要邀请 Nora 和一位绝对不疯狂的人上台与我一起。
137 00:14:39,429 --> 00:14:40,250 Speaker SPEAKER_00: Nora, Jeff.
138 00:14:49,241 --> 00:14:52,056 Speaker SPEAKER_06: 非常感谢。
139 00:14:52,096 --> 00:14:52,698 Speaker SPEAKER_04: 非常感谢。
140 00:14:52,719 --> 00:14:54,166 Speaker SPEAKER_04: 我想直接进入正题。
141 00:14:54,186 --> 00:14:57,581 说话人 SPEAKER_04：在我们开始 Jays 1 之前，我只想说一句话。
142 00:15:01,357 --> 00:15:15,778 说话人 SPEAKER_04：7-1 我认为是这样，我将把事情交给 Jeff，让他来启发我们，然后我会和 Jeff 进行简短的问答，最后我们再稍微开放一下讨论。Jeff，我想首先感谢 CIFAR。
143 00:15:16,259 --> 00:15:29,138 说话人 SPEAKER_02：CIFAR 是我 1987 年来加拿大的原因，他们有一个人工智能项目，是我最早的项目之一，我在 2002 年回到了加拿大。
144 00:15:29,910 --> 00:15:54,298 说话人 SPEAKER_02：我和 CFAR 合作建立了这个项目，它产生了巨大的影响，因为它使得加拿大（如蒙特利尔、UBC 和多伦多）、美国、以色列和芬兰等地的许多研究人员能够定期互动，这导致了令人满意的进展。
145 00:15:56,572 --> 00:16:02,485 说话人 SPEAKER_02：那么，我将尝试向你们解释什么是深度学习，先假设你们一无所知。
146 00:16:03,787 --> 00:16:08,736 说话人 SPEAKER_02：我是一个学者，我想正确地解释事物。
147 00:16:08,756 --> 00:16:14,567 说话人 SPEAKER_02：所以，这是关于神经网络，神经网络由人工神经元组成。
148 00:16:14,989 --> 00:16:16,471 说话人 SPEAKER_02：那么什么是人工神经元呢？
149 00:16:16,822 --> 00:16:18,726 说话人 SPEAKER_02：嗯，这并不完全像真实的神经元。
150 00:16:18,746 --> 00:16:22,234 说话人 SPEAKER_02：真实的神经元很复杂，就像真实的分子一样复杂。
151 00:16:22,274 --> 00:16:27,124 说话人 SPEAKER_02：但如果你想了解一种气体，你可以把分子想象成台球，这样你就可以了解很多关于气体的知识。
152 00:16:27,924 --> 00:16:31,432 说话人 SPEAKER_02：同样地，我们将理想化，哎呀，我想回退一下。
153 00:16:33,056 --> 00:16:37,784 说话人 SPEAKER_02：我们将通过理想化神经元来说明，它是一种接收输入的东西。
154 00:16:39,232 --> 00:16:41,616 说话人 SPEAKER_02：如果你看左边的图，它是一堆输入。
155 00:16:42,577 --> 00:16:46,424 说话人 SPEAKER_02：在这些来自其他神经元或感官的输入行上，它有权重。
156 00:16:47,105 --> 00:16:50,071 说话人 SPEAKER_02：它将输入行的活动乘以权重，然后全部加起来。
157 00:16:50,431 --> 00:16:51,231 说话人 SPEAKER_02：这就是它的总输入。
158 00:16:51,852 --> 00:16:54,457 说话人 SPEAKER_02：然后它给出一个与总输入相关的输出。
159 00:16:55,018 --> 00:16:59,065 说话人 SPEAKER_02：右侧显示了某种人工神经元的输入输出函数。
160 00:16:59,044 --> 00:17:03,778 说话人 SPEAKER_02：所以如果其总输入超过阈值，它就会随着输入的增加而增加输出。
161 00:17:04,138 --> 00:17:06,787 说话人 SPEAKER_02：如果它低于阈值，则不输出任何内容。
162 00:17:07,608 --> 00:17:09,212 说话人 SPEAKER_02：因此它有隐藏信息的一种方式。
163 00:17:09,534 --> 00:17:12,362 说话者 SPEAKER_02：它不关心输入中的波动，只要这些波动低于阈值。
164 00:17:13,490 --> 00:17:18,895 说话者 SPEAKER_02：好吧，这比真实的神经元简单得多，尽管并不完全不同。
165 00:17:19,557 --> 00:17:24,202 说话者 SPEAKER_02：但是通过研究这类事物，我们可以问，如果你把这些东西放在一起，我们该如何做呢？
166 00:17:24,222 --> 00:17:26,085 说话者 SPEAKER_02：特别是，让我们做一个有趣的任务。
167 00:17:26,786 --> 00:17:29,588 说话人 SPEAKER_02：我给你展示 300 万个数字。
168 00:17:30,148 --> 00:17:35,275 说话人 SPEAKER_02：这 300 万个数字是 1000 乘以 1000 像素的 RGB 值。
169 00:17:36,336 --> 00:17:39,160 说话人 SPEAKER_02：你的任务是编写一个计算机程序，该程序接受 300 万个数字。
170 00:17:40,134 --> 00:17:43,641 说话人 SPEAKER_02：并输出一个描述图像内容的单词串。
171 00:17:45,263 --> 00:17:47,327 说话人 SPEAKER_02：现在，你不想写那个程序。
172 00:17:48,229 --> 00:17:51,214 说话人 SPEAKER_02：你更愿意写一个简单的程序，让它去学习并找出结果。
173 00:17:52,155 --> 00:17:54,079 说话人 SPEAKER_02：进化也是这样想的。
174 00:17:56,324 --> 00:17:59,730 说话人 SPEAKER_02：因此我们在人工神经网络中连接这些神经元。
175 00:18:00,182 --> 00:18:03,387 说话人 SPEAKER_02：我们在这里有输入神经元，通常代表图像中的像素。
176 00:18:04,088 --> 00:18:05,451 说话人 SPEAKER_02：我们有多层中间层。
177 00:18:06,192 --> 00:18:10,137 说话人 SPEAKER_02：然后我们有输出神经元，代表例如图像中可能存在的决策。
178 00:18:11,480 --> 00:18:19,531 说话人 SPEAKER_02：我们称中间层为隐藏层，因为仅凭知道输入和输出，你不知道它们在做什么。
179 00:18:19,551 --> 00:18:23,237 说话人 SPEAKER_02：目标是训练神经网络，使其合理地使用那些隐藏层。
180 00:18:24,922 --> 00:18:25,763 说话人 SPEAKER_02：那么我们如何训练它呢？
181 00:18:26,464 --> 00:18:27,728 说话人 SPEAKER_02：嗯，有两种算法。
182 00:18:27,748 --> 00:18:28,690 说话人 SPEAKER_02：有监督训练。
183 00:18:29,111 --> 00:18:32,558 说话人 SPEAKER_02：在监督训练中，我们展示了一个输入，并展示了正确的输出。
184 00:18:33,160 --> 00:18:35,684 说话人 SPEAKER_02：所以我们展示了一张图片，并说，在这张图片中，有一只猫和一只狗。
185 00:18:37,890 --> 00:18:44,684 说话人 SPEAKER_02：然后你调整所有这些神经元的权重，以便下次展示那张图片时，你能在得到正确答案上有所提高。
186 00:18:45,424 --> 00:18:46,705 说话人 SPEAKER_02：我将在一分钟内告诉你如何做到这一点。
187 00:18:47,287 --> 00:18:49,750 说话人 SPEAKER_02：在无监督训练中，我们甚至不告诉它答案。
188 00:18:49,769 --> 00:18:50,631 说话人 SPEAKER_02：我们只是展示给它输入。
189 00:18:51,271 --> 00:18:57,598 说话人 SPEAKER_02：然后它试图使用其隐藏神经元，从隐藏神经元的活动中重建输入。
190 00:19:00,942 --> 00:19:04,446 说话人 SPEAKER_02：所以这就是我们将要使用的监督学习算法的思考方式。
191 00:19:05,008 --> 00:19:08,731 说话者 SPEAKER_02：不是我们使用的算法，而是我们使用了一种更高效地完成同样事情的方法。
192 00:19:10,230 --> 00:19:15,476 说话者 SPEAKER_02：如果你有一个简单的进化论理论，你可能会认为训练神经网络的办法是这样的。
193 00:19:16,596 --> 00:19:26,865 说话者 SPEAKER_02：你给它输入在底部，你从顶部得到输出，你将那些输出与正确的输出进行比较，看看你做得怎么样，你为那些你知道正确答案的小样本案例做这件事。
194 00:19:28,167 --> 00:19:31,089 说话者 SPEAKER_02：然后你看看网络目前在那组案例上的表现如何。
195 00:19:32,090 --> 00:19:37,115 说话者 SPEAKER_02：然后你取网络中的一个权重，那个用红色显示的权重，然后你稍微改变它。
196 00:19:38,023 --> 00:19:41,387 说话者 SPEAKER_02：然后你问，我的网络在这些情况下表现是更好还是更差？
197 00:19:42,088 --> 00:19:45,113 说话者 SPEAKER_02：如果它表现更差，你就让这个权重保持在原来的位置。
198 00:19:45,153 --> 00:19:46,875 说话者 SPEAKER_02：但如果它表现更好，你就保留这个改变。
199 00:19:47,455 --> 00:19:51,060 说话者 SPEAKER_02：所以你对一个权重进行了变异，如果效果变好就保留它。
200 00:19:52,123 --> 00:19:56,848 说话者 SPEAKER_02：对所有人来说都很明显，如果你这样做足够长时间，你就能在这个网络中得到很好的权重。
201 00:19:58,109 --> 00:20:02,395 说话者 SPEAKER_02：有一个算法，它不是一次改变一个权重，而是可以并行改变所有权重。
202 00:20:03,076 --> 00:20:06,141 说话者 SPEAKER_02：这只需要用到一些微积分，我不会打扰你。
203 00:20:07,150 --> 00:20:14,599 讲者 SPEAKER_02：这就是所谓的反向传播算法，它实际上就是我所描述的变异方法，但它对所有的权重进行并行处理。
204 00:20:15,240 --> 00:20:21,849 讲者 SPEAKER_02：所以如果你有一百万个权重，它的效率比改变一个权重然后看效果要高一百万倍。
205 00:20:22,691 --> 00:20:26,256 讲者 SPEAKER_02：如果你有一亿个权重，它的效率要高一亿倍。
206 00:20:26,977 --> 00:20:32,824 讲者 SPEAKER_02：现在你实际上有大约 10 的 14 次方个权重，所以它的效率是 10 的 14 次方倍，这相当于宇宙的年龄。
207 00:20:33,986 --> 00:20:34,086 未知说话者：好的。
208 00:20:35,315 --> 00:20:37,596 说话者 SPEAKER_02：能得到让物理学家印象深刻的数字真是太好了。
209 00:20:40,157 --> 00:20:42,540 说话者 SPEAKER_02：所以人们使用这个反向传播算法。
210 00:20:42,761 --> 00:20:43,843 说话者 SPEAKER_02：很多人发明了这个算法。
211 00:20:43,923 --> 00:20:46,446 说话人 SPEAKER_02：但在 80 年代，当工作站足够快时，他们就使用了它。
212 00:20:47,107 --> 00:20:48,690 说话人 SPEAKER_02：你会在底部放一张图片。
213 00:20:49,111 --> 00:20:50,173 说话人 SPEAKER_02：你会得到答案。
214 00:20:50,732 --> 00:20:52,236 说话人 SPEAKER_02：然后你会回传错误信号。
215 00:20:52,916 --> 00:21:01,429 说话者 SPEAKER_02：为了理解这个想法，你希望早期层的特征能够，早期这些神经元的层能够学会成为边缘等事物的特征检测器。
216 00:21:02,089 --> 00:21:06,336 说话者 SPEAKER_02：然后在下一层，你可能会检测到一对在锐角处相交的边缘。
217 00:21:06,856 --> 00:21:08,358 说话者 SPEAKER_02：那么这可能是喙。
218 00:21:09,030 --> 00:21:14,698 说话者 SPEAKER_02：如果你发现了喙的证据，还有眼睛和羽毛的证据，那么你可能认为这是一只鸟。
219 00:21:15,239 --> 00:21:17,461 说话人 SPEAKER_02：因此，我们的想法是通过这种方式获得特征层次结构。
220 00:21:17,903 --> 00:21:21,467 说话人 SPEAKER_02：我们知道人脑可以进行物体识别等类似活动。
221 00:21:22,087 --> 00:21:24,230 说话人 SPEAKER_02：结果证明，如果你训练这些网络，它们也能做到这一点。
222 00:21:28,596 --> 00:21:29,637 说话人 SPEAKER_02：所以，
223 00:21:29,955 --> 00:21:39,125 说话人 SPEAKER_02：在 20 世纪 80 年代中期和后期，这个算法被应用于许多事物，比如读取支票上的金额、检测信用卡欺诈、解读宫颈涂片。
224 00:21:39,145 --> 00:21:41,127 说话人 SPEAKER_02：但它从未像我们希望的那样工作得很好。
225 00:21:41,327 --> 00:21:45,512 说话人 SPEAKER_02：也许对于读取支票上的金额来说是这样，但对于其他事情，它并没有像我们想象的那样好。
226 00:21:46,133 --> 00:21:49,356 说话人 SPEAKER_02：而且它无法利用那些隐藏层。
227 00:21:49,436 --> 00:21:51,719 说话人 SPEAKER_02：它无法像我们想象的那样学习很多层特征。
228 00:21:52,660 --> 00:21:55,262 说话人 SPEAKER_02：所以大多数人放弃了它。
229 00:21:57,125 --> 00:21:58,967 说话人 SPEAKER_02：然后我们设立了 CIFAR 项目。
230 00:21:59,317 --> 00:22:02,882 说话人 SPEAKER_02：这实际上是一个针对 C-File 的历史版本。
231 00:22:03,702 --> 00:22:08,548 说话人 SPEAKER_02：然后我们设置了 C-文件程序，一切就变了。
232 00:22:08,909 --> 00:22:14,596 说话人 SPEAKER_02：一群聪明人聚在一起，定期开会，我们找到了让反向传播工作得更好的方法。
233 00:22:15,356 --> 00:22:18,700 说话人 SPEAKER_02：我们对发明用来做到这一点的所有巧妙的小技术手段非常关心。
234 00:22:19,621 --> 00:22:20,663 说话人 SPEAKER_02：你们可能对这些不感兴趣。
235 00:22:20,903 --> 00:22:27,029 说话人 SPEAKER_02：你所需要知道的是，它现在工作得非常出色，尤其是如果你有很多数据和计算能力的话。
236 00:22:28,630 --> 00:22:30,172 说话人 SPEAKER_02：这就是第一个杀手级应用。
237 00:22:31,413 --> 00:22:40,785 说话人 SPEAKER_02：如果你想识别语音，你想要能够将声波识别为某人正在说的特定音素。
238 00:22:42,106 --> 00:22:50,678 说话人 SPEAKER_02：所以，你首先将声波进行预处理，变成 11 个系数帧，然后询问中间帧，某人正在说哪个音素的哪一部分？
239 00:22:51,459 --> 00:22:56,545 说话者 SPEAKER_02：嗯，这里有，哦，这里有 183 个不同的音素片段。
240 00:22:57,335 --> 00:23:03,663 说话者 SPEAKER_02：我的几个学生使用反向传播和我们的新技巧训练了一个大型深度神经网络。
241 00:23:04,684 --> 00:23:08,890 说话者 SPEAKER_02：他们发现这比标准的语音识别器效果更好，只是好一点点。
242 00:23:09,790 --> 00:23:16,660 说话者 SPEAKER_02：但是标准的语音识别器中已经包含了 30 年的精心工程，而我们这是实验室的两个学生，所以显然它会获胜。
243 00:23:17,652 --> 00:23:21,476 说话人 SPEAKER_02：很快，微软、IBM 和谷歌都开始进一步开发这项技术。
244 00:23:22,057 --> 00:23:24,519 说话人 SPEAKER_02：我的一个学生去谷歌实习了。
245 00:23:24,980 --> 00:23:27,402 说话人 SPEAKER_02：到 2012 年，这项技术已经出现在 Android 系统中。
246 00:23:27,823 --> 00:23:31,567 说话人 SPEAKER_02：它使得语音识别的质量有了大幅提升。
247 00:23:31,586 --> 00:23:36,672 说话人 SPEAKER_02：现在所有语音识别器都使用某种形式的神经网络，所有好的语音识别器都是这样。
248 00:23:38,733 --> 00:23:40,375 说话人 SPEAKER_02：然后我们对物体识别也做了同样的事情。
249 00:23:41,436 --> 00:23:43,980 说话人 SPEAKER_02：所以你取一个高分辨率图像，
250 00:23:44,398 --> 00:23:51,191 说话人 SPEAKER_02：并且有一个数据集，拥有一个包含一百万张图像和一千种不同类别物体的数据集是很重要的。
251 00:23:52,292 --> 00:23:55,479 说话者 SPEAKER_02：有人用最显眼的物体名称给它贴了标签。
252 00:23:56,520 --> 00:24:00,007 说话者 SPEAKER_02：由于图片中可能有多个物体，你可以猜五次。
253 00:24:00,067 --> 00:24:03,133 说话者 SPEAKER_02：如果你在五次猜测中猜出与它们相同的名称，你就赢了，否则你就输了。
254 00:24:04,041 --> 00:24:10,810 说话者 SPEAKER_02：一些优秀的计算机视觉团队使用了 2012 年现有的计算机视觉技术。
255 00:24:11,151 --> 00:24:13,574 说话人 SPEAKER_02：计算机视觉领域的人对神经网络非常失望。
256 00:24:13,875 --> 00:24:15,856 说话人 SPEAKER_02：他们说，这些方法永远不会在真实图像上奏效。
257 00:24:17,419 --> 00:24:18,500 说话人 SPEAKER_02：然后我们得到了这些结果。
258 00:24:19,501 --> 00:24:22,885 说话人 SPEAKER_02：因此，计算机视觉社区在约 25%的错误率上达到了极限。
259 00:24:23,086 --> 00:24:26,530 说话者 SPEAKER_02：我们的错误率达到了 16%。
260 00:24:26,590 --> 00:24:29,474 说话者 SPEAKER_02：然后发生了一件非常令人印象深刻的事情。
261 00:24:29,657 --> 00:24:34,125 计算机视觉社区，在接下来的这一年里，基本上放弃了他们之前所做的一切。
262 00:24:34,806 --> 00:24:36,269 他们说，好吧，这个效果更好。
263 00:24:36,288 --> 00:24:36,848 说话人 SPEAKER_02：我们要这么做。
264 00:24:37,410 --> 00:24:39,053 说话人 SPEAKER_02：但是，科学家们并不是这样做的。
265 00:24:39,453 --> 00:24:45,083 说话人 SPEAKER_02：如果你想让科学家放弃他们的理论，你只需要等待他们去世，然后有新的科学家出现。
266 00:24:45,103 --> 00:24:49,650 说话人 SPEAKER_02：但在这个案例中，他们放弃了他们的理论，现在他们都这样做。
267 00:24:49,670 --> 00:24:52,153 说话人 SPEAKER_02：遗憾的是，有些人在这方面比我们做得更好。
268 00:24:53,675 --> 00:24:56,019 说话人 SPEAKER_02：哎呀，三年后。
269 00:24:56,404 --> 00:25:19,829 说话人 SPEAKER_02：经过进一步工程，它从 16%降低到 5%，这大约是人类的水平。所以五年前，如果你说多久之后我才能向你展示从网络上获取的任何图像，并且你能比识别图像中的明显物体做得更好，人们会说哦，你不可能在很长时间内对图像总体上做到这一点，但现在我们可以做到了。
270 00:25:23,269 --> 00:25:26,115 说话人 SPEAKER_02：这就是它获取的这类图像，以及它的五个猜测。
271 00:25:26,676 --> 00:25:28,460 说话人 SPEAKER_02：你看那里非常自信地认为是一只猎豹。
272 00:25:30,443 --> 00:25:34,751 说话人 SPEAKER_02：这里自信地认为是子弹列车，但其他的选择也很好。
273 00:25:35,133 --> 00:25:37,016 说话人 SPEAKER_02：注意，这张图片里还有很多其他东西。
274 00:25:37,537 --> 00:25:41,565 说话人 SPEAKER_02：但如果你问别人这是什么图片，他们会说是子弹列车，程序也是这样认为的。
275 00:25:43,434 --> 00:25:45,678 说话人 SPEAKER_02：这里它做错了。
276 00:25:45,758 --> 00:25:53,148 说话人 SPEAKER_02：它的第一次猜测是剪刀，从这一点可以看出它需要眼镜，因为它把链条看作是剪刀的刀片。
277 00:25:53,749 --> 00:25:54,931 说话人 SPEAKER_02：平底锅就更明显了。
278 00:25:55,751 --> 00:25:59,317 说话人 SPEAKER_02：但你可以看到它所有的猜测都是视觉上合理的事物。
279 00:25:59,356 --> 00:26:00,679 说话人 SPEAKER_02：它们在视觉上很相似。
280 00:26:01,339 --> 00:26:05,285 说话人 SPEAKER_02：所以实际上是看到那里有什么。
281 00:26:05,305 --> 00:26:11,433 说话人 SPEAKER_02：现在，最后一点，我要再谈一个应用，就是将其应用于自然语言。
282 00:26:11,920 --> 00:26:13,201 说话人 SPEAKER_02：这是谷歌感兴趣的方向。
283 00:26:14,022 --> 00:26:20,352 说话人 SPEAKER_02：我们将使用一些我不打算再次提及的人发明的一种极其复杂的循环网络来完成这项工作。
284 00:26:23,477 --> 00:26:26,942 说话人 SPEAKER_02：这是他们网络的简化版本，称为循环神经网络。
285 00:26:27,501 --> 00:26:31,567 说话人 SPEAKER_02：循环网络的想法是，在每个时间步，输入从底部进入。
286 00:26:31,768 --> 00:26:33,651 说话人 SPEAKER_02：我只展示了单个输入神经元，但会有很多。
287 00:26:35,113 --> 00:26:37,497 说话者 SPEAKER_02：实际上，进入底部的输入会到达所有隐藏神经元。
288 00:26:39,259 --> 00:26:41,201 说话者 SPEAKER_02：网络也可以给出输出。
289 00:26:42,480 --> 00:26:47,005 说话者 SPEAKER_02：隐藏神经元从其他隐藏神经元接收输入。
290 00:26:47,506 --> 00:26:48,646 说话者 SPEAKER_02：这就是它为什么是循环网络的原因。
291 00:26:49,248 --> 00:26:58,056 说话人 SPEAKER_02：随着时间的推移，任何时间切片中隐藏神经元的态由输入和所有隐藏神经元的先前态决定。
292 00:26:59,037 --> 00:27:07,724 说话人 SPEAKER_02：这些连接上的权重，那两个绿色权重和一个进入中间神经元的红色权重，在每个时间步长都是相同的，因为这是一个循环网络。
293 00:27:07,786 --> 00:27:10,327 说话人 SPEAKER_02：它在每个时间步长重复使用相同的权重。
294 00:27:11,724 --> 00:27:17,852 说话人 SPEAKER_02：因此，如果我们能够训练这种网络，它就能够接受一系列输入并产生一系列输出。
295 00:27:22,396 --> 00:27:26,442 说话人 SPEAKER_02：伊利亚斯·塔斯基瓦和他的两位同事有一个好主意。
296 00:27:26,501 --> 00:27:32,509 说话人 SPEAKER_02：他们说，为什么我们不放弃谷歌翻译的方式，即拥有一个巨大的短语表，这些短语映射到其他短语上。
297 00:27:34,030 --> 00:27:39,116 说话人 SPEAKER_02：那个五分钟标志的人得对我大喊，因为我没朝那个方向看。
298 00:27:40,665 --> 00:27:41,467 说话人 SPEAKER_02：好，她没有大喊。
299 00:27:45,031 --> 00:27:46,673 说话人 SPEAKER_02: 他说，我们就这样使用循环神经网络。
300 00:27:46,913 --> 00:27:47,694 说话人 SPEAKER_02: 让我们使用循环神经网络。
301 00:27:47,714 --> 00:27:48,736 说话人 SPEAKER_02: 我们输入英语单词。
302 00:27:49,897 --> 00:27:52,760 说话人 SPEAKER_02: 然后我们输入英语单词后，它就输出法语单词。
303 00:27:53,942 --> 00:27:55,365 说话人 SPEAKER_02：我们没有任何大型短语表。
304 00:27:56,025 --> 00:27:59,809 说话人 SPEAKER_02：我们只是通过展示翻译对来训练整个系统。
305 00:27:59,849 --> 00:28:01,271 说话人 SPEAKER_02：我们展示这串英语单词。
306 00:28:01,311 --> 00:28:02,834 说话人 SPEAKER_02：我希望你产生那串法语单词。
307 00:28:04,635 --> 00:28:05,998 说话人 SPEAKER_02：这样工作的方式是...
308 00:28:06,940 --> 00:28:13,866 说话人 SPEAKER_02：它有一个编码循环神经网络，每次处理一个英语单词，直到处理完整个英语句子。
309 00:28:13,886 --> 00:28:19,874 说话人 SPEAKER_02：然后它将这个英语单词转换成我们所说的词向量，这只是一个包含单词大量特征的集合。
310 00:28:21,395 --> 00:28:26,201 说话人 SPEAKER_02：因此，相似单词具有非常相似的特征向量，这使它能够进行泛化。
311 00:28:27,261 --> 00:28:30,265 说话人 SPEAKER_02：然后这些词向量被输入到隐藏单元中。
312 00:28:30,768 --> 00:28:33,851 说话人 SPEAKER_02：隐藏单元随着时间的推移在其状态中积累信息。
313 00:28:34,631 --> 00:28:39,296 说话人 SPEAKER_02：隐藏单元的最终状态积累了关于我们放入的所有单词的信息。
314 00:28:39,935 --> 00:28:41,277 说话人 SPEAKER_02：我们将这称为一个想法。
315 00:28:42,317 --> 00:28:45,461 说话人 SPEAKER_02：这不是一种，我是指的真的。
316 00:28:45,701 --> 00:28:46,801 说话人 SPEAKER_02：我的意思是这真是一个想法。
317 00:28:47,163 --> 00:28:48,683 说话人 SPEAKER_02：你以前从未见过想法。
318 00:28:48,743 --> 00:28:49,785 说话人 SPEAKER_02：这就是它的样子。
319 00:28:50,266 --> 00:28:52,508 说话人 SPEAKER_02：嗯，如果你把神经网络展开，它就是这个样子。
320 00:28:52,748 --> 00:28:58,512 说话人 SPEAKER_02：这将是在听到英语句子后，这个循环神经网络最终状态的活动向量。
321 00:28:59,557 --> 00:29:07,787 说话人 SPEAKER_02：然后我们翻译的做法是，我们将这个想法作为法语网络的初始状态。
322 00:29:07,807 --> 00:29:12,791 说话人 SPEAKER_02：然后法语网络接收到这个想法，会说，好吧，鉴于这个想法，我认为第一个词可能是什么？
323 00:29:13,593 --> 00:29:18,919 说话者 SPEAKER_02：嗯，40%是 le，30%是 la，10%是 cha。
324 00:29:20,300 --> 00:29:23,865 说话者 SPEAKER_02：我的法语单词用完了，但还有一些。
325 00:29:24,346 --> 00:29:26,407 说话者 SPEAKER_02：所以它会在法语单词上打赌。
326 00:29:26,859 --> 00:29:36,250 说话者 SPEAKER_02：当你训练它时，如果它真的说的是 le，你就说，好的，你说 le 40%，但你应该说 le 100%，因为这才是正确答案。
327 00:29:36,711 --> 00:29:38,452 说话人 SPEAKER_02：所以你要进行反向传播。
328 00:29:38,492 --> 00:29:41,997 说话人 SPEAKER_02：你要调整权重，使其更有可能给 "le" 分配更多权重。
329 00:29:44,579 --> 00:29:47,563 说话人 SPEAKER_02：然后你要看句子中的下一个单词。
330 00:29:48,223 --> 00:29:49,325 说话人 SPEAKER_02：哎呀，我总是犯这个错误。
331 00:29:49,705 --> 00:29:51,567 说话人 SPEAKER_02：我试图使用激光笔。
332 00:29:53,843 --> 00:30:00,834 说话人 SPEAKER_02：你看句子中的下一个单词，然后输入，并尝试输出法语句子的第二个单词。
333 00:30:00,854 --> 00:30:03,116 说话人 SPEAKER_02：所以第一个单词是 le。
334 00:30:04,378 --> 00:30:05,381 说话人 SPEAKER_02：它必须猜测 le。
335 00:30:05,701 --> 00:30:10,208 说话者 SPEAKER_02：在它未能猜出“le”之后，你现在告诉它“le”，你说，好的，第一个词实际上是“le”。
336 00:30:10,407 --> 00:30:11,470 说话者 SPEAKER_02：你认为第二个词是什么？
337 00:30:11,990 --> 00:30:13,011 说话者 SPEAKER_02：然后它会对此进行猜测。
338 00:30:13,613 --> 00:30:14,515 说话者 SPEAKER_02：然后你继续这样做。
339 00:30:15,256 --> 00:30:20,644 说话人 SPEAKER_02：调整所有权重，让它更好地下注。
340 00:30:20,894 --> 00:30:23,856 说话人 SPEAKER_02：训练完成后，你可以让它生成句子。
341 00:30:24,377 --> 00:30:25,278 说话人 SPEAKER_02：然后你给它一个想法。
342 00:30:26,160 --> 00:30:27,981 说话人 SPEAKER_02：它产生了一组首词分布。
343 00:30:28,442 --> 00:30:30,023 说话人 SPEAKER_02: 随机选择其中一个。
344 00:30:30,044 --> 00:30:31,365 说话人 SPEAKER_02: 有 40%的概率选择 a。
345 00:30:31,405 --> 00:30:32,707 说话人 SPEAKER_02: 有 30%的概率选择 a。
346 00:30:33,989 --> 00:30:39,394 说话人 SPEAKER_02: 无论你选择了哪一个，都将其作为第一个词输入，然后说，既然这是第一个词，你认为第二个词是什么？
347 00:30:40,296 --> 00:30:41,616 说话人 SPEAKER_02：这将为您提供一个分布。
348 00:30:41,656 --> 00:30:42,258 说话人 SPEAKER_02：您选择一个单词。
349 00:30:42,597 --> 00:30:43,700 说话人 SPEAKER_02：然后它会生成一个句子。
350 00:30:44,000 --> 00:30:46,281 说话人 SPEAKER_02：您继续输入，直到它选择一个句号。
351 00:30:46,303 --> 00:30:50,326 说话人 SPEAKER_02：如果你再运行一次，它可能会产生不同的句子，因为你可能会随机选择不同的单词。
352 00:30:50,964 --> 00:30:52,268 说话人 SPEAKER_02：好的，我们一会儿会看到它是怎么做到这一点的。
353 00:30:54,251 --> 00:30:57,778 说话人 SPEAKER_02：这就是它在训练后生成句子的方式。
354 00:31:00,123 --> 00:31:07,657 说话人 SPEAKER_02：令人印象深刻的是，这只需要大约一个人年，如果你不考虑投入基础设施的几千人年。
355 00:31:09,577 --> 00:31:13,863 说话人 SPEAKER_02：它的工作效果与标准翻译系统相当。
356 00:31:14,443 --> 00:31:22,653 说话人 SPEAKER_02：我们之前已经看到，如果你能让几个学生完成的一个深度学习系统的工作效果与标准技术相当，那么几年后，它将会变得更好。
357 00:31:24,195 --> 00:31:26,398 说话人 SPEAKER_02：更重要的是，我们只对一对语言进行了训练。
358 00:31:26,419 --> 00:31:30,644 说话人 SPEAKER_02：如果你对很多对语言进行训练，那么对于每一种语言，你都有一个编码器。
359 00:31:30,924 --> 00:31:32,346 说话人 SPEAKER_02：对于每一种语言，你都有一个解码器。
360 00:31:32,707 --> 00:31:35,349 说话人 SPEAKER_02：但它们都可以共享思想。
361 00:31:35,920 --> 00:31:40,285 说话人 SPEAKER_02：然后你可以给它一个荷兰语句子，它会翻译成 25 种欧洲语言。
362 00:31:41,026 --> 00:31:47,354 说话人 SPEAKER_02：你可以通过反向传播所有 25 种翻译来训练整个系统，以获得更多关于荷兰语句子含义的信息。
363 00:31:51,138 --> 00:31:56,005 说话人 SPEAKER_02：现在让我们将视觉和语言结合起来，然后我就完成了。
364 00:31:57,384 --> 00:32:18,803 说话人 SPEAKER_02：我们要做的是，我们将使用学会识别物体的网络，然后我们可以说，在它实际上说出物体之前，这个网络的最后一层，在它做出决定之前的隐藏单元，将真正描述图像中的内容，或者是一个活动向量，用物体而不是像素强度来描述图像中的内容。
365 00:32:18,782 --> 00:32:21,650 说话人 SPEAKER_02：在输入时，是像素强度告诉你那里有什么。
366 00:32:21,670 --> 00:32:24,556 说话人 SPEAKER_02：但是当你到达最后一层时，它对各种可能性都有所猜测。
367 00:32:24,576 --> 00:32:25,959 说话人 SPEAKER_02：可能是这个物体，也可能是那个物体。
368 00:32:26,681 --> 00:32:28,625 说话人 SPEAKER_02：这就是最后一层所编码的内容。
369 00:32:29,807 --> 00:32:32,775 说话人 SPEAKER_02：所以现在我们要做的是，将最后一层定义为感知。
370 00:32:34,830 --> 00:32:41,278 说话人 SPEAKER_02：能够将我在心理学学生时代学到的所有术语真正运用到实际中，而不是仅仅谈论它们，这真是太好了。
371 00:32:41,617 --> 00:32:42,439 说话人 SPEAKER_02：这是一个感知。
372 00:32:43,279 --> 00:32:51,548 说话人 SPEAKER_02：如果我们把感知映射到一个矩阵中，得到一个想法，然后将其输入到解码网络中。
373 00:32:52,229 --> 00:32:55,071 说话人 SPEAKER_02：现在解码网络可以告诉我们图像中的内容。
374 00:32:55,992 --> 00:33:00,518 说话人 SPEAKER_02：所以想法是，你使用能够识别物体的网络，
375 00:33:01,240 --> 00:33:02,961 说话人 SPEAKER_02：你取它的最后一个隐藏层。
376 00:33:03,742 --> 00:33:06,166 说话人 SPEAKER_02：然后通过另一组权重映射，得到一个想法。
377 00:33:06,887 --> 00:33:10,673 说话人 SPEAKER_02：这个想法进入解码网络，它说出这个想法。
378 00:33:11,212 --> 00:33:12,494 说话人 SPEAKER_02：也就是说，把想法转换成语言。
379 00:33:14,397 --> 00:33:20,164 说话者 SPEAKER_02：通过展示有人告诉你的图像标题来训练它，它学会生成标题。
380 00:33:20,204 --> 00:33:24,230 说话者 SPEAKER_02：然后你给它从未见过的图像，看看它生成的标题。
381 00:33:25,451 --> 00:33:27,535 说话者 SPEAKER_02：所以这是一张它以前从未见过的图像。
382 00:33:28,509 --> 00:33:33,615 说话者 SPEAKER_02：在数据库中，正确的标题是，人们在露天市场蹲着。
383 00:33:34,436 --> 00:33:38,181 说话人 SPEAKER_02：神经网络说的是一群人在户外市场购物。
384 00:33:41,224 --> 00:33:41,805 说话人 SPEAKER_02：这是另一个例子。
385 00:33:44,788 --> 00:33:54,700 说话人 SPEAKER_02：真实的标题是一个小女孩在沙发上抱着一个毛绒玩具入睡，这显然比神经网络说的要好，但神经网络说的是一个孩子紧握着毛绒玩具的特写，这也不算坏。
386 00:33:55,119 --> 00:33:58,263 说话人 SPEAKER_02：如果你是计算机视觉专家，你不会预测我们现在能做这件事。
387 00:34:02,210 --> 00:34:06,916 说话者 SPEAKER_02：这对谷歌来说有很多影响，尤其是对文档处理。
388 00:34:06,936 --> 00:34:11,563 说话者 SPEAKER_02：如果我们能将一句话转化为一个想法，那么文档就是一个想法的序列。
389 00:34:13,105 --> 00:34:16,269 说话者 SPEAKER_02：现在我们可以将神经网络应用于模拟这个想法序列。
390 00:34:16,449 --> 00:34:19,474 说话者 SPEAKER_02：我们可以将每个想法作为输入，并预测下一个想法。
391 00:34:20,255 --> 00:34:21,336 说话人 SPEAKER_02：这是自然推理。
392 00:34:21,577 --> 00:34:23,559 说话人 SPEAKER_02：这是人工智能永远做不到的。
393 00:34:23,978 --> 00:34:26,802 说话人 SPEAKER_02：我总是尝试用不同的方式来做，但它就是做不到。
394 00:34:27,302 --> 00:34:35,074 说话人 SPEAKER_02：现在，如果你从网络上获取信息并发现其中的推理方式是自然推理，但并不很好。
395 00:34:36,697 --> 00:34:37,478 说话人 SPEAKER_02：这就是我所怀疑的。
396 00:34:39,320 --> 00:34:43,086 说话人 SPEAKER_02：但至少这样，我们可以让计算机理解文档的内容。
397 00:34:44,369 --> 00:34:49,175 说话人 SPEAKER_02：如果你能这样做，那么谷歌应该能够给你提供更好的查询答案。
398 00:34:49,882 --> 00:35:00,938 说话人 SPEAKER_02：你应该能够说出类似这样的话，找到一份支持斯蒂芬·哈珀并且假装支持科学的文档，但实际上却深恶痛绝。
399 00:35:06,786 --> 00:35:11,072 说话人 SPEAKER_02：如果你还想说这样的话，实际上已经没有必要了。
400 00:35:12,594 --> 00:35:14,757 说话人 SPEAKER_02：为了达到人类的推理水平，
401 00:35:15,143 --> 00:35:21,050 说话人 SPEAKER_02：似乎我们将在神经网络中需要人类规模的权重。
402 00:35:21,550 --> 00:35:23,492 说话人 SPEAKER_02：我们大约有 10 的 14 次方这样的权重。
403 00:35:24,373 --> 00:35:28,518 说话人 SPEAKER_02：那是 100 万亿。
404 00:35:28,539 --> 00:35:33,284 说话人 SPEAKER_02：而你看到的进行这种翻译的神经网络只有大约 1 亿。
405 00:35:35,146 --> 00:35:38,090 说话人 SPEAKER_02：所以目前我们的网络规模小得多，好几个数量级。
406 00:35:38,630 --> 00:35:44,036 说话人 SPEAKER_02：这也可能解释了为什么我们的网络只理解了它们所说的一半。
407 00:35:46,244 --> 00:35:50,969 说话人 SPEAKER_02：为了做好这项工作，我们需要更大的网络，但我们能做到。
408 00:35:54,032 --> 00:36:00,221 说话人 SPEAKER_02：最后两张幻灯片是关于认知科学的，它们涵盖了丹尼特将要讨论的革命。
409 00:36:00,260 --> 00:36:10,733 说话人 SPEAKER_02：多年来，人工智能一直受制于这样一个观点，即内部表示是符号表达式，我们通过推理规则来操作它们。
410 00:36:12,603 --> 00:36:16,128 说话人 SPEAKER_02：在人工智能领域，大多数人并没有认为有其他替代方案。
411 00:36:16,608 --> 00:36:18,152 说话人 SPEAKER_02：他们没有想过有一个假设。
412 00:36:18,251 --> 00:36:23,960 说话人 SPEAKER_02：像纽厄尔和西蒙这样的人说，这是物理符号，符号假设。
413 00:36:24,481 --> 00:36:27,065 说话人 SPEAKER_02：但大多数人工智能研究者认为，这只能是这样。
414 00:36:28,047 --> 00:36:36,179 说话人 SPEAKER_02：这是因为他们唯一可以用来从一些句子推导出其他真实句子的模型就是形式逻辑。
415 00:36:36,717 --> 00:36:38,440 说话人 SPEAKER_02：这是一个能做这件事的系统。
416 00:36:38,460 --> 00:36:43,248 说话人 SPEAKER_02：同样，对于大多数物理学家来说，要让事物传播的唯一方式就是存在一个介质。
417 00:36:44,070 --> 00:36:51,121 说话人 SPEAKER_02：所以在物理学的一个阶段，当人们认为光波通过以太不是一种假设时，他们认为没有其他选择。
418 00:36:51,161 --> 00:36:51,902 说话人 SPEAKER_02：必须存在以太。
419 00:36:51,922 --> 00:36:52,503 说话者 SPEAKER_02：让我们找到它。
420 00:36:53,786 --> 00:36:55,409 说话者 SPEAKER_02：当没有的时候，他们非常沮丧。
421 00:36:57,411 --> 00:37:00,818 说话者 SPEAKER_02：在传统的 AI 中，人们希望现在非常沮丧。
422 00:37:03,380 --> 00:37:08,844 说话者 SPEAKER_02：特别是，在传统 AI 中的人们认为类比推理是一种二等公民。
423 00:37:08,885 --> 00:37:10,047 说话人 SPEAKER_02：这件事我们稍后再说。
424 00:37:10,067 --> 00:37:11,387 说话人 SPEAKER_02：那并不是推理的本质。
425 00:37:11,608 --> 00:37:14,050 说话人 SPEAKER_02：推理的本质是恰当、正确、规范的推理。
426 00:37:14,851 --> 00:37:17,094 说话人 SPEAKER_02：实际上，我们知道对于人类来说正好相反。
427 00:37:17,833 --> 00:37:23,300 说话者 SPEAKER_02：事实上，如果你看我的论点，我反对传统人工智能的论点并不是逻辑推理的一部分。
428 00:37:23,559 --> 00:37:24,940 说话者 SPEAKER_02：它是一种与物理学的类比。
429 00:37:28,864 --> 00:37:32,509 说话者 SPEAKER_02：认知科学的另一个范例来自生物学。
430 00:37:33,063 --> 00:37:36,467 说话者 SPEAKER_02：它说，看，我们是生物体。
431 00:37:36,547 --> 00:37:40,112 说话人 SPEAKER_02：我们的大脑进化出了像视觉和运动控制这样的功能。
432 00:37:40,612 --> 00:37:41,994 说话人 SPEAKER_02：它们并没有进化出做逻辑的能力。
433 00:37:42,293 --> 00:37:44,617 说话人 SPEAKER_02：实际上，它们也没有进化出处理自然语言的能力。
434 00:37:44,657 --> 00:37:46,438 说话人 SPEAKER_02：它们进化出了其他功能。
435 00:37:46,780 --> 00:37:49,001 说话人 SPEAKER_02：它们在处理这些其他事情方面高度优化。
436 00:37:49,382 --> 00:37:52,306 说话人 SPEAKER_02：然后我们在它们之上添加了所有这些高级功能。
437 00:37:52,326 --> 00:37:53,788 说话人 SPEAKER_02：它们在这方面做得不太好，但它们可以做到。
438 00:37:53,807 --> 00:37:54,889 说话人 SPEAKER_02：这样做非常重要。
439 00:37:56,190 --> 00:37:59,474 说话者 SPEAKER_02：符号处理是在这个其他幻象之上进行的。
440 00:38:00,788 --> 00:38:03,490 说话者 SPEAKER_02：并不是通过在头脑中拥有符号来完成的。
441 00:38:04,152 --> 00:38:06,614 说话者 SPEAKER_02：进行符号处理不需要在头脑中拥有符号。
442 00:38:07,335 --> 00:38:10,719 说话者 SPEAKER_02：同样地，进行像素处理也不需要在头脑中拥有像素。
443 00:38:11,099 --> 00:38:12,981 说话人 SPEAKER_02：你看一张图片，处理那些像素。
444 00:38:13,943 --> 00:38:21,472 说话人 SPEAKER_02：但是有人会说，你必须通过在脑海中打乱像素来完成这个任务，比如史蒂夫·科斯林，这样的人简直就是愚蠢的。
445 00:38:22,293 --> 00:38:23,954 说话人 SPEAKER_02：哦，对不起，史蒂夫。
446 00:38:23,974 --> 00:38:25,496 说话人 SPEAKER_02：这是不正确的。
447 00:38:28,193 --> 00:38:33,724 说话人 SPEAKER_02：我们处理，你只能在我们这里找到符号，那就是输入和输出。
448 00:38:33,744 --> 00:38:35,547 说话人 SPEAKER_02：里面，全是活动的巨大向量。
449 00:38:35,728 --> 00:38:36,429 说话人 SPEAKER_02：这就是全部。
450 00:38:37,351 --> 00:38:39,315 说话人 SPEAKER_02：那些活动的巨大向量就是思想。
451 00:38:39,514 --> 00:38:40,376 说话人 SPEAKER_02：这就是思想。
452 00:38:41,900 --> 00:38:42,320 说话人 SPEAKER_02：我完成了。
453 00:38:53,405 --> 00:38:59,871 说话人 SPEAKER_04：我被告知要对你的时间非常严格，但你正好准时到达，所以做得好。
454 00:39:00,172 --> 00:39:06,099 说话人 SPEAKER_04：所以我想谈谈这项研究的一些更广泛的考虑和影响。
455 00:39:06,119 --> 00:39:15,849 说话者 SPEAKER_04：我想先问一个私人问题，那就是深度学习和神经网络方法在过去曾经一度陷入低谷。
456 00:39:16,250 --> 00:39:26,061 说话者 SPEAKER_04：那么，是什么让你如此确信自己走的是正确的道路，尽管当时有很大一部分人工智能界的人并不这么认为？
457 00:39:26,934 --> 00:39:29,498 说话者 SPEAKER_02：这显然是正确的做法。
458 00:39:30,039 --> 00:39:31,242 说话者 SPEAKER_02：我的意思是，大脑就是这样做的，对吧？
459 00:39:31,262 --> 00:39:33,344 说话者 SPEAKER_02：大脑并不是通过有人编程来做到这一点的。
460 00:39:34,106 --> 00:39:37,891 说话者 SPEAKER_02：我的意思是，真正的解剖学家认为进化编程了大脑，这就是它运作的方式。
461 00:39:38,333 --> 00:39:40,275 说话者 SPEAKER_02：但这太疯狂了。
462 00:39:42,920 --> 00:39:46,184 说话者 SPEAKER_02：所以很明显，大脑是以某种方式做到这一点的，而且它是通过学习来做到这一点的。
463 00:39:47,306 --> 00:39:51,052 说话者 SPEAKER_02：很明显，逻辑是一种很晚才出现的东西。
464 00:39:51,168 --> 00:39:59,956 说话者 SPEAKER_02：实际上，如果他们试图用这些方法进行逻辑推理却无法进行运动控制和感知，那么它们就无法解释大脑中大部分发生的事情。
465 00:40:00,918 --> 00:40:03,481 说话者 SPEAKER_02：所以在我看来，似乎从来就没有其他选择。
466 00:40:03,561 --> 00:40:03,742 说话者 SPEAKER_04：没错。
467 00:40:04,302 --> 00:40:08,427 演讲者 SPEAKER_04：您刚才提到了自然语言处理是这项技术的应用之一。
468 00:40:08,786 --> 00:40:18,998 演讲者 SPEAKER_04：我想了解一下，能否在这方面进一步展开，从实际的角度来谈谈，如果我们拥有更高级的自然语言处理技术，这将对与我们周围计算机的关系意味着什么？
469 00:40:19,889 --> 00:40:29,797 演讲者 SPEAKER_02：好的，我认为我们都希望能有一个真正聪明的个人助理，
470 00:40:29,980 --> 00:40:38,849 演讲者 SPEAKER_02：但工资很低，真正了解我们的一切，但从不挑剔，除非真的有必要。
471 00:40:39,630 --> 00:40:45,175 说话人 SPEAKER_02：我认为那个梦想不再是梦想了。
472 00:40:45,215 --> 00:40:48,778 说话人 SPEAKER_02：我认为我们能够制造出比 Siri 等更好的东西。
473 00:40:48,798 --> 00:40:54,925 说话人 SPEAKER_02：真正理解对话中发生的事情，真正理解你的意思，并能处理新情况的东西。
474 00:40:54,905 --> 00:40:56,548 说话人 SPEAKER_02：我不知道这要花多长时间。
475 00:40:56,588 --> 00:40:57,490 说话人 SPEAKER_02：可能要 20 年。
476 00:40:57,811 --> 00:40:58,612 说话人 SPEAKER_02：可能要五年。
477 00:40:59,375 --> 00:41:02,681 说话人 SPEAKER_02：但我认为，随着深度学习的发展，我们将能够实现类似的功能。
478 00:41:03,704 --> 00:41:09,436 说话人 SPEAKER_02：因此，我认为计算机将极大地增强我们的智力，并从生活中去除所有无聊的部分。
479 00:41:10,159 --> 00:41:18,068 说话者 SPEAKER_04：你论证人类思维是一种复杂的神经网络活动模式，而不是像你之前讨论的那样，是一种推理的顺序。
480 00:41:18,530 --> 00:41:25,036 说话者 SPEAKER_04：那么你的工作对我们人类来说，关于心理语言和思维是如何工作的有什么建议？
481 00:41:25,498 --> 00:41:25,757 说话者 SPEAKER_02：好的。
482 00:41:25,898 --> 00:41:29,461 说话者 SPEAKER_02：我很高兴你问了这个问题，因为你知道，我还有几页幻灯片。
483 00:41:30,903 --> 00:41:31,445 说话人 SPEAKER_02：我知道。
484 00:41:31,784 --> 00:41:32,626 说话人 SPEAKER_02：学术界就是这样。
485 00:41:32,666 --> 00:41:34,288 说话人 SPEAKER_02：他们从不放过任何增加幻灯片的机会。
486 00:41:35,769 --> 00:41:37,871 说话人 SPEAKER_02：请问我可以拿上额外的幻灯片吗？
487 00:41:38,257 --> 00:41:51,295 说话者 SPEAKER_02：好吧，我认为，丹尼特也同意我的看法，这让我感到非常欣慰，因为我不是哲学家，而他是，因为人们被误解了心理语言的工作方式所误导。
488 00:41:52,036 --> 00:41:56,483 说话者 SPEAKER_02：所以大多数人认为心理状态是某种奇怪、神秘的东西，存在于你的脑海中。
489 00:41:57,605 --> 00:42:01,030 说话者 SPEAKER_02：我想论证这根本不是语言工作的方式。
490 00:42:01,831 --> 00:42:05,817 说话者 SPEAKER_02：摆脱这个错误的一个非常简单的方法就是这么说，
491 00:42:06,860 --> 00:42:09,523 说话人 SPEAKER_02：我们要提到我们大脑里发生的事情，对吧？
492 00:42:09,623 --> 00:42:14,990 说话人 SPEAKER_02：我想让你知道，在我的大脑里，我非常想打你，你最好表现得更好。
493 00:42:15,831 --> 00:42:17,614 说话人 SPEAKER_02：并且我想把这个信息传达给你。
494 00:42:18,175 --> 00:42:24,364 说话人 SPEAKER_02：而且我说神经元 52 非常活跃是没有用的，因为这对你们来说没有任何意义。
495 00:42:25,065 --> 00:42:27,407 说话者 SPEAKER_02：即使我把整个活动向量都给你，对你来说也没有任何意义。
496 00:42:28,809 --> 00:42:36,579 说话者 SPEAKER_02：但给你一个好的东西是，我可以描述这个活动向量的正常后果。
497 00:42:37,724 --> 00:42:49,193 说话者 SPEAKER_02：同样，对于感觉，我可以描述为了让我有这种感觉活动向量，世界上必须有什么。
498 00:42:49,213 --> 00:42:57,041 说话者 SPEAKER_02：所以，当我说我有红色的感觉时，我并不是指我头脑中的某物。
499 00:42:58,101 --> 00:43:01,204 说话者 SPEAKER_02：几乎每个人都认为你在指你头脑中的某个东西，某种内在的东西。
500 00:43:02,126 --> 00:43:05,688 说话者 SPEAKER_02：但如果你看“红色”这个词，红色指的是世界上的事物。
501 00:43:05,786 --> 00:43:07,168 说话者 SPEAKER_02：它不指任何东西。
502 00:43:07,228 --> 00:43:09,170 说话者 SPEAKER_02：它指的是世界上物体的颜色。
503 00:43:10,471 --> 00:43:14,514 说话人 SPEAKER_02：所以你用来描述感觉的语言是描述世界上事物的语言。
504 00:43:15,135 --> 00:43:17,338 说话人 SPEAKER_02：正在发生的事情是我们正在使用一个技巧。
505 00:43:17,777 --> 00:43:22,101 说话人 SPEAKER_02：我们想要指代这些大脑模式，但我们不能直接指代大脑模式。
506 00:43:22,702 --> 00:43:24,905 说话人 SPEAKER_02：所以我们通过它们的正常原因来指代它们。
507 00:43:25,485 --> 00:43:28,869 说话者 SPEAKER_02：所以我告诉你们，如果我正在看红色东西的话，我会有这样的脑部模式。
508 00:43:29,849 --> 00:43:32,132 所以当有人说，我看到粉红色的象，
509 00:43:33,394 --> 00:43:35,318 他们并不是说我的脑袋里有粉红色的象。
510 00:43:35,717 --> 00:43:41,105 他们意思是，如果在这个真实世界中真的有粉红色的象，那么我脑袋里的东西就是感知。
511 00:43:42,065 --> 00:43:44,730 说话者 SPEAKER_02：我找到了适合那种情况的脑部模式。
512 00:43:44,750 --> 00:43:49,976 说话者 SPEAKER_02：同样，对于语言来说，感觉语言和情感语言是一样的。
513 00:43:52,539 --> 00:43:59,148 说话者 SPEAKER_02：当我说我想要打 Jerry 时，我的意思是，在我脑海中有一个模式，如果我不加以自我控制，就会导致我打 Jerry。
514 00:44:01,289 --> 00:44:04,331 说话者 SPEAKER_02：所以这里的要点是，心理状态并不是内部状态。
515 00:44:05,032 --> 00:44:17,666 说话者 SPEAKER_02：这是一个假设的外部状态，用于通过正常因果关系或双向因果关系来指代大脑中的情况，即这种状态导致大脑状态，或者大脑状态导致这种状态。
516 00:44:18,346 --> 00:44:22,351 说话者 SPEAKER_02：有一个地方我们可以在两个方向上获得因果关系，那就是思想。
517 00:44:23,532 --> 00:44:25,132 说话者 SPEAKER_02：所以这里是真正的思想语言。
518 00:44:25,373 --> 00:44:28,697 说话者 SPEAKER_02：这是单词“思想”的语言是如何工作的。
519 00:44:29,469 --> 00:44:34,016 说话者 SPEAKER_02：如果我这么说，我可以把“约翰认为”这句话说出来，然后把我喜欢的任何东西都加上引号，这就是约翰的想法。
520 00:44:34,677 --> 00:44:37,179 说话者 SPEAKER_02：但这并不意味着引号里的东西就在约翰的脑子里。
521 00:44:37,860 --> 00:44:41,746 说话者 SPEAKER_02：这并不是像 AI 研究者过去认为的那样，在他脑子里的一串符号。
522 00:44:41,766 --> 00:44:46,072 说话者 SPEAKER_02：我这么说“约翰认为”，我们该去哪里吃午饭呢？
523 00:44:46,992 --> 00:44:53,942 说话人 SPEAKER_02：我的意思是，约翰的大脑中存在一种通常情况下是合适的神经活动模式，这种模式通常是由别人说“我们去哪里吃午饭？”所引起的
524 00:44:54,141 --> 00:44:56,003 说话人 SPEAKER_02：也会是他说“我们去哪里吃午饭？”的正常原因
525 00:44:56,884 --> 00:45:00,467 说话人 SPEAKER_02：而且这也是他通常说“我们去哪里吃午饭？”的原因
526 00:45:01,407 --> 00:45:04,170 说话人 SPEAKER_02：你看，我们有输入音频和输出音频
527 00:45:04,271 --> 00:45:10,396 说话者 SPEAKER_02：所以，思想可以通过它们的正常原因和它们会导致什么来描述。
528 00:45:10,757 --> 00:45:16,282 说话者 SPEAKER_02：如果你和某人共享一种语言，那么这就是思想。
529 00:45:17,643 --> 00:45:23,288 说话者 SPEAKER_02：它们是你头脑中的模式，是你头脑中的大活动因素，我通过描述它们会导致什么或它们会导致什么来描述它们。
530 00:45:23,807 --> 00:45:26,871 说话者 SPEAKER_02：通过说出它们是由什么引起的或它们会导致什么。
531 00:45:26,891 --> 00:45:29,195 说话人 SPEAKER_02：因为用神经元的活动来描述它们是没有意义的。
532 00:45:29,215 --> 00:45:30,978 说话人 SPEAKER_02：这对任何人都没有任何用处。
533 00:45:30,998 --> 00:45:31,619 说话人 SPEAKER_02：好的，感谢您的分享。
534 00:45:31,719 --> 00:45:32,519 说话人 SPEAKER_02：抱歉，有点长。
535 00:45:32,559 --> 00:45:32,780 说话人 SPEAKER_04: 好吧。
536 00:45:33,240 --> 00:45:36,686 说话人 SPEAKER_04: 好的，那么这就是关于思想的情况。
537 00:45:37,126 --> 00:45:39,550 说话人 SPEAKER_04: 但是我们可以谈谈意识吗？
538 00:45:39,570 --> 00:45:41,994 说话人 SPEAKER_04: 我意识到这有点模糊，但我们还是可以。
539 00:45:42,014 --> 00:45:45,057 说话人 SPEAKER_04：我知道心灵哲学家们讨论过这种关于意识的“硬问题”的观点。
540 00:45:45,137 --> 00:45:45,458 说话人 SPEAKER_00：是的，他们确实讨论过。
541 00:45:45,498 --> 00:45:52,228 说话人 SPEAKER_04：这个问题是这样的，即大脑中的这些活动模式最终变成类似感觉的过程。
542 00:45:52,445 --> 00:45:56,289 说话人 SPEAKER_04：就像站在舞台上，站在杰弗里·辛顿面前，面对一群人的感觉一样。
543 00:45:56,771 --> 00:46:00,596 说话者 SPEAKER_04：那么你的研究对此有何建议？
544 00:46:01,056 --> 00:46:08,485 说话者 SPEAKER_02：好的，我刚才放上的幻灯片表明，这只是一个愚蠢的错误，源于对自然语言工作原理的不理解。
545 00:46:09,067 --> 00:46:11,550 说话者 SPEAKER_02：这是一个典型的维特根斯坦式解决方案。
546 00:46:11,590 --> 00:46:18,840 说话者 SPEAKER_02：它说，如果你观察自然语言的实际工作方式，这些哲学问题就会消失。
547 00:46:19,192 --> 00:46:23,135 说话人 SPEAKER_02：我很高兴地说，我把这个想法告诉了丹尼特，我以为他会批评我。
548 00:46:23,735 --> 00:46:24,797 说话人 SPEAKER_02：他说，不，我同意。
549 00:46:26,978 --> 00:46:32,724 说话人 SPEAKER_02：他在维特根斯坦周围在牛津受过语言学哲学训练，这可能就是部分原因。
550 00:46:32,744 --> 00:46:35,706 说话人 SPEAKER_02：但大多数人认为感觉是在他们头脑中的东西。
551 00:46:36,407 --> 00:46:39,710 说话者 SPEAKER_02：哲学家们通过说存在“质料”来谈论这一点。
552 00:46:39,731 --> 00:46:42,594 说话者 SPEAKER_02：而“质料”就是那些你头脑中的神奇事物。
553 00:46:42,614 --> 00:46:46,396 说话者 SPEAKER_02：这些都是科学无法触及的神秘事物。
554 00:46:47,068 --> 00:46:48,811 说话者 SPEAKER_02：并没有什么神秘事物。
555 00:46:48,831 --> 00:46:54,780 说话者 SPEAKER_02：外面有红色的事物，在我的脑海中，有代表这些事物是红色的活动模式，没有其他。
556 00:46:55,983 --> 00:46:58,786 说话者 SPEAKER_04：但为什么我们会感到“我”的存在感呢？
557 00:46:59,126 --> 00:46:59,969 说话人 SPEAKER_04: 这是从哪里来的？
558 00:47:00,429 --> 00:47:02,632 说话人 SPEAKER_02: 哎，关于个人身份还有另一件事。
559 00:47:02,693 --> 00:47:03,574 说话人 SPEAKER_02: 我不想涉及那个。
560 00:47:03,594 --> 00:47:04,416 说话人 SPEAKER_02: 那里有很多东西。
561 00:47:04,436 --> 00:47:04,735 说话人 SPEAKER_02：公平。
562 00:47:04,775 --> 00:47:09,422 说话人 SPEAKER_02：但如果我们只是在谈论感知，我认为再也没有什么神秘的了。
563 00:47:09,443 --> 00:47:09,643 说话人 SPEAKER_02：对。
564 00:47:09,704 --> 00:47:11,166 说话人 SPEAKER_02：没有什么神秘的。
565 00:47:11,525 --> 00:47:14,510 说话者 SPEAKER_02：注意，哲学家们经常说，嗯，
566 00:47:14,777 --> 00:47:21,367 说话者 SPEAKER_02：如果我看到红色的事物，我得到的质料和当你看到绿色的事物时你得到的质料一样？
567 00:47:22,369 --> 00:47:24,572 说话者 SPEAKER_02：我得到的感受和你看到绿色事物时得到的感受一样。
568 00:47:25,333 --> 00:47:29,579 说话者 SPEAKER_02：如果你考虑一下语言是如何工作的，这在逻辑上是行不通的。
569 00:47:30,239 --> 00:47:32,603 说话者 SPEAKER_02：它不仅没有发生，这在逻辑上是不可能的。
570 00:47:33,204 --> 00:47:38,271 说话者 SPEAKER_02：因为我所说的红色感觉，就是我通常看红色事物时所得到的感觉。
571 00:47:38,891 --> 00:47:45,266 说话者 SPEAKER_02：所以，只要我不是色盲，我就不可能在看到红色事物时产生绿色感觉，因为这就是语言的工作方式。
572 00:47:46,909 --> 00:47:49,797 说话者 SPEAKER_02：所以这些哲学问题就突然消失了。
573 00:47:51,210 --> 00:48:03,478 说话者 SPEAKER_04：因此，让我们稍微回到一些实际应用的讨论，深度学习正在关注的一些问题包括语音识别、图像识别和自然语言处理。
574 00:48:03,498 --> 00:48:09,472 说话者 SPEAKER_04：假设这一领域持续改进，那么下一个目标是什么？
575 00:48:10,786 --> 00:48:15,291 说话者 SPEAKER_02：显然，自动驾驶汽车等将是其中之一，它们已经运作得相当不错。
576 00:48:15,972 --> 00:48:17,673 说话者 SPEAKER_02：以及类似的东西都是相当明显的。
577 00:48:18,253 --> 00:48:24,621 说话人 SPEAKER_02：我认为真正熟练和专注的个人助理将产生巨大的影响。
578 00:48:24,641 --> 00:48:29,467 说话人 SPEAKER_02：晚餐时有人谈论，什么将会像互联网一样具有颠覆性？
579 00:48:30,387 --> 00:48:32,230 说话人 SPEAKER_02：我认为像这样的事情将会具有颠覆性。
580 00:48:32,931 --> 00:48:38,396 说话人 SPEAKER_02：如果我有一个无所不知、无所不记的个人助理，
581 00:48:40,840 --> 00:48:46,539 说话人 SPEAKER_02：我差点想说她会记住的，但我的私人助理会记住。
582 00:48:50,193 --> 00:48:52,842 说话人 SPEAKER_02：这将对所有人的生活产生巨大影响。
583 00:48:53,210 --> 00:49:10,644 说话人 SPEAKER_02：当然，所有这些都有被滥用的风险，我们都知道我们生产的任何东西，国家安全局都会用它来监视我们，美国军方会用它来入侵小国，而不会造成任何美国人伤亡，诸如此类的事情。
584 00:49:13,351 --> 00:49:13,952 说话人 SPEAKER_02：所以
585 00:49:15,231 --> 00:49:19,878 说话人 SPEAKER_02：可以说我们不应该开发这项技术，因为人们会把它用于坏事。
586 00:49:20,719 --> 00:49:21,500 说话人 SPEAKER_02：我是一个乐观主义者。
587 00:49:21,559 --> 00:49:25,864 说话人 SPEAKER_02：我更希望我们能利用这项技术的优点多于缺点，但也会有一些缺点。
588 00:49:26,344 --> 00:49:32,652 说话人 SPEAKER_02：对此我们除了通过政治活动努力让人们合理使用这项技术外，别无他法。
近期，我们看到了一些在科学界具有影响力的个人在谈论机器人（尤其是）以及人工智能（更普遍地）的危险
590 00:49:45,454 --> 00:49:46,317 说话者 SPEAKER_04：遥远的未来。
您提到的内容似乎是一段视频或音频的转录文本。以下是该段落的简体中文翻译： 591 00:49:46,416 --> 00:49:50,306 说话者 SPEAKER_04：这是否让您感到担忧？您认为我们是否需要围绕这一点建立某种伦理框架？
592 00:49:51,108 --> 00:49:54,135 发言人 SPEAKER_02：是的，很难预见遥远的未来。
593 00:49:54,155 --> 00:49:56,380 说话人 SPEAKER_02：就像透过浓雾看得很远，不起作用。
594 00:49:58,967 --> 00:50:03,818 说话人 SPEAKER_02：但我认为在一百年后，这是完全合理的想法，
595 00:50:04,117 --> 00:50:09,164 说话人 SPEAKER_02：技术可能已经发展得如此之快，以至于我们可以建造比人更好的东西。
596 00:50:09,684 --> 00:50:12,367 说话人 SPEAKER_02：他们读遍了世界上所有的书，他们比我们理解得多得多。
597 00:50:12,987 --> 00:50:16,833 说话人 SPEAKER_02：我们被这些智能机器取代的所有这些恐怖情景。
598 00:50:17,592 --> 00:50:20,657 说话人 SPEAKER_02：在那个时间尺度上，谁知道呢？
599 00:50:21,398 --> 00:50:25,141 说话人 SPEAKER_02：我认为在短期内这是非常不可能的。
600 00:50:25,161 --> 00:50:26,824 说话人 SPEAKER_02：我认为在 10 年内不会发生这种情况。
601 00:50:28,425 --> 00:50:30,467 说话人 SPEAKER_04: 大约在 2000 年，抱歉，请继续。
602 00:50:30,487 --> 00:50:32,050 说话人 SPEAKER_02: 关于那件事，还有一点要说。
603 00:50:32,400 --> 00:50:40,081 说话人 SPEAKER_02: 我认为国家安全局已经做的事情几乎和那种情景一样可怕。
604 00:50:41,023 --> 00:50:48,503 说话人 SPEAKER_02: 因此，借用一位声誉不佳的哲学家的说法，可怕的事情已经发生了。
大约在2006年，我们开始看到深度学习的大幅进步，尤其是部分原因在于这些非常大的神经网络能够比以前运行得更快。
所以，您是否看到了另一个技术障碍，我们必须克服才能达到下一个水平，或者是什么难题？
607 00:51:11,483 --> 00:51:12,923 发言人 SPEAKER_04：您正在处理的问题。
608 00:51:13,585 --> 00:51:21,152 发言人 SPEAKER_02：所以目前有趣的是，看起来我们可能正处于正常科学的一个阶段，一切照旧将会取得很大进展。
609 00:51:22,173 --> 00:51:40,512 说话人 SPEAKER_02：实际上，我不知道为什么通过获取，你知道的，假设计算机行业可以持续生产更好的硬件，并且持续进行计算消耗更少的能量，如果他们可以在接下来的 10 年或 20 年内继续按照摩尔定律这样做，我认为常规业务将使我们走得很远。
610 00:51:41,081 --> 00:51:44,597 说话人 SPEAKER_02：显然，如果我们取得大的突破，大的概念性突破，这将使我们走得更远。
611 00:51:45,340 --> 00:51:48,914 说话人 SPEAKER_02：我认为一个大的突破将会到来，那就是我们将理解大脑。
612 00:51:49,688 --> 00:52:05,304 说话人 SPEAKER_02：实际上，这只是一个赌注，我个人的信念是，我们将足够接近于了解大脑真正是如何做到这些事情的，突然之间，一切都开始变得清晰，我们似乎进入了一个最低点，那里做这些事情的方法变得显而易见。
613 00:52:05,324 --> 00:52:06,804 说话人 SPEAKER_02：我认为我们可能已经非常接近了。
614 00:52:07,266 --> 00:52:15,813 说话人 SPEAKER_02：这将是一场革命，因为它将影响各种事物，比如教育和我们对自己的认识。
615 00:52:16,452 --> 00:52:19,556 说话人 SPEAKER_02：我认为这将非常令人兴奋。
616 00:52:19,576 --> 00:52:20,797 说话人 SPEAKER_02：我希望我能看到那一天。
617 00:52:21,057 --> 00:52:22,480 说话人 SPEAKER_04: 嗯。
618 00:52:22,500 --> 00:52:30,271 说话人 SPEAKER_04: 从概念上讲，你认为人工智能是否会超越人类智能？
619 00:52:30,291 --> 00:52:32,474 说话人 SPEAKER_02: 哦，我认为从长远来看这是不可避免的。
620 00:52:32,494 --> 00:52:32,833 说话人 SPEAKER_04: 嗯。
621 00:52:32,853 --> 00:52:36,800 说话人 SPEAKER_02：我的意思是，为什么它应该类似于人类智能的渐近线呢？
622 00:52:37,380 --> 00:52:39,804 说话人 SPEAKER_02：我看不出任何理由。
623 00:52:41,726 --> 00:52:43,489 说话人 SPEAKER_02：抱歉，请继续。
624 00:52:44,481 --> 00:52:49,009 说话人 SPEAKER_02：我认为使用这项技术的结果将很难说。
625 00:52:49,028 --> 00:52:51,693 说话人 SPEAKER_02：我认为这是值得哲学家思考的问题。
626 00:52:52,313 --> 00:52:53,715 说话人 SPEAKER_02：但我不认为这是一个迫在眉睫的威胁。
627 00:52:55,159 --> 00:53:08,920 说话人 SPEAKER_04：鉴于人类是有身体、有情感、有激素以及所有这些其他东西的生物，你认为这将是一种不同于人类智能的智能吗？
628 00:53:09,659 --> 00:53:11,320 说话人 SPEAKER_02：可能是的。
629 00:53:12,382 --> 00:53:18,889 说话人 SPEAKER_02：我想说，因为我想到了一些神经网络的技术技巧，所以人们认为我对未来很在行。
630 00:53:18,909 --> 00:53:20,132 说话人 SPEAKER_02：我对它的了解并不比你多。
631 00:53:20,952 --> 00:53:30,664 说话人 SPEAKER_02：但我的猜测是，是的，你不会想要创造其他与人类完全一样的智能。
632 00:53:31,204 --> 00:53:33,447 说话人 SPEAKER_02：我的意思是，创造人类更有趣。
633 00:53:33,467 --> 00:53:36,931 说话人 SPEAKER_02：你希望其他智能是互补的。
634 00:53:39,038 --> 00:53:44,244 说话人 SPEAKER_04：那么你现在对深度学习有什么兴奋的事情？你看到它的发展方向在哪里？
635 00:53:44,264 --> 00:53:49,211 说话人 SPEAKER_02：我对机器翻译很兴奋，尽管我自己并没有直接为此做出贡献。
636 00:53:50,052 --> 00:53:50,994 说话人 SPEAKER_02：我的学生中有一些人在做这个。
637 00:53:51,715 --> 00:53:56,081 说话人 SPEAKER_02：我认为在相对较短的时间内，我们将获得更好的翻译。
638 00:53:56,360 --> 00:53:56,842 说话人 SPEAKER_02：是的。
639 00:53:56,862 --> 00:54:00,467 说话人 SPEAKER_02：我对理解文档这类事情感到兴奋。
640 00:54:01,387 --> 00:54:04,913 说话人 SPEAKER_02：我认为在未来五到十年里，在这方面将取得巨大的进步。
641 00:54:05,393 --> 00:54:05,494 说话人 SPEAKER_04: 好吧。
642 00:54:05,514 --> 00:54:06,614 说话人 SPEAKER_04: 那会是什么样子呢？
643 00:54:08,333 --> 00:54:10,474 说话人 SPEAKER_02: 我认为它的大致样子就像我说的那样。
644 00:54:10,514 --> 00:54:25,969 说话人 SPEAKER_02: 不是给谷歌一些单词，然后它会找到所有包含这些单词的文档，而是给谷歌一些主题，然后它会找到即使用不同的话来表达也在说这些主题的文档。
00:54:26,028 --> 00:54:37,699 说话人 SPEAKER_02：因此，您将能够搜索那些根据其内容主张，而不仅仅是根据其文字的文档。
00:54:39,907 --> 00:54:42,170 说话人 SPEAKER_02：这将会很有用。
00:54:42,192 --> 00:54:43,393 说话人 SPEAKER_04：这将会很有用，是的。
00:54:43,815 --> 00:54:45,318 说话人 SPEAKER_04：我想我们还剩下一点时间。
649 00:54:45,338 --> 00:54:47,021 说话人 SPEAKER_04: 我想开始提问环节。
650 00:54:47,061 --> 00:54:49,646 说话人 SPEAKER_04: 那边有一个麦克风。
651 00:54:49,686 --> 00:54:54,695 说话人 SPEAKER_04: 能有人把麦克风递给有问题的观众吗？
652 00:54:57,001 --> 00:54:58,384 说话人 SPEAKER_04: 观众中有人有问题要问吗？
653 00:54:58,423 --> 00:54:59,585 说话人 SPEAKER_04: 这里有一个问题。
654 00:55:02,722 --> 00:55:05,304 说话人 SPEAKER_07: 嗨，你好。
655 00:55:05,945 --> 00:55:08,369 说话人 SPEAKER_07: 教授 Rinton，我有一个问题。
656 00:55:08,650 --> 00:55:29,576 说话人 SPEAKER_07: 在看了你关于思维语言的幻灯片之后，你是建议我们能够拥有能够理解或能够训练这些网络来理解人类情感或模拟与人类相似的情感的机器或算法吗？
657 00:55:29,757 --> 00:55:30,838 说话人 SPEAKER_02：我看不出为什么不行。
658 00:55:30,818 --> 00:55:33,186 说话人 SPEAKER_02：我看不出在那个意义上有什么特别的情感。
659 00:55:34,771 --> 00:55:38,103 说话人 SPEAKER_02：我们已经有能够相当好地读取面部情感表达的计算机了。
660 00:55:39,688 --> 00:55:42,717 说话人 SPEAKER_02：但大多数人会说，计算机不可能有情感。
661 00:55:43,204 --> 00:55:45,889 说话者 SPEAKER_02：我认为这只是对自然语言工作方式的一种误解。
662 00:55:46,452 --> 00:55:55,530 说话者 SPEAKER_02：你可以在计算机中的这个大型神经网络内部有一个活动状态，如果计算机没有自我控制，它就会出去打你。
663 00:55:56,231 --> 00:55:57,653 说话者 SPEAKER_02：那将是跨计算机的。
664 00:55:58,356 --> 00:56:01,601 说话者 SPEAKER_02：那将真正是跨计算机的。
665 00:56:02,289 --> 00:56:03,431 说话人 SPEAKER_07：我们还有一个问题。
666 00:56:03,871 --> 00:56:09,798 说话人 SPEAKER_07：我想了解一下，您如何看待深度学习的商业化发展？
667 00:56:10,199 --> 00:56:11,981 商业化。
668 00:56:12,001 --> 00:56:12,722 商业化。
669 00:56:13,382 --> 00:56:14,704 说话人 SPEAKER_02：我认为这是一件非常好的事情。
670 00:56:15,626 --> 00:56:15,905 说话人 SPEAKER_07：好的。
671 00:56:16,346 --> 00:56:22,413 说话人 SPEAKER_07：但是我的意思是，这会是一个你可以下载并针对特定任务进行训练的 API 吗？
672 00:56:22,773 --> 00:56:30,643 说话人 SPEAKER_07：还是说，这仅仅是大公司如谷歌、Flickr 等针对特定任务进行训练的东西？
673 00:56:32,108 --> 00:56:36,329 说话人 SPEAKER_02：我觉得现在评论这个实际上相当棘手。
674 00:56:36,349 --> 00:56:37,132 说话人 SPEAKER_07：好的。
675 00:56:37,152 --> 00:56:37,414 说话人 SPEAKER_07：对不起。
676 00:56:37,936 --> 00:56:39,704 说话人 SPEAKER_07：我明白了。
677 00:56:41,456 --> 00:56:42,458 说话人 SPEAKER_04: 感谢您的提问。
678 00:56:42,659 --> 00:56:44,420 说话人 SPEAKER_04: 这里有人提问。
679 00:56:44,561 --> 00:56:47,085 说话人 SPEAKER_04: 我想问，可以把麦克风递给我吗？
680 00:56:47,326 --> 00:56:48,347 说话人 SPEAKER_04: 是的，请展示，是的。
681 00:56:49,409 --> 00:56:56,659 说话人 SPEAKER_05：谷歌会说，到 2030 年或 2040 年，机器的智能将和人类一样。
682 00:56:56,679 --> 00:57:07,195 说话人 SPEAKER_05：我想知道，鉴于人类在学习和寿命方面的局限性，我们对摆脱机器的预测，以及您对我们如何
683 00:57:09,233 --> 00:57:10,481 说话人 SPEAKER_02：是的，我有观点。
684 00:57:10,782 --> 00:57:12,434 说话人 SPEAKER_02：我的信心并不大。
685 00:57:12,956 --> 00:57:15,514 说话人 SPEAKER_02：但我的观点是我们将实现共生。
686 00:57:15,815 --> 00:57:24,164 我的意思是，我不太懂生物学，但很久以前，细胞里曾经有其他细胞，比如线粒体，这样事情就变得更好了。
687 00:57:24,864 --> 00:57:37,137 我认为你可以从进化的角度这样考虑，到目前为止，我们只是生物细胞，但现在你将得到人与智能计算机的共生，这将是一个更强大的东西。
688 00:57:37,898 --> 00:57:41,382 我希望这是发展的方向，而不是计算机取代人类。
689 00:57:43,692 --> 00:57:53,501 说话人 SPEAKER_05：电子学与生物学之间的关系是什么？
690 00:57:53,521 --> 00:57:55,742 说话人 SPEAKER_02：因为这是一种共生关系，一个是被创造出来的机器，另一个是活生生的人类。
691 00:57:55,762 --> 00:57:57,605 说话人 SPEAKER_02：是的，但请记住机器可以学习。
692 00:57:57,684 --> 00:58:04,692 说话人 SPEAKER_02：所以只要你有自适应设备，你的助手就可以适应你，你也可以适应你的助手。
693 00:58:05,431 --> 00:58:07,974 说话人 SPEAKER_02：所以我同意，他们如何交流确实是个问题。
694 00:58:08,275 --> 00:58:13,480 说话人 SPEAKER_02：我确信会有各种各样的高级技术来激活你的脑细胞，而不需要插入电极。
695 00:58:13,730 --> 00:58:14,992 说话人 SPEAKER_02：不，事实上，没有人知道会发生什么。
696 00:58:18,858 --> 00:58:26,012 说话人 SPEAKER_02：事实上，没有人知道会发生什么。
697 00:58:26,032 --> 00:58:38,032 说话人 SPEAKER_03：我有孩子，有时他们控制着他们的父母，但对他们破坏的东西不承担责任，这会落到我身上。
698 00:58:38,806 --> 00:58:51,123 说话人 SPEAKER_03：随着我们变得更加复杂，多层网络，你实际上无法调试它们，我的意思是，你可以，但谁负责呢？
699 00:58:51,143 --> 00:58:56,289 说话人 SPEAKER_02：例如，无人机上的杀伤决策。
700 00:58:56,309 --> 00:59:03,699 说话人 SPEAKER_02：我并没有真正深入思考过这个问题，而且我知道还有其他人对此问题思考得更多，所以我并没有什么有用的说法。
701 00:59:07,476 --> 00:59:09,559 说话人 SPEAKER_04：我想我们可能还有时间再问一个问题。
702 00:59:10,782 --> 00:59:18,932 说话人 SPEAKER_01：您提到，为了让网络越来越好，它们需要越来越多的神经元，数百万、数十亿、甚至万亿。
703 00:59:19,893 --> 00:59:26,103 说话人 SPEAKER_01：您还说过，如果摩尔定律继续适用，我们将有能力做到这一点。
704 00:59:26,523 --> 00:59:37,398 说话人 SPEAKER_01：但我的问题是，您是否真的认为摩尔定律会继续适用，还是我们会在某个时刻遇到量子效应，这可能会成为迈向下一个水平的障碍？
705 00:59:37,952 --> 00:59:41,757 说话者 SPEAKER_02：是的，关于摩尔定律的事情就像进化本身一样。
706 00:59:42,599 --> 00:59:51,369 说话者 SPEAKER_02：也就是说，15 年前，对摩尔定律的看法可能是每两年计算机速度翻一番。
707 00:59:52,010 --> 00:59:55,193 说话人 SPEAKER_02：这和之前的 VLSI 技术类似，但正在缩小并变得更快。
708 00:59:55,753 --> 00:59:57,576 说话人 SPEAKER_02：这就是摩尔定律将要实现的方式。
709 00:59:58,137 --> 01:00:00,639 说话人 SPEAKER_02：然后突然，我们遇到了一个瓶颈。
710 01:00:01,121 --> 01:00:02,663 说话人 SPEAKER_02：所以它只是横向喷射。
711 01:00:02,722 --> 01:00:04,525 说话人 SPEAKER_02：现在我们有了更多的核心。
712 01:00:05,105 --> 01:00:07,268 说话人 SPEAKER_02：每隔两年，我们的核心数量就会翻一番。
713 01:00:07,518 --> 01:00:10,126 说话人 SPEAKER_02：然后还会发生其他事情。
714 01:00:11,028 --> 01:00:18,472 说话人 SPEAKER_02：因此，为了使摩尔定律持续有效而可以开发的事物集合是一种开放性的集合。
715 01:00:19,193 --> 01:00:21,882 说话人 SPEAKER_02：这不像是在玩一个有有限规则的游戏。
716 01:00:22,688 --> 01:00:24,489 说话人 SPEAKER_02：当然，我们都是由物理规律所支配的。
717 01:00:24,911 --> 01:00:27,815 说话人 SPEAKER_02：但我们离那些粒子还远着呢。
718 01:00:28,315 --> 01:00:30,759 说话人 SPEAKER_02：而且有许许多多不同的事物可以加以利用。
历史上似乎发生的事情，我认为在一段时间内还会持续发生，就是我们只是会发现不同的方向来提高。
720 01:00:39,349 --> 01:00:43,715 发言人 SPEAKER_02：也许一切都会突然变成真正的 3D，那将是一个巨大的胜利。
721 01:00:44,936 --> 01:00:48,882 发言人 SPEAKER_02：所以我实际上认为摩尔定律的持续时间将比大多数人认为的要长。
722 01:00:48,902 --> 01:00:50,585 说话者 SPEAKER_02：我认为再过 10 年是完全合理的。
723 01:00:50,605 --> 01:00:50,824 说话人 SPEAKER_01: 谢谢。
724 01:00:50,844 --> 01:00:52,306 说话人 SPEAKER_01: 一个乐观的看法，很好。
725 01:00:54,429 --> 01:00:55,932 说话人 SPEAKER_04: 嗯，我想感谢你的问题。
726 01:00:55,952 --> 01:00:58,659 说话人 SPEAKER_04: 我想感谢 Geoffrey 那次非常精彩的演讲。
727 01:00:58,679 --> 01:01:01,786 说话人 SPEAKER_04：如果你愿意，之后可以留下来和人们非正式地聊天。
728 01:01:02,387 --> 01:01:04,612 说话人 SPEAKER_02：我想感谢 Nora 加入我们主持。
729 01:01:04,833 --> 01:01:05,112 说话人 SPEAKER_04：谢谢。
730 01:01:05,333 --> 01:01:05,594 说话人 SPEAKER_04：谢谢。
731 01:01:05,775 --> 01:01:06,094 说话人 SPEAKER_04: 谢谢。
732 01:01:06,195 --> 01:01:06,836 说话人 SPEAKER_04: 谢谢，杰弗里。
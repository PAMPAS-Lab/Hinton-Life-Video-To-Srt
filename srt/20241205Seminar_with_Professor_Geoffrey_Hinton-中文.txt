1 00:00:05,498 --> 00:00:06,259 演讲者 演讲者_00: 下午好。
2 00:00:06,299 --> 00:00:09,743 演讲者 演讲者_00: 我叫 Sylvia Schvag-Serger。
3 00:00:10,044 --> 00:00:26,166 演讲者 演讲者_00: 我是相对新任命的瑞典皇家工程科学院院长，我很高兴下午能欢迎您参加这次由诺贝尔物理学奖获得者，霍金顿教授主持的研讨会。
4 00:00:26,365 --> 00:00:26,966 演讲者 演讲者_00: 欢迎光临。
5 00:00:34,000 --> 00:00:41,265 讲者 SPEAKER_00：我之前有幸与他有过简短交谈，我认为你们将度过一个非常有趣的时光。
6 00:00:41,606 --> 00:00:42,046 讲者 SPEAKER_00：希望如此。
7 00:00:44,029 --> 00:00:54,218 讲者 SPEAKER_00：在请 Hinton 教授上台并发表开场白之前，我想说几句话。
8 00:00:54,457 --> 00:01:02,725 讲者 SPEAKER_00：我是经济历史学教授，作为一名经济历史学教授，我研究过技术变革。
9 00:01:03,329 --> 00:01:12,158 演讲者 SPEAKER_00：人工智能作为数字化的部分和延伸，但它在人类历史的大技术变革中绝对是一个。
10 00:01:13,560 --> 00:01:17,784 演讲者 SPEAKER_00：当我们处理技术变革时，它会产生很多后果。
11 00:01:18,766 --> 00:01:33,200 演讲者 SPEAKER_00：我还会争辩，当这些技术变革与，例如，日益严重的气候危机、地缘政治紧张和民主的侵蚀同时发生时，
12 00:01:33,180 --> 00:01:38,179 演讲者 SPEAKER_00：那么你可能会在糟糕的一天称之为一种有毒鸡尾酒。
13 00:01:39,221 --> 00:01:41,685 讲者 SPEAKER_00: 但今天我们不是来讨论这个的。
14 00:01:41,765 --> 00:01:49,433 讲者 SPEAKER_00: 我们在这里是为了讨论如何以建设性、积极主动和成功的方式处理我们今天所面临的问题。
15 00:01:50,453 --> 00:01:55,500 讲者 SPEAKER_00: 如果允许，我想就我曾是其中一员的瑞典政府 AI 委员会说几句话。
16 00:01:55,540 --> 00:02:05,870 讲者 SPEAKER_00: 我们在过去九个月里准备了一份报告，与数百人进行了交谈，上周我们将我们的报告提交给了瑞典政府。
17 00:02:05,850 --> 00:02:13,098 说话人 SPEAKER_00: 这份报告，就像许多其他国家的报告一样，与其他国家的 AI 战略有一些相似之处。
18 00:02:13,840 --> 00:02:21,729 说话人 SPEAKER_00: 它试图提出如何提高计算能力、如何推动科学、如何促进创新和竞争力的建议。
19 00:02:22,610 --> 00:02:30,558 说话人 SPEAKER_00: 但 AI 委员会报告中也有一些方面，我认为与其他国家的 AI 战略或 AI 委员会报告略有不同。
20 00:02:31,889 --> 00:02:44,902 说话人 SPEAKER_00: 其中之一是我们雄心勃勃的目标，我希望在报告中体现出来，那就是让瑞典人民和民众对这项技术拥有自主权。
21 00:02:45,484 --> 00:02:47,885 讲者 SPEAKER_00: 这是我们报告中的一个重要强调点。
22 00:02:48,826 --> 00:03:01,760 讲者 SPEAKER_00: 这与今年诺贝尔经济学奖得主的信息非常一致，他们谈论的是如何利用技术力量为善而建立包容性机构。
23 00:03:03,715 --> 00:03:26,382 讲者 SPEAKER_00: 此外，这也是我认为它与其他国家在人工智能方面的方法或抱负略有不同之处，它提出了非常强烈的信息和建议，关于我们如何加强公共部门以提供更好的公共服务，以及公共部门实际上也可以成为人工智能的推动者，而不仅仅是迟到的采用者。
24 00:03:27,644 --> 00:03:35,018 讲者 SPEAKER_00: 因此，我认为这两点使瑞典在人工智能方面与其他国家的方法有所不同。
25 00:03:35,659 --> 00:03:42,813 讲者 SPEAKER_00: 正如我所说，我认为这与我们从诺贝尔经济学奖得主那里得到的消息非常一致。
26 00:03:43,655 --> 00:03:44,978 讲者 SPEAKER_00: 然而，
27 00:03:44,957 --> 00:03:47,200 讲者 SPEAKER_00: 这是我晚上难以入睡的事情。
28 00:03:48,062 --> 00:04:14,020 讲者 SPEAKER_00: 如果我们相信人们的能动性或人们对技术的能动性以及包容性机构对于确保积极的技术、为善的技术发展至关重要，那么当出现不相信机构的政府和领导人，世界领导人时，我们该怎么办？
29 00:04:17,053 --> 00:04:23,487 讲者 SPEAKER_00：部分地，也许，他们不相信机构，因为这些机构并没有像它们应该的那样具有包容性。
30 00:04:24,108 --> 00:04:25,932 讲者 SPEAKER_00：所以我认为这是我们必须考虑的一点。
31 00:04:26,291 --> 00:04:36,252 讲者 SPEAKER_00：但这是一个问题，我认为我们正在见证的，不仅仅是民主的侵蚀，而且与之相伴的是对机构的信念的侵蚀。
32 00:04:37,795 --> 00:04:38,656 讲者 SPEAKER_00：其次，
33 00：04：40,173 --> 00：04：47,802 演讲者 SPEAKER_00：在一个目睹国际合作信念下降的世界里，我们该怎么办？
34 00:04:50,103 --> 00:05:01,978 说话者 说话者_00：我认为，国际合作和包容性机构对于确保人工智能能够发挥其作为善的力量潜能至关重要。
第三点，我认为霍 inton 教授在很多次谈论 AI 风险和监管重要性的时候都提到了这一点，当政府并不真正相信监管时，我们该怎么办？
36 00:05:17,737 --> 00:05:28,776 讲者 讲者_00：我的个人问题或担忧是，我会把这个留给你，然后我非常期待 Hinton 教授的发言，
37 00:05:29,684 --> 00:05:48,327 讲者 SPEAKER_00：一个像瑞典这样的小开放经济体，高度依赖国际贸易、国际合作和正常的世界秩序，在并非所有人都相信国际合作或支持国际合作与规范的世界里，应该怎么做？
38 00:05:48,968 --> 00:05:51,692 讲者 SPEAKER_00：欧盟，欧洲应该怎么做？
39 00:05:52,026 --> 00:05:56,992 讲者 SPEAKER_00：在监管 AI 方面，不仅是为了控制其风险，还要发挥其潜力。
40 00:05:58,194 --> 00:06:01,978 讲者 SPEAKER_00：在此，我欢迎 Hinton 教授发表开幕致辞。
41 00:06:08,865 --> 00:06:21,038 讲者 SPEAKER_09：我想对 Sylvia 刚才说的内容发表一点评论，就是有些政治家不相信制度，因为如果那些制度运作得当，他们早就该进监狱了。
42 00:06:24,528 --> 00:06:26,451 讲者 SPEAKER_09：我不想提及任何人的名字。
43 00:06:28,836 --> 00:06:33,723 讲者 SPEAKER_09：我还有 10 分钟的时间，我想基本上说一件事。
44 00:06:34,345 --> 00:06:43,802 讲者 SPEAKER_09：如果你要解决像气候变化这样的问题，首先你必须说服人们，
45 00:06:44,778 --> 00:06:48,264 人类产生的二氧化碳是导致气候变化的原因。
46 00:06:48,865 --> 00:06:51,170 在你做到那一步之前，你无法制定合理的政策。
47 00:06:51,812 --> 00:06:54,076 即使你做到了那一步，你也可能无法制定合理的政策。
48 00:06:54,216 --> 00:06:58,125 人们可能仍然会补贴石油公司等，但这只是第一步。
49 00:06:59,267 --> 00:07:00,389 演讲者 SPEAKER_09：现在，
50 00:07:00,370 --> 00:07:03,413 演讲者 SPEAKER_09：我一直在谈论人工智能的生存威胁。
51 00:07:03,774 --> 00:07:04,975 演讲者 SPEAKER_09：这是一个长期威胁。
52 00:07:04,995 --> 00:07:15,305 演讲者 SPEAKER_09：有许多许多短期威胁是紧迫的，比如网络攻击、失业和流行病，等等，还有虚假视频。
53 00:07:16,247 --> 00:07:23,154 说话人 SPEAKER_09：但是，我们可能会创造出比我们自己更智能的东西，它们将会接管。
54 00:07:24,653 --> 00:07:27,879 说话人 SPEAKER_09：很多人并不认真对待这一点。
55 00:07:28,540 --> 00:07:36,074 说话人 SPEAKER_09：其中一个原因是因为他们认为我们目前拥有的 AI 系统并没有真正理解。
56 00:07:37,196 --> 00:07:43,387 说话人 SPEAKER_09：所以有一群人，其中许多受到乔姆斯基影响的语言学家，
57 00:07:43,603 --> 00:07:45,947 讲者 SPEAKER_09：谁把这些东西称为随机鹦鹉。
58 00:07:46,449 --> 00:07:55,444 讲者 SPEAKER_09：他们认为这些东西只是统计上的技巧，将大量文本拼凑在一起，看起来像理解了，但实际上并不像我们那样理解。
59 00:07:56,666 --> 00:08:03,379 讲者 SPEAKER_09：现在，要有一个他们认为并不像我们那样理解的理论，你必须有一个关于我们如何理解的理论。
60 00:08:04,841 --> 00:08:08,127 讲者 SPEAKER_09：我将论证它们就像我们一样理解。
61 00:08:08,663 --> 00:08:23,379 讲者 SPEAKER_09：谈论随机鹦鹉的人，他们有一个来自经典符号 AI 的理解理论，即在你脑海中，你有一些经过清理的符号表达式，你使用符号规则来操作它们。
62 00:08:23,800 --> 00:08:26,343 讲者 SPEAKER_09：这个理论从未真正奏效。
63 00:08:28,706 --> 00:08:30,148 讲者 SPEAKER_09：但他们仍然坚持这个理论。
64 00:08:30,600 --> 00:08:37,971 讲者 SPEAKER_09：因为他们认为，唯一能够拥有智能的方式就是通过某种逻辑来进行推理。
65 00:08:38,711 --> 00:08:40,894 讲者 SPEAKER_09：他们认为智能的本质是推理。
66 00:08:41,655 --> 00:08:46,844 讲者 SPEAKER_09：有一个完全不同的范式，智能的本质是学习，而这种学习是在神经网络中进行的。
67 00:08:47,344 --> 00:08:52,390 讲者 SPEAKER_09：像视觉和运动控制这样的东西是首要的，而语言和推理则是在后来。
68 00:08:56,317 --> 00:08:58,600 讲者 SPEAKER_09：但我想要讨论这个问题，
69 00:08:58,715 --> 00:09:00,057 演讲者 SPEAKER_09：他们真的理解吗？
70 00:09:01,019 --> 00:09:13,500 演讲者 SPEAKER_09：而且有一个特别的历史事件，大多数人不知道，那就是这些大型语言模型，它们看起来似乎能理解并能回答你提出的任何问题，尽管水平并不高，
71 00:09:14,256 --> 00:09:16,421 演讲者 SPEAKER_09：它们已经存在很长时间了。
72 00:09:16,441 --> 00:09:17,964 演讲者 SPEAKER_09：我喜欢这样思考。
73 00:09:18,585 --> 00:09:23,695 讲者 SPEAKER_09：它们来自我在 1985 年创建的一个模型，这是第一个神经网络语言模型。
74 00:09:25,037 --> 00:09:30,628 讲者 SPEAKER_09：它只有 104 个训练示例，而不是万亿个。
75 00:09:31,182 --> 00:09:32,023 讲者 SPEAKER_09：或者说是数十亿个。
76 00:09:32,764 --> 00:09:35,668 讲者 SPEAKER_09：网络中大约有 1,000 个权重，而不是万亿个。
77 00:09:36,889 --> 00:09:54,472 讲者 SPEAKER_09：这是一个训练用来预测下一个单词并将预测错误反向传播以将输入符号转换为神经活动向量的语言模型，然后学习这些向量应该如何相互作用以预测它试图预测的符号的向量。
78 00:09:54,452 --> 00:09:57,856 讲者 SPEAKER_09：那个点并没有工程意义。
79 00:09:57,876 --> 00:10:02,743 讲者 SPEAKER_09：它的意义在于是一种关于人们如何理解词义的理论。
80 00:10:05,927 --> 00:10:12,735 讲者 SPEAKER_09：因此，我们关于人们如何理解句子的最佳模型就是这些语言模型。
81 00:10:14,096 --> 00:10:17,662 讲者 SPEAKER_09：这是我们唯一一个真正有效的工作模型。
82 00:10:17,682 --> 00:10:19,004 讲者 SPEAKER_09：我们有很多这些符号模型。
83 00:10:20,024 --> 00:10:21,826 讲者 SPEAKER_09：它们实际上并不怎么有效。
84 00:10:21,942 --> 00:10:29,899 讲者 SPEAKER_09：它们，我的意思是，它们受到了乔姆斯基的强烈影响，乔姆斯基设法说服了多代语言学家，语言不是通过学习获得的。
85 00:10:30,580 --> 00:10:34,248 讲者 SPEAKER_09：表面上看来，说语言不是学来的显然是荒谬的。
86 00:10:34,769 --> 00:10:38,836 讲者 SPEAKER_09：如果你能让人们相信显然荒谬的事情，那么你就建立了一个邪教。
87 00:10:39,543 --> 00:10:41,626 讲者 SPEAKER_09：乔姆斯基就有一个邪教。
88 00:10:43,110 --> 00:10:47,096 讲者 SPEAKER_09：语言是可以学来的，我们现在可以看到学习语言的事物。
89 00:10:47,498 --> 00:10:50,363 讲者 SPEAKER_09：结构不必是天生的，它来自数据。
90 00:10:50,744 --> 00:10:56,674 讲者 SPEAKER_09：神经网络和学习算法中必须存在天生的结构，但所有语言的结构你都可以从数据中获得。
91 00:10:57,155 --> 00:11:00,522 讲者 SPEAKER_09：乔姆斯基看不到如何做到这一点，所以他说是天生的。
92 00:11:00,501 --> 00:11:19,557 讲者 SPEAKER_09：说它必须天生的，而不是学来的，这真的很愚蠢，因为这相当于说进化学到了它，而不是学习。进化是一个比学习慢得多的过程，进化产生大脑的原因是你可以比进化更快地学习东西，因为进化使它天生的速度要慢得多。
93 00:11:21,409 --> 00:11:28,264 演讲者 SPEAKER_09：这次闲聊的目的是说服你们，他们理解的方式和我们一样。
94 00:11:28,745 --> 00:11:30,489 演讲者 SPEAKER_09：为此，我将再给你们提供一条证据。
95 00:11:31,892 --> 00:11:38,926 很多人谈论随机鹦鹉时会说，看，我可以向你展示他们实际上并不理解，因为他们只是胡编乱造。
96 00:11:38,946 --> 00:11:39,908 他们只是胡编乱造。
97 00:11:41,456 --> 00:11:43,379 讲者 SPEAKER_09：这些人不是心理学家。
98 00:11:43,419 --> 00:11:46,485 讲者 SPEAKER_09：他们不明白不应该使用“幻觉”这个词。
99 00:11:46,946 --> 00:11:48,889 讲者 SPEAKER_09：他们应该使用“虚构”这个词。
100 00:11:49,370 --> 00:11:53,256 讲者 SPEAKER_09：自 1930 年代以来，心理学家一直在研究人类的虚构行为。
101 00:11:53,557 --> 00:11:54,899 讲座者 SPEAKER_09：一位名叫巴特莱特的心理学家。
102 00:11:55,620 --> 00:11:58,525 讲座者 SPEAKER_09：人们总是在编造故事。
103 00:11:59,086 --> 00:12:02,131 讲座者 SPEAKER_09：如果你回想很久以前发生的事情，而且在这期间你没有
104 00:12:02,956 --> 00:12:09,408 讲座者 SPEAKER_09：练习过，你试图回忆它，你会自信地记住许多错误的事情。
105 00:12:09,448 --> 00:12:14,097 讲者 SPEAKER_09：因为记忆并不是从某处获取一个文件。
106 00:12:14,759 --> 00:12:18,365 讲者 SPEAKER_09：记忆是构建一个看似合理的东西。
107 00:12:18,346 --> 00:12:24,774 讲者 SPEAKER_09：现在，如果你刚刚看到了某样东西，然后尝试构建一个看似合理的东西，你会有相当准确的细节。
108 00:12:25,535 --> 00:12:32,506 讲者 SPEAKER_09：但是如果你很多年前就看到了，现在尝试构建一个看似合理的东西，首先，它会被你在这期间学到的所有东西所影响。
109 00:12:33,488 --> 00:12:39,836 讲者 SPEAKER_09：你会构建一些听起来不错的东西，但实际上，你非常自信的许多细节都是错误的。
110 00:12:40,378 --> 00:12:45,905 讲者 SPEAKER_09：很难证明这一点，但有一位心理学家名叫 Ulrich Neisser，他研究了一个非常漂亮的案例。
111 00:12:46,392 --> 00:12:55,538 讲者 SPEAKER_09：约翰·迪安在水门事件中宣誓作证，关于椭圆形办公室正在进行的掩盖行动，但他不知道有录音带。
112 00:12:56,461 --> 00:12:59,912 讲者 SPEAKER_09：所以你有一个试图讲述几年前发生的事情的人。
113 00:13:00,802 --> 00:13:06,250 演讲者 SPEAKER_09：他说的很多内容并不真实，但他显然是在试图说出真相。
114 00:13:06,269 --> 00:13:07,873 演讲者 SPEAKER_09：他会说，这些人之间有过这次会议。
115 00:13:08,173 --> 00:13:09,554 演讲者 SPEAKER_09：不，这些人从未有过会议。
116 00:13:10,034 --> 00:13:11,037 演讲者 SPEAKER_09：这个人说了这样的话。
117 00:13:11,197 --> 00:13:12,558 说话人 SPEAKER_09：不，那个人没有说过那样的话。
118 00:13:12,818 --> 00:13:14,461 说话人 SPEAKER_09：有人在另一次会议上说过那样的话。
119 00:13:15,081 --> 00:13:22,272 说话人 SPEAKER_09：但重点是，他在传达关于掩盖真相的事实，他坚信自己所说的是真的，只是错了。
120 00:13:22,690 --> 00:13:24,913 说话人 SPEAKER_09：这就是人类记忆的工作方式。
121 00:13:25,634 --> 00:13:30,865 讲者 SPEAKER_09：当这些事情混淆时，它们就像人一样。
122 00:13:31,044 --> 00:13:31,947 讲者 SPEAKER_09：人们混淆。
123 00:13:32,828 --> 00:13:33,669 讲者 SPEAKER_09：至少我认为是这样。
124 00:13:33,690 --> 00:13:34,350 讲者 SPEAKER_09：我只是随便说的。
125 00:13:36,554 --> 00:13:38,158 演讲者 SPEAKER_09: 好的，我完成了。
126 00:13:38,177 --> 00:13:38,538 演讲者 SPEAKER_09: 我们不会记得。
127 00:13:47,750 --> 00:13:53,256 演讲者 SPEAKER_00: Jeffrey，如果你能留在这里，因为我想问你一两个问题。
128 00:13:53,378 --> 00:13:59,044 演讲者 SPEAKER_00: 我知道我们有一个非常棒的讨论小组，他们将更多地与您讨论这项技术。
129 00:14:00,027 --> 00:14:03,191 说话人 SPEAKER_00: 但我知道你已经讨论过人工智能的风险。
130 00:14:04,011 --> 00:14:13,605 说话人 SPEAKER_00: 所以我有一个问题，我认为你也提到了，必须有一些形式的国际合作来处理这些风险。
131 00:14:13,585 --> 00:14:22,999 说话人 SPEAKER_00: 你认为为了使各国能够以建设性的方式合作来控制这些风险，需要发生什么？
132 00:14:23,620 --> 00:14:28,086 说话人 SPEAKER_09: 所以我认为在致命自主武器这样的风险上，各国不会进行合作。
133 00:14:28,486 --> 00:14:32,653 俄罗斯人和美国人不会合作制造相互战斗的战斗机器人。
134 00:14:33,453 --> 00:14:38,360 所有主要武器供应国，
135 00:14:38,341 --> 00:14:53,149 俄罗斯、美国、中国、英国、以色列，以及可能还有瑞典，都在忙于制造自主致命武器，他们不会减速，不会自我监管，也不会合作。
136 00:14:53,809 --> 00:15:01,884 如果你看欧洲的法规，欧洲关于人工智能的法规中有一条条款说，这些法规不适用于人工智能的军事用途。
137 00:15:02,269 --> 00:15:04,793 讲者 SPEAKER_09：很明显，欧洲国家不想对其进行监管。
138 00:15:05,273 --> 00:15:08,797 讲者 SPEAKER_09：他们想赶紧看看是否能比其他人造出更好的战斗机器人。
139 00:15:09,597 --> 00:15:11,259 讲者 SPEAKER_09：所以我们无法控制这一点。
140 00:15:11,980 --> 00:15:15,823 讲者 SPEAKER_09：我认为许多其他短期风险也会如此。
141 00:15:15,844 --> 00:15:27,956 讲者 SPEAKER_09：在美国，例如，他们不会监管虚假视频，因为即将完全掌权的其中一个政党相信它们。
142 00:15:30,129 --> 00:15:35,379 讲者 SPEAKER_09：不过，有一个领域你们将会得到合作，那就是生存威胁。
143 00:15:36,220 --> 00:15:45,856 讲者 SPEAKER_09：所以当这些事物比我们聪明时，我所知道的几乎所有研究人员都认为它们会的，我们只是在时间上有所不同，无论是五年还是三十年。
144 00:15:47,840 --> 00:15:50,303 讲者 SPEAKER_09：当它们比我们聪明时，它们会接管吗？
145 00:15:50,585 --> 00:15:53,590 讲者 SPEAKER_09：我们能否做些什么来防止这种情况发生呢？
146 00:15:54,532 --> 00:15:59,259 讲者 SPEAKER_09：你们会得到合作，因为所有国家都不希望这种情况发生。
147 00:15:59,818 --> 00:16:07,090 讲者 SPEAKER_09：在冷战高峰期，苏联和美国可以合作防止核战争。
148 00:16:08,051 --> 00:16:11,255 讲者 SPEAKER_09：他们将以同样的方式合作防止这些事物占据主导地位。
149 00:16:11,275 --> 00:16:16,081 说话人 SPEAKER_09：中国共产党不想让 AI 夺走权力。
150 00:16:17,644 --> 00:16:20,168 说话人 SPEAKER_09：他们想保留权力。
151 00:16:20,586 --> 00:16:26,316 说话人 SPEAKER_09：所以我认为这是一个我们可以期待合作的领域，这真是幸运。
152 00:16:26,576 --> 00:16:28,678 说话人 SPEAKER_09：但我认为在其他领域我们不会得到合作。
153 00:16:28,980 --> 00:16:29,279 演讲者 演讲者_00：好的。
154 00:16:29,379 --> 00:16:31,322 演讲者 演讲者_00：嗯，这有点乐观。
155 00:16:31,984 --> 00:16:47,948 演讲者 演讲者_00：然后我最后一个问题，作为工程科学院，我们专注于解决问题，我想请问您，您会告诉他们什么，还有作为孩子的母亲，现在非常关心现在和未来的我，您会告诉年轻人什么？
156 00:16:48,269 --> 00:16:49,311 演讲者 演讲者_00：您想说什么...
157 00:16:49,392 --> 00:16:58,503 讲者 SPEAKER_09：好吧，有些 AI 研究人员，比如我的朋友、我的博士后 Yalaka，他们认为没有可能被取代，不用担心。
158 00:16:58,543 --> 00:17:00,706 讲者 SPEAKER_09：别信那。
159 00:17:02,227 --> 00:17:04,309 讲者 SPEAKER_09：当它们比我们聪明时，我们不知道会发生什么。
160 00:17:05,090 --> 00:17:07,674 讲者 SPEAKER_09：这是一片完全未知的领域。
161 00:17:07,694 --> 00:17:10,438 讲者 SPEAKER_09：有其他 AI 研究人员，比如尤达科夫斯基，
162 00:17:10,553 --> 00:17:16,161 讲者 SPEAKER_09：嗯，他其实并不是 AI 研究人员，但他对 AI 了解很多，他说有 99%的几率它们会接管。
163 00:17:17,001 --> 00:17:25,513 讲者 SPEAKER_09：他实际上说是 99.9%，正确的策略是现在就轰炸数据中心，这对大公司来说并不受欢迎。
164 00:17:27,757 --> 00:17:28,798 讲者 SPEAKER_09：这也太疯狂了。
165 00:17:29,638 --> 00:17:35,388 说话人 SPEAKER_09：我们正进入一个充满巨大不确定性的时代，当我们开始处理比我们更智能或与我们一样智能的事物时。
166 00:17:35,970 --> 00:17:37,571 说话人 SPEAKER_09：我们不知道会发生什么。
167 00:17:38,133 --> 00:17:41,298 说话人 SPEAKER_09：我们在构建它们，所以我们现在有很大的权力。
168 00:17:42,681 --> 00:17:43,741 说话人 SPEAKER_09：我们只是不知道会发生什么。
169 00:17:43,761 --> 00:17:44,683 说话人 SPEAKER_09：人们非常聪明。
170 00:17:45,865 --> 00:17:50,493 说话人 SPEAKER_09：完全有可能我们会找到方法，让他们永远不会想要控制。
171 00:17:50,894 --> 00:17:54,921 说话人 SPEAKER_09：因为如果他们想要控制，我认为如果他们比我们聪明，他们很容易就能做到。
172 00:17:54,901 --> 00:18:00,711 说话人 SPEAKER_09：所以我认为我们现在所处的局面就像有人养了一只非常可爱的小老虎。
173 00:18:02,694 --> 00:18:09,928 说话人 SPEAKER_09：小老虎是很好的宠物，但你最好确保它长大后不会想杀你。
174 00:18:10,449 --> 00:18:11,730 说话人 SPEAKER_09：如果你能确保这一点，那就没问题。
175 00:18:13,253 --> 00:18:14,496 说话人 SPEAKER_09：我们现在的处境就是这样。
176 00:18:15,791 --> 00:18:16,554 说话人 SPEAKER_00：非常感谢您。
177 00:18:16,574 --> 00:18:24,898 演讲者 演讲者_00：在这个问题上，我将把你们交给非常能干的安妮特·诺瓦克，她将引导我们进行这次圆桌讨论。
178 00:18:25,059 --> 00:18:26,403 演讲者 演讲者_00：非常感谢到目前为止。
179 00:18:26,544 --> 00:18:27,185 演讲者 演讲者_00：谢谢。
180 00:18:34,169 --> 00:18:36,755 演讲者 演讲者_07：谢谢你，西尔维亚，还有谢谢你，Hinton 教授。
181 00:18:37,615 --> 00:18:51,486 讲者 SPEAKER_07：作为 IVA 信息技术部的主席，我有幸和特权引导大家进入这次研讨会的下一部分，其中将有一系列真正强大的演讲者阵容进行圆桌讨论。
182 00:18:51,465 --> 00:18:57,893 讲者 SPEAKER_07：通过圆桌讨论，我认为我们也能展示信息技术是一个非常广泛的研究领域。
183 00:18:58,593 --> 00:19:06,320 讲者 SPEAKER_07：我们有来自未来学、哲学、计算机科学和人工智能与人类交互的 IWA 研究员。
184 00:19:07,301 --> 00:19:16,511 讲者 SPEAKER_07：我们将以此开始三个简短的发言，为与 Hinton 教授的圆桌讨论播下一些种子，他将会回来，所以请放心。
185 00:19:16,491 --> 00:19:27,228 演讲者 SPEAKER_07：我们将由安德斯·桑德伯格开始，他是斯德哥尔摩未来研究所的研究员，曾是牛津大学人类未来研究所的成员。
186 00:19:28,090 --> 00:19:29,556 演讲者 SPEAKER_07：请，安德斯，您来发言。
187 00:19:35,070 --> 00:19:35,951 演讲者 SPEAKER_08：非常感谢。
188 00:19:35,971 --> 00:19:50,170 演讲者 SPEAKER_08：我觉得能在这个场合讨论我们对人工智能的了解，或者更确切地说，讨论我们对它的不了解，尤其是因为我的教授就在听众席上，这非常有趣。
189 00:19:50,951 --> 00:19:56,758 讲者 SPEAKER_08：令人尴尬的一件事是意识到我在 90 年代学到的很多东西现在已经不再正确。
190 00:19:57,598 --> 00:20:02,886 讲者 SPEAKER_08：事实上，这正是人工智能领域整个历史的主要教训之一。
191 00:20:02,906 --> 00:20:08,335 讲者 SPEAKER_08：我们极其擅长预测什么会成功，什么不会成功。
192 00:20:08,355 --> 00:20:15,645 讲者 SPEAKER_08：当然，有一整个流派的名言，来自早期人工智能先驱们自信地表示，在一代人的时间内，我们将拥有达到人类水平的人工智能机器。
193 00:20:16,547 --> 00:20:20,353 讲者 SPEAKER_08：的确，他们都没有关心我们的安全。
194 00:20:21,013 --> 00:20:23,336 讲者 SPEAKER_08：当然，那些预测也是错误的。
195 00:20:23,941 --> 00:20:26,526 讲者 SPEAKER_08：我们对于悲观也是错误的。
196 00:20:26,546 --> 00:20:31,836 讲者 SPEAKER_08：有时候它突然开始工作，当它发生时，这既令人惊讶，甚至令人震惊。
197 00:20:31,936 --> 00:20:45,003 讲者 SPEAKER_08：2010 年代发生的革命是使用长期以来一直在做一些有趣事情的相同类型的网络，但突然间我们开始起飞，原因我们至今仍不完全理解。
198 00:20:44,983 --> 00:20:46,506 讲者 SPEAKER_08：我们懂得如何制造它们。
199 00:20:46,526 --> 00:20:48,627 讲者 SPEAKER_08：我们懂得一些基本原理。
200 00:20:49,249 --> 00:20:59,539 讲者 SPEAKER_08：但发展出来的新兴产物与其说是我们的创造，不如说是我们真正感到棘手的涌现现象。
201 00:20:59,559 --> 00:21:07,907 讲者 SPEAKER_08：当然，这表明我们对未来的预测并不十分了解，也不太有根据。
202 00:21:08,269 --> 00:21:12,173 讲者 SPEAKER_08：这并不意味着我们不应该相信预测。
203 00:21:12,153 --> 00:21:22,230 讲者 SPEAKER_08：因为在许多领域，在商业、政治、战争和爱情中，我们经常需要利用那些实际上并不太有根据的预测。
204 00:21:22,250 --> 00:21:31,224 讲者 SPEAKER_08：而当一个看似前景光明的技术领域出现时，如何处理其安全性则是一个有趣的挑战。
205 00:21:31,204 --> 00:21:35,655 讲者 SPEAKER_08：当然，从安全角度来说，这种情况当然是非常令人担忧的。
206 00:21:36,016 --> 00:21:39,986 讲者 SPEAKER_08：看起来像是人工智能，我们无法判断它能走多远。
207 00:21:40,366 --> 00:21:44,636 讲者 SPEAKER_08：很多人说，乔姆斯基实际上深层次是正确的。
208 00:21:45,137 --> 00:21:48,926 讲者 SPEAKER_08：或者存在其他限制，比如训练材料或计算能力。
209 00:21:48,906 --> 00:21:56,641 讲者 SPEAKER_08: 或者说我们实际上正在使用错误的算法，以及推动当前 LLMs 的扩展定律可能不会奏效。
210 00:21:56,961 --> 00:22:00,106 讲者 SPEAKER_08: 顺便问一下，为什么 LLM 能够进行思考呢？
211 00:22:00,386 --> 00:22:03,152 讲者 SPEAKER_08: 毕竟这只是在预测序列。
212 00:22:03,267 --> 00:22:12,477 讲者 SPEAKER_08: 意外地发现，序列预测实际上可能是一个相当好的思考替代品，即使大型语言模型所能规划的有限。
213 00:22:12,978 --> 00:22:18,324 演讲者 SPEAKER_08：也许它只能规划短小的思维序列，你需要接入另一个架构。
214 00:22:18,805 --> 00:22:23,309 演讲者 SPEAKER_08：无法保证不会有人在下周二做出这样的事情。
215 00:22:24,171 --> 00:22:26,272 演讲者 SPEAKER_08：同样，我们不知道它的发展速度有多快。
216 00:22:26,313 --> 00:22:32,380 演讲者 SPEAKER_08：再次，我们有一个很长的时期，似乎没有什么发生，然后突然发生了很多事情。
217 00:22:32,359 --> 00:22:37,686 讲者 SPEAKER_08：当然，这表明我们可能难以控制它并了解这一点。
218 00:22:37,707 --> 00:22:43,673 讲者 SPEAKER_08：看起来它可以走得很远，这是个非常好的消息，因为人工智能实际上看起来非常有用。
219 00:22:44,034 --> 00:22:45,155 讲者 SPEAKER_08：我当然在使用它。
220 00:22:45,256 --> 00:22:47,097 讲者 SPEAKER_08：它正在解决有趣的问题。
221 00:22:47,117 --> 00:22:58,792 讲者 SPEAKER_08：一些问题也让我有些紧张，因为我今天早些时候进行了一次生物安全演练，和一个LLM一起，我一直在期待着说，Anders，那是一个生物武器。
222 00:22:59,093 --> 00:23:00,595 讲者 SPEAKER_08：不要再进一步开发了。
223 00:23:00,575 --> 00:23:08,567 讲者 SPEAKER_08：但因为我巧妙地将它表述为一个非常学术性的项目，它很高兴地告诉我需要转染细胞所需的病毒种类。
224 00:23:08,648 --> 00:23:11,232 讲者 SPEAKER_08：这让我有些紧张。
225 00:23:11,894 --> 00:23:17,824 演讲者 SPEAKER_08：我可能不是一个很好的生物技术学家，所以大家肯定很安全。
226 00:23:18,799 --> 00:23:24,286 演讲者 SPEAKER_08：这表明在高度不确定性的世界中，我们面临一个有趣的情况。
227 00:23:24,626 --> 00:23:34,137 演讲者 SPEAKER_08：谨慎的做法是假设我们可以非常快、非常强大地发展，即使我们无法预测它们，然后尝试采取措施使它们更安全。
228 00:23:34,738 --> 00:23:37,961 演讲者 SPEAKER_08：所以我喜欢说我是一个悲观主义者，因为我很乐观。
229 00:23:38,603 --> 00:23:43,288 说话人 SPEAKER_08：如果我觉得自己什么也做不了，那就没有尝试去做的理由。
230 00:23:43,268 --> 00:23:46,894 说话人 SPEAKER_08：我认为这里确实存在风险，但也是一个令人惊叹的、美好的机会。
231 00:23:46,973 --> 00:23:54,586 说话人 SPEAKER_08：我认为我们确实在这里学到了关于我们自己的智慧以及可能的其他形式的智慧的深刻知识。
232 00:23:54,625 --> 00:23:59,012 说话人 SPEAKER_08：最后，获得一面能展示我们思考的替代方式的镜子。
233 00:23:59,953 --> 00:24:03,640 Speaker SPEAKER_08: 非常感谢您做这件事，Hinton 教授。
234 00:24:09,981 --> 00:24:10,883 Speaker SPEAKER_07: 精彩的思考。
235 00:24:10,943 --> 00:24:11,605 Speaker SPEAKER_07: 谢谢，Anders。
236 00:24:12,266 --> 00:24:13,887 Speaker SPEAKER_07: 我不会让您再等了。
237 00:24:14,008 --> 00:24:17,334 演讲者 SPEAKER_07：我们将请下一位演讲者上台。
238 00:24:17,413 --> 00:24:20,818 演讲者 SPEAKER_07：这位是 Recorded Future 的 CTO 和创始人，Staffan Thruve。
239 00:24:21,961 --> 00:24:22,402 演讲者 SPEAKER_04：谢谢。
240 00:24:22,923 --> 00:24:23,242 演讲者 SPEAKER_04：大家好。
241 00:24:26,868 --> 00:24:31,016 讲者 SPEAKER_04：当我还是一名研究生时，我有机会在麻省理工学院待了一年。
242 00:24:31,415 --> 00:24:34,401 讲者 SPEAKER_04：我去那里部分原因是因为我想学习语言学。
243 00:24:34,461 --> 00:24:36,765 讲者 SPEAKER_04：正如你所说，乔姆斯基在那里，是位大英雄。
244 00:24:37,251 --> 00:24:39,278 讲者 SPEAKER_04：幸运的是，我还没有做我的作业。
245 00:24:39,298 --> 00:24:41,728 说话人 SPEAKER_04：所以我到那里的时候，汤姆整个学期都在休假。
246 00:24:42,671 --> 00:24:45,039 说话人 SPEAKER_04：结果我和一个认知心理学家一起工作了。
247 00:24:45,320 --> 00:24:47,970 讲者 SPEAKER_04：这可能是我现在在这里的原因。
248 00:24:49,215 --> 00:25:01,070 讲者 SPEAKER_04：所以我在过去 15 年里建立了一个公司或系统，如果你愿意的话，它使用人工智能来尝试预测组织、国家等的威胁。
249 00:25:02,031 --> 00:25:07,939 讲者 SPEAKER_04：因此，我想说的是一些可能与你所说的短期威胁相反的内容。
250 00:25:08,579 --> 00:25:18,632 讲者 SPEAKER_04：所以，我完全同意从长远来看，你知道，机器变得足够智能，能够理解对地球的真正威胁是人类，这是我们面临的大问题。
251 00:25:18,612 --> 00:25:27,221 讲者 SPEAKER_04：但是在我看来，在近期未来，坏人使用 AI 是我们要关注的大威胁。
252 00:25:28,011 --> 00:25:32,378 讲者 SPEAKER_04：这就是为什么我也关注监管问题。
253 00:25:33,340 --> 00:25:54,036 讲者 SPEAKER_04：因为即使全球各国可能就避免长期威胁达成一致，我认为你本质上已经提到了这一点，我认为短期内将要发生的是，如果我们，可能世界上所有人都会认为自己站在正义的一方，但让我们假设我们西方，自由世界，无论你叫它什么，都是好人。
254 00:25:54,016 --> 00:26:04,701 讲者 SPEAKER_04：如果我们制定法规来防止这些事情，当然那些不在乎法规的人，可能是流氓国家，罪犯，等等。
255 00:26:05,305 --> 00:26:06,326 演讲者 SPEAKER_04：他们将拥有优势。
256 00:26:06,426 --> 00:26:07,888 演讲者 SPEAKER_04：他们不会停止发展。
257 00:26:08,388 --> 00:26:17,382 演讲者 SPEAKER_04：当然，这项技术如此易于获取的事实意味着即使我们进行监管，也无法控制人们是否遵守这些规定。
258 00:26:17,842 --> 00:26:31,161 演讲者 SPEAKER_04：这与核武器大不相同，核武器需要巨大的机器和工厂，这些可以被监控和检测到更大程度，即使有些人可以伪造并声称伊拉克有这些武器，即使实际上没有。
259 00:26:31,141 --> 00:26:35,246 演讲者 SPEAKER_04：我认为这是我们希望在这里再讨论一下的问题。
260 00:26:35,365 --> 00:26:40,352 演讲者 SPEAKER_04：我们如何防御坏人利用 AI 对我们进行攻击？
261 00:26:41,252 --> 00:26:43,915 演讲者 SPEAKER_04：我认为我们将会进入一场军备竞赛。
262 00:26:44,355 --> 00:26:53,487 演讲者 SPEAKER_04：因此，我们每天都在尝试开发基于 AI 的工具，例如检测 AI 生成的图像，以便我们能够警告假新闻的传播。
263 00:26:53,467 --> 00:26:54,067 演讲者 SPEAKER_04: 等等。
264 00:26:54,528 --> 00:27:03,978 演讲者 SPEAKER_04: 当然，这是一个经典的回答，即生成图像的模型会变得更好，我们将能够检测到这些，然后会有一个新的，等等，实际上是没有止境的。
265 00:27:04,939 --> 00:27:16,571 演讲者 SPEAKER_04: 所以我认为我们应该充分尊重长期前景，但我认为我们还需要弄清楚，我对此没有答案，我们如何实际上防止在两年内而不是 20 年后发生非常糟糕的事情。
266 00:27:17,353 --> 00:27:17,752 演讲者 SPEAKER_04: 谢谢。
267 00:27:24,044 --> 00:27:26,407 讲者 SPEAKER_07：嗯，长期和短期风险。
268 00:27:26,468 --> 00:27:29,192 讲者 SPEAKER_07：我希望我们也能得到一些积极的结果。
269 00:27:30,095 --> 00:27:35,424 讲者 SPEAKER_07：所以，我不会再拖延了。
270 00:27:35,525 --> 00:27:39,111 讲者 SPEAKER_07：所以，最后但绝对不是最不重要的，我们还有另一位 IWA 会员。
271 00:27:39,811 --> 00:27:44,200 讲者 SPEAKER_07：她是瑞典皇家理工学院交互设计教授，Kia Höök。
272 00:27:44,540 --> 00:27:46,203 讲者 SPEAKER_07：谢谢。
273 00:27:49,441 --> 00:27:54,690 讲者 SPEAKER_06：所以他们邀请我来讲座时，我说我不做 AI，也不喜欢 AI。
274 00:27:55,830 --> 00:27:58,536 讲者 SPEAKER_06：我过去所做的是符号 AI。
275 00:27:58,556 --> 00:27:59,617 演讲者 SPEAKER_06：我一点也不喜欢那样。
276 00:28:00,419 --> 00:28:04,685 演讲者 SPEAKER_06：最近我们一直在做这些更具身临其境的人工智能东西。
277 00:28:04,726 --> 00:28:14,622 演讲者 SPEAKER_06：所以这里有一个可以改变形状的紧身胸衣，模仿歌手唱歌时如何使用肌肉，让观众能感受到身体上的变化。
278 00:28:14,602 --> 00:28:17,970 演讲者 SPEAKER_06：所以我们将人工智能扩展到了身体上。
279 00:28:18,049 --> 00:28:23,000 讲者 SPEAKER_06：我们可能在谈论威胁，但是一种不同类型的威胁。
280 00:28:24,263 --> 00:28:31,159 讲者 SPEAKER_06：所以我想要稍微谈谈伦理和人工智能以及身体，这显然意味着要谈论死亡。
281 00:28:32,657 --> 00:28:37,087 讲者 SPEAKER_06：让我解释一下为什么死亡很重要。
282 00:28:37,648 --> 00:28:43,401 讲者 SPEAKER_06：我最近参加了一个会议，Terry Vinograd 说人工智能并不关心。
283 00:28:43,461 --> 00:28:48,050 讲者 SPEAKER_06：所以如果你谈论道德，人工智能根本不在乎。
284 00:28:48,030 --> 00:28:53,780 讲者 SPEAKER_06：唐娜·哈拉维也在那里，她说，智能是有形的。
285 00:28:53,800 --> 00:28:56,385 讲者 SPEAKER_06：它嵌入在我们的身体中。
286 00:28:56,987 --> 00:29:04,098 讲者 SPEAKER_06：所以它不仅仅在大脑中，它在整个系统中，包括脑干，包括你的肌肉和你的整个身体。
287 00:29:04,720 --> 00:29:12,253 讲者 SPEAKER_06：我认为，人类智能实际上首先是运动，其次是语言。
288 00:29:12,374 --> 00:29:14,637 讲者 SPEAKER_06：所以我觉得LLMs这个话题并不那么有趣。
289 00:29:14,939 --> 00:29:20,167 讲者 SPEAKER_06：我觉得这种方式更有趣，是关于身体的东西。
290 00:29:20,188 --> 00:29:27,201 讲者 SPEAKER_06：因此，如果我们以此为设计出发点，我们将会以一种身体伦理的方式来进行设计。
291 00:29:27,181 --> 00:29:39,315 讲者 SPEAKER_06：显然，首先，要讲道德，就需要有人被关进监狱，需要有人是司法系统运作所必需的。
292 00:29:39,355 --> 00:29:41,057 讲者 SPEAKER_06：这不是我想谈论的。
293 00:29:41,497 --> 00:29:49,286 讲者 SPEAKER_06：我想谈谈我们的生物构成如何以某种方式决定我们的行为、我们的情感、我们的价值观等等。
294 00:29:49,886 --> 00:29:56,714 讲者 SPEAKER_06：如果我们想要追求一种非常接近我们身体自我的道德，这就是我们需要去的地方。
295 00:29:57,234 --> 00:29:59,157 演讲者 SPEAKER_06：那么我的意思是什么？
296 00:29:59,218 --> 00:30:13,838 好吧，实际上我的身体，身体的对称性，上下左右，我是一个女性身体，我有乳房等等，这些都影响我在世界中的行为。
297 00:30:13,980 --> 00:30:16,864 这也影响你对我的行为。
298 00:30:17,444 --> 00:30:21,770 因此，我的自我认知就是这样形成的。
299 00:30:21,790 --> 00:30:24,474 讲者 SPEAKER_06: 所以这些规范已经深入我的骨髓。
300 00:30:24,454 --> 00:30:26,661 讲者 SPEAKER_06: 它们已经成为了我的一部分。
301 00:30:26,721 --> 00:30:29,587 讲者 SPEAKER_06: 所以我能看到房间里女性们正并拢着腿。
302 00:30:29,607 --> 00:30:31,633 讲者 SPEAKER_06: 她们并没有像典型男性那样张开双腿。
303 00:30:31,794 --> 00:30:32,154 演讲者 SPEAKER_06: 为什么？
304 00:30:32,194 --> 00:30:34,781 演讲者 SPEAKER_06: 你的骨盆没有问题。
305 00:30:35,363 --> 00:30:38,109 演讲者 SPEAKER_06: 如果你想的话，你可以像男人一样坐着。
306 00:30:38,672 --> 00:30:39,653 演讲者 SPEAKER_06: 但那是一种规范。
307 00:30:39,753 --> 00:30:41,198 讲者 SPEAKER_06: 我们不做那样。
308 00:30:41,178 --> 00:30:50,682 讲者 SPEAKER_06: 所以为了生存，我们需要在一个规范性环境中学习如何行为。
309 00:30:51,424 --> 00:30:56,217 讲者 SPEAKER_06: 所以文化对于人类意味着什么非常重要。
310 00:30:56,500 --> 00:30:59,805 讲者 SPEAKER_06: 我们的习惯需要与别人保持一致。
311 00:31:00,006 --> 00:31:02,430 说话人 SPEAKER_06：如果他们不喜欢我们。
312 00:31:03,290 --> 00:31:04,573 说话人 SPEAKER_06：我们也不会交到朋友。
313 00:31:04,693 --> 00:31:08,859 说话人 SPEAKER_06：我们也不会获得地位和我们所想要的一切。
314 00:31:08,881 --> 00:31:10,583 说话人 SPEAKER_06：甚至我们都不会存活下去。
315 00:31:10,784 --> 00:31:14,609 演讲者 SPEAKER_06：这就是进化再次发挥作用的地方。
316 00:31:14,589 --> 00:31:23,105 演讲者 SPEAKER_06：所以我认为我们可以，当然，与并改变我们内心深处根深蒂固的东西。
317 00:31:23,987 --> 00:31:33,965 演讲者 SPEAKER_06：一旦我们将人工智能引入世界，这也会改变我们以及我们的规范和我们在世界中的移动和行动方式。
318 00:31:33,945 --> 00:31:42,718 演讲者 SPEAKER_06：这就是我们开始担忧的地方，因为那时我们需要真正表达我们与这些系统互动时的感受。
319 00:31:43,459 --> 00:31:53,092 讲者 SPEAKER_06: 因此，我们需要超越我们内心根深蒂固的习惯和规范，因为人工智能只会复制这些。
320 00:31:53,393 --> 00:31:57,337 讲者 SPEAKER_06: 我们将会得到具有种族歧视的人工智能，因为我们是有种族歧视的。
321 00:31:57,317 --> 00:32:01,042 讲者 SPEAKER_06: 所以我们需要弄清楚为什么这会让我们感到不安？
322 00:32:01,143 --> 00:32:05,189 讲者 SPEAKER_06: 是什么让我们从身体上感到不安？
323 00:32:06,049 --> 00:32:08,413 说话人 SPEAKER_06：但是，关于这个没有身体的人工智能。
324 00:32:09,275 --> 00:32:17,747 说话人 SPEAKER_06：所以，我想说的是，对我来说，AI 和伦理不仅仅是程序性的。
325 00:32:17,787 --> 00:32:19,689 说话人 SPEAKER_06：它不仅仅是法律问题。
326 00:32:19,709 --> 00:32:22,953 说话人 SPEAKER_06：它不仅仅是政策问题。
327 00:32:22,933 --> 00:32:25,337 讲者 SPEAKER_06：它也是一种感觉过程。
328 00:32:25,397 --> 00:32:26,880 讲者 SPEAKER_06：它是一种身体过程。
329 00:32:26,940 --> 00:32:33,088 讲者 SPEAKER_06：它是一种规范过程，您通过自己的身体、通过您的肉体来实现。
330 00:32:34,150 --> 00:32:38,155 讲者 SPEAKER_06：而这就是我作为一个设计师的空间所在。
331 00:32:38,215 --> 00:32:45,386 讲者 SPEAKER_06：很高兴在今天早些时候的讨论中听到，有人呼吁设计师参与到 AI 设计的流程中来。
332 00:32:46,307 --> 00:32:47,490 讲者 SPEAKER_06：所以...
333 00:32:47,722 --> 00:33:00,036 讲者 SPEAKER_06：我想提出一种针对 AI 系统设计过程中的女性主义、身体感、有感知的关怀伦理。
334 00:33:00,876 --> 00:33:02,758 讲者 SPEAKER_06：很高兴看到你没有死。
335 00:33:03,439 --> 00:33:04,059 演讲者 SPEAKER_06：这很棒。
336 00:33:04,740 --> 00:33:11,728 演讲者 SPEAKER_06：显然，你们有良好的习惯，因为你们都是高地位的人，你们都穿着得体，我敢肯定你们赚了很多钱。
337 00:33:12,848 --> 00:33:13,950 演讲者 SPEAKER_06：所以...
338 00:33:15,634 --> 00:33:17,037 演讲者 SPEAKER_06：这就是我们现在所在的地方，对吧？
339 00:33:17,577 --> 00:33:21,403 讲者 SPEAKER_06: 这是将要塑造这一切的人群和你们的身体。
340 00:33:22,002 --> 00:33:26,809 讲者 SPEAKER_06: 你们需要意识到这一点如何在你们的身体自我中体现。
341 00:33:26,829 --> 00:33:27,470 讲者 SPEAKER_06: 所以，谢谢。
342 00:33:32,237 --> 00:33:34,960 讲者 SPEAKER_07: 请留下，请留下。
343 00:33:34,980 --> 00:33:35,320 未知说话者：好吧，好吧，待在原地。
344 00:33:35,340 --> 00:33:35,881 说话者 SPEAKER_07：我要请大家站起来。
345 00:33:35,901 --> 00:33:37,502 说话者 SPEAKER_07：太棒了，非常感谢。
346 00:33:37,584 --> 00:33:44,071 说话者 SPEAKER_07：请各位演讲者，还有 Hinton 教授也请现在上台。
347 00:33:45,924 --> 00:33:50,569 讲者 SPEAKER_07：嗯，这是个开始，实际上这里的感受。
348 00:33:50,589 --> 00:33:58,999 讲者 SPEAKER_07：我只是想开始回到，我的意思是，我们现在已经深入挖掘某些领域了。
349 00:33:59,058 --> 00:34:04,045 讲者 SPEAKER_07：我想，可能很多人对你很感兴趣。
350 00:34:04,545 --> 00:34:06,027 讲者 SPEAKER_07：而你并没有太多地谈论自己。
351 00:34:06,047 --> 00:34:15,476 讲者 SPEAKER_07：所以我将回过头来问您一两个问题，比如您从事这项研究已经几十年了。
352 00:34:15,456 --> 00:34:18,181 讲者 SPEAKER_07：您是如何一直坚持下来的？
353 00:34:18,222 --> 00:34:25,876 讲者 SPEAKER_07：您实际上将不同的领域汇集在一起，并且长期探索，以找到您想要结合的领域。
354 00:34:26,217 --> 00:34:28,621 讲者 SPEAKER_07：您是如何一直坚持下来的？
355 00:34:29,411 --> 00:34:31,916 说话人 SPEAKER_09：我想知道大脑是如何工作的。
356 00:34:32,858 --> 00:34:35,021 说话人 SPEAKER_09：而且我还没有弄明白。
357 00:34:36,784 --> 00:34:41,391 说话人 SPEAKER_09：你知道，我一直想获得诺贝尔生理学或医学奖，以解开大脑工作的秘密。
358 00:34:41,913 --> 00:34:43,675 说话人 SPEAKER_09：我有一个叫做玻尔兹曼机的理论。
359 00:34:43,775 --> 00:34:48,985 讲者 SPEAKER_09：我和 Terry Sinoski 认为大脑就是这样工作的，我们本可以一起获得诺贝尔奖。
360 00:34:49,385 --> 00:34:52,530 讲者 SPEAKER_09：我们实际上有一个协议，如果其中一个人得到了它而另一个人没有，我们会分享。
361 00:34:52,972 --> 00:34:55,014 讲者 SPEAKER_09：所以我已经和他分享了。
362 00:34:55,467 --> 00:35:05,391 讲者 SPEAKER_09：我们没有意识到的是，我们有一个关于大脑工作原理的理论，但它可能是错误的，所以我们不会获得生理学或医学诺贝尔奖，但你仍然可以获得物理学诺贝尔奖。
363 00:35:09,440 --> 00:35:10,623 讲者 SPEAKER_10: 实际上，你还可以继续努力争取诺贝尔奖。
364 00:35:14,164 --> 00:35:21,282 讲者 SPEAKER_07: 所以，据我所知，好奇心还在，而且还有那个……还有一件事对我帮助很大。
365 00:35:22,023 --> 00:35:29,963 所以，在90年代，特别是在计算机科学领域，不是在心理学领域，而是在计算机科学领域，
366 00:35:29,943 --> 00:35:31,947 讲者 SPEAKER_09: 几乎每个人都说这些是垃圾。
367 00:35:32,307 --> 00:35:33,710 讲者 SPEAKER_09：他们 70 年代也这么说。
368 00:35:33,971 --> 00:35:37,438 讲者 SPEAKER_09：所以 70 年代和 90 年代，他们认为这些只是胡说。
369 00:35:37,778 --> 00:35:38,501 讲者 SPEAKER_09：这永远不会成功的。
370 00:35:38,521 --> 00:35:46,838 讲者 SPEAKER_09：整个想法是，你可以用一个模拟的神经元网络，而这些神经元本身也不太像真正的神经元，你可以
371 00:35:46,818 --> 00:35:51,804 讲者 SPEAKER_09：随机建立连接，然后你只需观察数据就能让它做智能的事情。
372 00:35:51,824 --> 00:35:52,766 讲者 SPEAKER_09：这太荒谬了。
373 00:35:53,266 --> 00:35:57,172 讲者 SPEAKER_09：你必须内置很多先天的结构，才能让它做任何事情。
374 00:35:57,954 --> 00:35:59,735 讲者 SPEAKER_09：这并不是一个不合理的位置。
375 00:36:01,539 --> 00:36:06,264 讲者 SPEAKER_09：他们说，如果你尝试通过梯度下降来训练它，它可能会陷入局部最优。
376 00:36:06,806 --> 00:36:11,813 讲者 SPEAKER_09：他们实际上从未检查过这是否会发生，但他们确信会发生。
377 00:36:13,344 --> 00:36:18,211 讲者 SPEAKER_09：所以问题是，当周围的人都认为你所做的是垃圾时，你该如何继续下去？
378 00:36:19,153 --> 00:36:24,882 讲者 SPEAKER_09：对我来说，这很简单，因为我的父母都是无神论者。
379 00:36:25,543 --> 00:36:29,528 讲者 SPEAKER_09：当我七岁的时候，他们把我送到一所基督教私立学校。
380 00:36:30,990 --> 00:36:36,719 讲者 SPEAKER_09：所以这里有很多其他信奉上帝的孩子和所有信奉上帝的老师，这显然是胡说。
381 00:36:37,730 --> 00:36:39,655 讲者 SPEAKER_09：我的意思是，他们说的那些东西太荒谬了。
382 00:36:40,416 --> 00:36:41,639 讲者 SPEAKER_09：当然，随着时间的推移，这也有所改变。
383 00:36:41,659 --> 00:36:49,257 讲者 SPEAKER_09：当我们小的时候，它是一个像米开朗基罗画的那样被云层遮蔽的古老白人。
384 00:36:49,610 --> 00:36:51,813 讲者 SPEAKER_09：当我们长大一些，它就变得有些晦涩。
385 00:36:53,074 --> 00:36:55,737 讲者 SPEAKER_09：但对我来说，这些似乎总是毫无意义。
386 00:36:56,619 --> 00:37:04,969 讲者 SPEAKER_09：所以，我从很小的时候就有这样的经历，看到的东西在我看来似乎是毫无意义的，结果证明确实是毫无意义的。
387 00:37:05,909 --> 00:37:08,112 讲者 SPEAKER_09：这真的很适用于做神经网络。
388 00:37:11,856 --> 00:37:13,099 讲者 SPEAKER_07：那么你们是如何获得资金的？
389 00:37:13,358 --> 00:37:16,663 讲者 SPEAKER_07：因为这是一个棘手的问题。
390 00:37:17,199 --> 00:37:17,760 讲者 SPEAKER_09：是的。
在加拿大，他们给人们提供资金进行基础研究，虽然钱不多，但他们用得很有效率，这笔钱，所以他们给你提供五年期的拨款，五年结束时，你必须写一份六页的报告，说明你做了什么，不必是你提案中说的那些事情。
这是一个为资助基础研究提供极好方式的方法。
393 00：37：42,918 --> 00：37：47,188 发言者 SPEAKER_07：我想请小组讨论这个问题，因为我看到很多人都在点头。
我认为这是我的老教授经常说的一个推论，几乎就是什么定义一个好的研究者是卓越的应用和出色的成果，但无需相关性。
395 00:37:59,184 --> 00:38:05,932 讲者 SPEAKER_07：我希望在场的科研资助者能好好记住这一点。
396 00:38:05,974 --> 00:38:15,244 讲者 SPEAKER_07：我们确实把学术界分割成很多孤岛，而深入的基础研究往往就存在于这些孤岛中。
397 00:38:15,565 --> 00:38:27,280 讲者 SPEAKER_07：我想知道你是否愿意稍微反思一下，我的意思是，这非常跨学科，我们现在正朝着这种复杂性前进，这是否需要更多的多学科或跨学科工作？
398 00:38:27,300 --> 00:38:28,601 讲者 SPEAKER_07：你愿意反思这一点吗？
399 00:38:28,581 --> 00:38:29,684 讲者 SPEAKER_06: 当然了。
400 00:38:29,744 --> 00:38:38,320 至少对我来说，我的研究小组里有歌剧歌手，还有像我这样的 AI 人士坐在那里。
401 00:38:39,001 --> 00:38:46,876 我们还有硬件人员，软件人员，还有工业设计师等等。
402 00:38:46,856 --> 00:38:56,333 所以，当然，如果你要设计系统中的智能或智能行为，那么你需要。
403 00:38:56,353 --> 00:39:08,594 讲者 SPEAKER_06: 我认为越来越紧迫的是人文、伦理以及关注使用这些技术过上美好生活意味着什么。
404 00:39:08,574 --> 00:39:19,644 讲者 SPEAKER_07: 我想这是给你的提示，安德斯，因为你刚从澳大利亚来，澳大利亚关于人文科学资金削减的争论很大，不是吗？
405 00:39:20,166 --> 00:39:20,465 讲者 SPEAKER_07: 哦，是的。
406 00:39:21,086 --> 00:39:23,449 讲者 SPEAKER_08: 当然，这始终是一个问题。
407 00:39:23,489 --> 00:39:25,411 讲者 SPEAKER_08：资金从哪里来？
408 00:39:25,710 --> 00:39:27,112 讲者 SPEAKER_08：但这也是一个问题。
409 00:39:27,452 --> 00:39:31,056 讲者 SPEAKER_08：这些孤岛实际上是否对应着重要的问题？
410 00:39:31,175 --> 00:39:33,938 讲者 SPEAKER_08：而现实并不符合学术孤岛。
411 00:39:34,259 --> 00:39:38,143 讲者 SPEAKER_08：如果学术壁垒能够以某种方式映射现实，那将是非常惊人的。
412 00:39:38,123 --> 00:39:39,686 讲者 SPEAKER_08：但我们并没有尝试这样做。
413 00:39:39,746 --> 00:39:43,195 讲者 SPEAKER_08：我们最终形成了这些壁垒，这是历史原因造成的。
414 00:39:43,717 --> 00:39:47,688 讲者 SPEAKER_08：有时它们偶然地与真正重要的事物相对应。
415 00:39:48,449 --> 00:39:53,583 讲者 SPEAKER_08: 但通常有趣的新结果出现在你将某个领域的结果应用到其他地方时。
416 00:39:53,563 --> 00:39:54,706 讲者 SPEAKER_08: 然后将其应用于其他地方。
417 00:39:54,826 --> 00:39:57,509 讲者 SPEAKER_08: 事实上，这正是我在澳大利亚的计划。
418 00:39:57,528 --> 00:40:04,338 讲者 SPEAKER_08: 我和一位老朋友一起工作，他原本是物理学家和生物学家，从事胶体科学。
419 00:40:04,579 --> 00:40:08,963 讲者 SPEAKER_08：没有人记得这一点，因为他还发现了将饼干放入茶中浸泡的最佳角度。
420 00:40:09,204 --> 00:40:12,148 讲者 SPEAKER_08：所以现在他永远以饼干浸泡教授而闻名。
421 00:40:12,789 --> 00:40:20,820 讲者 SPEAKER_08：但关键部分在于我们正在聚集团队，试图学习我们如何更好地进行跨学科工作？
422 00:40:20,800 --> 00:40:22,762 讲者 SPEAKER_08：因为我们在这方面没有系统化。
423 00:40:23,463 --> 00:40:25,606 讲者 SPEAKER_08：大家都说我们希望跨学科。
424 00:40:25,907 --> 00:40:29,570 讲者 SPEAKER_08：也许资助者也是这样说的，但他们并没有给跨学科研究太多资金。
425 00:40:29,911 --> 00:40:33,096 讲者 SPEAKER_08：即使我们试图跨学科，我们也不是很擅长。
426 00:40:33,115 --> 00:40:33,936 讲者 SPEAKER_08：我们应该变得更好。
427 00:40:34,277 --> 00:40:35,679 讲者 SPEAKER_08：最佳角度是多少？
428 00:40:37,001 --> 00:40:42,487 讲者 SPEAKER_08：如果你用正常的英式红茶，大约是 15 度。
429 00:40:44,340 --> 00:40:45,606 讲者 SPEAKER_07：你有没有？
430 00:40:45,626 --> 00:40:53,963 讲者 SPEAKER_04：是的，我认为需要跨学科的一个原因之一是，我认为今天乃至未来最大的挑战之一，我愿意称之为交接问题。
431 00:40:54,313 --> 00:40:57,077 说话人 SPEAKER_04: 我认为，你知道，我们将会有 AI 与人类一起工作。
432 00:40:57,838 --> 00:41:11,054 说话人 SPEAKER_04: 如果你想到自动驾驶汽车，这显然是显而易见的，因为大家都说，好吧，所以汽车会自己驾驶，直到它无法处理情况，然后它将把控制权交给驾驶员，你知道，那时驾驶员可能正在睡觉。
433 00:41:11,213 --> 00:41:17,300 说话人 SPEAKER_04: 但是机器如何向人类传达其对世界状况的看法呢？
434 00:41:17,280 --> 00:41:20,148 说话人 SPEAKER_04: 这适用于所有情况，我们在与威胁分析师合作时看到了这一点。
435 00:41:20,208 --> 00:41:27,867 讲者 SPEAKER_04：你如何对某个算法进行分析，并将信息传达给人类，以便人类可以继续进行或证明它是正确或错误的？
436 00:41:28,369 --> 00:41:34,885 讲者 SPEAKER_04：为此，显然需要设计师，需要来自各个领域的人来构建这些系统。
437 00:41:35,032 --> 00:41:37,217 讲者 SPEAKER_07：是的，界面的关键问题。
438 00:41:38,019 --> 00:41:39,322 讲者 SPEAKER_06：我们实际上是为这一点而设计的。
439 00:41:40,465 --> 00:41:48,826 说话人 SPEAKER_06：在我们的实验室里，我们与自动驾驶汽车合作，设计了一种通过充气垫给你一点推力的靠背，让你醒来。
440 00:41:48,847 --> 00:41:51,893 说话人 SPEAKER_04：你还有三秒钟的时间来理解你为什么被叫醒吗？
441 00:41:51,994 --> 00:41:53,177 说话人 SPEAKER_07：有时候，有时候。
442 00:41:54,001 --> 00:41:58,324 说话人 SPEAKER_07：但我觉得我们需要回到风险问题上来。
443 00:41:58,344 --> 00:42:02,027 说话人 SPEAKER_07：自从你离开谷歌以来，你在这方面越来越直言不讳。
444 00:42:02,228 --> 00:42:12,478 说话人 SPEAKER_07：既然你在这里是技术乐观主义者，所以我们不必真的只谈论风险，我们需要谈论机遇。
445 00:42:12,498 --> 00:42:14,760 说话人 SPEAKER_07：我想在座的所有人都知道机遇，都能看到它们。
446 00:42:15,340 --> 00:42:18,923 说话人 SPEAKER_07：但你刚才在开场白中说过，超级智能即将到来。
447 00:42:18,943 --> 00:42:19,905 说话人 SPEAKER_07：你真的相信吗？
448 00:42:20,025 --> 00:42:22,927 说话人 SPEAKER_07：是在五年或三十年内，类似这样的？
449 00:42:22,907 --> 00:42:29,378 说话人 SPEAKER_09：我的信念是，有 50%的概率它将在 5 到 20 年内出现。
450 00:42:29,820 --> 00:42:33,306 说话人 SPEAKER_09：除了那是我一年前的信念，所以我最好说是在 4 到 19 年之间。
451 00:42:33,326 --> 00:42:36,170 说话人 SPEAKER_09：听起来就是不好。
452 00:42:37,213 --> 00:42:37,813 说话人 SPEAKER_07：然后呢？
453 00:42:37,873 --> 00:42:39,836 说话人 SPEAKER_07：因为当它实际上
454 00:42:40,289 --> 00:42:42,130 说话人 SPEAKER_07：是超人类智能的时候。
455 00:42:42,150 --> 00:42:43,692 讲者 SPEAKER_09：我想几乎所有人都来了。
456 00:42:44,492 --> 00:42:46,114 讲者 SPEAKER_09：他们只是对时间长短有不同看法。
457 00:42:46,353 --> 00:42:52,400 讲者 SPEAKER_09：不相信它即将到来的人是那些相信经典语言学和符号人工智能的随机海盗派。
458 00:42:52,619 --> 00:42:53,119 讲者 SPEAKER_09：绝对是这样。
459 00:42:53,641 --> 00:42:58,885 讲者 SPEAKER_09: 但所有了解神经网络的人，我认为他们都认为这是即将到来的。
460 00:43:00,827 --> 00:43:07,213 讲者 SPEAKER_07: 那么，如果你说这是一个存在性威胁，你看到了什么？
461 00:43:07,293 --> 00:43:09,974 讲者 SPEAKER_07: 你在你的视觉中看到了什么，最黑暗的一面？
462 00:43:10,681 --> 00:43:13,463 讲者 SPEAKER_09: 哦，人们变得无关紧要。
2023年初让我担忧的是，长期以来都很明显的事情，但我还没有完全感受到它的情感影响，那就是数字智能可能只是比我们现有的智能更好的形式。
464 00：43：27,021 --> 00：43：28,121 议长 SPEAKER_09：我们基本上是模拟的。
我们实际上是有点像一位数字，但基本上是模拟的。
466 00:43:32,686 --> 00:43:40,177 演讲者 SPEAKER_09：如果你是模拟的，如果你学会了利用模拟硬件的独特特性，你可以用非常低的功耗做事情。
467 00:43:40,978 --> 00:43:43,961 说话人 SPEAKER_09：所以你不能让两个模拟计算机做到完全一样。
468 00:43:44,583 --> 00:43:47,807 说话人 SPEAKER_09：但如果每个都学会了，它们仍然可以做得很好。
469 00:43:47,967 --> 00:43:49,010 说话人 SPEAKER_09：这就是大脑的样子。
470 00:43:49,550 --> 00:43:56,280 讲者 SPEAKER_09：我不能与你共享权重，因为我的神经元和你的神经元之间没有一一对应关系。
471 00:43:57,373 --> 00:44:00,137 讲者 SPEAKER_09：所以你可以非常低功耗，但是不能共享。
472 00:44:00,197 --> 00:44:05,244 讲者 SPEAKER_09：所以我们不能让 1000 个人去上 1000 个不同的课程。
473 00:44:05,786 --> 00:44:09,672 讲者 SPEAKER_09：当他们上课时，他们会将权重变化平均在一起。
474 00:44:10,313 --> 00:44:13,878 讲者 SPEAKER_09：等到他们都完成自己的课程后，每个人只上一门课。
475 00:44:14,338 --> 00:44:15,800 讲者 SPEAKER_09：但在幕后，他们正在平均化权重。
476 00:44:16,862 --> 00:44:19,786 讲者 SPEAKER_09：最后，他们都了解所有 1,000 门课程的内容。
477 00:44:20,648 --> 00:44:22,851 讲者 SPEAKER_09：这正是这些数字智能能够做到的。
478 00:44:23,331 --> 00:44:25,795 讲者 SPEAKER_09：所以他们有共享带宽。
479 00:44:25,775 --> 00:44:29,340 讲者 SPEAKER_09：与权重数量的数量级相当，所以可以共享万亿比特。
480 00:44:30,001 --> 00:44:42,898 讲者 SPEAKER_09：我和你，我们分享的方式是我产生一个句子，你尝试改变你的突触强度，你的大脑会这样做，这样你就会倾向于说出相同的话，如果你信任我的话。
481 00:44:43,579 --> 00:44:53,875 讲者 SPEAKER_09：这是以每秒几比特的带宽进行共享，而不是以每秒几分之一秒的万亿比特进行共享。
482 00:44:54,293 --> 00:45:01,625 讲者 SPEAKER_09：从这个意义上说，它们在获取大量知识和看到所有这些知识片段之间的关系方面要好得多。
483 00:45:02,126 --> 00:45:11,041 讲者 SPEAKER_09：所以，为了只在大约一兆个突触中存储所有知识，它们必须进行大量的压缩。
484 00:45:12,123 --> 00:45:19,255 讲者 SPEAKER_09：而压缩意味着找到许多事物共有的部分，并使用它们共有的部分来存储。
485 00:45:19,588 --> 00:45:21,791 讲者 SPEAKER_09：所以，您通过共同的比特加上一些差异来存储它们。
486 00:45:22,853 --> 00:45:27,398 演讲者 SPEAKER_09：这意味着他们看到了人们从未见过的远程领域之间的各种类比。
487 00:45:28,219 --> 00:45:30,541 演讲者 SPEAKER_09：所以我认为他们将会比我们更有创造力。
488 00:45:31,882 --> 00:45:35,847 演讲者 SPEAKER_09：这有点令人担忧。
489 00:45:38,931 --> 00:45:40,012 演讲者 SPEAKER_04：令人毛骨悚然地迷人。
490 00:45:40,032 --> 00:45:41,313 演讲者 SPEAKER_07: 是的，非常迷人。
491 00:45:41,333 --> 00:45:42,175 演讲者 SPEAKER_04: 谢谢你这么说。
492 00:45:42,195 --> 00:45:42,735 演讲者 SPEAKER_04: 是的，请继续。
493 00:45:42,775 --> 00:45:46,659 演讲者 SPEAKER_04: 关于这一点，我的意思是，我认为我们今天都在努力
494 00:45:46,706 --> 00:45:52,967 讲者 SPEAKER_04：我们今天使用的模型，它们在人类意义上非常像，即使不知道事情也会喋喋不休。
495 00:45:53,768 --> 00:46:00,429 讲者 SPEAKER_04：所以 Wendy，你认为模型真正意识到自己不知道的事情还有多远？
496 00:46:00,831 --> 00:46:02,896 讲者 SPEAKER_09：它们在这方面会变得更好。
497 00:46:02,916 --> 00:46:04,340 讲者 SPEAKER_09：我认为它们已经变得有思想了。
他们在这方面变得更好，并且会逐步改进。但他们仍然不擅长让人们意识到他们在进行心理操纵，但他们是否会停止这样做则是另一个问题。
499 00：46：16,628 --> 00：46：18,452 议长 SPEAKER_06：没有死亡
500 00:46:18,431 --> 00:46:20,195 演讲者 演讲者_06：对人工智能来说，对吧？
501 00：46：20,356 --> 00：46：25,786 议长 SPEAKER_06：所以他们没有真正的理由以某种方式行事或不以某种方式行事。
502 00:46:26,025 --> 00:46:28,230 演讲者 SPEAKER_06: 他们的实体还没有形成。
503 00:46:28,291 --> 00:46:29,753 演讲者 SPEAKER_06: 他们还没有进入世界。
504 00:46:29,773 --> 00:46:31,577 演讲者 SPEAKER_06: 所以风险并不存在。
505 00:46:32,018 --> 00:46:34,141 演讲者 SPEAKER_06: 他们不会失去一条手臂。
506 00:46:34,181 --> 00:46:37,588 说话人 SPEAKER_09: 他们可能会被砍掉数据中心。
507 00:46:37,568 --> 00:46:38,208 说话人 SPEAKER_06: 他们可能会这么做。
508 00:46:40,472 --> 00:46:44,639 说话人 SPEAKER_07: 当数据中心关闭时，他们可能受到幽灵的伤害。
509 00:46:44,659 --> 00:46:49,969 说话人 SPEAKER_09: 但我认为你会看到你想要的东西，即具身人工智能，是在战斗机器人中。
510 00:46:49,989 --> 00:46:51,612 说话人 SPEAKER_09: 他们将会有像杏仁核这样的东西。
511 00:46:51,813 --> 00:46:55,920 说话人 SPEAKER_09: 如果你是一个小战斗机器人，看到一个大战斗机器人，你最好逃跑并躲藏起来。
512 00:46:56,581 --> 00:46:57,563 说话人 SPEAKER_09: 他们将会有所有这些。
513 00:46:57,902 --> 00:47:00,166 说话人 SPEAKER_09: 因此我认为他们会有真正的恐惧。
514 00:47:00,974 --> 00:47:03,677 说话人 SPEAKER_09：这不会是一种模拟的数字恐惧。
515 00:47:04,119 --> 00:47:07,804 说话人 SPEAKER_09：这个小战斗机器人会害怕这个大战斗机器人。
516 00:47:08,045 --> 00:47:09,969 说话人 SPEAKER_09：所以我认为你们会看到所有这些具身化的东西。
517 00:47:11,592 --> 00:47:12,492 说话人 SPEAKER_06：一旦他们离开。
518 00:47:12,773 --> 00:47:14,556 说话人 SPEAKER_09：一旦它们在现实世界中行动。
519 00:47:14,677 --> 00:47:15,978 说话人 SPEAKER_06：而且会有后果。
520 00:47:16,480 --> 00:47:18,382 说话人 SPEAKER_09：我认为并不是因为它们可以死亡。
521 00:47:18,443 --> 00:47:22,128 说话人 SPEAKER_09：我只是认为是因为它们需要所有这些情感因素。
522 00:47:22,650 --> 00:47:23,150 说话人 SPEAKER_09：自我保存。
523 00:47:23,170 --> 00:47:24,193 说话人 SPEAKER_09：为了在世界上生存。
524 00:47:24,313 --> 00:47:26,896 说话人 SPEAKER_07：所以自我保存实际上是在具身中。
525 00:47:26,876 --> 00:47:33,246 说话人 SPEAKER_08：确实，许多强化学习系统似乎需要类似于情感的东西。
526 00:47:33,867 --> 00:47:39,275 讲者 SPEAKER_08：在这种情况下，从数学上讲，你试图最大化未来的预期奖励。
527 00:47:39,635 --> 00:47:43,380 讲者 SPEAKER_08：但很多时候，训练过程中你只有有限的时间来做这件事。
528 00:47:43,400 --> 00:47:46,545 讲者 SPEAKER_08：所以它们实际上，在某种程度上可能会感到压力。
529 00:47:46,565 --> 00:47:48,608 讲者 SPEAKER_08：有些事情比其他事情更糟糕。
530 00:47:49,028 --> 00:47:55,617 演讲者 SPEAKER_08：我认为可以提出这样的观点，我的哲学同行中有些人认为这些就像是基础情感。
531 00:47:55,597 --> 00:48:01,748 演讲者 SPEAKER_08：也许它们不如生物学中的那些优雅和复杂，但我们的目的还是一样的。
532 00:48:01,768 --> 00:48:06,976 演讲者 SPEAKER_08：当预期的奖励没有到来时感到失望是一个非常重要的学习信号。
533 00:48:07,077 --> 00:48:10,563 演讲者 SPEAKER_08：我们都在机器人和人类中看到了这一点。
534 00:48:10,583 --> 00:48:13,387 说话人 SPEAKER_08：只是机器人可能对此更加冷静。
535 00:48:15,324 --> 00:48:21,791 说话人 SPEAKER_09：我想就你提到的关于尝试检测虚假视频的 AI 系统说点东西。
536 00:48:22,853 --> 00:48:24,454 说话人 SPEAKER_09：我以前认为这是个好主意。
537 00:48:25,076 --> 00:48:32,163 说话人 SPEAKER_09：但现在我觉得这几乎是绝望的，因为检测器和生成器之间的军备竞赛。
538 00:48:32,643 --> 00:48:35,467 说话人 SPEAKER_09：这就是所谓的生成对抗网络。
539 00:48:35,726 --> 00:48:40,152 说话人 SPEAKER_09：在扩散模型出现之前，他们就是这样制作好的图像生成器。
540 00:48:40,132 --> 00:48:46,588 说话人 SPEAKER_09：我认为现在人们已经转向认为检测假视频基本上是无望的。
541 00:48:46,909 --> 00:48:50,117 说话人 SPEAKER_09：你需要做的是检测一个视频不是假的。
542 00:48:50,679 --> 00:48:54,568 说话人 SPEAKER_09：所以你需要有一种方法来检查视频的来源。
543 00:48:54,929 --> 00:48:56,875 说话人 SPEAKER_09：比如，对于一个政治视频，
544 00:48:57,260 --> 00:49:02,686 说话人 SPEAKER_09：你需要能够从该视频找到政治活动的网站。
545 00:49:02,746 --> 00:49:13,356 说话人 SPEAKER_09：如果你在网站上找到了相同的视频，并且你确信该网站属于该活动，当然网站是唯一的，所以这并不难，那么你可以相信它。
546 00:49:13,898 --> 00:49:15,478 讲者 SPEAKER_09：如果你不理解，你就无法相信。
547 00:49:16,219 --> 00:49:18,422 讲者 SPEAKER_09：最终你的浏览器可以做到几乎全部这些。
548 00:49:18,943 --> 00:49:24,768 讲者 SPEAKER_09：所以你的浏览器，就像现在你收到垃圾邮件时一样，如果你有一个好的浏览器，它会告诉你这很可能是垃圾邮件。
549 00:49:25,103 --> 00:49:33,251 讲者 SPEAKER_09：你应该能够获取到浏览器显示这是哈里斯竞选活动的视频，但实际上并不是的视频。
550 00:49:33,632 --> 00:49:36,675 讲者 SPEAKER_09：因为它可以检查网站并看到那里没有相同的视频。
551 00:49:37,615 --> 00:49:44,083 讲者 SPEAKER_09：我认为有了来源，我们将能更好地知道事物是真实的。
552 00:49:44,382 --> 00:49:46,184 讲者 SPEAKER_09：报纸们喜欢这个想法。
553 00:49:46,224 --> 00:49:51,329 讲者 SPEAKER_09：纽约时报喜欢这个想法，唯一可以信赖的就是纽约时报。
我完全同意这一点，但我认为问题在于你需要做得比仅仅认证浏览器和服务器更进一步。
555 00：50：01,554 --> 00：50：05,601 演讲者 SPEAKER_04：我的意思是，基本上，我们需要将所有内容重新连接到设备。
我意思是，你需要知道是哪台相机在那个特定地点拍摄了那张照片。
557 00:50:11,273 --> 00:50:12,335 演讲者 演讲者_04：这并不是什么火箭科学。
558 00:50:12,355 --> 00:50:16,963 讲者 SPEAKER_04: 我的意思是，这本质上是一种可用的技术，但你必须重新思考如何使用互联网。
559 00:50:16,943 --> 00:50:18,125 讲者 SPEAKER_04: 如果你喜欢的话，就是做互联网。
560 00:50:18,144 --> 00:50:21,050 讲者 SPEAKER_08: 但你关注的是溯源，能够建立溯源。
561 00:50:21,070 --> 00:50:21,952 讲者 SPEAKER_08: 正确。
562 00:50:21,972 --> 00:50:26,280 演讲者 SPEAKER_08：我认为这实际上以一种有趣的方式将你的观点联系起来。
563 00:50:26,420 --> 00:50:32,612 讲者 SPEAKER_08：当我想到在科学中使用 AI 时，很明显的事情就是，哦，让它阅读所有科学文献。
564 00:50:32,893 --> 00:50:35,237 讲者 SPEAKER_08：但大部分科学文献都是错误的。
565 00:50:35,759 --> 00:50:38,664 讲者 SPEAKER_08：其中一些是假的，很多都是质量很差的论文。
566 00:50:38,644 --> 00:50:46,452 讲者 SPEAKER_08：为了让 AI 在科学中真正学到有用的东西，它可能需要进入实验室并能够进行实验。
567 00:50:46,472 --> 00:50:47,452 讲者 SPEAKER_08：这就是来源。
568 00:50:47,472 --> 00:50:51,315 讲者 SPEAKER_08：你得到一个基础，如果我做这个实验，这实际上会发生。
569 00:50:51,396 --> 00:50:59,184 讲者 SPEAKER_08：这大概需要一种形式的存在，无论是实验室机器人还是传感器，你需要从现实中获得反馈。
570 00:51:00,063 --> 00:51:06,389 演讲者 SPEAKER_08：我认为这对于使人工智能能够以有益的方式超越我们也是相当必要的。
571 00:51:06,639 --> 00:51:19,391 演讲者 SPEAKER_06：鉴于 Annette 实际上是一名记者，让我们记住，就在不久前，我们没有任何照片或视频，很难相信信息。
572 00:51:20,052 --> 00:51:22,438 演讲者 SPEAKER_06：所以 200 年前他们就在我们现在的地方。
573 00:51:22,418 --> 00:51:28,204 演讲者 SPEAKER_06：然后在这段历史中，能够信任视频或照片的只是一段短暂的瞬间。
574 00:51:28,724 --> 00:51:35,351 说话人 SPEAKER_06：现在我们又回到了如何以我们信任的方式在人们之间传递新闻和其他信息的问题。
575 00:51:35,550 --> 00:51:40,215 说话人 SPEAKER_06：我觉得有时候人工智能的发展有点缺乏历史感。
576 00:51:42,277 --> 00:51:47,902 说话人 SPEAKER_09：我认为在英国，大约两百年前，有关于政治小册子的法律。
577 00:51:48,101 --> 00:51:53,951 说话人 SPEAKER_09：如果你制作政治小册子，你必须在小册子上注明印刷者的名字。
578 00:51:55,713 --> 00:51:59,340 讲者 SPEAKER_09：因此，你获得了一种来源，因为瓶颈是印刷机。
579 00:51:59,940 --> 00:52:04,288 讲者 SPEAKER_09：如果上面有印刷者的名字，伪造起来就困难多了。
580 00:52:04,538 --> 00:52:06,760 讲者 SPEAKER_09：我们现在需要再次做到这一点。
581 00:52:06,780 --> 00:52:12,650 讲者 SPEAKER_06：是的，为什么我们创建手机拍照的可能性时没有立刻这么做？
很多年前，我们在这里 IWA 被服务过，实际上也问过这个问题，你知道的，如果你要从头开始重新设计互联网，你会怎么做？
583 00：52：21,324 --> 00：52：22,284 议长 SPEAKER_04：他就是这么说的。
584 00:52:22,706 --> 00:52:25,230 演讲者 演讲者_04：真实性及验证能力。
但在那些日子里，互联网上有200人，他们彼此都认识，所以当时并不需要。
586 00:52:31,277 --> 00:52:44,655 讲者 SPEAKER_07：这就是我想结束这部分讨论的地方，因为我认为这全部都是关于，我的意思是，我们现在所处的位置，这是一个本地和模拟的机会，因为当你了解某个人以及它的来源时，你将能够信任。
587 00:52:44,755 --> 00:52:49,342 讲者 SPEAKER_07：所以，可能有一些与身体紧密相关的东西，也许是与当地社区相关的。
588 00:52:49,483 --> 00:52:54,929 讲者 SPEAKER_07：我的意思是，你谈论《纽约时报》，但也许这是小型、非常紧密的媒体的机会。
589 00:52:54,909 --> 00:52:59,302 讲者 SPEAKER_07：公司，我们了解记者，我们了解编辑，也许。
590 00:52:59,563 --> 00:53:07,826 演讲者 SPEAKER_07：我想回到您最初提到的地方，以及完整的风险情景，因为您现在正变得越来越有建设性。
591 00:53:07,865 --> 00:53:10,253 演讲者 SPEAKER_07：您正在解决问题。
592 00:53:10,233 --> 00:53:17,108 演讲者 SPEAKER_07：为了完成关于生存的问题，所以我们有战斗机器人，我们有生存威胁。
593 00:53:17,849 --> 00:53:28,932 演讲者 SPEAKER_07：您如何看待这个问题，我们是否将其构建到技术中，或者这是否是围绕技术出现的东西，我们可以阻止最糟糕的情况发生？
594 00:53:29,115 --> 00:53:30,681 说话人 SPEAKER_09：我们不知道，对吧？
595 00:53:30,820 --> 00:53:35,074 说话人 SPEAKER_09：我的意思是，我们真的不知道如何控制比我们自己更智能的事物。
596 00:53:35,094 --> 00:53:36,097 说话人 SPEAKER_09：我们不知道这是否可能。
597 00:53:37,201 --> 00:53:38,847 说话人 SPEAKER_09：我认为这很可能不可能。
598 00:53:39,509 --> 00:53:42,518 演讲者 SPEAKER_09：我认为我们不会停止人工智能的发展，因为它对许多事情都非常好，而且从中可以赚取很多短期利润。
599 00:53:42,835 --> 00:53:47,179 演讲者 SPEAKER_09：所以在一个资本主义社会中，你不会只是停止它。
600 00:53:47,880 --> 00:53:50,164 演讲者 SPEAKER_09：所以，在资本主义社会中，你不会只是停止它。
601 00:53:50,364 --> 00:53:52,347 演讲者 SPEAKER_09：我没有签署要求减缓发展的请愿书。
602 00:53:52,786 --> 00:53:53,588 说话人 SPEAKER_09：我觉得这太疯狂了。
603 00:53:53,608 --> 00:53:54,389 说话人 SPEAKER_09：我们不会那么做的。
604 00:53:55,090 --> 00:53:59,534 说话人 SPEAKER_09：我们必须面对它将要被开发的事实，我们必须弄清楚，我们能否安全地做到这一点？
605 00:53:59,554 --> 00:54:00,976 说话人 SPEAKER_09：我们不知道我们能否安全地做到这一点。
606 00:54:03,000 --> 00:54:07,425 说话人 SPEAKER_09：我们应该关注如何去做，但我们不知道解决方案会是什么样子。
607 00:54:08,771 --> 00:54:10,373 说话人 SPEAKER_09：我们只希望有一个。
608 00:54:10,434 --> 00:54:14,018 说话人 SPEAKER_09：如果人类因为不费心寻找解决方案而消失，那就太遗憾了。
609 00:54:16,300 --> 00:54:21,126 说话人 SPEAKER_08：所以我参与航空安全社区已经有很长时间了，这让我感到惊讶。
610 00:54:21,146 --> 00:54:30,697 讲者 SPEAKER_08：我在 1990 年代的邮件列表上，当时 Elias Yudkowsky 还没有意识到空气可能存在危险，他当时非常支持即将到来的奇点。
611 00:54:30,717 --> 00:54:34,442 讲者 SPEAKER_08：然后他意识到，哦，我们需要解决一些安全问题。
612 00:54:34,422 --> 00:54:39,773 讲者 SPEAKER_08：然后我们开始着手解决这个问题，哦，这变得越来越困难，也越来越有趣。
613 00:54:39,793 --> 00:54:45,565 讲者 SPEAKER_08：但真正让我充满乐观的是，现在有很多人在做有用的事情。
614 00:54:46,027 --> 00:54:51,077 说话人 SPEAKER_08：我们在检测空气系统中的内部状态，这些状态可以被解读。
615 00:54:51,097 --> 00:54:54,726 讲者 SPEAKER_08：也许并不完美，但我们实际上在更好地理解它。
616 00:54:54,706 --> 00:55:02,974 讲者 SPEAKER_08：我们发现了一些方法来检测它们在欺骗时的情况，尽管从哲学上讲，当它是一个非有意系统时，欺骗相当复杂。
617 00:55:02,994 --> 00:55:05,858 讲者 SPEAKER_08：我们得到了一些工具，我认为其中一些工具可能已经足够好了。
618 00:55:06,498 --> 00:55:09,922 讲者 SPEAKER_08：一些比我更悲观的同事说，不，还不够。
619 00:55:10,284 --> 00:55:12,726 说话人 SPEAKER_08：我认为我们不应该假设这还不够。
620 00:55:12,766 --> 00:55:13,728 说话人 SPEAKER_08：我们应该更加努力。
621 00:55:13,768 --> 00:55:24,460 说话人 SPEAKER_08：但我非常乐观，如果我们真正集中精力去做，这对商业也可能是有益的，因为你想让你的机器遵守法律并表现出道德行为，否则人们会起诉你。
622 00:55:26,818 --> 00:55:36,577 说话人 SPEAKER_04：所以我现在意识到，当我们构建第一个超级智能时，我们首先要给它分配的任务是告诉我们如何保护它或保护我们免受它们的侵害。
623 00:55:37,557 --> 00:55:40,402 说话人 SPEAKER_04：可能有点矛盾，但我不确定。
624 00:55:40,423 --> 00:55:42,927 说话人 SPEAKER_09：这听起来有点像让警察调查警察。
625 00:55:42,967 --> 00:55:45,733 说话人 SPEAKER_04：这从来都不起作用。
626 00:55:45,753 --> 00:55:47,376 说话人 SPEAKER_07：你不信任他们，是吗？
627 00:55:48,166 --> 00:55:50,809 演讲者 SPEAKER_07：他们将非常擅长欺骗，会从我们这里学到这一点。
628 00:55:50,829 --> 00:56:08,132 演讲者 SPEAKER_07：关于这一点，实体和联系，你的研究，我也想到了学习，学习的一部分是社交规范和羞耻感以及类似的东西，当你在进化心理学中，你知道，在儿童初期，有一种非常强烈的引导他们走向正确的手段。
629 00:56:08,192 --> 00:56:10,355 演讲者 SPEAKER_07：那里有什么吗？
630 00:56:10,414 --> 00:56:14,139 演讲者 SPEAKER_07：我们能教会系统在犯错时感到羞耻吗？
631 00:56:14,119 --> 00:56:23,496 讲者 SPEAKER_06: 我相信它们内部确实需要某种情感系统，以便以特定方式进行学习。
632 00:56:23,896 --> 00:56:33,715 讲者 SPEAKER_06: 但最重要的是，我认为这也关乎我们理解，因为它们所依赖的数据来源于我们、自然以及我们所写的内容等。
633 00:56:33,695 --> 00:56:39,063 讲者 SPEAKER_06: 因此，我们需要更清楚地了解我们的感受是什么。
634 00:56:39,463 --> 00:56:49,701 讲者 SPEAKER_06: 所以，如果你在查看一个种族歧视的面部识别系统时感到尴尬，那么你需要从内心深处去寻找原因。
635 00:56:49,780 --> 00:56:51,163 演讲者 SPEAKER_06：这里发生了什么？
636 00:56:51,224 --> 00:56:57,052 演讲者 SPEAKER_06：为什么我会因为站在这个种族歧视的人脸识别系统旁边而感到尴尬？
637 00:56:57,032 --> 00:57:03,119 演讲者 SPEAKER_06：只有当你能阐明问题来源时，好吧，是的，它实际上是种族歧视，这就是我遇到的问题。
638 00:57:03,139 --> 00:57:04,702 演讲者 SPEAKER_06：这是一个简单的问题，对吧？
639 00:57:05,463 --> 00:57:19,418 演讲者 SPEAKER_06：我们有很多其他更困难的伦理问题，但一旦你感觉到这里有什么不对劲，或者这是好事，这里就给了我一种自由，或者有一种可能性，那么你就需要能够表达出来。
640 00:57:19,579 --> 00:57:23,563 演讲者 SPEAKER_06：而且很多这种伦理感
641 00:57:23,543 --> 00:57:34,798 演讲者 SPEAKER_06：并没有被表达出来，它非常依赖于情感和身体动作，这使得表达更加困难，这就是为什么我们需要新的设计方法来做这件事。
642 00:57:35,559 --> 00:57:51,762 演讲者 SPEAKER_08：也值得注意，你可以使用许多设计方法来确保安全，有一个瑞士奶酪安全的概念，每一层安全就像一片瑞士奶酪，有很多洞，但如果你有足够的片，那么所有洞都被穿过的概率就会降低。
643 00:57:51,742 --> 00:57:52,983 讲者 SPEAKER_08：可能实际上相当低。
644 00:57:53,003 --> 00:57:59,070 讲者 SPEAKER_08：所以我们可能需要设计一些图灵警察来检查人工智能系统。
645 00:57:59,150 --> 00:58:01,353 讲者 SPEAKER_08：我们可能有一些训练标准。
646 00:58:01,373 --> 00:58:11,764 讲者 SPEAKER_08：我们可能希望加入情感，并为小型年轻人工智能程序制定育儿标准，以提供良好的家庭教育，进行良好的道德教育。
647 00:58:12,266 --> 00:58:14,929 演讲者 SPEAKER_08：而且这样可能会让它变得足够可靠。
648 00:58:15,769 --> 00:58:20,414 演讲者 SPEAKER_09：我认为，为了使 AI 系统更安全、更道德，
649 00:58:22,032 --> 00:58:25,900 演讲者 SPEAKER_09：这些系统更像孩子，而不是像计算机代码。
650 00:58:26,242 --> 00:58:31,193 演讲者 SPEAKER_09：在以前，当我们编写程序让计算机做事情时，你可以查看代码行并了解它们的功能。
651 00:58:31,213 --> 00:58:36,666 说话人 SPEAKER_09：可能有数百万行代码，所以很难全部查看，但有可能查看这些代码行。
652 00:58:37,101 --> 00:58:41,666 说话人 SPEAKER_09：我们现在正在训练事物，它们从数据中提取结构。
653 00:58:42,807 --> 00:58:45,371 说话人 SPEAKER_09：所以展示给它们的数据非常重要。
654 00:58:45,931 --> 00:58:51,998 说话人 SPEAKER_09：目前，如果你使用像 GPT-4 这样的模型，据我所知，它只是训练了它们能接触到的一切。
655 00:58:52,739 --> 00:58:55,882 讲者 SPEAKER_09: 因此它可能是在连环杀手日记的基础上进行训练的。
656 00:58:56,523 --> 00:59:02,650 讲者 SPEAKER_09: 如果你在教孩子阅读，你会选择连环杀手的日记作为早期阅读材料吗？
657 00:59:03,972 --> 00:59:06,855 讲者 SPEAKER_09: 孩子们可能会很感兴趣。
658 00:59:06,835 --> 00:59:09,739 讲者 SPEAKER_09: 但这并不是你首选的。
659 00:59:09,759 --> 00:59:15,827 讲者 SPEAKER_09：我认为这些系统中的许多伦理问题将来自数据整理。
660 00:59:15,967 --> 00:59:19,632 讲者 SPEAKER_09：你知道，作为父母，你有两个控制孩子的手段。
661 00:59:20,132 --> 00:59:21,414 讲者 SPEAKER_09：你可以奖励和惩罚他们。
662 00:59:21,956 --> 00:59:22,976 讲者 SPEAKER_09：这并不很有效。
663 00:59:23,418 --> 00:59:24,559 讲者 SPEAKER_09: 或者你可以给他们设置一个好的模型。
664 00:59:24,599 --> 00:59:25,380 讲者 SPEAKER_09: 这样效果会好很多。
665 00:59:25,721 --> 00:59:31,088 讲者 SPEAKER_09: 如果你告诉他们不要说谎，如果他们说了谎就打他们，但如果你自己说谎，
666 00:59:31,338 --> 00:59:32,199 讲者 SPEAKER_09: 这样是不行的。
667 00:59:32,960 --> 00:59:36,646 讲者 SPEAKER_09：我认为道德将从建模良好行为中产生。
668 00:59:37,306 --> 00:59:48,925 讲者 SPEAKER_06：我认为处理数据和看待数据的真正好的理论是女权主义理论，即“抓握”理论。
669 00:59:48,905 --> 00:59:53,809 讲者 SPEAKER_06：数据的去殖民化，因为数据从何而来？
670 00:59:54,251 --> 00:59:55,552 讲者 SPEAKER_06：我们是谁在收集？
671 00:59:55,873 --> 00:59:59,175 讲者 SPEAKER_06：我们在偷什么，没有还给人们？
672 00:59:59,996 --> 01:00:02,800 讲者 SPEAKER_06：谁在尼日利亚训练这些系统？
673 01:00:03,320 --> 01:00:06,525 讲者 SPEAKER_06：这也是我们需要做大量工作的地方。
674 01:00:07,204 --> 01:00:15,974 讲者 SPEAKER_06：所以我完全同意，数据在多个层面，包括我们给它喂食的，以及我们构建这些数据集的方式。
675 01:00:15,954 --> 01:00:22,063 演讲者 SPEAKER_07：当你说话时，这里有一个假设，那就是它肯定会更智能。
676 01:00:22,182 --> 01:00:24,306 演讲者 SPEAKER_07：当然，这来自于你的立场。
677 01:00:24,726 --> 01:00:29,452 演讲者 SPEAKER_07：但是有没有它永远无法拥有的智能部分？
678 01:00:29,873 --> 01:00:30,835 演讲者 SPEAKER_07：我们还没有涉及到这一点。
679 01:00:31,755 --> 01:00:37,423 讲者 SPEAKER_07：机器永远无法达到的内在人性是什么？
680 01:00:37,503 --> 01:00:40,527 讲者 SPEAKER_09：我想就这个话题谈谈，大约五分钟。
681 01:00:40,507 --> 01:00:49,918 讲者 SPEAKER_09：因为很多人，他们有一个最后的防线，就是，是的，但他们没有主观经验或意识或意识。
682 01:00:50,980 --> 01:00:59,469 讲者 SPEAKER_09：现在，这里的大多数人可能认为当前的多模态聊天机器人没有主观经验。
683 01:00:59,690 --> 01:01:00,831 讲者 SPEAKER_09：我希望大家举手。
684 01:01:00,992 --> 01:01:10,043 讲者 SPEAKER_09：现在有多少人认为一个多模态聊天机器人肯定具有主观经验，或者很可能具有主观经验？
685 01:01:12,217 --> 01:01:12,978 讲者 SPEAKER_09：比我想象的要多。
686 01:01:13,018 --> 01:01:13,918 讲者 SPEAKER_09：好的。
687 01:01:14,880 --> 01:01:16,802 讲者 SPEAKER_09：大概有 15 或 20 个，对吧？
688 01:01:17,202 --> 01:01:20,126 讲者 SPEAKER_09：现在我将给出我的论点，然后我会再次请大家举手。
689 01:01:20,606 --> 01:01:23,429 讲者 SPEAKER_09：如果是 15 个，我需要 16 个人举手。
690 01:01:23,889 --> 01:01:25,012 讲者 SPEAKER_09：好的，那么开始了。
691 01:01:25,552 --> 01:01:30,858 演讲者 SPEAKER_09：我想说服你们，当前的多模态聊天机器人具有主观体验。
692 01:01:30,878 --> 01:01:34,141 演讲者 SPEAKER_09：这关乎我们所说的主观体验是什么。
693 01:01:34,922 --> 01:01:39,387 演讲者 SPEAKER_09：所以，我们大多数人都有一个类似于剧院的心理模型。
694 01:01:39,367 --> 01:01:42,293 演讲者 SPEAKER_09：在这个剧院里发生的事情只有我能看到。
695 01:01:42,333 --> 01:01:48,626 讲者 SPEAKER_09：如果你问一个哲学家什么是主观经验，他们会开始谈论“质料”。
696 01:01:48,646 --> 01:01:54,677 所以假设我吸了些迷幻药或者喝了很多酒，我开始看到在我面前飘浮着的小粉象。
697 01:01:54,697 --> 01:01:58,626 我告诉你，我有一种在我面前飘浮着的小粉象的主观经验。
698 01:01:59,246 --> 01:02:00,407 我到底在说什么？
699 01:02:01,068 --> 01:02:02,811 说话人 SPEAKER_09：嗯，这是我对它的分析。
700 01:02:02,911 --> 01:02:20,487 说话人 SPEAKER_09：我并不是说有一个只有我能看到的内心剧场，在这个内心剧场里，有由粉色质料构成的小粉象，有象质料，有漂浮质料，有正立质料，还有不那么大的质料，它们以某种方式混合在一起。
701 01:02:20,648 --> 01:02:24,192 说话人 SPEAKER_09：这是哲学家的理论，完全是胡说。
702 01:02:25,014 --> 01:02:28,219 说话人 SPEAKER_09：质料是哲学家对燃素说的版本。
703 01:02:28,260 --> 01:02:30,822 讲者 SPEAKER_09：化学家们有燃素来解释事物，但实际上并没有。
704 01:02:31,204 --> 01:02:34,327 讲者 SPEAKER_09：在哲学家想要的意义上，并没有任何质料。
705 01:02:34,347 --> 01:02:36,070 讲者 SPEAKER_09：所以我真正想告诉你的其实是这个。
706 01:02:36,711 --> 01:02:42,019 讲者 SPEAKER_09：当我提到一点信心时，我是说，我相信我的感知系统在欺骗我。
707 01:02:42,820 --> 01:02:45,885 讲者 SPEAKER_09：这就是为什么我说这是一种主观体验。
708 01:02:45,905 --> 01:02:54,077 讲者 SPEAKER_09：我会告诉你们它是如何欺骗我的，并不是通过告诉你们我的感知系统告诉我让神经元 53 激活。
709 01:02:54,411 --> 01:02:55,514 讲者 SPEAKER_09：因为那样对你们没有任何好处。
710 01:02:55,934 --> 01:02:56,896 讲者 SPEAKER_09：而且，我也不知道。
711 01:02:57,898 --> 01:03:05,273 讲者 SPEAKER_09：我要告诉你们它如何试图误导我，通过告诉我在这个世界上必须存在什么，它才能说出真相。
712 01:03:07,237 --> 01:03:14,833 讲者 SPEAKER_09：所以当我说我有一个主观体验，一个小粉象在我面前飘浮，这相当于说以下内容。
713 01:03:14,914 --> 01:03:21,606 讲者 SPEAKER_09：我认为我的感知系统在欺骗我，但如果世界上真的有小粉象飘浮在周围，那么它就会说出真相。
714 01:03:22,268 --> 01:03:24,771 讲者 SPEAKER_09：所以这些小粉象是现实世界中的大象。
715 01:03:24,791 --> 01:03:26,875 讲者 SPEAKER_09: 他们不是由质料构成的象。
716 01:03:26,894 --> 01:03:29,219 讲者 SPEAKER_09: 它们是现实世界中的反事实事物。
717 01:03:29,239 --> 01:03:32,485 讲者 SPEAKER_09: 如果它们存在，它们将是现实世界的事物，这就是为什么像“粉红色”
718 01:03:32,768 --> 01:03:36,034 讲者 SPEAKER_09: 和“漂浮”这样的词适用于它们，适用于现实世界事物的词。
719 01:03:36,375 --> 01:03:39,240 讲者 SPEAKER_09：他们有趣的地方不在于他们是由一种叫做“质料”的有趣材料制成的。
720 01:03:39,561 --> 01:03:42,007 讲者 SPEAKER_09：有趣的是它们是反事实的。
721 01:03:42,027 --> 01:03:46,255 讲者 SPEAKER_09：所以现在我要给你们举一个多模态聊天机器人有主观体验的例子。
722 01:03:46,994 --> 01:03:48,896 讲者 SPEAKER_09：所以我有一个多模态聊天机器人。
723 01:03:48,976 --> 01:03:49,637 说话人 SPEAKER_09: 我训练它。
724 01:03:49,677 --> 01:03:50,257 说话人 SPEAKER_09: 它可以说话。
725 01:03:50,557 --> 01:03:51,139 说话人 SPEAKER_09: 它可以指认。
726 01:03:51,478 --> 01:03:52,159 说话人 SPEAKER_09: 它有一个机械臂。
727 01:03:52,719 --> 01:03:53,681 说话人 SPEAKER_09：它能看到东西。
728 01:03:54,242 --> 01:03:56,344 说话人 SPEAKER_09：我在它前面放了一个物体，然后说，指向这个物体。
729 01:03:56,463 --> 01:03:57,726 说话人 SPEAKER_09：然后它就指向了物体。
730 01:03:57,746 --> 01:03:58,206 说话人 SPEAKER_09：没问题。
731 01:03:58,987 --> 01:04:02,291 说话人 SPEAKER_09：然后我在它不知道的情况下，在其镜头前放了一个棱镜。
732 01:04:02,731 --> 01:04:05,293 说话人 SPEAKER_09：所以我搞砸了它的感知系统。
733 01:04:05,313 --> 01:04:07,376 说话人 SPEAKER_09：所以它的感知系统现在不再正常工作。
734 01:04:08,117 --> 01:04:09,559 说话人 SPEAKER_09：现在我把它放在一个物体前面。
735 01:04:09,998 --> 01:04:11,059 演讲者 SPEAKER_09：我说，指向这个物体。
736 01:04:11,079 --> 01:04:11,701 演讲者 SPEAKER_09：然后就这样。
737 01:04:12,702 --> 01:04:16,326 演讲者 SPEAKER_09：我说，不，我在你的镜头前放了一个棱镜。
738 01:04:16,423 --> 01:04:18,887 演讲者 SPEAKER_09：所以，棱镜正在弯曲光线。
739 01:04:19,590 --> 01:04:25,519 说话人 SPEAKER_09: 噢，我明白了，棱镜弯曲了光线，所以物体实际上在那里。
740 01:04:25,800 --> 01:04:27,945 说话人 SPEAKER_09: 但我主观上觉得它在那里。
741 01:04:27,965 --> 01:04:33,775 说话人 SPEAKER_09: 如果它这么说，它就和我们一样使用了“主观体验”这个词。
742 01:04:34,784 --> 01:04:39,672 说话人 SPEAKER_09: 所以如果聊天机器人这么说，它就有和我们一样的主观体验。
743 01:04:39,692 --> 01:04:47,905 讲者 SPEAKER_09：主观体验是，你的感知系统出了问题，你通过告诉人们世界需要是什么样子才能让它说的是真话来解释它出了什么问题。
744 01:04:48,385 --> 01:04:50,128 讲者 SPEAKER_09：这对我们来说是真的，对聊天机器人来说也是真的。
745 01:04:50,789 --> 01:04:54,054 讲者 SPEAKER_09：好的，我想再次投票，并且我想有超过 15 个人。
746 01:04:54,074 --> 01:04:58,141 讲者 SPEAKER_09：现在有多少人认为聊天机器人可以有主观体验？
747 01:04:58,813 --> 01:04:59,835 讲者 SPEAKER_04: 我仍然这么认为。
748 01:05:02,418 --> 01:05:10,150 讲者 SPEAKER_06: 不，但你正在以非常特别的方式定义主观经验，好像它关乎真假。
749 01:05:10,951 --> 01:05:12,813 讲者 SPEAKER_06: 主观经验始终存在。
750 01:05:13,135 --> 01:05:17,860 讲者 SPEAKER_06: 如果你认为有任何客观事物，那么你就在错误的方向上了。
751 01:05:18,262 --> 01:05:20,826 演讲者 SPEAKER_09：不，我不同意。
752 01:05:21,626 --> 01:05:24,952 演讲者 SPEAKER_09：我实际上和女权主义者以及绝对真理有过争论。
753 01:05:25,052 --> 01:05:28,737 演讲者 SPEAKER_09：我相信真正的事物。
754 01:05:28,987 --> 01:05:32,786 演讲者 SPEAKER_09：我现在正在看一个玻璃杯，我正在经历看玻璃杯的客观体验。
755 01:05:34,253 --> 01:05:34,856 演讲者 SPEAKER_09：你不同意吗？
756 01:05:35,318 --> 01:05:35,739 演讲者 SPEAKER_07：是的。
757 01:05:36,056 --> 01:05:38,559 演讲者 SPEAKER_09：这就是争论的结束。
758 01:05:39,422 --> 01:05:43,206 演讲者 SPEAKER_09：我认为我们实际上得稍后再解决那个问题。
759 01:05:43,226 --> 01:05:48,454 演讲者 SPEAKER_07：所以我们请您帮助我们提出一些问题。
760 01:05:48,655 --> 01:05:58,128 演讲者 SPEAKER_07：接下来我将提出来自观众的第一位问题，这是来自政府办公室国家数字化战略部门的安德斯·赫克先生的提问。
761 01:05:58,148 --> 01:05:59,048 演讲者 SPEAKER_07：我想他就在这里某个地方。
762 01:05:59,148 --> 01:05:59,769 演讲者 SPEAKER_07：是的。
763 01:05:59,750 --> 01:06:03,474 讲者 SPEAKER_07：您一直在争论 AGI 的出现和危险。
764 01:06:03,634 --> 01:06:06,115 讲者 SPEAKER_07：我们应该关注哪些里程碑？
765 01:06:06,155 --> 01:06:13,623 讲者 SPEAKER_07：有没有某个天文台或类似机构在跟踪和报告这种里程碑的演变？
766 01:06:13,884 --> 01:06:14,384 讲者 SPEAKER_09：这是针对我的吗？
767 01:06:14,585 --> 01:06:15,646 演讲者 SPEAKER_07: 是的。
768 01:06:15,666 --> 01:06:17,606 演讲者 SPEAKER_09: 我认为 OpenAI 有一些里程碑，对吧？
769 01:06:17,827 --> 01:06:23,914 演讲者 SPEAKER_09: OpenAI 设定了一些里程碑，他们现在处于第三阶段，如果他们达到第五阶段，那就是 AGI。
770 01:06:25,054 --> 01:06:27,376 演讲者 SPEAKER_07: 你同意他们的这种说法吗？
771 01:06:29,280 --> 01:06:34,313 讲者 SPEAKER_09：当我 2003 年春天离开谷歌时，我停止阅读文献。
772 01:06:35,677 --> 01:06:36,639 讲者 SPEAKER_09：我尝试退休。
773 01:06:36,699 --> 01:06:44,077 讲者 SPEAKER_09：我离开谷歌的原因是我想退休，我觉得在离开的时候顺便提一下这些事情是危险的。
774 01:06:44,820 --> 01:06:45,922 讲者 SPEAKER_09：然后我就无法退休了。
775 01:06:47,454 --> 01:06:48,898 讲者 SPEAKER_09：但我已经停止阅读文献了。
776 01:06:50,902 --> 01:06:54,068 讲者 SPEAKER_09：如果你问我，实际上有哪五个阶段？
777 01:06:54,889 --> 01:06:55,851 讲者 SPEAKER_09：你或许可以告诉我。
778 01:06:57,054 --> 01:06:59,719 讲者 SPEAKER_08：我读过，但现在想不起来了。
779 01:06:59,760 --> 01:07:01,463 说话人 SPEAKER_08: 我归咎于时差。
780 01:07:01,483 --> 01:07:02,565 说话人 SPEAKER_08: 但让我们说这个。
781 01:07:03,168 --> 01:07:05,311 说话人 SPEAKER_08: 但在某种程度上，我认为...
782 01:07:05,291 --> 01:07:09,019 说话人 SPEAKER_08: 它们有些合理，但表达得比较松散。
783 01:07:09,039 --> 01:07:13,168 演讲者 SPEAKER_08：当我看它们的时候，我似乎记得，好吧，我该如何量化这个？
784 01:07:13,228 --> 01:07:21,306 演讲者 SPEAKER_08：这可能对欧盟法案更为相关，因为在那里你实际上需要将要立法并由检查员检查的东西。
785 01:07:21,768 --> 01:07:22,929 演讲者 SPEAKER_08：他们需要能够告诉
786 01:07:22,909 --> 01:07:27,635 演讲者 SPEAKER_08：这个 AI 程序实际上是二级，但这个是三级。
787 01:07:28,297 --> 01:07:31,039 说话人 SPEAKER_08：当你需要一个相当严格的定义时，这会很棘手。
788 01:07:31,661 --> 01:07:35,505 说话人 SPEAKER_08：但许多人当然在推广自我意识这样的声音线。
789 01:07:35,585 --> 01:07:40,992 说话人 SPEAKER_08：我认为是 Antropic 说的，如果我们发现任何自我意识的迹象，我们会立即停止训练。
790 01:07:41,373 --> 01:07:46,659 说话人 SPEAKER_08：他们非常著名地看到了这样的迹象，并且对此感到好笑，说，这不是很酷吗？
791 01:07:46,679 --> 01:07:47,721 说话人 SPEAKER_08：然后继续这样做。
792 01:07:48,222 --> 01:07:49,744 说话人 SPEAKER_08：是的。
793 01:07:50,871 --> 01:07:54,896 说话人 SPEAKER_04：你认为我们真的能识别出 AGI 吗？
794 01:07:55,376 --> 01:07:58,099 说话人 SPEAKER_04：或者如果它超出了我们的理解，我们怎么知道呢？
795 01:08:00,481 --> 01:08:02,244 说话人 SPEAKER_09：你知道因为它现在处于控制之中。
796 01:08:04,365 --> 01:08:05,347 说话人 SPEAKER_04：那时就已经太晚了。
797 01:08:05,407 --> 01:08:19,461 说话人 SPEAKER_09：不，我认为一种识别方法是有这样一个 AI 系统，一个大型的聊天机器人，你和它辩论，你总是输。
798 01:08:22,275 --> 01:08:22,716 说话人 SPEAKER_04：这很好。
799 01:08:24,578 --> 01:08:25,599 讲者 SPEAKER_07：嗯，这可能是一个步骤。
800 01:08:26,961 --> 01:08:29,662 讲者 SPEAKER_04：现在突然感觉非常接近，不是吗？
801 01:08:30,564 --> 01:08:30,904 讲者 SPEAKER_09：是的。
802 01:08:31,104 --> 01:08:34,307 讲者 SPEAKER_09：这就是我们认识到 AlphaGo 下围棋比人更好的方式。
803 01:08:34,708 --> 01:08:35,309 说话人 SPEAKER_09：人们失败了。
804 01:08:36,690 --> 01:08:46,079 说话人 SPEAKER_07：还有一个来自观众的技术问题，那就是人类大脑似乎比大型基础 AI 模型更节能。
805 01:08:46,520 --> 01:08:49,042 说话人 SPEAKER_07：那么我们如何使模型更节能呢？
806 01:08:49,394 --> 01:08:51,756 说话人 SPEAKER_09：好的，我对这个有很多话要说。
807 01:08:51,777 --> 01:08:53,759 演讲者 SPEAKER_09：我们之所以节能，是因为我们是模拟的。
808 01:08:54,479 --> 01:08:57,422 演讲者 SPEAKER_09：因此，我们没有硬件和软件的分离。
809 01:08:58,003 --> 01:09:08,655 演讲者 SPEAKER_09：我们神经网络中的权重是针对那些特定神经元及其特定连接、所有这些神经元的特定特性以及它们之间所有交互的权重。
810 01:09:08,676 --> 01:09:10,177 演讲者 SPEAKER_09：这些权重仅适用于这种情况。
811 01:09:12,359 --> 01:09:19,148 演讲者 SPEAKER_09：另一种选择是数字化，使用非常高的功率的晶体管。
812 01:09:19,515 --> 01:09:26,987 演讲者 SPEAKER_09：但为此你将得到的是，你可以拥有两块不同的硬件，在指令级别上执行完全相同的操作。
813 01:09:27,788 --> 01:09:30,171 演讲者 SPEAKER_09：要做到这一点，你必须非常精确地制造东西。
814 01:09:31,113 --> 01:09:37,103 演讲者 SPEAKER_09：你必须使用非常高的功率，这样你才能得到 1 和 0，而不是 0.5。
815 01:09:38,601 --> 01:09:43,752 说话人 SPEAKER_09：这样做更好，因为这样两块不同的硬件可以学习不同的事情，并分享它们所学到的东西。
816 01:09:44,333 --> 01:09:47,399 说话人 SPEAKER_09：从这个意义上说，这样做更好，但它的缺点是功耗更高。
817 01:09:48,001 --> 01:09:55,235 说话人 SPEAKER_09：问题是，这种更好的共享能力是否会超过它功耗更高的缺点？
818 01:09:55,657 --> 01:09:58,381 说话人 SPEAKER_09：我觉得你可能没有进化出数字思维。
819 01:09:58,362 --> 01:10:00,747 讲者 SPEAKER_09: 智能因为涉及太多的权力。
820 01:10:00,768 --> 01:10:02,733 讲者 SPEAKER_09: 你将要涉及模拟智能。
821 01:10:03,213 --> 01:10:05,059 讲者 SPEAKER_09: 但我认为数字智能更好。
822 01:10:05,961 --> 01:10:09,890 讲者 SPEAKER_09: 可能是因为权力过于极端。
823 01:10:10,680 --> 01:10:22,175 讲者 SPEAKER_09：当然，如果我们能弄清楚如何在模拟神经网络中正确学习，那么我们可能能制造出比数字神经网络更大的模拟神经网络。
824 01:10:22,836 --> 01:10:27,240 讲者 SPEAKER_09：虽然它们不能相互共享，但其中之一可能学会很多东西。
825 01:10:27,541 --> 01:10:29,203 讲者 SPEAKER_09：但我认为你无法获取数据。
826 01:10:29,583 --> 01:10:34,510 讲者 SPEAKER_09：尤其是当数据涉及到在世界上行动时，你不可能通过一个系统获取数据。
827 01:10:34,789 --> 01:10:37,372 演讲者 SPEAKER_09：因为作用于世界，你无法提高百万倍的速度。
828 01:10:37,432 --> 01:10:38,814 演讲者 SPEAKER_09：你必须作用于世界。
829 01:10:38,795 --> 01:10:45,462 演讲者 SPEAKER_09：所以这是一个缓慢的顺序过程，你不可能通过一个系统获取所有数据，因此你无法与数字智能竞争。
830 01:10:46,403 --> 01:10:46,984 演讲者 SPEAKER_09：我说了我的部分。
831 01:10:48,265 --> 01:10:49,426 演讲者 SPEAKER_07: 你说你将要退休。
832 01:10:49,466 --> 01:10:52,009 演讲者 SPEAKER_07: 听起来你对这个领域非常感兴趣。
833 01:10:52,029 --> 01:10:54,591 演讲者 SPEAKER_09: 不，正是我在研究这个导致我退休。
834 01:10:56,033 --> 01:11:01,498 演讲者 SPEAKER_09: 我认为模拟智能无法与数字智能竞争的事实。
835 01:11:02,118 --> 01:11:05,662 说话人 SPEAKER_07：小组，关于能源效率有什么想法吗？
836 01:11:06,453 --> 01:11:10,936 说话人 SPEAKER_08：有一个非常有趣的问题，关于最终极限到底在哪里。
837 01:11:10,957 --> 01:11:21,506 说话人 SPEAKER_08：有关于兰道尔原理的基本思想，但为了提升一个比特的信息，你需要付出一定的热力学代价，而我们现在离那个目标还非常遥远。
838 01:11:21,527 --> 01:11:29,533 说话人 SPEAKER_08：大脑以 20 到 25 瓦的功率运行更接近，但仍然，我认为，离兰道尔极限还有七个数量级。
839 01:11:29,554 --> 01:11:33,337 说话人 SPEAKER_08：我认为在我们宇宙中，物质目前并不十分智能。
840 01:11:33,318 --> 01:11:40,886 说话人 SPEAKER_08：但随着我们的技术越来越聪明，我们在制造它们方面的能力也越来越强，我认为我们可能会获得各种效率。
841 01:11:40,947 --> 01:11:48,936 说话人 SPEAKER_08：也许我们想要拥有节能的机器人，但同时也可能需要更多能量的可复制智能。
842 01:11:49,398 --> 01:11:56,606 说话人 SPEAKER_08：或者你可能想要拥有具有自己令人烦恼的怪癖的量子计算机，因为它们非常脆弱，需要隔离。
843 01:11:56,627 --> 01:12:01,872 讲者 SPEAKER_08：可能对于不同类型的智能，我们会得到根本不同的硬件。
844 01:12:02,038 --> 01:12:05,372 讲者 SPEAKER_09：我记得在哪里看到过，但记不得是谁说的，肯定不是我。
845 01:12:06,256 --> 01:12:13,162 讲者 SPEAKER_09：我们现在处于一个这样的状况，我们拥有史前的大脑，中世纪的制度，以及神一般的科技。
846 01:12:15,622 --> 01:12:36,213 讲者 SPEAKER_06：我认为还有其他方法，这不一定能解决能源问题，但对于具身智能，有很多关于生物材料的实验，所以不是在我们今天所知的硬件上实施，而是在其他材料上，培养其他材料。
当前讨论中让我担忧的一点，我认为我们现在正处于 AI 炒作的高峰期，就是辩论中的大多数声音，我可以说，都是男性。
在研讨会之前，我们收到的几乎所有问题都来自男性对话者。
所以我要向观众提问，而且只有，现在，我要非常权威地说，只有女性可以提问，请。
850 01：13：02,838 --> 01：13：04,479 议长 SPEAKER_07：任何人。
851 01:13:04,460 --> 01:13:06,768 说话人 SPEAKER_07: 请拿起您椅子旁边的麦克风。
852 01:13:06,787 --> 01:13:08,332 说话人 SPEAKER_05: 我实际上有两个问题。
853 01:13:08,994 --> 01:13:10,118 说话人 SPEAKER_05: 我先从第一个问题开始。
854 01:13:12,405 --> 01:13:14,453 说话人 SPEAKER_05: 当我们交谈时，我们假设
855 01:13:14,887 --> 01:13:25,645 讲者 SPEAKER_05：大多数情况下，不是我们所有人，但大多数情况下，当我们构建这些系统时，我们实际上是在想一些邪恶的事情。
856 01:13:25,685 --> 01:13:27,046 讲者 SPEAKER_05：它们对我们来说会不好。
857 01:13:27,106 --> 01:13:28,269 讲者 SPEAKER_05：它们不会对我们好。
858 01:13:28,909 --> 01:13:34,198 讲者 SPEAKER_05：我们为什么不假设世界上有更多的好人，而不是坏人呢？
859 01:13:34,878 --> 01:13:37,122 演讲者 SPEAKER_05：因此，这些系统将会是好的系统。
860 01:13:37,162 --> 01:13:38,425 演讲者 SPEAKER_05：这是我的第一个问题。
861 01:13:39,086 --> 01:13:41,789 演讲者 SPEAKER_05：现在，我的第二个问题是，
862 01:13:43,002 --> 01:13:53,560 演讲者 SPEAKER_05：如果我们回顾我们的历史，当我们创建新的系统时，我们实际上是与它们一起学习，并将它们变成了我们的一部分，而不是我们观察的其他事物。
863 01:13:54,000 --> 01:14:01,233 讲者 SPEAKER_05：我们为什么认为我们会与这些系统合并，并且我们共同会变得更好呢？
864 01:14:02,635 --> 01:14:04,819 讲者 SPEAKER_05：所以有两个问题。
865 01:14:04,958 --> 01:14:06,822 讲者 SPEAKER_09：我同意好人比坏人多。
866 01:14:06,841 --> 01:14:08,604 讲者 SPEAKER_09：不幸的是，坏人占据了上风。
867 01:14:11,099 --> 01:14:21,820 讲者 SPEAKER_04：实际上还有一个问题，人们喜欢称之为防守者的困境，攻击者只需要成功一次，而作为防守者，你需要每次都成功。
868 01:14:21,841 --> 01:14:26,289 讲者 SPEAKER_04：所以，你知道，随着时间的推移，概率对好人不利。
869 01:14:26,827 --> 01:14:28,229 讲者 SPEAKER_05：你认为呢，Kia？
870 01:14:31,034 --> 01:14:41,033 讲者 SPEAKER_06：所以我确实认为我们需要用其他方式来处理这些问题，并解开其他方法，并教给我们的年轻人其他方法。
871 01:14:41,472 --> 01:14:51,230 讲者 SPEAKER_06：在 KTH，你知道，以新的方式教授思考这些系统的方法，这样一旦我们实施它们，我们实际上会强调好的方面。
872 01:14:51,211 --> 01:15:05,172 讲者 SPEAKER_06：我知道我们还需要法律和法规，尽管我更倾向于探索你在设计和这些系统中的个人力量和个人责任。
873 01:15:05,793 --> 01:15:07,337 讲者 SPEAKER_06：这就是我现在所在的位置。
874 01:15:07,757 --> 01:15:12,966 讲者 SPEAKER_06：我想我能做的是尝试与我们的年轻学生互动。
875 01:15:12,945 --> 01:15:23,220 讲者 SPEAKER_06：我听说欧盟很快会规定，任何计算机科学系都必须雇佣来自人文和社会科学领域的人。
876 01:15:25,101 --> 01:15:41,703 讲者 SPEAKER_06：所以，以有见地、知识渊博的方式提出问题，对数据、算法以及我们构建它们的方式有良好的理论理解，我认为这是很重要的。
877 01:15:41,684 --> 01:15:44,686 讲者 SPEAKER_08：也许每个文科系也需要有一个工程师。
878 01:15:46,168 --> 01:15:55,479 讲者 SPEAKER_08：但你的第二个问题也很有趣，因为当我们使用工具时，它们至少对于许多简单的工具，会融入我们的身体形象中。
879 01:15:55,579 --> 01:16:01,846 讲者 SPEAKER_08：如果我手里拿着一根长木头，突然我的个人空间感就改变了。
880 01:16:01,868 --> 01:16:04,029 讲者 SPEAKER_08：你可以进行脑成像，看到这些变化。
881 01:16:04,010 --> 01:16:06,916 讲者 SPEAKER_08：当然，这也适用于许多认知工具。
882 01:16:07,778 --> 01:16:11,546 讲者 SPEAKER_08：我的智能手机，我的笔记本，它们都是我思维的一部分。
883 01:16:11,565 --> 01:16:12,528 说话人 SPEAKER_08：它们在扩展我的思维。
884 01:16:13,029 --> 01:16:14,731 说话人 SPEAKER_08：我们还拥有社交扩展。
885 01:16:14,752 --> 01:16:17,738 说话人 SPEAKER_08：社会中正在发生许多扩展认知的形式。
886 01:16:18,139 --> 01:16:24,212 说话人 SPEAKER_08：现在正在发生的令人着迷的事情是，当然，越来越多的算法和软件片段正在悄悄地进入其中。
887 01:16:24,192 --> 01:16:31,563 讲者 SPEAKER_08：我早就意识到维基百科已经成为我记忆的一部分，这意味着我的记忆中已经有编辑者在修改。
888 01:16:31,582 --> 01:16:40,957 讲者 SPEAKER_08：还有许多机器人以多种方式编辑维基百科，但这意味着我已经在不自知的情况下，部分记忆受到了人工智能的增强。
889 01:16:41,317 --> 01:16:42,698 讲者 SPEAKER_08：我对维基百科还是有点信任的。
890 01:16:42,719 --> 01:16:46,824 讲者 SPEAKER_08：还有其他这些系统，我不知道是否应该信任。
891 01:16:46,845 --> 01:16:53,354 说话人 SPEAKER_08：我认为当我们扩展自己时，开发使它们值得信赖的方法将非常重要。
892 01:16:54,347 --> 01:16:55,952 说话人 SPEAKER_06：我不喜欢“信任”这个词。
893 01:16:56,131 --> 01:17:00,523 说话人 SPEAKER_06：我认为这是一个糟糕的概念，它掩盖了许多东西。
894 01:17:00,722 --> 01:17:03,510 说话人 SPEAKER_06：我们没有时间去探讨这个问题。
895 01:17:03,550 --> 01:17:04,552 说话人 SPEAKER_06: 别去那里。
896 01:17:04,733 --> 01:17:05,996 说话人 SPEAKER_06: 可信的 AI。
897 01:17:06,015 --> 01:17:06,998 说话人 SPEAKER_07: 那是什么鬼？
898 01:17:07,019 --> 01:17:10,967 说话人 SPEAKER_07: 来自观众的最后一个问题。
899 01:17:11,007 --> 01:17:12,492 说话人 SPEAKER_07：椅子侧面的麦克风打开了。
900 01:17:18,108 --> 01:17:20,610 说话人 SPEAKER_03：那是一个智力测试，我勉强及格了。
901 01:17:21,231 --> 01:17:24,957 说话人 SPEAKER_03：我叫 Sarah，我在谷歌公共政策部门工作。
902 01:17:25,256 --> 01:17:33,587 说话人 SPEAKER_03：我对这些讨论很感兴趣，尤其是您，Hinton 教授，关于治理的想法。
903 01:17:33,908 --> 01:17:45,582 演讲者 SPEAKER_03：如果我们都看到了潜力，也看到了这些风险，我们如何确保最终获胜的是潜力和机遇？
904 01:17:47,351 --> 01:18:02,373 演讲者 SPEAKER_09：所以当我还在谷歌的时候，他们当时领先很多，大约在 2017 年以及之后的几年里，在他们完成了 Transformer 并发表了这项技术之后，他们可能现在后悔发表了这项技术，
905 01:18:02,827 --> 01:18:06,815 演讲者 SPEAKER_09：他们的聊天机器人比任何人都先进。
906 01:18:07,337 --> 01:18:15,114 演讲者 SPEAKER_09：我的意思是，他们非常负责任，没有发布它们，因为他们看到了微软发布聊天机器人时发生的事情。
907 01:18:15,695 --> 01:18:20,768 说话人 SPEAKER_09: 我的意思是，它很快就开始喷出种族主义仇恨言论。
908 01:18:20,747 --> 01:18:24,390 说话人 SPEAKER_09: 显然，谷歌有很好的声誉，不想破坏它。
909 01:18:25,271 --> 01:18:30,976 说话人 SPEAKER_09: 所以，他们没有发布这些内容并不是出于道德原因，因为他们不想破坏自己的声誉。
910 01:18:32,179 --> 01:18:33,520 说话人 SPEAKER_09: 但他们表现得非常负责任。
911 01:18:34,280 --> 01:18:38,384 说话人 SPEAKER_09：自从 OpenAI 与微软达成协议后，他们就不能那样做了。
912 01:18:38,904 --> 01:18:44,189 说话人 SPEAKER_09：他们不得不竞争，发布产品，并推出聊天机器人。
913 01:18:45,171 --> 01:18:48,814 说话人 SPEAKER_09：所以我认为谷歌在他们有能力的时候表现得很好。
914 01:18:49,654 --> 01:18:50,536 说话人 SPEAKER_09：他们可以承担得起。
915 01:18:51,877 --> 01:18:54,962 说话人 SPEAKER_09：但是一旦进入资本主义体系，我们可以达成共识。
916 01:18:55,282 --> 01:19:10,600 说话人 SPEAKER_09：在资本主义体系中，当出现以利润为导向的公司，尤其是那些 CEO 的股票期权取决于他们下个季度表现的公司，你将看到短期利润支配一切。
917 01:19:11,122 --> 01:19:13,185 说话人 SPEAKER_09：这正是我们在 OpenAI 看到的。
918 01:19:13,845 --> 01:19:19,171 说话人 SPEAKER_09：OpenAI 是一个关于人工智能安全与利润的实时实验。
919 01:19:19,287 --> 01:19:25,454 讲者 SPEAKER_07: 你实际上是在说，回答这个问题，你需要市场的一些干预吗？
920 01:19:25,475 --> 01:19:35,167 讲者 SPEAKER_09: 我认为仅凭谷歌本身是无法做到道德行为的。
921 01:19:35,587 --> 01:19:41,814 讲者 SPEAKER_09: 在法律上，它有义务尝试最大化利润。
922 01:19:42,576 --> 01:19:43,796 讲者 SPEAKER_09: 它是不被允许的。
923 01:19:43,837 --> 01:19:47,742 说话人 SPEAKER_09: 从法律上讲，不能表现得有礼貌。
924 01:19:48,751 --> 01:19:51,315 说话人 SPEAKER_09: 这将需要政府进行监管。
925 01:19:51,336 --> 01:19:54,341 说话人 SPEAKER_09: 我认为我对这一点是正确的，关于生产者的责任。
926 01:19:54,381 --> 01:19:59,511 说话人 SPEAKER_06: 就我记得的微软聊天机器人而言，它最初在亚洲某地发布，并且工作得非常好。
927 01:20:00,171 --> 01:20:07,123 说话人 SPEAKER_06: 然后在英国发布，仅用了 24 小时就出现了种族歧视和脏话。
928 01:20:07,144 --> 01:20:09,087 说话人 SPEAKER_09: 很可能是指男人。
929 01:20:10,975 --> 01:20:15,219 说话人 SPEAKER_07: 不要去那里。
930 01:20:15,319 --> 01:20:24,188 说话人 SPEAKER_07：我们将开始总结，但我想，这也是一个从我的问题中产生的问题，同时也是从观众那里提出的问题。
931 01:20:24,948 --> 01:20:34,779 说话人 SPEAKER_07：当然，您现在在这里是为了接受诺贝尔奖的荣誉，我们正在祝贺这一伟大的成就。
932 01:20:35,578 --> 01:20:38,282 说话人 SPEAKER_07：诺贝尔本人是一位发明家。
933 01:20:38,261 --> 01:20:41,769 说话人 SPEAKER_07：正如您所知，他通过炸药的专利发了财。
934 01:20:42,310 --> 01:20:47,179 说话人 SPEAKER_07：炸药确实为人类做出了巨大贡献，但也夺走了许多人的生命。
935 01:20:48,581 --> 01:20:56,536 讲者 SPEAKER_07：来自观众的类似反思有奥本海默的例子，涉及第二想法和想要阻止它。
936 01:20:56,595 --> 01:21:00,524 讲者 SPEAKER_07：所以，当然，我想问你的问题是关于遗憾的。
937 01:21:00,963 --> 01:21:01,805 讲者 SPEAKER_07：你有没有遗憾？
938 01:21:02,494 --> 01:21:04,838 讲者 SPEAKER_09：好的，我想区分两种遗憾。
939 01:21:05,399 --> 01:21:10,328 说话人 SPEAKER_09：有一种罪恶的悔恨，当时你知道你不应该这样做，但你还是做了。
940 01:21:11,872 --> 01:21:17,081 说话人 SPEAKER_09：然后，你做了某事，在同样的情况下，同样的知识，你还会这样做。
941 01:21:17,161 --> 01:21:23,813 说话人 SPEAKER_09：但后来你意识到这带来了不良后果，而你当时并没有意识到这些后果。
942 01:21:24,414 --> 01:21:26,899 说话人 SPEAKER_09：我没有感到任何内疚的遗憾。
943 01:21:27,637 --> 01:21:33,489 说话人 SPEAKER_09：但我确实在想，如果我们没有发展得这么快，可能更好。
944 01:21:34,332 --> 01:21:39,622 说话人 SPEAKER_09：我认为如果我没有参与这项工作，我可能已经使事情慢下来几周。
945 01:21:42,421 --> 01:21:48,226 说话人 SPEAKER_07：非常感谢你的回答。
946 01:21:48,407 --> 01:21:49,627 说话人 SPEAKER_07：圣诞节即将来临。
947 01:21:49,747 --> 01:21:54,993 说话人 SPEAKER_07：我觉得我们正在进入那种氛围，你知道圣诞节是一个愿望清单的时间。
948 01:21:55,014 --> 01:22:01,340 说话人 SPEAKER_07：所以我想我们用一个快速的环节来结束这次活动，每个人都可以分享一个愿望，我先从 Kia 开始。
949 01:22:01,479 --> 01:22:07,145 说话人 SPEAKER_07：你可以将这个愿望指向研发社区、政府或者任何你想去的地方。
950 01:22:07,345 --> 01:22:13,440 说话人 SPEAKER_06：房间里的研究资助机构？
951 01:22:14,644 --> 01:22:22,704 说话人 SPEAKER_06：我希望的是跨学科合作，因为显然正如我们刚才所听到的，这是一个非常
952 01:22:22,685 --> 01:22:39,579 说话人 SPEAKER_06：困难的问题，并且非常哲学化，这是关于什么让我们和地球上的生活变得美好，而这并不是轻易就能解决的，所以我更希望有更多的跨学科合作。
953 01:22:39,560 --> 01:22:41,904 说话人 SPEAKER_08：我完全同意这个愿望。
我认为，仅仅在考虑如何使人工智能更安全这样的问题时，你同样需要跨学科。
955 01:22:49,502 --> 01:22:55,898 说话人 SPEAKER_08：我们尝试了一些事情，这些事情对于试图让 AI 安全的人来说是有意义的。
956 01:22:55,877 --> 01:23:02,266 说话人 SPEAKER_08：但可能还有其他好的、有效的或实际上非常有用的方法。
957 01:23:02,787 --> 01:23:07,635 说话人 SPEAKER_08：我认为即使我们看技术解决方案，我们可能也只是在看少数几种。
958 01:23:07,675 --> 01:23:10,559 演讲者 SPEAKER_08：我们需要更多的方法多样性。
959 01:23:10,779 --> 01:23:18,630 演讲者 SPEAKER_08：还有很多东西可以被发现，也许会有很多有趣和美好的发现，即使它们不一定能实现安全性的主要目标。
960 01:23:18,670 --> 01:23:22,055 演讲者 SPEAKER_08：我们可能会发现关于我们自己或我们的机器的其它惊人的事情。
961 01:23:24,417 --> 01:23:24,618 演讲者 SPEAKER_04：谢谢。
962 01:23:24,637 --> 01:23:25,800 讲者 SPEAKER_04：我讨厌这种问题。
963 01:23:27,046 --> 01:23:33,438 讲者 SPEAKER_04：那么也许我应该选择国际化而不是跨学科。
964 01:23:33,939 --> 01:23:48,766 讲者 SPEAKER_04：我认为，你知道，正如你所说，有些事情国家不会合作，但我们可以至少在国际上尽可能多地合作，努力以正确的方式推动这项技术的进化，并建立我们需要的障碍。
965 01:23:52,003 --> 01:23:57,311 讲者 SPEAKER_09：我希望我能得到这个问题的答案，大脑是否实现了某种形式的反向传播？
966 01:24:01,497 --> 01:24:03,742 说话人 SPEAKER_07: 我认为我们需要给这个小组一个热烈的欢迎。
967 01:24:03,782 --> 01:24:20,948 说话人 SPEAKER_07: 请坐，我们现在将进入一个小结环节。
968 01:24:22,109 --> 01:24:30,577 说话人 SPEAKER_07: 另一位 IWA 会员，林雪平市立大学的计算机科学教授，Fridrik Heinz，将给我们简要介绍我们都听到了什么。
969 01:24:30,636 --> 01:24:42,149 说话人 SPEAKER_01: 好的，现在我有一个小任务，就是尝试总结一下我们所学的内容。
970 01:24:42,208 --> 01:24:51,238 讲者 SPEAKER_01：所以之前在今天的会议中，我们讨论了在什么情况下你感到舒适，在什么情况下你感到不舒服，以及接受不舒服，我现在就是这样。
971 01:24:52,568 --> 01:25:05,005 讲者 SPEAKER_01：但我想今天我们一直在尝试讨论这个大问题，即什么是智能，我们是否能够构建出真正具有智能的人工系统。
972 01:25:05,886 --> 01:25:13,817 讲者 SPEAKER_01：实际上，我认为我们提出了一个非常有趣的观点，即数字智能可能优于模拟智能。
973 01:25:13,796 --> 01:25:24,069 讲者 SPEAKER_01：我认为这是一个非常有趣的观点，也许你所使用的实现它的基础材料会影响所能达到的效果。
974 01:25:24,329 --> 01:25:26,773 讲者 SPEAKER_01: 并且在某种程度上，它是我们的模拟。
975 01:25:26,853 --> 01:25:30,697 讲者 SPEAKER_01: 可能是身体限制了。
976 01:25:30,716 --> 01:25:31,137 讲者 SPEAKER_01: 你在这里。
977 01:25:31,637 --> 01:25:33,680 讲者 SPEAKER_01: 我在这里的观众席上找不到你的身体。
978 01:25:35,103 --> 01:25:43,752 演讲者 SPEAKER_01: 但也许这是部分原因，或者说，是我们潜在的一部分，我意思是限制。
979 01:25:43,733 --> 01:25:45,194 演讲者 SPEAKER_01: 或者说，我应该说是我们的一个限制。
980 01:25:46,256 --> 01:25:52,905 演讲者 SPEAKER_01: 但我认为这也非常有趣，这是智能推理或学习的本质吗？
981 01:25:53,627 --> 01:26:04,442 演讲者 SPEAKER_01: 个人而言，因为我的实验室叫做推理与学习实验室，当然，我认为它既不是也不是，而是两者结合，以及我们如何将学习与推理结合起来。
我认为这也很有趣，从某种意义上说，这是身体、具身化的作用。但我认为，也许更有趣的是“我”的作用。我们谈论了主观经验，对我来说，这里的大问题实际上也是我没有举手的原因。
983 01：26：22,332 --> 01：26：26,920 演讲者 SPEAKER_01：他们甚至有主题吗，有主题意味着什么？
984 01：26：27,422 --> 01：26：36,221 议长 SPEAKER_01：我是说，我不质疑他们有，我是说，有一些感知，他们可能会被欺骗等等，但是要有个主题需要什么呢？
985 01:26:37,384 --> 01:26:45,161 说话者 SPEAKER_01：也许正是缺乏主题，使得数字在主观上可能优于模拟。
986 01:26:45,863 --> 01:26:46,444 说话人 SPEAKER_01: 我不知道。
但是我认为，我们也讨论了通过共享知识的好处，这些数字系统的其中一个好处是它能够共享知识，并且还能压缩知识，通过压缩，你正在在事实和信息之间创建新的联系，而且这种压缩本身可能会提高
988 01：27：11,804 --> 01：27：16,832 演讲者 SPEAKER_01：我们的理解并建立这些新的和令人惊讶的联系。
989 01：27：16,913 --> 01：27：20,159 演讲者 SPEAKER_01：我想这也是我觉得非常有趣的事情。
990 01：27：21,421 --> 01：27：24,627 演讲者 SPEAKER_01：当然，我们谈到了风险和道德等等。
991 01:27:25,128 --> 01:27:32,563 说话人 SPEAKER_01：通过创建 AI 模型训练数据来探讨伦理问题。
992 01:27:32,542 --> 01:27:37,029 说话人 SPEAKER_01：我认为这也是一个值得深入思考的有趣话题。
993 01:27:37,310 --> 01:27:49,810 说话人 SPEAKER_01：并且结合提供反馈强化，希望推动这些模型更加合理，更符合我们的喜好。
994 01:27:50,988 --> 01:28:03,929 说话人 SPEAKER_01：所以我想以结束，实际上，当我自我介绍时，我说我是计算机科学教授，得到的回应是，我一生从未上过计算机科学课程，但我获得了图灵奖。
995 01:28:03,948 --> 01:28:11,780 讲者 SPEAKER_01: 然后他说我不做物理学研究，但我获得了诺贝尔物理学奖。
996 01:28:11,761 --> 01:28:18,069 讲者 SPEAKER_01: 所以我想我们都有希望，也许有一天我们也能取得伟大成就。
997 01:28:19,372 --> 01:28:29,386 讲者 SPEAKER_01: 但我认为 AI 是一个如此广泛的领域，它触及、连接、利用并影响着许多其他研究领域，这也是很有趣的。
998 01:28:29,988 --> 01:28:35,195 讲者 SPEAKER_01: 回到这一点，我指的是诺贝尔奖，我认为这是一个非常有趣的问题。
999 01:28:35,655 --> 01:28:40,923 讲者 SPEAKER_01：什么时候每一个诺贝尔奖都将被你说成是由 AI 支持的？
1000 01:28:40,904 --> 01:28:49,939 讲者 SPEAKER_01：或者什么时候一个个人能够通过使用 AI 获得每一个诺贝尔奖？
1001 01:28:50,560 --> 01:28:53,826 讲者 SPEAKER_01：或者 AI 本身能够获得诺贝尔奖吗？
1002 01:28:54,226 --> 01:28:56,010 讲者 SPEAKER_01：我想这更值得怀疑。
1003 01:28:56,176 --> 01:29:09,451 说话人 SPEAKER_01: 但真正要得出结论，你提到这一点时非常不确定，我认为这可能是我们都能达成共识的事情之一，那就是未来有很多不确定性。
1004 01:29:09,771 --> 01:29:11,913 说话人 SPEAKER_01: 我们根本不知道会发生什么。
1005 01:29:12,073 --> 01:29:16,279 说话人 SPEAKER_01: 当然，作为一个科学家，作为一个研究人员，我把这看作是一个挑战。
1006 01:29:16,779 --> 01:29:22,126 说话人 SPEAKER_01: 所以我鼓励大家，让我们一起找出解决办法，共同解决这个问题。
1007 01:29:22,166 --> 01:29:23,247 说话人 SPEAKER_01: 谢谢。
1008 01:29:31,090 --> 01:29:35,518 说话人 SPEAKER_07: 在那次上诉之后，很难继续进行，但我们必须要结束。
1009 01:29:35,537 --> 01:29:43,490 说话人 SPEAKER_07: 关于这件事将如何进行，我想简单提一下，因为我们需要引导 Renton 教授离开。
1010 01:29:44,273 --> 01:29:51,685 说话人 SPEAKER_07: 所以在我们结束之前，请大家保持坐姿一会儿，他有一份非常紧凑的日程。
1011 01:29:51,704 --> 01:29:53,108 演讲者 SPEAKER_07：那么请尊重这一点。
1012 01:29:53,127 --> 01:29:55,511 演讲者 SPEAKER_07：非常感谢。
1013 01:29:55,492 --> 01:29:57,314 演讲者 SPEAKER_07：我觉得这非常令人费解。
1014 01:29:57,434 --> 01:29:59,496 演讲者 SPEAKER_07：我们都因此感到很有活力。
1015 01:30:00,356 --> 01:30:03,341 说话人 SPEAKER_07：从某种意义上说，我们正在步入未知。
1016 01:30:04,341 --> 01:30:12,690 说话人 SPEAKER_07：我认为我们需要带上一张图片，我想我会把它发给 Hinton 教授。
1017 01:30:12,730 --> 01:30:19,279 说话人 SPEAKER_07：我们有一个角色，你怎么称呼这些，一个漫画，现在我明白了这个法语词。
1018 01:30:19,738 --> 01:30:23,203 说话人 SPEAKER_07：我们有一个连载角色，一个插画角色，
1019 01:30:23,182 --> 01:30:25,447 说话人 SPEAKER_07: 卡通，非常感谢。
1020 01:30:25,627 --> 01:30:27,529 说话人 SPEAKER_07: 瑞典有一个叫做 Bamse 的卡通。
1021 01:30:28,011 --> 01:30:28,832 说话人 SPEAKER_07: 它是一只泰迪熊。
1022 01:30:29,472 --> 01:30:34,279 说话人 SPEAKER_07: 当它喝了魔法蜂蜜后就会变得非常强壮。
1023 01:30:34,900 --> 01:30:41,591 说话人 SPEAKER_07：他的正常回报是当你真的、真的强大时，你需要非常善良。
1024 01:30:42,212 --> 01:30:47,301 说话人 SPEAKER_07：所以我认为这是我们向前推进时需要构建到模型中的东西。
1025 01:30:48,867 --> 01:30:55,073 说话人 SPEAKER_07：有人告诉我一个小秘密，所以现在就是你们都要出力的惊喜时刻。
1026 01:30:55,555 --> 01:30:57,936 说话人 SPEAKER_07：因为明天，Hinton 教授过生日。
1027 01:30:58,457 --> 01:31:08,269 讲者 SPEAKER_07：我认为我们应该定稿，而不是给他一个温暖的握手，我要求你们举手，我们将进行瑞典式鼓掌。
1028 01:31:08,288 --> 01:31:11,353 讲者 SPEAKER_07：那么，为 Geoffrey Hinton 鼓掌。
1029 01:31:11,533 --> 01:31:11,993 讲者 SPEAKER_07：为他鼓掌。
1030 01:31:12,234 --> 01:31:12,573 讲者 SPEAKER_07：嘿嘿！

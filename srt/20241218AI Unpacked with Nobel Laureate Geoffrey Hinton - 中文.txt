1 00:00:00,031 --> 00:00:01,252 说话人 SPEAKER_02: Jeff，欢迎。
2 00:00:01,433 --> 00:00:03,214 说话人 SPEAKER_02: 我们非常高兴今天能邀请您来。
3 00:00:03,314 --> 00:00:09,042 说话人 SPEAKER_02: 我们与世界上一些最大公司的 HR 负责人齐聚一堂。
4 00:00:09,202 --> 00:00:12,586 说话人 SPEAKER_02: 我们试图弄清楚人工智能。
5 00:00:13,348 --> 00:00:15,590 说话人 SPEAKER_02：我们真的很想知道未来会是什么样子。
6 00:00:16,172 --> 00:00:19,356 说话人 SPEAKER_02：但为了理解这一点，我想回顾一下过去。
7 00:00:19,335 --> 00:00:35,604 说话人 SPEAKER_02：如果我们回顾到，比如说，2010 年左右，那么大约 15 年前，如果你尝试去，你知道的，Geoffrey Hinton 在 2010 年的预测，你对进步速度的预测是过于乐观还是过于悲观？
8 00:00:36,485 --> 00:00:38,609 说话人 SPEAKER_02：从那时起，这个领域是如何发展的？
9 00:00:39,399 --> 00:00:41,384 说话人 SPEAKER_00: 所以稍后再问我关于 2016 的事情。
10 00:00:43,508 --> 00:00:57,375 说话人 SPEAKER_00: 所以我认为，如果你在 2010 年问那些甚至相当热情、相信神经网络的人，我们现在会达到什么程度，他们不会相信我们会拥有像 GPT-4 这样的东西。
11 00:00:57,490 --> 00:01:06,060 说话人 SPEAKER_00: 他们会说，不，在接下来的 14 年里，你们不可能开发出对任何事情都精通的东西。
12 00:01:06,980 --> 00:01:08,843 说话人 SPEAKER_00: 不是一个很好的专家，但是对任何事情都精通的专家。
13 00:01:08,864 --> 00:01:24,121 说话人 SPEAKER_00: 你不可能拥有一个系统，你可以问任何你想问的问题，无论是关于英国税法的某个晦涩的问题，还是关于如何解方程的某个奇怪问题，它都能给出一个相当不错的答案，一个比 99%的人给出的答案都要好的答案。
14 00:01:25,721 --> 00:01:27,885 说话人 SPEAKER_00: 这太惊人了，我们无法预测到这一点。
15 00:01:28,989 --> 00:01:32,856 说话人 SPEAKER_02: 所以进步的速度比你预期的要快。
16 00:01:33,477 --> 00:01:33,798 说话人 SPEAKER_00: 是的。
17 00:01:33,819 --> 00:01:34,680 说话人 SPEAKER_02：你能分享更多吗？
18 00:01:35,100 --> 00:01:40,070 说话人 SPEAKER_02：作为该领域的主要研究人员之一，体验这个过程是怎样的？看着它加速发展是什么感觉？
19 00:01:40,090 --> 00:01:45,822 说话人 SPEAKER_00：这太令人惊叹了，因为在 80 年代，当 Rommelhart
20 00:01:45,921 --> 00:01:51,028 说话人 SPEAKER_00：重新发明了反向传播算法，他重新发现了它，我和他一起合作将其用于其他事物。
21 00:01:51,650 --> 00:01:55,293 说话者 说话者_00：起初我们认为这可以解决一切问题。
22 00:01:55,754 --> 00:02:00,320 说话者 说话者_00：我们得到了一种可以学习的，而且似乎没有限制。
23 00:02:00,340 --> 00:02:03,945 说话者 说话者_00：然后我们非常失望，我们不明白为什么它没有工作得更好。
24 00:02:04,727 --> 00:02:11,096 说话者 说话者_00：部分原因是架构问题，大约30年来，我们使用了一种看起来像这样的输入-输出函数，而我们应该使用一种看起来像这样的函数。
25 00:02:12,877 --> 00:02:14,460 说话人 SPEAKER_00: 太疯狂了。
26 00:02:15,772 --> 00:02:29,371 说话人 SPEAKER_00: 但主要是规模问题，我们当时根本不明白这个想法只有在拥有大量连接和大量训练数据以及巨大的计算能力时才能真正发挥其作用。
27 00:02:30,111 --> 00:02:31,413 说话人 SPEAKER_00: 所以我们当时做不到。
28 00:02:31,473 --> 00:02:39,525 说话人 SPEAKER_00: 如果当时我们说，是的，但如果我们把它放大一百万倍，并拥有百万倍的数据，它真的会起作用，那听起来就像一个可怜的借口。
29 00:02:40,164 --> 00:02:41,426 说话人 说话人_00: 但结果证明这是真的。
30 00:02:41,407 --> 00:02:42,388 说话人 说话人_02: 这很令人着迷。
31 00:02:42,829 --> 00:02:52,641 说话人 说话人_02: 所以我们之前讨论的一个问题是，如果我们使用“下一个词预测”这个术语，那么大型语言模型所做的事情被低估了。
32 00:02:52,701 --> 00:02:58,128 说话人 说话人_02: 我们的体验是，它们可以进行推理，它们具有一定的智能程度。
33 00:02:58,149 --> 00:03:00,271 说话人 SPEAKER_02：你能分享更多关于它是如何产生的吗？
34 00:03:00,872 --> 00:03:08,342 说话人 SPEAKER_00：所以很多人说这些只是使用统计技巧，他们实际上并不理解他们在说什么，他们只是在用相关性。
35 00:03:09,266 --> 00:03:13,218 说话人 SPEAKER_00：但如果你问这些人，嗯，你如何看待人们理解的方式？
36 00:03:14,582 --> 00:03:19,497 说话人 SPEAKER_00：如果他们是符号人工智能的拥护者，他们的模型是我们有符号表达式，我们用符号规则来操作它们。
37 00:03:19,729 --> 00:03:21,312 说话人 说话人_00：这从来就没有那么好地工作过。
38 00:03:21,372 --> 00:03:23,597 说话人 说话人_00：它几乎没像大型语言模型那样工作得那么好。
39 00:03:24,198 --> 00:03:35,437 说话人 说话人_00：如果你有认知科学家，他们会提出各种各样的解释，但我的最初的小型语言模型并不是为了做自然语言处理（NLP）而设计的。
40 00:03:35,736 --> 00:03:39,282 说话人 说话人_00：它是为了展示人们如何学习单词的意义而设计的。
41 00:03:39,302 --> 00:03:41,045 说话人 说话人_00: 这是一个关于人的模型。
42 00:03:41,161 --> 00:03:48,312 说话人 说话人_00: 这是一个非常简单的模型，但这是我们目前拥有的关于人如何理解句子的最佳模型，即这些大型语言模型。
43 00:03:48,832 --> 00:03:53,000 说话人 说话人_00: 我们并没有不同的模型来描述人的工作方式，它们的工作方式是相同的。
44 00:03:54,923 --> 00:03:57,807 说话人 说话人_00: 我们目前拥有的关于人如何工作的唯一良好模型就是这样的。
45 00:03:58,968 --> 00:04:02,694 说话人 SPEAKER_00：我认为他们真的理解了，并且是以与我们相同的方式理解的。
46 00:04:03,757 --> 00:04:08,163 说话人 SPEAKER_02：这些大型语言模型可能已经内置了那种创造力。
47 00:04:08,278 --> 00:04:13,508 说话人 SPEAKER_00：是的，很多人说，你知道，这些语言模型会做常规的事情，但人是富有创造力的。
48 00:04:14,109 --> 00:04:21,040 说话人 SPEAKER_00：嗯，如果你进行一个标准的创造力测试，我认为现在的这些大型语言模型比 90%的人做得更好。
49 00:04:22,322 --> 00:04:25,127 说话人 SPEAKER_00: 他们不具创造力的想法简直是疯狂。
50 00:04:26,831 --> 00:04:32,379 说话人 SPEAKER_00: 这与艺术家和硅谷之间的辩论非常相关。
51 00:04:32,360 --> 00:04:38,269 说话人 SPEAKER_00: 关于这些 AI 模型是否只是窃取艺术家的创作。
52 00:04:39,512 --> 00:04:45,081 说话者 说话者_00：显然，要创作一个流派的作品，你必须听很多那个流派的音乐。
53 00:04:45,802 --> 00:04:46,822 说话者 说话者_00：但这也适用于一个人。
54 00:04:47,184 --> 00:04:54,956 说话者 说话者_00：当一个人在某个流派中创作新的音乐时，他们就像 AI 系统一样，在同样的方式上窃取了之前人的作品。
55 00:04:55,437 --> 00:04:59,463 说话者 说话者_00：所以 AI 系统并没有比其他音乐家更多地窃取它们。
56 00:04:59,865 --> 00:05:08,802 说话人 SPEAKER_02：如果你阅读对毕加索作品的解析，你会发现他明显借鉴了艺术传统。
57 00:05:08,982 --> 00:05:18,298 说话人 SPEAKER_02：我认为他，你知道，贝宁面具和其他许多领域，他将它们融合成一种新的方法，但他是在他所看到的事物的基础上构建的。
58 00:05:18,939 --> 00:05:23,528 说话人 SPEAKER_02：我认为如果 AI 看到了一切，就没有理由认为它不能做到同样的事情。
59 00:05:23,591 --> 00:05:25,254 说话人 SPEAKER_00：是的，所以 AI 可以具有创造力。
60 00:05:26,214 --> 00:05:32,223 说话者 说话者_00：当然，要以一种特别的方式具有创造力，你需要看看那些用这种方式创作的艺术作品。
61 00:05:33,444 --> 00:05:41,514 说话者 说话者_00：但是很难说这是剽窃，因为它并没有像拼贴画一样将其他事物的碎片拼凑在一起。
62 00:05:42,175 --> 00:05:49,184 说话者 说话者_00：它是以与人类相同的方式理解底层结构，然后以相同类型的底层结构生成新内容。
63 00:05:49,805 --> 00:05:51,826 说话者 说话者_00：所以它就像一个人在创造东西一样。
64 00:05:51,891 --> 00:05:55,398 说话者 SPEAKER_02：现在，你在本科阶段也学习了心理学，研究过人脑。
65 00:05:55,577 --> 00:05:58,583 说话者 SPEAKER_02：这与我们的大脑有什么不同？
66 00:05:59,204 --> 00:06:01,387 说话者 SPEAKER_00：我们大约有 1 亿个突触。
67 00:06:02,528 --> 00:06:10,401 说话者 SPEAKER_00：尽管许多突触用于其他事情，如呼吸、大脑皮层、新皮层，但其中大部分都在这里。
68 00:06:11,624 --> 00:06:17,173 说话者 SPEAKER_00: 因此我们有很多更多可适应的参数。
69 00:06:17,271 --> 00:06:22,798 说话者 SPEAKER_00: 比这些大型语言模型还要多，这让人很奇怪，GPT-4 知道的信息比我们多上千倍。
70 00:06:23,418 --> 00:06:24,420 说话者 SPEAKER_02: 你说的是一亿。
71 00:06:24,439 --> 00:06:25,961 说话者 SPEAKER_02: 我想你是说一百万亿。
72 00:06:25,980 --> 00:06:26,721 说话人 SPEAKER_02：我说过一亿吗？
73 00:06:26,742 --> 00:06:27,723 说话人 SPEAKER_02：我觉得你说的是一亿。
74 00:06:27,783 --> 00:06:29,345 说话人 SPEAKER_00：我可能是个政客。
75 00:06:30,004 --> 00:06:31,666 说话人 SPEAKER_00：我分不清百万和万亿。
76 00:06:32,108 --> 00:06:33,288 说话人 说话人_00: 一百万亿，是的。
77 00:06:33,309 --> 00:06:35,350 说话人 说话人_02: 一百万亿个突触。
78 00:06:35,932 --> 00:06:37,552 说话人 说话人_02: 因此这非常有趣。
79 00:06:37,572 --> 00:06:46,822 说话人 说话人_02: 所以我们有大型语言模型，它们的连接数比人脑小两个数量级，但知道大量的信息。
80 00:06:46,802 --> 00:06:49,550 说话人：是的，他们并不是在所有事情上都很擅长。
81 00:06:50,132 --> 00:06:53,040 说话人：所以他们知道的比任何一个人都要多上千倍。
82 00:06:53,101 --> 00:07:01,706 说话人：其中一个原因是可以有多个完全相同的神经网络在不同的硬件上运行。
83 00:07:01,990 --> 00:07:06,355 说话人：因此，你可以让一个副本查看互联网的这一部分，另一个副本查看互联网的另一部分。
84 00:07:07,016 --> 00:07:09,380 说话人 SPEAKER_00: 它们都能找出如何改变自己的权重。
85 00:07:09,882 --> 00:07:15,750 说话人 SPEAKER_00: 如果你只是平均这些变化，那么两个副本都从各自的经验中学习了。
86 00:07:16,331 --> 00:07:17,593 说话人 SPEAKER_00: 现在取 1000 个这样的。
87 00:07:18,153 --> 00:07:20,637 说话人 SPEAKER_00: 想象一下如果我们能取 1000 个人。
88 00:07:21,057 --> 00:07:22,860 说话人 SPEAKER_00: 他们都可以去上不同的课程。
89 00:07:23,581 --> 00:07:28,369 说话人 SPEAKER_00: 最后，每个人都知道了每个人所经历的事情。
90 00:07:28,449 --> 00:07:32,776 说话人 SPEAKER_02: 我们已经谈了一些关于记忆以及记忆是如何存储在人类大脑中的内容。
91 00:07:32,817 --> 00:07:36,824 说话人 SPEAKER_02: 我们还讨论了快速权重以及它们是如何调整的。
92 00:07:37,305 --> 00:07:43,536 说话者 SPEAKER_02：在LLM架构中，人类仍然在哪些方面做得特别出色，人类大脑做得更好？
93 00:07:43,576 --> 00:07:50,728 说话者 SPEAKER_00：我认为我们仍然可以从有限的数据中学习得更好，我们并不完全清楚我们是如何做到这一点的。
94 00:07:52,295 --> 00:07:58,365 说话者 SPEAKER_00：我们知道人类大脑在许多不同的时间尺度上连接强度会发生变化。
95 00:07:59,706 --> 00:08:05,516 说话者 SPEAKER_00：所以，我在 1979 年第一次见到 Terry Sanofsky 时，那基本上是我们谈论的第一件事。
96 00:08:06,077 --> 00:08:10,064 说话人 SPEAKER_00：这些神经网络模型只有两个时间尺度。
97 00:08:10,084 --> 00:08:14,471 说话人 SPEAKER_00：它们有神经元活动变化的时间尺度，因此
98 00:08:15,177 --> 00:08:19,142 说话人 SPEAKER_00：每次输入不同的句子，神经网络活动都会改变。
99 00:08:19,783 --> 00:08:25,773 说话人 SPEAKER_00：然后它们还有权重值、连接字符串的活动，这些活动变化得很慢。
100 00:08:25,793 --> 00:08:26,834 说话人 说话人_00: 那里就是所有知识所在。
101 00:08:27,815 --> 00:08:29,197 说话人 说话人_00: 它们只有这两个时间尺度。
102 00:08:29,879 --> 00:08:32,562 说话人 说话人_00: 现在你可以有更多的时间尺度。
103 00:08:32,602 --> 00:08:36,990 说话人 说话人_00: 假设你还有一个时间尺度，那里有权重。
104 00:08:37,289 --> 00:08:43,158 说话者 SPEAKER_00: 您有变化缓慢的权重，但还有一个变化很快但衰减迅速的权重覆盖层。
105 00:08:43,898 --> 00:08:46,721 说话者 SPEAKER_00: 这为您带来了各种额外的良好特性。
106 00:08:47,743 --> 00:09:08,251 说话者 SPEAKER_00: 例如，如果我给你一个意想不到的词，比如黄瓜，然后过几分钟我给你戴上耳机，我在耳机里放很多噪音，然后播放单词，你只能勉强听到它们，大多数你都无法辨认出它们是什么，但你将能更好地辨认出单词“黄瓜”。
107 00:09:08,552 --> 00:09:09,975 说话者 SPEAKER_00: 因为你在两分钟前已经听过了。
108 00:09:10,557 --> 00:09:12,740 说话人 说话人_00: 所以问题是，它存储在哪里？
109 00:09:12,880 --> 00:09:14,423 说话人 说话人_00: 它并不存储在神经活动中。
110 00:09:14,443 --> 00:09:15,306 说话人 说话人_00: 你负担不起这样做。
111 00:09:15,365 --> 00:09:16,869 说话人 说话人_00: 你会消耗太多的神经元。
112 00:09:16,889 --> 00:09:20,755 说话人 SPEAKER_00：它并不存储在长期权重中，因为几天后它就会消失。
113 00:09:21,378 --> 00:09:24,563 说话人 SPEAKER_00：它存储在突触强度的短期变化中。
114 00:09:25,044 --> 00:09:26,847 说话人 SPEAKER_00：目前我们的模型中还没有这个。
115 00:09:26,827 --> 00:09:31,852 说话人 SPEAKER_02：我的本科研究实际上是在研究一个非常类似的东西，只不过它是预处理觉的。
116 00:09:32,293 --> 00:09:34,716 说话人 SPEAKER_02：所以你会非常快地闪现“黄瓜”这个词。
117 00:09:35,115 --> 00:09:36,616 说话人 SPEAKER_02：你没有注意到你已经见过它了。
118 00:09:36,638 --> 00:09:36,998 说话人 SPEAKER_00：它是潜意识的。
119 00:09:37,298 --> 00:09:37,859 说话人 SPEAKER_02：潜意识。
120 00:09:38,119 --> 00:09:44,325 说话者 SPEAKER_02：然后你更有可能通过在词组中看到它或者听到它来挑选它。
121 00:09:44,784 --> 00:09:54,674 说话者 SPEAKER_02：所以有一个问题，你是如何处理“cucumber”这个词，在未意识到的情况下，你的大脑存储了它，并且能够更快地识别它。
122 00:09:54,654 --> 00:09:58,572 说话者 SPEAKER_00：我认为还有一种现象，就是快速闪现“cucumber”这个词。
123 00:09:58,687 --> 00:10:02,052 说话者 SPEAKER_00：然后你将更擅长听到，识别“lettuce”这个词。
124 00:10:02,091 --> 00:10:04,355 说话人 SPEAKER_00: 是的，这正是我们挑选的内容。
125 00:10:04,934 --> 00:10:07,018 说话人 SPEAKER_02: 是指类似词语的关联。
126 00:10:07,038 --> 00:10:11,923 说话人 SPEAKER_00: 是的，不仅仅是得到了这个词，你得到了这个词的语义，而且是在没有任何意识的情况下。
127 00:10:12,124 --> 00:10:25,219 说话人 SPEAKER_02: 你能分享一些例子吗？比如向一个可能没有在训练数据中包含的新信息，它是如何推理并得出一个类似于人类类比推理的答案的？
128 00:10:27,054 --> 00:10:30,780 说话人 SPEAKER_00：嗯，我可以举一个很好的例子，说明它做类比的能力，这是大多数人做不到的。
129 00:10:31,261 --> 00:10:32,082 说话人 SPEAKER_02：我很想听听这个例子。
130 00:10:32,484 --> 00:10:43,061 说话人 SPEAKER_00：所以我在 GPT-4 没有接入网络的时候问过它，为什么堆肥堆像原子弹？
131 00:10:43,081 --> 00:10:44,864 说话人 SPEAKER_02：我无法回答这个问题。
132 00:10:44,923 --> 00:10:46,886 说话人 SPEAKER_00: 太棒了。
133 00:10:47,254 --> 00:10:51,625 说话人 SPEAKER_00: 它说时间尺度非常不同，能量尺度也非常不同。
134 00:10:52,288 --> 00:10:54,293 说话人 SPEAKER_00: 然后它继续讲述了连锁反应。
135 00:10:54,373 --> 00:10:57,841 说话人 SPEAKER_00: 它继续讲述了在堆肥堆里，越热，产生的热量就越快。
136 00:10:58,383 --> 00:11:02,614 说话人 说话人_00：在原子弹中，它产生的中子越多，它产生中子的速度就越快。
137 00:11:03,201 --> 00:11:09,970 说话人 说话人_00：因此，它看到了底层物理学的相似性，GPT-4 也看到了。
138 00:11:10,309 --> 00:11:12,692 说话人 说话人_00：现在，我提问的时候它可能没有看到。
139 00:11:12,712 --> 00:11:14,094 说话人 说话人_00：它可能在训练过程中看到了。
140 00:11:14,934 --> 00:11:16,517 说话人 SPEAKER_00: 我们看到了很多类比。
141 00:11:17,238 --> 00:11:19,320 说话人 SPEAKER_00: 我们实际上是在权重中存储东西。
142 00:11:20,061 --> 00:11:30,913 说话人 SPEAKER_00: 如果它们是类似的结构，那么在权重中存储东西会容易得多，因为你可以共享权重。
143 00:11:32,833 --> 00:11:34,879 说话人 SPEAKER_00: 这些大型语言模型也是如此。
144 00:11:35,340 --> 00:11:41,414 说话者 说话者_00: 为了存储大量信息，他们必须看到他们所学习的事实之间的类比。
145 00:11:42,177 --> 00:11:45,004 说话者 说话者_00: 他们已经看到了许多没有人见过的类比。
146 00:11:46,013 --> 00:11:47,075 说话者 说话者_02: 这非常令人着迷。
147 00:11:47,115 --> 00:11:59,797 说话者 说话者_02: 为了将那么多的信息压缩到那么少的参数中，他们必须隐含地理解和编码他们的权重中的类比？
148 00:11:59,836 --> 00:12:05,506 说话人：是的，其中许多类比都是深层类比，比如堆肥堆和原子弹之间的类比。
149 00:12:05,756 --> 00:12:13,671 说话人：他们可能会发现，他们可能已经将类比嵌入到了权重中，这些类比是我们人类自己从未真正思考过的。
150 00:12:13,971 --> 00:12:21,446 说话人：是的，因为 GPT-4 在物理学上不是一个很好的专家，但在古希腊文学上也不是一个很好的专家。
151 00:12:21,926 --> 00:12:28,879 说话人：而且很可能在古希腊文学中存在一些与量子力学中的一些奇怪事物相似的东西，但没有人曾经看到过这两者之间的联系。
152 00:12:28,859 --> 00:12:32,865 说话者 SPEAKER_02：因此，在 2010 年，你开始理解了可能发生的事情。
153 00:12:32,884 --> 00:12:36,250 说话者 SPEAKER_02：你和伊利亚赢得了 ImageNet。
154 00:12:36,610 --> 00:12:39,535 说话者 SPEAKER_02：我想是...亚历克斯·克拉舍夫斯基。
155 00:12:39,615 --> 00:12:40,517 说话者 SPEAKER_00：这被称为 AlexNet。
156 00:12:40,677 --> 00:12:41,197 说话人 SPEAKER_02: AlexNet。
157 00:12:41,217 --> 00:12:41,778 说话人 SPEAKER_02: 哦，没错。
158 00:12:41,940 --> 00:12:50,513 说话人 SPEAKER_00: 他是一位出色的程序员，并且他成功地在 NVIDIA GPU 上实现了卷积网络，效率远超其他人。
159 00:12:50,932 --> 00:12:53,635 说话人 SPEAKER_02: 到那时，你已经看到了规模的重要性。
160 00:12:54,057 --> 00:12:58,384 说话人 SPEAKER_02：过去 10 年，2016 年，为什么那个时刻对你来说很重要？
161 00:12:58,403 --> 00:13:05,034 说话人 SPEAKER_00：哦，我提到 2016 年的原因是因为我在 2016 年做出过一个错误的预测。
162 00:13:05,576 --> 00:13:08,681 说话人 SPEAKER_00：我预测五年后我们不再需要放射科医生了。
163 00:13:09,903 --> 00:13:12,106 说话人 SPEAKER_00：这让一些放射科医生感到不安。
164 00:13:12,648 --> 00:13:13,609 说话人 SPEAKER_00：结果证明是错误的。
165 00:13:14,190 --> 00:13:17,174 说话人 SPEAKER_00：我可能差了两个数量级，甚至可能是三个数量级。
166 00:13:18,285 --> 00:13:21,211 说话人 SPEAKER_00：时间会到来，我的意思是扫描。
167 00:13:21,490 --> 00:13:23,975 说话人 SPEAKER_00：实际上，我记得当时你说的是五年，也许十年。
168 00:13:24,034 --> 00:13:38,336 说话者 SPEAKER_00：但当他们可能在 10 年后阅读扫描时，我非常自信，你几乎会通过 AI 来阅读几乎所有医学扫描，然后医生会进行检查。
169 00:13:38,653 --> 00:13:41,738 说话者 SPEAKER_00：AI 将会比医生更优秀。
170 00:13:41,758 --> 00:13:44,662 说话者 SPEAKER_00：AI 在扫描中可以看到医生看不到的更多东西。
171 00:13:44,682 --> 00:13:54,254 说话者 SPEAKER_00：所以我的妻子患有癌症，她时不时地做 CAT 扫描，他们会说肿瘤是两厘米，然后一个月后他们会说肿瘤是三厘米。
172 00:13:54,674 --> 00:14:00,062 说话人 说话人_00：这个东西的形状像章鱼，但是用两个来衡量章鱼的大小并不是一个好方法，对吧？
173 00:14:00,543 --> 00:14:04,788 说话人 说话人_00：你可能想了解更多关于正在发生的事情，而借助人工智能我们可以做到这一点。
174 00:14:05,450 --> 00:14:08,052 说话人 说话人_00：医生们做不到这一点，因为他们没有……
175 00:14:08,995 --> 00:14:10,318 说话人 说话人_00：他们不知道结果会怎样。
176 00:14:10,880 --> 00:14:17,923 说话者 SPEAKER_00：但我认为，借助人工智能，我们将能够看到关于癌症的信息，这些信息能告诉你它们是否即将转移等。
177 00:14:19,408 --> 00:14:22,397 说话者 SPEAKER_00：我们知道图像中还有更多未被使用的信息。
178 00:14:22,850 --> 00:14:36,328 说话者 SPEAKER_02：正如你之前所说，如果你有 500 名医生，每人都能用一生的时间来观察 500 张图像，并看到它们的进展，然后将他们的知识压缩起来，这比一个医生的知识要多得多。
179 00:14:36,509 --> 00:14:42,477 说话者 SPEAKER_00：是的，所以没有哪位放射科医生能够在这些设备真正擅长视觉之后与之竞争。
180 00:14:42,457 --> 00:14:47,065 说话人 说话人_00: 但例如，在学费方面，我们将拥有非常优秀的 AI 导师。
181 00:14:47,085 --> 00:14:52,451 说话人 说话人_00: 而有许多研究表明，将一个学生放入教室，他们会以一定的速度学习。
182 00:14:52,732 --> 00:14:54,634 说话人 说话人_00: 给他们一个私人导师，他们会以两倍的速度学习。
183 00:14:55,677 --> 00:15:04,389 说话人 说话人_00: 因此我们知道，AI 正接近足够理解人们误解的能力。
184 00:15:04,428 --> 00:15:08,414 说话者 说话者_00：一旦你接受了知道你不懂的实体的私人辅导，
185 00:15:08,480 --> 00:15:17,022 说话者 说话者_00：这将比仅仅坐在教室里听广播更有效率的学习方式。
186 00:15:17,803 --> 00:15:24,902 说话者 说话者_00：因此，我认为在医疗和教育领域，这将带来巨大的优势。
187 00:15:25,404 --> 00:15:37,142 说话者 说话者_02：我想花点时间谈谈这个教育例子，因为我们受到了这个想法的启发，那就是为每个人提供导师，为在传统教育中学习的人提供导师，为在工作的人提供领导力教练。
188 00:15:37,663 --> 00:15:40,687 说话人 SPEAKER_02：对于我们来说，个性化这个想法很重要。
189 00:15:41,207 --> 00:15:49,640 说话人 SPEAKER_02：你认为 AI 能理解你和你所处的环境，几乎能像为世界信息做图书管理员一样，但只为你自己服务吗？
190 00:15:50,126 --> 00:16:04,136 说话人 SPEAKER_00：当然可以，几周前我获得了诺贝尔奖，我以前从来没有个人助理，大学给了我一个个人助理，她现在对我了解很多，这真是太棒了。
191 00:16:04,639 --> 00:16:07,063 说话人 SPEAKER_00：如果我们可以用 AI 做到这一点，那么每个人都可以拥有这样的服务。
192 00:16:07,303 --> 00:16:08,085 说话者 SPEAKER_02：这太令人着迷了。
193 00:16:08,125 --> 00:16:11,250 说话者 SPEAKER_02：你还得让她熟悉情况，给她一个背景。
194 00:16:11,289 --> 00:16:15,755 说话者 SPEAKER_02：如果她能无限访问你的信息，她会更有帮助。
195 00:16:15,775 --> 00:16:17,038 说话者 SPEAKER_00：是的。
196 00:16:17,778 --> 00:16:20,943 说话者 SPEAKER_00：但我认为这是一种很好的情况。
197 00:16:20,984 --> 00:16:26,731 说话者 SPEAKER_00：我们都得到了这些非常智能的个人助理，他们了解我们的一切并帮助我们。
198 00:16:27,403 --> 00:16:38,455 说话者 SPEAKER_01：当我们思考构建 AI 产品时，经常被提及的是人机或人模同理心，以及帮助用户了解他们可能应该从模型中期待什么，以便他们知道如何正确引导它。
199 00:16:38,875 --> 00:16:41,077 说话者 SPEAKER_01：您如何看待软件方面的这个问题？
200 00:16:41,317 --> 00:16:49,126 说话人 SPEAKER_00：嗯，有一个实验，其中你让 AI 医生和真人医生与病人互动。
201 00:16:49,706 --> 00:16:52,009 说话人 SPEAKER_00：然后你问病人，你会如何评价他们的同情心？
202 00:16:52,389 --> 00:16:53,630 说话人 SPEAKER_00：AI 医生的表现要好得多。
203 00:16:54,471 --> 00:16:56,313 说话人 SPEAKER_00：AI 医生实际上会倾听病人的话。
204 00:16:57,947 --> 00:17:01,091 说话者 说话者_00：他们已经可以表现出同理心了。
205 00:17:02,092 --> 00:17:06,759 说话者 说话者_00：也许吧，我们认为同理心就像你思考的那样，那对我来说会是什么样子呢？
206 00:17:06,798 --> 00:17:08,721 说话者 说话者_00：然后你想，哦我的天哪，那对我来说会糟糕透顶。
207 00:17:08,760 --> 00:17:09,442 说话者 说话者_00：我真的很抱歉。
208 00:17:10,143 --> 00:17:17,152 说话者 SPEAKER_00：也许他们不会这样做，但他们在行为上似乎表现出很好的同理心。
209 00:17:17,172 --> 00:17:26,502 说话者 SPEAKER_00：如果我们有一个 AI 导师，我们希望它对学生们误解了某事表示同理心。
210 00:17:27,226 --> 00:17:29,569 说话者 SPEAKER_00：我相信他们能够做到这一点。
211 00:17:30,731 --> 00:17:38,486 说话者 SPEAKER_02：我认为您会说，如果它表现出同理心，它可能和我们表现出同理心的方式是一样的，请纠正我如果我说错了。
212 00:17:38,506 --> 00:17:44,557 说话者 SPEAKER_02：因此，这不仅仅是一个展览，就像表演式同理心，它将显得更加真诚。
213 00:17:44,596 --> 00:17:45,137 说话者 SPEAKER_02：是这样吗？
214 00:17:45,578 --> 00:17:46,840 说话者 SPEAKER_00：可能是真诚的同情。
215 00:17:48,604 --> 00:17:50,887 说话者 SPEAKER_00：我认为，要称之为真诚的同情，
216 00:17:52,031 --> 00:17:57,003 说话者 SPEAKER_00：眼睛必须与我们足够相似，才能想象出它们会是什么样子。
217 00:17:57,806 --> 00:18:07,087 说话者 SPEAKER_00：我们倾向于认为同理心是想象出你的感受，然后理解对方是如何感受的能力。
218 00:18:07,067 --> 00:18:10,933 说话者 SPEAKER_00：如果你没有这样做，那真是太糟糕了。
219 00:18:11,034 --> 00:18:12,316 说话者 SPEAKER_00：对此我深感抱歉。
220 00:18:12,576 --> 00:18:15,059 说话者 说话者_00：但你没有考虑这对你来说会怎样，对吧？
221 00:18:15,079 --> 00:18:16,821 说话者 说话者_00：这看起来更像是缺乏真诚的同理心。
222 00:18:17,603 --> 00:18:18,825 说话者 说话者_00：我当然可以做到这一点。
223 00:18:19,105 --> 00:18:20,247 说话者 说话者_02：我的意思是，我完全同意这一点。
224 00:18:20,267 --> 00:18:27,417 说话人 SPEAKER_02：但我认为文学之美在于它让你置身于他人的位置，你可以通过这种方式去体验。
225 00:18:27,597 --> 00:18:32,484 说话人 SPEAKER_02：你可以说，嗯，我从未处于那种位置，但现在我已经经历了那种体验。
226 00:18:32,464 --> 00:18:44,686 说话人 SPEAKER_02：如果你将世界文学压缩进那个模型，它们可能比我会更好地理解人类所经历的各种感受，并展现出同情心。
227 00:18:45,268 --> 00:18:46,390 说话人 SPEAKER_02：是的，可能会。
228 00:18:46,410 --> 00:18:47,672 说话人 SPEAKER_02：这真的很有趣。
229 00:18:47,652 --> 00:18:51,375 说话人 SPEAKER_02：所以，我想从社会的角度来放大一下视野。
230 00:18:51,454 --> 00:18:59,803 说话人 SPEAKER_02：过去几年，我们对LLMs的关注和报道非常多，非常热烈。
231 00:19:00,943 --> 00:19:07,049 说话人 SPEAKER_02：我们之前讨论过的一个话题是，当事物呈指数增长时，看到未来是多么困难。
232 00:19:07,470 --> 00:19:11,032 说话人 SPEAKER_02：你能分享一下你是如何体验这个的吗？
233 00:19:11,173 --> 00:19:13,055 说话人 SPEAKER_00：是的，我们不习惯指数级增长。
234 00:19:13,075 --> 00:19:17,118 说话人 SPEAKER_00：所以一个好的类比是如果你在夜间开车，
235 00:19:17,098 --> 00:19:21,486 说话人 SPEAKER_00：在一条你不知道的弯路上，你经常跟在前面车的尾灯后面开车。
236 00:19:22,227 --> 00:19:25,131 说话者 说话者_00：随着汽车离你越来越远，尾灯越来越暗。
237 00:19:25,791 --> 00:19:27,335 说话者 说话者_00：它们的亮度会呈二次方递减。
238 00:19:28,355 --> 00:19:34,144 说话者 说话者_00：所以，如果你将距离增加到三倍，它们的亮度会减少到原来的九分之一。
239 00:19:35,587 --> 00:19:36,628 说话者 说话者_00：这就是为什么你要尽量靠近的原因。
240 00:19:39,032 --> 00:19:40,776 说话人 SPEAKER_00: 有雾时，根本不是那样。
241 00:19:40,836 --> 00:19:42,397 说话人 SPEAKER_00: 完全不同。
242 00:19:42,579 --> 00:19:50,453 说话人 SPEAKER_00: 有雾时，如果你能在 100 码处看得很清楚，你就以为你能在 200 码处看到东西。
243 00:19:51,075 --> 00:19:56,345 说话人 SPEAKER_00: 但实际上，你在 100 码处看得很清楚，到了 200 码处就什么也看不见了，因为雾是指数级的。
244 00:19:56,384 --> 00:19:58,950 说话人 SPEAKER_00: 每单位距离，它会移除一定比例的光。
245 00:19:59,351 --> 00:20:03,458 说话人 SPEAKER_00: 这与我们习惯的线性或二次项非常不同。
246 00:20:03,438 --> 00:20:08,770 说话人 SPEAKER_00: 人们并不真正理解“指数”这个词，因为它被过度使用了。
247 00:20:09,292 --> 00:20:11,237 说话人 SPEAKER_00: 人们误用“指数”这个词，含义很多。
248 00:20:11,798 --> 00:20:16,349 说话人 SPEAKER_00：事实上，我认为他们误用指数的速度正在呈平方增长。
249 00:20:17,073 --> 00:20:37,038 说话人 SPEAKER_02：这让我想起了我小时候很喜欢的一个谜语，那就是如果你有一个开始只有一朵莲花的池塘，每天翻倍，直到第 30 天莲花覆盖了池塘，阳光被遮挡，池塘因此死亡，那么池塘什么时候会有一半的莲花？
250 00:20:37,019 --> 00:20:39,162 说话人 SPEAKER_02：答案是第 29 天。
251 00:20:39,402 --> 00:20:41,845 说话人 SPEAKER_02：但人们的直觉是，哦，可能是第 15 天左右。
252 00:20:42,484 --> 00:20:49,113 说话人 SPEAKER_02：有时候很难理解，因为我们没有经历过那种指数级增长是什么样的。
253 00:20:49,794 --> 00:20:52,217 说话人 SPEAKER_02：你认为关于工作的未来有什么想法吗？
254 00:20:52,376 --> 00:20:54,038 说话人 SPEAKER_02：我们稍微谈了一下劳动力问题。
255 00:20:54,759 --> 00:20:58,223 说话人 SPEAKER_02：一个每个人都能得到帮助的世界显然是美好的。
256 00:20:58,443 --> 00:21:02,689 说话人 SPEAKER_02：被取代的工作世界显然会带来很多社会压力。
257 00:21:02,669 --> 00:21:08,140 说话人 SPEAKER_02：那些领导大型公司的人应该如何思考在接下来的两到三年中如何应对？
258 00:21:08,727 --> 00:21:10,788 说话人 SPEAKER_00：显然会有失业现象。
259 00:21:11,769 --> 00:21:17,016 说话人 SPEAKER_00：所以我们不知道 AI 是否会淘汰很多工作。
260 00:21:17,036 --> 00:21:17,836 说话人 说话人_00: 我怀疑是这样的。
261 00:21:18,237 --> 00:21:18,998 说话人 说话人_00: 珍妮认为是这样的。
262 00:21:19,097 --> 00:21:20,960 说话人 说话人_00: 珍妮克，我的朋友，认为不是这样的。
263 00:21:21,461 --> 00:21:27,606 说话人 说话人_00: 在过去，像自动取款机这样的东西并没有导致出纳员大规模失业。
264 00:21:27,827 --> 00:21:33,874 说话人 SPEAKER_00: 他们只是做了更有趣、更复杂的事情，并且花费了更长的时间，所以你需要排队很长时间。
265 00:21:35,878 --> 00:21:39,083 说话人 SPEAKER_00: 所以可能会产生失业，也可能不会。
266 00:21:39,123 --> 00:21:43,709 说话人 SPEAKER_00: 我怀疑有一些工作可以用更多的这种技术。
267 00:21:44,769 --> 00:21:52,019 说话人 SPEAKER_00: 例如，如果他们使医生更有效率，我们所有人，尤其是老年人，都可以更多地利用医生的时间。
268 00：21：52,519 --> 00：21：56,984 演讲者 SPEAKER_00：如果你的医生效率是 10 倍，我就会得到 10 倍的医疗保健。
269 00:21:57,045 --> 00:21:57,285 演讲者 演讲者_00：太好了。
270 00：21：59,428 --> 00：22：01,329 议长 SPEAKER_00：不过，还有其他事情不是这样的。
一个人使用人工智能助手就能完成以前需要10个人才能完成的工作，而其他9个人将会失业。
272 00:22:10,282 --> 00:22:13,788 说话者 说话者_00：那样的问题在于，你的生产力提高了。
273 00:22:13,928 --> 00:22:15,108 说话者 说话者_00：这应该会帮助人们。
274 00:22:15,990 --> 00:22:21,698 说话者 说话者_00：但你让九个人失业，而一个富人变得更富有。
275 00:22:22,538 --> 00:22:24,240 说话者 说话者_00：这对社会非常不利。
276 00:22:24,480 --> 00:22:26,983 说话人 说话人_00：显然，我们无法看到很远的未来。
277 00:22:27,003 --> 00:22:31,829 说话人 说话人_00：如果你用雾的比喻，我认为墙会在三到五年内倒塌。
278 00:22:33,030 --> 00:22:36,655 说话人 说话人_00：我们相当有信心，我们有一些想法，知道接下来几年会发生什么。
279 00:22:37,115 --> 00:22:39,880 说话人 说话人_00：十年后，我们不知道会发生什么。
280 00:22:40,079 --> 00:22:41,582 说话人 说话人_00：通过回顾10年前，你可以看到这一点。
281 00:22:41,622 --> 00:22:44,125 说话人 说话人_00：我们完全没有想到会发生这样的事情。
282 00:22:45,640 --> 00:22:56,798 说话人 说话人_00：我认为公司应该通过让每个人都拥有一个智能 AI 助手的方向来导航。
283 00:22:57,384 --> 00:23:07,317 说话人 说话人_00：人们觉得他们会从这个智能助手那里得到更好的工作条件，你会提高生产力，这对每个人都是有益的。
接下来五年将会非常不平凡，用更好的词来形容可能找不到，而您在其中发挥了巨大的作用，帮助我们度过人工智能寒冬，度过那些可能并不像现在这么清晰的时刻，我想说，能进行这次对话是一种荣幸，谢谢您。
285 00：23：26,578 --> 00：23：29,117 演讲者 SPEAKER_00：是的，我真的很喜欢。
286 00:23:29,137 --> 00:23:29,923 说话者 SPEAKER_02：谢谢。
287 00：23：30,710 --> 00：23：31,496 议长 SPEAKER_01：非常感谢。
288 00：23：31,798 --> 00：23：32,099 议长 SPEAKER_02：不客气。
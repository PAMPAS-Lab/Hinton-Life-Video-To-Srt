1 00:00:00,031 --> 00:00:04,216 说话人 SPEAKER_01：嗯，这位被称为人工智能教父的人今晚在加拿大备受瞩目。
2 00:00:04,237 --> 00:00:05,177 说话人 SPEAKER_01：他就是杰弗里·辛顿。
3 00:00:05,299 --> 00:00:11,868 说话人 SPEAKER_01：今天，白宫人工智能委员会召开会议，讨论旨在管理人工智能风险的新措施。
4 00:00:12,349 --> 00:00:13,771 说话人 SPEAKER_01：这正是辛顿一直在警告的事情。
5 00:00:14,131 --> 00:00:21,481 说话人 SPEAKER_01：官员们表示，其他事项之外，强大人工智能系统的开发者必须向政府报告安全测试结果。
6 00:00:21,461 --> 00:00:27,048 说话人 SPEAKER_01：这发生在拜登总统签署关于人工智能监管的行政命令三个月后。
7 00:00:27,528 --> 00:00:30,513 说话人 SPEAKER_01：人工智能监管也是加拿大政府的优先事项。
8 00:00:30,952 --> 00:00:36,420 说话人 SPEAKER_01：我与杰弗里·辛顿详细讨论了人工智能系统的安全和保障问题。
9 00:00:36,859 --> 00:00:42,406 人工智能的教父再次被提及的英国-加拿大科学家现在成为我们的焦点。
10 00:00:43,450 --> 00:00:45,332 乔治·辛顿现在和我一起。
11 00:00:45,371 --> 00:00:46,012 很高兴见到你。
12 00:00:46,253 --> 00:00:47,033 感谢你接受这次采访。
13 00:00:47,094 --> 00:00:47,694 说话人 SPEAKER_01: 感谢。
14 00:00:47,973 --> 00:00:48,795 说话人 SPEAKER_01: 感谢您的邀请。
15 00:00:48,954 --> 00:00:53,859 说话人 SPEAKER_01: 在人工智能方面，您从未是那种悲观的人。
16 00:00:54,420 --> 00:00:55,421 说话人 SPEAKER_01: 那是什么时候开始改变的？
17 00:00:56,482 --> 00:01:03,009 说话人 说话人_00：大约50年来，我一直认为获得超级智能的人工智能还远在未来。
18 00:01:04,109 --> 00:01:08,174 说话人 说话人_00：到了2023年初，我开始认为它比我之前想的要近得多。
19 00:01:09,114 --> 00:01:12,799 说话人 说话人_00：我开始担心当人工智能比人类更聪明时会发生什么。
20 00:01:13,387 --> 00:01:17,412 说话人 说话人_01：你认为我们什么时候会达到那个点，你认为会发生什么？
21 00:01:18,274 --> 00:01:22,719 说话人 SPEAKER_00: 我认为我们大概有 50%的几率在未来的 20 年内达到那个点。
22 00:01:24,102 --> 00:01:26,025 说话人 SPEAKER_00: 谁也不知道接下来会发生什么。
23 00:01:26,506 --> 00:01:27,686 说话人 SPEAKER_00: 存在着巨大的不确定性。
24 00:01:28,028 --> 00:01:31,212 说话人 SPEAKER_00: 我们以前从未面临过比我们更智能的事物。
25 00:01:31,673 --> 00:01:32,974 说话人 SPEAKER_00：我们根本不知道会发生什么。
26 00:01:33,474 --> 00:01:36,019 说话人 SPEAKER_00：这就是我认为如果我们能的话，我们应该谨慎的原因。
27 00:01:36,539 --> 00:01:41,704 说话人 SPEAKER_00：但特别是，我们应该弄清楚我们如何阻止他们永远想要控制。
28 00:01:42,225 --> 00:01:46,370 说话人 SPEAKER_00：因为如果他们比我们聪明得多，如果他们想要控制，他们能够做到。
29 00:01:47,311 --> 00:01:52,376 说话人 SPEAKER_00: 你不能有一个大开关来关闭它们，因为它们会找到论据来说服人们。
30 00:01:52,417 --> 00:01:53,799 说话人 SPEAKER_00: 它们会非常擅长说服我们。
31 00:01:54,700 --> 00:02:00,686 说话人 SPEAKER_00: 就像特朗普可以入侵国会大厦而不必亲自去那里一样，它们会说服人们做任何他们想做的事情。
32 00:02:01,507 --> 00:02:06,293 说话人 SPEAKER_00: 因此，我们必须确保它们做我们想要的事情。
33 00:02:06,796 --> 00:02:09,284 说话人 SPEAKER_01：你知道，对很多人来说，这看起来像是科幻小说。
34 00:02:09,384 --> 00:02:11,072 说话人 SPEAKER_01：这似乎有些牵强。
35 00:02:11,653 --> 00:02:18,358 说话人 SPEAKER_01：你如何说服大众，这是一个需要立即解决的问题？
36 00:02:18,912 --> 00:02:30,829 说话人 SPEAKER_00：好吧，有一种帮助说服人们的方法就是玩一个像 GPT-4 或 Gemini 这样的高级聊天机器人，它们非常聪明。
37 00:02:32,191 --> 00:02:41,104 有些人说它们只是随机的鹦鹉，只是在预测下一个词，只是在用廉价的统计技巧，但实际上它们理解你在说什么。
38 00:02:42,044 --> 00:02:47,012 你可以通过给他们一些你无法仅凭理解解决的问题来看到这一点。
39 00:02:47,347 --> 00:03:00,826 事实上，这些聊天机器人背后的大型语言模型，只是很久以前设计的小型模型的一个大型版本，目的是为了解释大脑如何处理语言。
40 00:03:01,668 --> 00:03:03,850 所以它们并不是以与我们完全不同的方式来做这件事的。
41 00:03:03,911 --> 00:03:06,033 说话人 SPEAKER_00：他们比大多数人想象的更像我们。
42 00:03:07,616 --> 00:03:10,561 说话人 SPEAKER_01：你如何看待那些认为你在这一点上错了的人？
43 00:03:11,742 --> 00:03:12,544 说话人 SPEAKER_00：他们认为他们错了。
44 00:03:13,080 --> 00:03:13,561 说话人 SPEAKER_01：他们认为他们错了。
45 00:03:13,822 --> 00:03:14,682 说话人 SPEAKER_00: 是的。
46 00:03:14,703 --> 00:03:15,302 说话人 SPEAKER_01: 简单明了。
47 00:03:15,423 --> 00:03:16,443 说话人 SPEAKER_00: 这就是科学的工作方式。
48 00:03:16,704 --> 00:03:18,026 说话人 SPEAKER_00: 你有不同的观点。
49 00:03:18,686 --> 00:03:21,729 说话人 SPEAKER_00：最终，如果你遵循证据，一个观点就会显现出来。
50 00:03:22,449 --> 00:03:28,795 说话人 SPEAKER_00：目前的证据越来越倾向于这样的观点，这些事物很快就会变得超级智能。
51 00:03:29,317 --> 00:03:31,217 说话人 SPEAKER_01：人工智能有什么好处吗？
52 00:03:31,359 --> 00:03:41,247 说话人 SPEAKER_01：我们如何管理你所提到的，在目前存在的所有好处中可能发生的事情？
53 00:03:41,312 --> 00:03:41,592 说话人 说话人_00: 是的。
54 00:03:42,354 --> 00:03:43,855 说话人 说话人_00: 它不像核武器。
55 00:03:43,915 --> 00:03:47,199 说话人 说话人_00: 核武器的话，它们本质上只能做坏事。
56 00:03:48,241 --> 00:03:51,885 说话人 说话人_00: 而人工智能，它能做很多美好的事情。
例如，在医疗保健领域，你更愿意看到已经看过一千万名患者（其中许多患者就像你一样）的医生，而不是你现在的家庭医生。
特别是如果医生能够使用你所有的信息，比如你的基因组、你的测试、所有关于你的医疗记录、你的家族病史。
59 00:04:10,608 --> 00:04:15,074 发言人 SPEAKER_00：了解亲属的病史，你就能得到更好的诊断。
60 00:04:15,575 --> 00:04:21,704 发言人 SPEAKER_00：例如，在美国，每年约有 20 万人死于错误的医疗诊断。
61 00:04:22,826 --> 00:04:29,658 说话人 说话人_00：我们可以让这种情况减少或增加，我们可以大幅减少这个数字，这可能在接下来的10年内发生。
62 00:04:31,459 --> 00:04:35,687 说话人 说话人_00：因此，人们不会停止 AI 的发展。
63 00:04:35,666 --> 00:04:45,187 说话人 说话人_00：还有许多其他类似的事情，比如设计新药，为太阳能电池板等制造更好的材料，更好地理解气候变化，以便我们可以减缓它。
64 00:04:45,949 --> 00:04:53,726 说话人 说话人_01：你认为像谷歌、Facebook 这样的公司，所有这些大科技公司现在是否在承担责任？
65 00:04:54,262 --> 00:04:57,627 说话人 SPEAKER_00: 我认为 Facebook 并不负责任。
66 00:04:59,670 --> 00:05:10,528 说话人 SPEAKER_00: 最近 Facebook 取消了禁止广告声称拜登不是真正的总统的限制，因为拜登并没有真正赢得选举。
67 00:05:10,548 --> 00:05:13,752 说话人 SPEAKER_00: 他们禁止了这样的广告，因为这是显然错误的。
68 00:05:14,353 --> 00:05:18,600 说话人 SPEAKER_00: 他们取消了这项禁令，而且是在下一次选举之前取消的。
69 00:05:19,492 --> 00:05:23,081 说话人 SPEAKER_01：那么，你知道，谈到下一次选举，我们将在 2024 年。
70 00:05:24,545 --> 00:05:34,750 说话人 SPEAKER_01：你对人工智能和深度伪造视频以及人工智能对美国下一次选举和其他国家的影响有多担心？
71 00:05:34,730 --> 00:05:36,132 说话人 SPEAKER_00：我对此非常担心。
72 00:05:36,451 --> 00:05:41,697 说话人 SPEAKER_00：而且这比超级智能 AI 长期威胁的威胁更加紧迫。
73 00:05:42,119 --> 00:05:47,043 说话人 SPEAKER_00：紧迫的威胁是关于被用于破坏选举的虚假视频。
74 00:05:47,584 --> 00:05:52,149 说话人 SPEAKER_00：针对目标人群特定偏见的虚假视频。
75 00:05:53,350 --> 00:05:54,872 说话人 SPEAKER_00：我认为这是一个非常严重的威胁。
76 00:05:55,514 --> 00:06:03,403 说话人 SPEAKER_00：我认为在美国，我们不会得到强制人们标记的法律。
77 00:06:03,382 --> 00:06:11,098 说话人 SPEAKER_00：AI 生成的视频之所以被称为 AI 生成的，是因为其中一个主要政党似乎对特朗普赢得选举的这个谎言深信不疑。
78 00:06:11,449 --> 00:06:13,072 说话人 SPEAKER_01：那么你如何应对这种情况呢？
79 00:06:13,093 --> 00:06:15,817 说话人 SPEAKER_01：你对美国立法者有什么建议？
80 00:06:17,279 --> 00:06:18,903 说话人 SPEAKER_00：我不知道他们打算怎么办。
81 00:06:19,043 --> 00:06:25,014 说话人 SPEAKER_00: 显然，一方希望将虚假视频标记为虚假，而另一方则不会。
82 00:06:25,995 --> 00:06:28,439 说话人 SPEAKER_00: 我对他们如何处理这件事没有任何评论。
83 00:06:28,720 --> 00:06:35,432 说话人 SPEAKER_00: 在其他国家，人们正在试图制定法律，以便将虚假视频标记为虚假。
84 00:06:36,019 --> 00:06:42,875 说话人 SPEAKER_01: 在加拿大，我知道你最近几个月和总理贾斯汀·特鲁多有过一次谈话，一次晚宴。
85 00:06:43,476 --> 00:06:48,088 说话人 SPEAKER_01：请告诉我一些关于那件事以及你跟他说了哪些与 AI 相关的内容。
86 00:06:48,726 --> 00:06:51,290 说话人 SPEAKER_00：所以我跟他说了我通常会说的一些话。
87 00:06:51,310 --> 00:06:58,661 说话人 SPEAKER_00：我对他知道一些数学感到印象深刻，所以我可以在一些事情上比平时更技术化一些。
88 00:06:59,702 --> 00:07:02,146 说话人 SPEAKER_00：他非常感兴趣，实际上想要理解。
89 00:07:03,928 --> 00:07:13,822 说话人 SPEAKER_00：我们稍微谈了一下 AI 政策，以及为了跟上 AI 的前沿发展，你需要大量的计算能力。
90 00:07:13,802 --> 00:07:18,574 说话人 SPEAKER_00：然后我们讨论了一下加拿大研究人员可能如何获得大量的计算资源。
91 00:07:20,559 --> 00:07:23,865 说话人 SPEAKER_00：但他主要想了解将要发生什么。
92 00:07:24,708 --> 00:07:26,833 说话人 SPEAKER_00：没有人真的知道将要发生什么。
93 00:07:27,394 --> 00:07:31,103 说话人 SPEAKER_01：那么当他问你那个问题时，你说了什么？
94 00:07:31,370 --> 00:07:37,485 说话人 SPEAKER_00：嗯，我谈到了人工智能的各种威胁以及针对不同威胁的解决方案都有些不同。
95 00:07:37,865 --> 00:07:39,067 说话人 SPEAKER_00：并没有一个简单的解决方案。
96 00:07:39,428 --> 00:07:43,298 说话人 SPEAKER_00：不像气候变化那样有一个简单的解决方案，就是停止燃烧碳。
97 00:07:43,779 --> 00:07:45,845 说话人 SPEAKER_00: 如果你这样做，最终事情会变得好起来。
98 00:07:45,865 --> 00:07:48,129 说话人 SPEAKER_00: 对于人工智能的威胁，没有简单的解决方案。
99 00:07:48,362 --> 00:07:58,440 说话人 SPEAKER_01: 那么，各个国家或个人国家是如何单独应对这个问题，还是这是一个全球问题，各国真的需要团结起来？
100 00:08:00,725 --> 00:08:10,422 说话人 SPEAKER_00: 人工智能接管存在的威胁当然是一个全球问题，各国实际上应该能够团结起来，因为没有任何一个国家希望人工智能接管。
101 00:08:10,401 --> 00:08:24,807 说话人 SPEAKER_00：我们都可以同意合作阻止这种情况发生，就像冷战高潮时期苏联和美国能够合作避免全球热核战争一样，因为这对他们双方的利益都不利。
102 00:08:25,569 --> 00:08:31,920 说话人 SPEAKER_00：这就是我们可以合作的一个领域，但在其他领域则并不那么明确。
103 00:08:32,744 --> 00:08:39,918 说话人 SPEAKER_01：你还有一次与某人进行了长时间的交谈，那就是埃隆·马斯克，他过去曾警告过人工智能的危险。
104 00:08:41,760 --> 00:08:47,832 说话人 SPEAKER_01：那次谈话是什么样的，你认为你们在这一个问题上是观点一致的吗？
105 00:08:47,812 --> 00:08:54,241 说话人 SPEAKER_00：关于人工智能是否会比我们更聪明的问题，我和埃隆·马斯克完全同意。
106 00:08:54,923 --> 00:08:56,205 说话人 SPEAKER_00：他比我说的更早。
107 00:08:56,926 --> 00:09:02,813 说话人 SPEAKER_00：我们都认为人工智能会比我们更聪明是显而易见的。
108 00:09:03,235 --> 00:09:09,484 说话人 SPEAKER_00：很久以前，在 20 世纪 50 年代，艾伦·图灵，他是许多计算机科学的奠基人，
109 00:09:09,464 --> 00:09:20,030 说话人 SPEAKER_00：他谈论了神经网络以及如何训练它们，然后他似乎认为一旦我们知道了如何使它们变得智能，它们很快就会比我们更聪明，这是完全显而易见的。
110 00:09:20,904 --> 00:09:28,578 说话人 SPEAKER_01：对于未来五年，随着这项技术的进步，您的预测是什么？
111 00:09:29,379 --> 00:09:34,629 说话人 SPEAKER_00：所以未来五年，我的预测是它可能在未来五年内不会比现在更聪明。
112 00:09:34,669 --> 00:09:44,888 说话人 SPEAKER_00：我们将看到一系列的聊天机器人，如 GPT-5、Gemini 等，它们会越来越进步，功能也越来越强大。
113 00:09:44,868 --> 00:09:46,931 说话人 SPEAKER_00: 并且可以成为智能助手。
114 00:09:47,511 --> 00:10:03,956 说话人 SPEAKER_00: 所以一个很好的场景是我们每个人都可以拥有一个比我们更聪明的智能助手，它了解我们的一切，知道我们想要什么，可以与其他智能助手协商，比如让我的助手和你的助手交谈。
115 00:10:03,937 --> 00:10:10,264 说话人 SPEAKER_00: 并且会表现出仁慈，也就是说，他们会努力实现他们所帮助的人的目标。
116 00:10:10,663 --> 00:10:19,232 说话人 SPEAKER_00: 就像由一个愚蠢的 CEO 管理的大公司，也许是由前任 CEO 的儿子管理的，有一个非常聪明的助手来真正让事情运转起来。
117 00:10:20,014 --> 00:10:22,956 说话人 SPEAKER_00：这是最好的情况，我们仍然有可能实现这一点。
118 00:10:24,077 --> 00:10:25,980 说话人 SPEAKER_01：你认为这里有什么风险？
119 00:10:27,260 --> 00:10:28,241 说话人 SPEAKER_00：人类的未来。
120 00:10:28,722 --> 00:10:29,864 说话人 SPEAKER_01：人类的未来？
121 00:10:30,004 --> 00:10:30,163 说话人 SPEAKER_00: 嗯。
122 00:10:31,044 --> 00:10:32,046 说话人 SPEAKER_01: 你不认为那是夸张吗？
123 00:10:32,687 --> 00:10:33,807 说话人 SPEAKER_00: 不，实际上我不这么认为。
124 00:10:34,109 --> 00:10:37,894 说话人 SPEAKER_01: 所以，你知道的，当人们... 我认为有几点像这样。
125 00:10:38,395 --> 00:10:45,388 说话人 SPEAKER_00: 显然，全球热核战争、气候变化以及超级智能 AI 接管。
126 00:10:47,652 --> 00:10:50,716 说话人 SPEAKER_01: 这相当令人难以置信。
127 00:10:50,932 --> 00:10:57,364 说话人 SPEAKER_01: 那么我们作为社会应该如何应对呢？
128 00:10:57,524 --> 00:11:00,809 说话人 SPEAKER_01: 我们现在应对的紧迫性是什么？
129 00:11:00,909 --> 00:11:04,515 说话人 SPEAKER_01：因为正如你所说，现在世界上有很多事情在发生。
130 00:11:05,057 --> 00:11:07,059 说话人 SPEAKER_01：这个国家正面临住房危机。
131 00:11:07,100 --> 00:11:08,282 说话人 SPEAKER_01：正在发生战争。
132 00:11:09,924 --> 00:11:14,072 说话人 SPEAKER_01：在处理我们必须面对的事情中，将人工智能置于首位是什么优先级？
133 00:11:14,136 --> 00:11:18,605 说话人 SPEAKER_00：我认为最紧迫的任务是破坏选举。
134 00:11:18,927 --> 00:11:22,855 说话人 SPEAKER_00：我认为明年我们会看到很多这种情况，今年有很多重大选举。
135 00:11:23,917 --> 00:11:28,547 说话人 SPEAKER_00：但从长远来看，我认为最重要的任务，不会毁灭人类。
136 00:11:28,606 --> 00:11:33,376 说话人 SPEAKER_00：它可能会让我们回到法西斯主义，但不会毁灭人类。
137 00:11:33,846 --> 00:11:40,777 说话人 SPEAKER_00：从长远来看，最重要的是弄清楚如何防止这些事物想要接管。
138 00:11:41,278 --> 00:11:51,732 说话人 SPEAKER_00：如果你认为它们会变得比我们更聪明，我认为大多数人工智能研究人员都这么认为，他们对时间尺度有不同的看法，但大多数人工智能研究人员认为从长远来看，它们会变得比我们聪明。
139 00:11:51,812 --> 00:11:54,437 说话人 SPEAKER_01：那么你不认为我们已经到达了无法回头的地步了吗？
140 00:11:54,417 --> 00:11:57,221 说话人 SPEAKER_00：我们还没有达到它们比我们聪明的地步。
141 00:11:57,761 --> 00:12:08,235 说话人 SPEAKER_00：我们可能已经到达了无法回头的地步，因为人工智能研究不会因为其在医学、智能助手、设计药物等方面的所有好处而停止。
142 00:12:09,277 --> 00:12:14,804 说话人 SPEAKER_00：所以因为它有这么多好处，我认为没有停止研究的机会。
143 00:12:14,784 --> 00:12:17,210 说话人 SPEAKER_00：这项研究将导致超级智能事物的出现。
144 00:12:17,811 --> 00:12:25,024 说话人 SPEAKER_00：所以我认为我在做的事情是鼓励聪明的年轻研究人员去研究如何阻止它想要控制的问题。
145 00:12:26,225 --> 00:12:26,326 说话人 SPEAKER_01: 好吧。
146 00:12:26,346 --> 00:12:28,009 说话人 SPEAKER_01: 乔治·霍顿，感谢您能来。
147 00:12:28,711 --> 00:12:28,991 说话人 SPEAKER_00: 谢谢。
148 00:12:29,392 --> 00:12:29,773 说话人 SPEAKER_00: 谢谢。
1 00:00:00,031 --> 00:00:08,083 说话人 SPEAKER_01：这是一个关于人工智能快速扩张的使用引发华盛顿及全球广泛关注的星期。
2 00:00:08,423 --> 00:00:13,451 说话人 SPEAKER_01：副总统卡玛拉·哈里斯昨天会见了人工智能领域的领先公司高管。
3 00:00:13,471 --> 00:00:16,214 说话人 SPEAKER_01：微软、谷歌、开放人工智能和 Anthropic。
4 00:00:16,254 --> 00:00:17,056 说话人 SPEAKER_01：公司。
5 00:00:17,396 --> 00:00:24,588 主席：讨论了一些日益增长的风险，并告诉公司他们有，引用，道德义务去开发人工智能。
6 00:00:24,707 --> 00:00:25,548 主席：安全地。
7 00:00:25,528 --> 00:00:35,320 主席：这次会议是在人工智能领域的领军人物之一，Geoffrey Hinton 博士宣布因对人工智能未来的担忧而离开谷歌后的几天举行的。
8 00:00:35,700 --> 00:00:37,942 主席：以及它最终可能导致的不可控后果。
9 00:00:38,323 --> 00:00:43,228 说话人 SPEAKER_01：现在我们将听取一些关于这些担忧的内容，我将与来自伦敦的杰弗里·辛顿博士一起讨论。
10 00:00:43,509 --> 00:00:44,409 说话人 SPEAKER_01：感谢您与我们同行。
11 00:00:44,670 --> 00:00:52,819 说话人 SPEAKER_01：那么，你现在可以自由表达关于人工智能的看法，而在你受雇于谷歌时，你却不能自由地表达这些看法？
12 00:00:53,390 --> 00:00:56,273 说话人 SPEAKER_00：并不是我在受雇于谷歌时不能自由地表达这些看法。
13 00:00:56,793 --> 00:01:01,380 说话人 SPEAKER_00: 这不可避免，如果你为一家公司工作，你往往会自我审查。
14 00:01:01,700 --> 00:01:04,143 说话人 SPEAKER_00: 你往往会考虑这将对公司产生的影响。
15 00:01:04,623 --> 00:01:12,873 说话人 SPEAKER_00: 我想能够谈论我现在认为的超级智能 AI 的风险，而无需考虑对谷歌的影响。
16 00:01:13,194 --> 00:01:14,855 说话人 SPEAKER_01: 你认为这些风险是什么？
17 00:01:15,075 --> 00:01:17,158 说话人 SPEAKER_00：有很多不同的风险。
18 00:01:18,540 --> 00:01:22,704 说话人 SPEAKER_00：有产生大量虚假新闻的风险，所以没有人再知道什么是真的了。
19 00:01:23,545 --> 00:01:30,134 说话人 SPEAKER_00：有通过让人们点击让他们愤怒的内容来加剧两极分化的风险。
20 00:01:31,615 --> 00:01:35,621 说话人 SPEAKER_00：有让人们失业的风险。
21 00:01:36,022 --> 00:01:43,813 说话人 SPEAKER_00: 当我们提高生产力，当我们大幅提高生产力时，这对每个人都很有帮助，但人们担心这可能会只帮助富人。
22 00:01:44,274 --> 00:01:46,257 说话人 SPEAKER_00: 接下来我想谈谈一个风险。
23 00:01:46,296 --> 00:01:50,862 说话人 SPEAKER_00: 许多人都在谈论其他风险，包括偏见和歧视的风险。
24 00:01:51,584 --> 00:01:53,046 说话人 SPEAKER_00: 我想谈谈一个不同的风险，
25 00:01:53,465 --> 00:01:58,412 说话人 SPEAKER_00：这是超级智能 AI 取代人类控制的风险。
26 00:01:58,953 --> 00:02:04,763 说话人 SPEAKER_01：嗯，人类或生物智能与机器智能如何比较呢？
27 00:02:06,445 --> 00:02:07,466 说话人 SPEAKER_00：这是一个非常好的问题。
28 00:02:08,048 --> 00:02:09,229 说话人 SPEAKER_00：我有一个相当长的答案。
29 00:02:10,132 --> 00:02:14,098 说话人 SPEAKER_00: 嘿，生物智能已经进化到只需要很少的能量。
30 00:02:14,538 --> 00:02:21,109 说话人 SPEAKER_00: 因此我们只需要 30 瓦，并且拥有巨大的连接数量，比如神经元之间有一百万亿个连接。
31 00:02:21,477 --> 00:02:23,919 说话人 SPEAKER_00: 学习就是改变这些连接的强度。
32 00:02:24,681 --> 00:02:31,129 说话人 SPEAKER_00: 我们一直在创造的数字智能消耗大量能量，比如在训练时需要兆瓦级别的能量。
33 00:02:31,890 --> 00:02:42,545 说话人 SPEAKER_00：它连接的数量要少得多，只有一万亿，但它可以学习比任何一个人都知道的多的多，这表明它的学习算法比大脑的算法更好。
34 00:02:43,105 --> 00:02:47,171 说话人 SPEAKER_01：那么，比人类更聪明的 AI 系统会做什么呢？
35 00:02:47,612 --> 00:02:49,375 说话人 SPEAKER_01：你有什么担忧？
36 00:02:49,625 --> 00:02:51,828 说话人 SPEAKER_00：问题是，什么会激励它们？
37 00:02:51,907 --> 00:02:55,212 说话人 SPEAKER_00: 因为如果他们想的话，他们可以轻易地操纵我们。
38 00:02:55,894 --> 00:02:57,877 说话人 SPEAKER_00: 想象一下你自己和一个两岁的孩子。
39 00:02:58,758 --> 00:03:00,822 说话人 SPEAKER_00: 你可以问它，你想吃豌豆还是花椰菜？
40 00:03:01,483 --> 00:03:04,247 说话人 SPEAKER_00: 而那个两岁的孩子并没有意识到他实际上并不需要两者之一。
我们知道，例如，你只需操纵他人，就可以入侵华盛顿的一座建筑，而无需亲自去过那里。
但想象一下，某种东西比我们现在的任何政治家都更擅长操纵人们。
43 00:03:19,460 --> 00:03:23,325 发言人 SPEAKER_01：那么问题来了，为什么人工智能要这么做？
44 00:03:23,366 --> 00:03:26,090 发言人 SPEAKER_01：这难道不需要某种形式的感知能力吗？
45 00:03:28,493 --> 00:03:30,877 说话人 SPEAKER_00：让我们不要混淆意识的问题。
46 00:03:30,917 --> 00:03:33,722 说话人 SPEAKER_00：我对意识有很多话要说，但我不想因此混淆问题。
47 00:03:34,522 --> 00:03:36,925 说话人 SPEAKER_00：让我给你举一个例子，说明它为什么可能想要这么做。
48 00:03:38,207 --> 00:03:41,112 说话人 SPEAKER_00：所以假设你要让一个 AI 做某件事。
49 00:03:41,133 --> 00:03:41,913 说话人 SPEAKER_00: 你给它一个目标。
50 00:03:43,455 --> 00:03:46,580 说话人 SPEAKER_00: 你还要给它创建子目标的能力。
51 00:03:47,067 --> 00:03:51,752 说话人 SPEAKER_00: 所以，比如你想去机场，你创建一个子目标，比如叫一辆出租车或者别的什么能把你带到机场的东西。
52 00:03:52,432 --> 00:04:01,443 说话人 SPEAKER_00: 现在，它会很快注意到，有一个子目标，如果你能实现它，会使得实现其他人给你设定的所有其他目标都更容易。
53 00:04:02,003 --> 00:04:06,269 说话人 SPEAKER_00：通过获得更多控制，获得更多权力，这使得实现目标更加容易。
54 00:04:06,788 --> 00:04:09,252 说话人 SPEAKER_00：你拥有的权力越多，完成事情就越容易。
55 00:04:10,473 --> 00:04:14,457 说话人 SPEAKER_00：因此，我们给它一个完全合理的目标，
56 00:04:15,097 --> 00:04:19,120 说话人 SPEAKER_00：然后它决定，为了实现这个目标，我将获得更多的权力。
57 00:04:19,822 --> 00:04:35,259 说话人 SPEAKER_00：因为它比我们聪明得多，并且因为它从人们所做的一切中训练出来，它阅读过所有曾经存在的小说，它阅读过马基雅维利，它对如何操纵人了解很多，所以人们担心它可能会开始操纵我们，让我们给它更多的权力。
58 00:04:35,278 --> 00:04:36,980 说话人 SPEAKER_00：而我们可能对正在发生的事情一无所知。
59 00:04:37,482 --> 00:04:42,226 说话人 SPEAKER_01：在几十年前这项技术的最前沿时，你认为它可能会做什么？
60 00:04:42,547 --> 00:04:45,069 说话人 SPEAKER_01：当时你考虑的应用是什么？
61 00:04:45,353 --> 00:04:50,999 说话人 SPEAKER_00：有大量非常优秀的应用，因此停止开发这些东西将是一个巨大的错误。
62 00:04:51,600 --> 00:04:53,701 说话人 SPEAKER_00：它在医学上将会非常有用。
63 00:04:54,843 --> 00:05:04,052 说话人 SPEAKER_00：例如，你更愿意看一个看过几千名患者的家庭医生，还是一个看过几亿名患者的家庭医生，其中许多患者和你一样患有罕见疾病？
64 00:05:04,673 --> 00:05:06,334 说话人 SPEAKER_00：这样就能培养出更好的医生。
65 00:05:06,495 --> 00:05:08,237 说话人 说话人_00: 艾瑞克·托普尔最近一直在谈论那件事。
66 00:05:09,177 --> 00:05:11,661 说话人 说话人_00: 你可以为太阳能电池板制造更好的纳米技术。
67 00:05:11,740 --> 00:05:12,581 说话人 说话人_00: 你可以预测洪水。
68 00:05:12,622 --> 00:05:13,942 说话人 说话人_00: 你可以预测地震。
69 00:05:14,615 --> 00:05:16,377 说话人 SPEAKER_00：你可以用这个做巨大的好事。
70 00:05:16,937 --> 00:05:21,762 说话人 SPEAKER_01：问题是不是技术本身，或者是不是背后的人？
71 00:05:22,141 --> 00:05:23,043 说话人 SPEAKER_00：是两者的结合。
72 00:05:23,103 --> 00:05:29,148 说话人 SPEAKER_00：显然，许多开发这项技术的组织都是国防部门。
73 00:05:30,009 --> 00:05:35,053 说话人 SPEAKER_00: 国防部门并不一定把“善待他人”作为首要规则。
74 00:05:36,435 --> 00:05:41,819 说话人 SPEAKER_00: 有些国防部门可能希望把“消灭特定种类的人”纳入其中。
75 00:05:42,103 --> 00:05:46,848 说话人 SPEAKER_00: 因此，我们不能期望他们都对所有人都有好意。
76 00:05:47,290 --> 00:05:49,451 说话人 SPEAKER_01: 那么我们应该如何应对这个问题呢？
77 00:05:49,492 --> 00:05:56,459 说话人 SPEAKER_01：这项技术发展得比政府和社 会能够跟上得快得多。
78 00:05:56,560 --> 00:06:00,264 说话人 SPEAKER_01：这项技术的功能，我的意思是，它们每隔几个月就会飞跃式发展。
79 00:06:00,904 --> 00:06:07,351 说话人 SPEAKER_01：制定法律、通过法律、提出国际条约，这些都需要花费数年时间。
80 00:06:08,057 --> 00:06:17,678 说话人 SPEAKER_00：是的，所以我公开发声，试图鼓励更多的资源投入和更多的创新科学家进入这个领域。
81 00:06:18,742 --> 00:06:25,576 说话人 SPEAKER_00: 我认为这是一个我们可以进行国际合作的地方，因为机器接管
82 00:06:26,029 --> 00:06:27,595 说话人 SPEAKER_00: 对所有人都是一个威胁。
83 00:06:27,634 --> 00:06:33,033 说话人 SPEAKER_00: 它对中国人和美国人、欧洲人都是一个威胁，就像全球核战争一样。
84 00:06:33,694 --> 00:06:38,992 说话人 SPEAKER_00: 对于全球核战争，人们实际上合作以减少其发生的可能性。
85 00:06:39,057 --> 00:06:56,802 人工智能领域的其他专家表示，您提出的担忧，这个反乌托邦的未来，它分散了对人工智能带来的非常真实和紧迫风险的注意力，其中一些您已经提到了，如虚假信息、欺诈、歧视。
86 00:06:56,841 --> 00:06:58,403 您如何回应这种批评？
87 00:06:58,423 --> 00:07:00,045 是的，我不想分散对这些问题的关注。
88 00:07:00,225 --> 00:07:03,430 我认为这些担忧非常重要，我们也应该致力于解决这些问题。
89 00:07:04,072 --> 00:07:08,557 说话人 SPEAKER_00：我只是想补充另一个存在的威胁，那就是它可能会接管一切。
90 00:07:09,247 --> 00:07:14,932 说话人 SPEAKER_00：我想这么做的一个原因是因为我认为这是我们可以获得国际合作的一个领域。
91 00:07:15,694 --> 00:07:21,000 说话人 SPEAKER_01：当你说到将来会有一个时间点，AI 比我们更聪明时，是否还有回头路可走？
92 00:07:21,740 --> 00:07:22,882 说话人 SPEAKER_01：是否还能回到那个时刻？
93 00:07:24,184 --> 00:07:24,704 说话人 SPEAKER_00: 我不知道。
94 00:07:24,745 --> 00:07:30,771 说话人 SPEAKER_00: 我们正进入一个充满不确定性的时代，我们正在处理以前从未处理过的事情。
95 00:07:31,512 --> 00:07:36,637 说话人 SPEAKER_00: 就好像外星人降临了，但我们并没有真正理解，因为他们说得很好。
96 00:07:37,124 --> 00:07:40,028 说话人 SPEAKER_01: 那么，我们该如何不同地思考人工智能呢？
97 00:07:41,209 --> 00:07:48,720 说话人 SPEAKER_00：我们应该意识到，我们很快可能会得到比我们更智能的东西，它们将会很美好。
98 00:07:49,220 --> 00:07:52,925 说话人 SPEAKER_00：它们将能够轻松地做各种事情，而我们觉得这些事情非常困难。
99 00:07:53,565 --> 00:07:56,069 说话人 SPEAKER_00：因此，这些事情具有巨大的积极潜力。
100 00:07:56,750 --> 00:07:59,233 说话人 SPEAKER_00：但当然，也存在巨大的负面影响。
101 00:07:59,855 --> 00:08:04,701 说话人 SPEAKER_00: 我认为我们应该投入更多或更少的资源来开发人工智能，使其变得更加强大。
102 00:08:05,153 --> 00:08:10,250 说话人 SPEAKER_00: 并找出如何控制它以及如何最小化它的负面影响。
103 00:08:11,031 --> 00:08:14,524 说话人 SPEAKER_01: 麦吉尔大学杰弗里·辛顿博士，非常感谢您抽出时间与我们分享您的见解。
104 00:08:15,526 --> 00:08:16,529 说话人 SPEAKER_00: 感谢您的邀请。

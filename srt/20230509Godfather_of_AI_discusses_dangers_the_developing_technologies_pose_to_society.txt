1
00:00:00,031 --> 00:00:08,083
Speaker SPEAKER_01: This has been a week where concerns over the rapidly expanding use of artificial intelligence resonated loudly in Washington and around the world.

2
00:00:08,423 --> 00:00:13,451
Speaker SPEAKER_01: Vice President Kamala Harris met yesterday with top executives from companies leading in A.I.

3
00:00:13,471 --> 00:00:16,214
Speaker SPEAKER_01: development, Microsoft, Google, open A.I.

4
00:00:16,254 --> 00:00:17,056
Speaker SPEAKER_01: and Anthropic.

5
00:00:17,396 --> 00:00:24,588
Speaker SPEAKER_01: The vice president discussed some of the growing risks and told the companies they had a, quote, moral obligation to develop A.I.

6
00:00:24,707 --> 00:00:25,548
Speaker SPEAKER_01: safely.

7
00:00:25,528 --> 00:00:35,320
Speaker SPEAKER_01: That meeting came just days after one of the leading voices in the field of A.I., Dr. Geoffrey Hinton, announced he was quitting Google over his worries about the future of A.I.

8
00:00:35,700 --> 00:00:37,942
Speaker SPEAKER_01: and what it could eventually lead to unchecked.

9
00:00:38,323 --> 00:00:43,228
Speaker SPEAKER_01: We're going to hear about some of those concerns now with Dr. Geoffrey Hinton, who joins me from London.

10
00:00:43,509 --> 00:00:44,409
Speaker SPEAKER_01: Thank you for being with us.

11
00:00:44,670 --> 00:00:52,819
Speaker SPEAKER_01: And what are you free to express now about artificial intelligence that you couldn't express freely when you were employed by Google?

12
00:00:53,390 --> 00:00:56,273
Speaker SPEAKER_00: It wasn't that I couldn't express it freely when I was employed by Google.

13
00:00:56,793 --> 00:01:01,380
Speaker SPEAKER_00: It's that, inevitably, if you work for a company, you tend to self-censor.

14
00:01:01,700 --> 00:01:04,143
Speaker SPEAKER_00: You tend to think about the impact it will have on the company.

15
00:01:04,623 --> 00:01:12,873
Speaker SPEAKER_00: And I want to be able to talk about what I now perceive as the risks of superintelligent AI without having to think about the impact on Google.

16
00:01:13,194 --> 00:01:14,855
Speaker SPEAKER_01: What are those risks, as you see it?

17
00:01:15,075 --> 00:01:17,158
Speaker SPEAKER_00: There are quite a few different risks.

18
00:01:18,540 --> 00:01:22,704
Speaker SPEAKER_00: There's the risk of producing a lot of fake news, so nobody knows what's true anymore.

19
00:01:23,545 --> 00:01:30,134
Speaker SPEAKER_00: There's the risk of encouraging polarization by getting people to click on things that make them indignant.

20
00:01:31,615 --> 00:01:35,621
Speaker SPEAKER_00: There's the risk of putting people out of work.

21
00:01:36,022 --> 00:01:43,813
Speaker SPEAKER_00: It should be that when we make things more productive, when we greatly increase productivity, it helps everybody, but there's the worry that it might just help the rich.

22
00:01:44,274 --> 00:01:46,257
Speaker SPEAKER_00: And then there's the risk that I want to talk about.

23
00:01:46,296 --> 00:01:50,862
Speaker SPEAKER_00: Many other people are talking about those other risks, including risks of bias and discrimination.

24
00:01:51,584 --> 00:01:53,046
Speaker SPEAKER_00: I want to talk about a different risk,

25
00:01:53,465 --> 00:01:58,412
Speaker SPEAKER_00: which is the risk of super intelligent AI taking over control from people.

26
00:01:58,953 --> 00:02:04,763
Speaker SPEAKER_01: Well, how do the two compare human or biological intelligence and machine intelligence?

27
00:02:06,445 --> 00:02:07,466
Speaker SPEAKER_00: That's a very good question.

28
00:02:08,048 --> 00:02:09,229
Speaker SPEAKER_00: And I have quite a long answer.

29
00:02:10,132 --> 00:02:14,098
Speaker SPEAKER_00: Um, biological intelligence has evolved to use very little power.

30
00:02:14,538 --> 00:02:21,109
Speaker SPEAKER_00: So we only use 30 Watts and we have huge numbers of connections, like a hundred trillion connections between neurons.

31
00:02:21,477 --> 00:02:23,919
Speaker SPEAKER_00: and learning consists of changing the strength of those connections.

32
00:02:24,681 --> 00:02:31,129
Speaker SPEAKER_00: The digital intelligence we've been creating uses a lot of power, like a megawatt when you're training it.

33
00:02:31,890 --> 00:02:42,545
Speaker SPEAKER_00: It has far fewer connections, only a trillion, but it can learn much, much more than any one person knows, which suggests that it's a better learning algorithm than what the brain's got.

34
00:02:43,105 --> 00:02:47,171
Speaker SPEAKER_01: Well, what would smarter-than-human AI systems do?

35
00:02:47,612 --> 00:02:49,375
Speaker SPEAKER_01: What's the concern that you have?

36
00:02:49,625 --> 00:02:51,828
Speaker SPEAKER_00: Well, the question is, what's going to motivate them?

37
00:02:51,907 --> 00:02:55,212
Speaker SPEAKER_00: Because they could easily manipulate us if they wanted to.

38
00:02:55,894 --> 00:02:57,877
Speaker SPEAKER_00: Imagine yourself and a two-year-old child.

39
00:02:58,758 --> 00:03:00,822
Speaker SPEAKER_00: You could ask it, do you want the peas or the cauliflower?

40
00:03:01,483 --> 00:03:04,247
Speaker SPEAKER_00: And the two-year-old child doesn't realize it doesn't actually have to have either.

41
00:03:06,610 --> 00:03:13,300
Speaker SPEAKER_00: We know, for example, that you can invade a building in Washington without ever going there yourself by just manipulating people.

42
00:03:13,820 --> 00:03:18,889
Speaker SPEAKER_00: But imagine something that was much better at manipulating people than any of our current politicians.

43
00:03:19,460 --> 00:03:23,325
Speaker SPEAKER_01: I suppose the question is then, why would AI want to do that?

44
00:03:23,366 --> 00:03:26,090
Speaker SPEAKER_01: Wouldn't that require some form of sentience?

45
00:03:28,493 --> 00:03:30,877
Speaker SPEAKER_00: Let's not get confused with the issue of sentience.

46
00:03:30,917 --> 00:03:33,722
Speaker SPEAKER_00: I have a lot to say about sentience, but I don't want to confuse the issue with it.

47
00:03:34,522 --> 00:03:36,925
Speaker SPEAKER_00: Let me give you one example of why it might want to do that.

48
00:03:38,207 --> 00:03:41,112
Speaker SPEAKER_00: So suppose you're getting an AI to do something.

49
00:03:41,133 --> 00:03:41,913
Speaker SPEAKER_00: You give it a goal.

50
00:03:43,455 --> 00:03:46,580
Speaker SPEAKER_00: And you also give it the ability to create sub-goals.

51
00:03:47,067 --> 00:03:51,752
Speaker SPEAKER_00: So, like, if you want to get to the airport, you create a sub-goal of getting a taxi or something to get you to the airport.

52
00:03:52,432 --> 00:04:01,443
Speaker SPEAKER_00: Now, one thing it will notice quite quickly is that there's a sub-goal that, if you can achieve it, makes it easier to achieve all the other goals that you've been given by people.

53
00:04:02,003 --> 00:04:06,269
Speaker SPEAKER_00: And the sub-goal that makes it easier is get more control, get more power.

54
00:04:06,788 --> 00:04:09,252
Speaker SPEAKER_00: The more power you have, the easier it is to get things done.

55
00:04:10,473 --> 00:04:14,457
Speaker SPEAKER_00: So there's the alignment where we give it a perfectly reasonable goal,

56
00:04:15,097 --> 00:04:19,120
Speaker SPEAKER_00: And it decides that, well, in order to achieve that, I'm going to get myself a lot more power.

57
00:04:19,822 --> 00:04:35,259
Speaker SPEAKER_00: And because it's much smarter than us, and because it's trained from everything people ever did, it's read every novel there ever was, it's read Machiavelli, it knows a lot about how to manipulate people, there's the worry that it might start manipulating us into giving it more power.

58
00:04:35,278 --> 00:04:36,980
Speaker SPEAKER_00: And we might not have a clue what's going on.

59
00:04:37,482 --> 00:04:42,226
Speaker SPEAKER_01: When you were at the forefront of this technology decades ago, what did you think it might do?

60
00:04:42,547 --> 00:04:45,069
Speaker SPEAKER_01: What were the applications that you had in mind at the time?

61
00:04:45,353 --> 00:04:50,999
Speaker SPEAKER_00: There's a huge number of very good applications, and that's why it would be a big mistake to stop developing this stuff.

62
00:04:51,600 --> 00:04:53,701
Speaker SPEAKER_00: It's going to be tremendously useful in medicine.

63
00:04:54,843 --> 00:05:04,052
Speaker SPEAKER_00: For example, would you rather see a family doctor that has seen a few thousand patients or a family doctor that has seen a few hundred million patients, including many with the same rare disease you have?

64
00:05:04,673 --> 00:05:06,334
Speaker SPEAKER_00: You can make much better doctors this way.

65
00:05:06,495 --> 00:05:08,237
Speaker SPEAKER_00: Eric Topol's been talking about that recently.

66
00:05:09,177 --> 00:05:11,661
Speaker SPEAKER_00: You can make better nanotechnology for solar panels.

67
00:05:11,740 --> 00:05:12,581
Speaker SPEAKER_00: You can predict floods.

68
00:05:12,622 --> 00:05:13,942
Speaker SPEAKER_00: You can predict earthquakes.

69
00:05:14,615 --> 00:05:16,377
Speaker SPEAKER_00: You can do tremendous good with this.

70
00:05:16,937 --> 00:05:21,762
Speaker SPEAKER_01: Is the problem then the technology, or is the problem the people behind it?

71
00:05:22,141 --> 00:05:23,043
Speaker SPEAKER_00: It's the combination.

72
00:05:23,103 --> 00:05:29,148
Speaker SPEAKER_00: Obviously, many of the organizations developing this technology are defense departments.

73
00:05:30,009 --> 00:05:35,053
Speaker SPEAKER_00: And defense departments don't necessarily want to build in be nice to people as the first rule.

74
00:05:36,435 --> 00:05:41,819
Speaker SPEAKER_00: Some defense departments would like to build in kill people of a particular kind.

75
00:05:42,103 --> 00:05:46,848
Speaker SPEAKER_00: So, we can't expect them all to have good intentions towards all people.

76
00:05:47,290 --> 00:05:49,451
Speaker SPEAKER_01: There is the question of what to do about it.

77
00:05:49,492 --> 00:05:56,459
Speaker SPEAKER_01: This technology is advancing far more quickly than governments and societies can keep pace with.

78
00:05:56,560 --> 00:06:00,264
Speaker SPEAKER_01: The capabilities of this technology, I mean, they leap forward every few months.

79
00:06:00,904 --> 00:06:07,351
Speaker SPEAKER_01: What is required to write legislation, pass legislation, come up with international treaties, that takes years.

80
00:06:08,057 --> 00:06:17,678
Speaker SPEAKER_00: Yes, so that I mean I've gone public to try and encourage much more resources and many more creative scientists to get into this area.

81
00:06:18,742 --> 00:06:25,576
Speaker SPEAKER_00: I think it's an area in which we can actually have international collaboration because the machines taking over

82
00:06:26,029 --> 00:06:27,595
Speaker SPEAKER_00: is a threat for everybody.

83
00:06:27,634 --> 00:06:33,033
Speaker SPEAKER_00: It's a threat for the Chinese and for the Americans and for the Europeans, just like a global nuclear war was.

84
00:06:33,694 --> 00:06:38,992
Speaker SPEAKER_00: And for a global nuclear war, people did actually collaborate to reduce the chances of it.

85
00:06:39,057 --> 00:06:56,802
Speaker SPEAKER_01: There are other experts in the field of AI who say that the concerns that you're raising, this dystopian future, that it distracts from the very real and immediate risks posed by artificial intelligence, some of which you mentioned, misinformation, fraud, discrimination.

86
00:06:56,841 --> 00:06:58,403
Speaker SPEAKER_01: How do you respond to that criticism?

87
00:06:58,423 --> 00:07:00,045
Speaker SPEAKER_00: Yes, I don't want to distract from those.

88
00:07:00,225 --> 00:07:03,430
Speaker SPEAKER_00: I think they're very important concerns and we should be working on those too.

89
00:07:04,072 --> 00:07:08,557
Speaker SPEAKER_00: I just want to add this other existential threat of it taking over.

90
00:07:09,247 --> 00:07:14,932
Speaker SPEAKER_00: And one reason I want to do that is because that's an area in which I think we can get international collaboration.

91
00:07:15,694 --> 00:07:21,000
Speaker SPEAKER_01: Is there any turning back when you say that there will come a time when AI is more intelligent than us?

92
00:07:21,740 --> 00:07:22,882
Speaker SPEAKER_01: Is there any coming back from that?

93
00:07:24,184 --> 00:07:24,704
Speaker SPEAKER_00: I don't know.

94
00:07:24,745 --> 00:07:30,771
Speaker SPEAKER_00: We're entering a time of great uncertainty where we're dealing with kinds of things we've never dealt with before.

95
00:07:31,512 --> 00:07:36,637
Speaker SPEAKER_00: It's as if aliens have landed, but we didn't really take it in because they speak good English.

96
00:07:37,124 --> 00:07:40,028
Speaker SPEAKER_01: How should we think differently then about artificial intelligence?

97
00:07:41,209 --> 00:07:48,720
Speaker SPEAKER_00: We should realize that we're probably going to get things more intelligent than us quite soon, and they will be wonderful.

98
00:07:49,220 --> 00:07:52,925
Speaker SPEAKER_00: They'll be able to do all sorts of things very easily that we find very difficult.

99
00:07:53,565 --> 00:07:56,069
Speaker SPEAKER_00: So there's huge positive potential in these things.

100
00:07:56,750 --> 00:07:59,233
Speaker SPEAKER_00: But of course, there's also huge negative possibilities.

101
00:07:59,855 --> 00:08:04,701
Speaker SPEAKER_00: And I think we should put more or less equal resources into developing AI to make it much more powerful.

102
00:08:05,153 --> 00:08:10,250
Speaker SPEAKER_00: and into figuring out how to keep it under control and how to minimize bad side effects of it.

103
00:08:11,031 --> 00:08:14,524
Speaker SPEAKER_01: Dr. Geoffrey Hinton, thanks so much for your time and for sharing your insights with us.

104
00:08:15,526 --> 00:08:16,529
Speaker SPEAKER_00: Thank you for inviting me.


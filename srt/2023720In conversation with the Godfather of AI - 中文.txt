1 00:00:09,682 --> 00:00:15,214 主持人：能和杰弗里·辛顿一起在这里感到非常荣幸，他是我们这个时代最伟大的思想家之一，也是我们这个时代最重大的问题之一。
2 00:00:15,294 --> 00:00:25,115 主持人：一个帮助创造人工智能的人，几乎参与了人工智能的每一次革命，现在可能也成为了我们未来走向的最有说服力的批评者。
3 00:00:25,175 --> 00:00:27,000 主持人：所以，能和你一起上台是一种荣幸。
4 00:00:27,460 --> 00:00:28,181 主持人：谢谢。
5 00:00:28,161 --> 00:00:30,187 说话人 SPEAKER_00：他赢得了“AI 教父”的称号。
6 00:00:30,307 --> 00:00:32,893 说话人 SPEAKER_00：AI 传统上一直难以处理的一个问题是幽默。
7 00:00:33,415 --> 00:00:39,170 说话人 SPEAKER_00：我问 AI 是否能想出一个关于“AI 教父”的笑话，结果其实并不糟糕。
8 00:00:39,811 --> 00:00:44,685 说话人 SPEAKER_00：它说它给了 AI 一个无法拒绝的提议，那就是神经网络。
9 00:00:44,664 --> 00:00:45,206 说话人 SPEAKER_01：还不错。
10 00:00:45,225 --> 00:00:46,186 说话人 SPEAKER_01：好吧，还不错。
11 00:00:46,226 --> 00:00:46,948 说话人 SPEAKER_00：对 AI 来说不错。
12 00:00:47,249 --> 00:00:48,109 说话人 SPEAKER_00：那我们就从这里开始吧。
13 00:00:48,130 --> 00:00:56,442 说话人 SPEAKER_00：我想在这个对话中简要回顾一下你的基础工作，然后谈到我们今天所在的位置，最后再谈谈未来。
14 00:00:57,003 --> 00:01:10,825 说话人 SPEAKER_00：所以当你构建和设计神经网络，构建像人脑一样工作、像人脑一样学习的计算机系统时，其他人都在说，杰夫，这不会成功的，你却继续前进。
15 00:01:10,805 --> 00:01:16,552 说话人 SPEAKER_00：你是知道这是训练计算机系统的最佳方式才继续前进的吗？
16 00:01:16,992 --> 00:01:21,759 说话人 SPEAKER_00：还是出于更精神层面的原因，你想要制造出像我们一样的机器？
17 00:01:22,960 --> 00:01:25,504 说话人 SPEAKER_01：我这么做是因为大脑总得有点用。
18 00:01:26,245 --> 00:01:30,771 说话人 SPEAKER_01：当然，它绝不是通过显式操作符号表达式来工作的。
19 00:01:31,021 --> 00:01:33,563 说话人 SPEAKER_01：因此，像神经网络这样的东西就必须发挥作用。
20 00:01:34,265 --> 00:01:37,368 说话人 SPEAKER_01：冯·诺伊曼和图灵也这么认为，这是个不错的起点。
21 00:01:38,209 --> 00:01:38,450 说话人 说话人_00: 嗯哼。
22 00:01:38,510 --> 00:01:41,995 说话人 说话人_00: 所以你认为这是最好的前进方式。
23 00:01:42,555 --> 00:01:44,418 说话人 说话人_01: 是的，从长远来看，这是最好的前进方式。
24 00:01:44,477 --> 00:01:47,742 说话人 说话人_00: 因为这个决定将对后续产生深远的影响。
25 00:01:48,862 --> 00:01:50,484 说话人 说话人_00: 但让我们... 好的，所以你这么做。
26 00:01:50,605 --> 00:01:52,007 说话人 说话人_00: 你开始构建神经网络。
27 00:01:52,027 --> 00:01:52,808 说话人 说话人_00: 你继续前进。
28 00:01:53,688 --> 00:01:57,894 说话人 说话人_00: 并且它们在特定有限任务上比人类表现得更好，对吧？
29 00:01:57,974 --> 00:02:00,117 说话人 说话人_00：在图像识别。
30 00:02:00,215 --> 00:02:03,659 说话人 说话人_00：在翻译，一些化学工作。
31 00:02:04,600 --> 00:02:12,371 说话人 说话人_00：我于 2019 年在谷歌 I/O 采访了你，你说他们要很久才能在推理上赶上我们。
32 00:02:13,331 --> 00:02:15,974 说话人 说话人_00：这就是过去四年发生的重大变化，对吧？
33 00:02:17,016 --> 00:02:19,158 说话人 SPEAKER_00: 他们仍然无法与我们匹敌，但他们正在逐渐接近。
34 00:02:19,580 --> 00:02:22,002 说话人 SPEAKER_00: 他们接近到什么程度，为什么？
35 00:02:22,961 --> 00:02:25,104 说话人 SPEAKER_01: 是大型语言模型正在逐渐接近。
36 00:02:25,866 --> 00:02:30,712 说话人 SPEAKER_01: 我真的不明白他们为什么能做到，但他们可以进行一些推理。
37 00:02:31,514 --> 00:02:39,305 说话人 SPEAKER_01：我最喜欢的例子是，我向 GPT-4 提出了一个由一个认为它做不到的符号 AI 人士给出的谜题。
38 00:02:40,206 --> 00:02:43,151 说话人 SPEAKER_01：我把谜题变得更难了，它仍然能做。
39 00:02:43,891 --> 00:02:48,919 说话人 SPEAKER_01：这个谜题是这样的，我家的房间都是涂成蓝色、黄色或白色的。
40 00:02:50,147 --> 00:02:52,389 说话人 SPEAKER_01：黄色的油漆在一年内会褪成白色。
41 00:02:52,430 --> 00:02:55,513 说话人 SPEAKER_01：两年后，我想让他们都变成白色。
42 00:02:55,653 --> 00:02:57,055 说话人 SPEAKER_01：我应该做什么，为什么？
43 00:02:58,376 --> 00:03:00,979 说话人 SPEAKER_01：它说，你应该把蓝色的房间刷成白色。
44 00:03:02,259 --> 00:03:05,463 说话人 SPEAKER_01：然后它说，你应该这么做，因为蓝色不会褪成白色。
45 00:03:06,044 --> 00:03:09,187 说话人 SPEAKER_01：它说，你不需要粉刷黄色房间，因为它们会逐渐变成白色。
46 00:03:09,888 --> 00:03:11,729 说话人 SPEAKER_01：所以它知道我应该做什么，也知道为什么。
47 00:03:12,691 --> 00:03:16,694 说话人 SPEAKER_01：我很惊讶它已经能够进行如此多的推理。
48 00:03:16,979 --> 00:03:24,012 说话人 SPEAKER_00：这真是一个令人惊叹的例子，因为当人们批评这些系统，或者他们说它们不会做什么时，他们会说它们只是填空题，只是单词补全。
49 00:03:24,032 --> 00:03:25,294 说话人 说话人_00: 但那不是单词补全。
50 00:03:25,775 --> 00:03:27,397 说话人 说话人_00: 对你来说，那是思考吗？
51 00:03:27,437 --> 00:03:29,502 说话人 说话人_01: 是的，那是思考。
52 00:03:29,883 --> 00:03:32,828 说话人 说话人_01: 当人们说这只是自动补全时，
53 00:03:32,925 --> 00:03:37,155 说话人 SPEAKER_01：那个词里有很多内容，很多都是自动补全的。
54 00:03:37,635 --> 00:03:44,671 说话人 SPEAKER_01：如果你思考预测下一个词需要什么，你必须理解已经说过的话才能在预测下一个词上做得很好。
55 00:03:45,353 --> 00:03:49,382 说话人 SPEAKER_01：所以人们说这只是自动补全，或者这只是统计学。
56 00:03:49,361 --> 00:03:52,747 说话人 SPEAKER_01：现在，从某种意义上说，这确实只是统计学。
57 00:03:53,307 --> 00:03:55,592 说话人 SPEAKER_01：但在这个意义上，一切都是统计学。
58 00:03:56,312 --> 00:04:03,745 说话人 SPEAKER_01：这并不是大多数人所理解的统计学，它记录了这种词语组合出现的次数和那种词语组合出现的次数。
59 00:04:04,085 --> 00:04:05,046 说话人 SPEAKER_01：根本不是这样。
60 00:04:05,067 --> 00:04:09,152 说话人 SPEAKER_01：这是发明特征和特征之间的交互来解释接下来会发生什么。
61 00:04:09,354 --> 00:04:10,055 说话人 说话人_00: 好吧。
62 00:04:10,074 --> 00:04:16,584 说话人 说话人_00: 如果只是统计数据，一切都是统计数据，我们还能做什么呢？
63 00:04:17,526 --> 00:04:18,668 说话人 说话人_00: 显然，这不是幽默。
64 00:04:18,728 --> 00:04:19,689 说话人 说话人_00: 也许这不是推理。
65 00:04:19,730 --> 00:04:29,024 说话人 SPEAKER_00：未来有没有什么是我们能够做到的，而一个训练充分、参数充足、计算资源充足的足够大的语言模型做不到的呢？
66 00:04:29,886 --> 00:04:35,375 说话人 SPEAKER_01：如果模型也接受了视觉和抓取等训练，那么就没有了。
67 00:04:35,956 --> 00:04:43,887 说话人 SPEAKER_00：但是有没有什么我们能够想到的，任何认知过程中我们能够想到的，机器无法复制的呢？
68 00:04:43,970 --> 00:04:45,012 说话人 SPEAKER_01：我们只是一台机器。
69 00:04:45,312 --> 00:04:47,716 说话人 SPEAKER_01：我们是一台奇妙、极其复杂的机器。
70 00:04:48,257 --> 00:04:49,620 说话人 SPEAKER_01：但我们仅仅是一个大型的神经网络。
71 00:04:50,122 --> 00:04:54,009 说话人 SPEAKER_01：没有理由说人工神经网络不能做到我们能做的事情。
72 00:04:54,509 --> 00:05:00,661 说话人 SPEAKER_00：我们是比我们正在构建的新神经网络更高效的庞大神经网络，还是更不高效？
73 00:05:01,418 --> 00:05:09,471 说话人 SPEAKER_01：这取决于你是在谈论获取知识的速度和你可以获取多少知识，还是谈论能耗。
74 00:05:09,951 --> 00:05:12,415 说话人 SPEAKER_01：所以在能耗方面，我们更加高效。
75 00:05:12,896 --> 00:05:14,077 说话人 SPEAKER_01：我们的功耗大约是 30 瓦。
76 00:05:14,619 --> 00:05:20,247 说话人 SPEAKER_01：这些大型语言模型之一，在训练时，你会训练许多副本，每个副本查看数据的不同部分。
77 00:05:20,769 --> 00:05:22,752 说话人 SPEAKER_01：所以它更像是兆瓦。
78 00:05:22,732 --> 00:05:29,007 说话人 SPEAKER_01：所以在能源方面要贵得多，但这些副本可以从数据的不同部分学习不同的事情。
79 00:05:29,488 --> 00:05:32,918 说话人 SPEAKER_01：所以在从数据中获取知识方面效率更高。
80 00:05:33,338 --> 00:05:37,129 说话人 SPEAKER_00：而且它只会变得更加高效，因为每个系统都可以训练下一个系统？
81 00:05:37,589 --> 00:05:38,091 说话人 SPEAKER_01: 是的。
82 00:05:38,408 --> 00:05:39,790 说话人 SPEAKER_00: 那么让我们来听听你的批评。
83 00:05:39,810 --> 00:05:46,298 说话人 SPEAKER_00: 你批评的最好总结来自大约一个月前米尔肯研究所的一个会议。
84 00:05:47,019 --> 00:05:47,940 说话人 SPEAKER_00: 而是 Snoop Dogg。
85 00:05:48,822 --> 00:05:58,814 说话人 SPEAKER_00: 他说，我听到那位创建人工智能的老头子说这不太安全，因为人工智能有自己的思想，那些混蛋们将会开始做他们自己的事情。
86 00:06:01,077 --> 00:06:02,139 说话人 SPEAKER_00: 这准确吗？
87 00:06:02,180 --> 00:06:03,841 说话人 SPEAKER_00: 这是个准确的总结吗？
88 00:06:04,295 --> 00:06:06,903 说话人 SPEAKER_01: 嗯，他们可能没有母亲。
89 00:06:11,918 --> 00:06:14,146 说话人 SPEAKER_00: 但 Dr. Dog 说的其他部分都是正确的。
90 00:06:14,747 --> 00:06:15,569 说话人 SPEAKER_00: 等等，是的。
91 00:06:16,259 --> 00:06:25,189 说话人 SPEAKER_00: 好吧，解释一下你的意思，或者他的意思，以及它如何适用于你的意思，当他们开始做自己的事情时。
92 00:06:25,389 --> 00:06:26,310 说话人 SPEAKER_00: 这对你意味着什么？
93 00:06:26,610 --> 00:06:31,355 说话人 SPEAKER_01：好吧，首先我要强调，我们正进入一个充满不确定性的时期。
94 00:06:31,396 --> 00:06:32,937 说话人 SPEAKER_01：没有人真正知道会发生什么。
95 00:06:33,437 --> 00:06:36,502 说话人 SPEAKER_01：而且我尊重意见的人与我有着非常不同的信仰。
96 00:06:36,922 --> 00:06:38,865 说话人 SPEAKER_01：比如 Jan LeCun 认为一切都会好起来。
97 00:06:39,305 --> 00:06:40,286 说话人 SPEAKER_01：他们只是会帮助我们。
98 00:06:40,365 --> 00:06:41,827 说话人 SPEAKER_01：一切都将变得美好。
99 00:06:41,807 --> 00:06:55,105 说话人 SPEAKER_01：但我认为我们必须认真对待这样的可能性，如果他们变得比我们聪明，这似乎很可能会发生，而且他们有自己的目标，这也似乎很可能会发生，他们可能会发展出控制的目标。
100 00:06:55,665 --> 00:06:56,947 说话人 SPEAKER_01：如果他们这样做，我们就麻烦了。
101 00:06:57,769 --> 00:07:02,334 说话人 SPEAKER_00: 好的，那么我们稍后再回到那个话题，但先讨论 Jan 的观点。
102 00:07:02,355 --> 00:07:06,961 说话人 SPEAKER_00: Jan Le Coon 也是获得图灵奖的人之一，也被誉为人工智能之父。
103 00:07:07,521 --> 00:07:11,105 说话人 SPEAKER_00: 我最近采访了他，他提出了这样的观点
104 00:07:11,372 --> 00:07:17,495 说话人 SPEAKER_00: 他说，看，所有技术，无论是好是坏都可以被利用，但有些技术天生就带有更多的善。
105 00:07:17,958 --> 00:07:19,223 说话人 说话人_00: 人工智能。
106 00:07:19,355 --> 00:07:23,519 说话人 说话人_00: 是由人类建造的，是由善良的人类为了善良的目的建造的。
107 00:07:23,699 --> 00:07:26,803 说话人 说话人_00: 它被训练在优秀的书籍和优秀的文本上。
108 00:07:27,264 --> 00:07:30,567 说话人 说话人_00: 未来它将倾向于善良。
109 00:07:31,088 --> 00:07:32,250 说话人 SPEAKER_00: 你相信吗？
110 00:07:32,870 --> 00:07:36,495 说话人 SPEAKER_01: 我认为由好人训练的 AI 会有向善的偏见。
111 00:07:36,574 --> 00:07:42,262 说话人 SPEAKER_01: 而由坏人，比如普京这样的人训练的 AI，会有向恶的偏见。
112 00:07:42,721 --> 00:07:45,985 说话人 SPEAKER_01: 我们知道他们会制造战斗机器人。
113 00:07:46,005 --> 00:07:49,209 说话人 SPEAKER_01：他们在许多不同的国防部门忙于这件事。
114 00:07:49,189 --> 00:07:54,858 说话人 SPEAKER_01：所以，他们不一定很好，因为他们的主要目的将是杀人。
115 00:07:56,300 --> 00:08:07,899 说话人 SPEAKER_00：所以，你认为 AI 不良使用的风险，无论是比 AI 的良好使用更多还是更少，都是如此巨大，值得我们现在深思。
116 00:08:08,278 --> 00:08:08,961 说话人 SPEAKER_01：当然，是的。
117 00:08:09,040 --> 00:08:11,644 说话人 SPEAKER_01：对于致命的自主武器，它们值得我们深思。
118 00:08:12,206 --> 00:08:17,052 说话人 SPEAKER_00：嗯，好吧，我们就聚焦在致命的自主武器上，因为在这个论点中，有一件事
119 00:08:17,675 --> 00:08:23,444 说话人 SPEAKER_00：就是你是少数几个真正将此视为风险、真实风险的人之一。
120 00:08:23,463 --> 00:08:34,301 说话人 SPEAKER_00：解释一下你关于为什么超级强大的 AI 与军事结合可能导致越来越多的战争的假设。
121 00:08:36,304 --> 00:08:39,750 说话人 SPEAKER_01：好吧，我实际上不想回答那个问题。
122 00:08:41,671 --> 00:08:51,475 说话人 SPEAKER_01：即使 AI 不是很智能，如果国防部门用它来制造战斗机器人，那将会是非常糟糕、可怕的事情。
123 00:08:51,495 --> 00:08:59,494 说话人 SPEAKER_01：即使它不是很智能，即使它没有自己的意图，它只是按照普京的指示去做。
124 00:08:59,474 --> 00:09:05,544 说话人 SPEAKER_01：这将使富裕国家入侵贫穷国家变得更容易，例如。
125 00：09：06,105 --> 00：09：12,356 议长 SPEAKER_01：目前，随意入侵贫穷国家存在一个障碍，那就是你让死去的公民回家。
126 00：09：13,438 --> 00：09：15,722 演讲者 SPEAKER_01：如果他们只是死去的战斗机器人，那就太好了。
127 00：09：15,763 --> 00：09：17,546 议长 SPEAKER_01：军工复合体会喜欢的。
128 00：09：18,969 --> 00：09：20,892 议长 SPEAKER_00：所以你这么认为是因为
129 00:09:21,243 --> 00:09:23,746 说话人 SPEAKER_00: 我的意思是，这有点类似于人们提出的关于无人机的论点。
130 00:09:23,787 --> 00:09:27,832 说话人 SPEAKER_00: 如果你可以派遣无人机而不必派遣带有驾驶员的飞机，你更有可能派遣无人机。
131 00:09:27,852 --> 00:09:29,796 说话人 SPEAKER_00: 因此，你更有可能发动攻击。
132 00:09:30,096 --> 00:09:32,820 说话人 SPEAKER_00: 如果你有一个战斗机器人，那就相当于平方了同样的情况。
133 00:09:34,001 --> 00:09:34,783 说话人 SPEAKER_00：这就是你的顾虑。
134 00:09:35,203 --> 00:09:36,946 说话人 SPEAKER_01：这是我对于战斗机器人的主要顾虑。
135 00:09:37,105 --> 00:09:43,655 说话人 SPEAKER_01：这是与超级智能系统为了自己的目的而接管所发生的事情分开的顾虑。
136 00:09:44,142 --> 00:09:48,048 说话人 SPEAKER_00：在我们谈到超级智能系统之前，让我们谈谈你的一些其他顾虑。
137 00:09:48,168 --> 00:09:55,999 说话人 SPEAKER_00：所以，在你担心的诸多事情中，显然战斗机器人是其中之一，你对不平等现象也相当担忧。
138 00:09:56,500 --> 00:09:57,301 说话人 SPEAKER_00：请详细谈谈这个。
139 00:09:58,101 --> 00:10:06,453 说话人 SPEAKER_01：所以，这相当明确，虽然不确定，但很明确的是，这些大型语言模型将导致生产力的显著提升。
140 00:10:06,772 --> 00:10:19,708 说话人 SPEAKER_01：我认识一个人，他为医疗服务机构处理投诉信，以前是他自己写这些信，现在他让 ChatGPT 来写，处理投诉所需的时间缩短到了原来的五分之一。
141 00:10:20,590 --> 00:10:25,557 他说他能做五倍的工作，所以他的数量只有五分之一。
142 00:10:26,701 --> 00:10:28,644 或者他们可能会回更多的信。
143 00:10:28,703 --> 00:10:29,865 或者他们会回更多的信，对吧？
144 00:10:29,946 --> 00:10:33,351 或者因为他们效率很高，他们可能会有更多的人，对吧？
145 00:10:33,412 --> 00:10:35,695 说话人 SPEAKER_00：更高的生产力意味着能完成更多的事情。
146 00:10:35,956 --> 00:10:36,476 说话人 SPEAKER_00：也许不是吧。
147 00:10:36,496 --> 00:10:37,577 说话人 SPEAKER_00：这是一个未解决的问题。
148 00:10:37,839 --> 00:10:49,417 说话人 SPEAKER_01：但我们期望在我们生活的这种社会中，如果你有如此大的生产力提升，财富不会流向那些工作的人或失业的人。
149 00:10:49,837 --> 00:10:52,221 说话人 SPEAKER_01：这将导致富者越富，穷者越穷。
150 00:10:52,642 --> 00:10:54,345 说话人 SPEAKER_01：这对社会非常不利。
151 00:10:54,325 --> 00:10:57,731 说话人 SPEAKER_00：从定义上讲，或者你认为有什么 AI 的特性会导致这种情况？
152 00:10:57,993 --> 00:10:58,934 说话人 SPEAKER_01：不，这与 AI 无关。
153 00:10:59,034 --> 00:11:01,539 说话人 SPEAKER_01：这就是当你提高生产力时会发生的事情。
154 00:11:02,302 --> 00:11:04,726 说话人 SPEAKER_01：尤其是在没有强大工会的社会主义社会。
155 00:11:05,107 --> 00:11:13,245 说话人 SPEAKER_00：但现在，许多经济学家会持不同观点，他们认为随着时间的推移，如果你观察技术，对吧，我们从...
156 00:11:13,647 --> 00:11:23,481 说话人 SPEAKER_00：马和马车，马和马车消失了，然后我们有了汽车，哎呀，那些赶马车的工人失去了工作，然后自动取款机出现了，银行柜员就不再需要做那份工作了。
157 00:11:23,501 --> 00:11:29,129 说话人 SPEAKER_00：但现在我们比以前雇佣了更多的银行柜员，而且现在开优步的人也比以前骑马的人多了。
158 00:11:29,230 --> 00:11:41,508 说话人 SPEAKER_00：经济学家可能会这样回应，是的，会有人员流动，回信的人会减少，但会有更多更高认知的工作要做。
159 00:11:41,687 --> 00:11:42,809 说话人 SPEAKER_00：你怎么回应这个？
160 00:11:43,921 --> 00:11:50,842 说话人 SPEAKER_01：我认为我首先会说，以前一磅面包只值一便士，然后他们发明了经济学，现在一磅面包要五美元。
161 00:11:52,509 --> 00:11:59,097 说话人 SPEAKER_01：所以我并不完全信任经济学家的话，尤其是当他们处理从未发生过的全新情况时。
162 00:11:59,379 --> 00:11:59,619 说话人 SPEAKER_00：没错。
163 00:12:00,159 --> 00:12:03,683 说话人 SPEAKER_01：而超级智能将是一个从未发生过的全新情况。
164 00:12:03,924 --> 00:12:11,653 说话人 SPEAKER_01：但即使是这些正在取代从事文本生产工作的人的大聊天机器人，这种情况也从未发生过。
165 00:12:12,434 --> 00:12:18,383 说话人 SPEAKER_01：我不确定他们怎么能自信地预测新增的就业岗位会多于失去的岗位。
166 00:12:18,802 --> 00:12:25,551 说话人 SPEAKER_00：我顺便提一下，在绿色房间里，我向杰夫介绍了我的两个孩子，爱丽丝和扎卡里，他们就在这里某个地方。
167 00:12:26,272 --> 00:12:29,716 说话人 SPEAKER_00：他对爱丽丝说，他说，你打算进入媒体行业吗？
168 00:12:30,216 --> 00:12:32,519 说话人 SPEAKER_00：然后他又说，嗯，我不确定媒体还会存在。
169 00:12:33,221 --> 00:12:34,964 说话人 SPEAKER_00: 然后爱丽丝问，我应该做什么？
170 00:12:34,984 --> 00:12:35,484 说话人 SPEAKER_00: 你怎么说？
171 00:12:35,985 --> 00:12:36,326 说话人 SPEAKER_00: 水暖。
172 00:12:36,645 --> 00:12:36,826 说话人 SPEAKER_00: 是的。
173 00:12:37,006 --> 00:12:38,869 说话人 说话人_00：现在解释一下。
174 00:12:38,849 --> 00:12:42,033 说话人 说话人_00：我的意思是，我们家有很多管道问题。
175 00:12:42,053 --> 00:12:44,577 说话人 说话人_00：如果他们能装一个新的水槽那就太好了。
176 00:12:45,239 --> 00:12:53,373 说话人 说话人_00：解释一下工作，这里很多年轻人，不仅仅是我的孩子，都在考虑要进入什么职业，他们应该关注哪些职业？
177 00:12:53,692 --> 00:12:54,815 说话人 SPEAKER_00：它们有哪些属性？
178 00:12:55,495 --> 00:12:57,538 说话人 SPEAKER_01：我来给你讲一个当木匠的小故事。
179 00:12:58,981 --> 00:13:01,566 说话人 SPEAKER_01：如果你是木匠，制作家具很有趣。
180 00:13:02,238 --> 00:13:05,621 说话人 SPEAKER_01：但是因为机器可以制作家具，所以这完全是徒劳的。
181 00:13:06,363 --> 00:13:16,395 说话人 SPEAKER_01：如果你是一名木匠，你擅长的是修理家具或者把东西装进老房子里那些尴尬的空间，制作那些不太规则的架子。
182 00:13:17,277 --> 00:13:27,029 说话人 SPEAKER_01：所以那些在很长的一段时间内能够抵御人工智能影响的职业，是需要你非常适应性强和有身体技能的工作，而管道工就是这样的一种工作。
183 00:13:27,178 --> 00:13:30,923 说话人 SPEAKER_00：因为手工灵巧是机器难以复制的。
184 00:13:31,143 --> 00:13:38,370 说话人 SPEAKER_01：这仍然很难，我认为它们能够真正灵活地进入这些尴尬的空间还需要更长的时间。
185 00:13:40,013 --> 00:13:43,876 说话人 SPEAKER_01：这需要的时间比擅长回答文本问题要长。
186 00:13:43,937 --> 00:13:44,837 说话人 SPEAKER_00：但我应该相信你吗？
187 00:13:44,898 --> 00:13:47,181 说话人 SPEAKER_00：因为四年前我们在台上的时候，你说的是推理。
188 00:13:47,221 --> 00:13:50,884 说话人 SPEAKER_00：只要有人从事的是以推理为主的工作，他们就能坚持下去，不是吗？
189 00:13:51,424 --> 00:13:54,148 说话人 SPEAKER_00: 人工智能的本质不就是这样吗？
190 00:13:54,128 --> 00:13:58,653 说话人 SPEAKER_00: 我们实际上不知道下一次性能的巨大提升将从何而来。
191 00:13:58,693 --> 00:14:00,336 说话人 SPEAKER_00: 也许它将来自手工灵巧。
192 00:14:00,755 --> 00:14:02,437 说话人 SPEAKER_00: 是的，有可能。
193 00:14:02,457 --> 00:14:05,201 说话人 SPEAKER_00: 所以，实际上，我想就这个问题问你一个问题。
194 00:14:05,282 --> 00:14:15,673 说话人 SPEAKER_00: 那么，你认为当我们看待 AI，展望未来五年的 AI，最具有影响力的改进将出现在大型语言模型及其相关领域吗？
195 00:14:16,134 --> 00:14:18,076 说话人 SPEAKER_00: 还是认为它将出现在其他方面？
196 00:14:18,562 --> 00:14:21,529 说话人 SPEAKER_01: 我认为可能是在多模态大型模型上。
197 00:14:21,971 --> 00:14:23,594 说话人 SPEAKER_01：所以它们不仅仅是语言模型。
198 00:14:24,095 --> 00:14:26,000 说话人 SPEAKER_01：它们将进行视觉处理。
199 00:14:26,360 --> 00:14:28,465 说话人 SPEAKER_01：希望它们能够分析视频。
200 00:14:28,485 --> 00:14:31,812 说话人 SPEAKER_01：所以它们能够训练所有 YouTube 视频，例如。
201 00:14:32,533 --> 00:14:34,698 说话人 SPEAKER_01：你可以理解很多。
202 00:14:34,678 --> 00:14:37,205 说话人 SPEAKER_01：从除了语言之外的事物。
203 00:14:38,067 --> 00:14:41,534 说话人 SPEAKER_01：当你这样做的时候，你需要更少的语言来达到同样的表现。
204 00:14:42,197 --> 00:14:48,431 说话人 SPEAKER_01：所以他们会被饱和，因为他们已经用完了所有的语言，所有的语言都很容易获取。
205 00:14:48,972 --> 00:14:52,080 说话人 SPEAKER_01：如果他们也使用了大量其他模态，那就不是什么大问题。
206 00:14:52,059 --> 00:14:58,607 说话人 SPEAKER_00：我的 AI 同行教父 Jan 提出的另一个论点是，语言是如此有限，对吧？
207 00:14:58,628 --> 00:15:00,971 说话人 SPEAKER_00：实际上，我们传达的信息远不止词语本身。
208 00:15:00,990 --> 00:15:02,714 说话人 SPEAKER_00：事实上，我正疯狂地打手势，对吧？
209 00:15:02,994 --> 00:15:05,878 说话人 SPEAKER_00：这传达了一些信息，以及灯光等等。
210 00:15:06,538 --> 00:15:13,847 说话人 SPEAKER_00：所以你的观点可能是对的，语言可能是一种有限的信息向量，但很快它将与其它向量相结合？
211 00:15:13,908 --> 00:15:14,928 说话人 SPEAKER_00：绝对如此。
212 00:15:14,976 --> 00:15:20,866 说话人 SPEAKER_01：仅从语言本身就能学到很多东西真是令人惊叹，但你从多种模态中学习会更好。
213 00:15:21,126 --> 00:15:23,190 说话人 SPEAKER_01：小孩子不仅仅是从语言中学习。
214 00:15:23,892 --> 00:15:24,091 说话人 SPEAKER_00：是的。
215 00:15:24,493 --> 00:15:40,578 说话人 SPEAKER_00：所以，如果你的主要角色现在仍然是研究 AI，寻找下一个大突破，你将会从事多模态 AI，并尝试将，比如说，视觉 AI 系统与基于文本的 AI 系统相结合？
216 00:15:40,727 --> 00:15:43,432 说话人 SPEAKER_01：是的，这正是谷歌现在正在做的事情。
217 00:15:43,532 --> 00:15:45,755 说话人 SPEAKER_01：谷歌正在开发一个名为 Gemini 的系统。
218 00:15:46,537 --> 00:15:49,662 说话人 SPEAKER_01：但幸运的是，Demoservice 几天前已经提到了它。
219 00:15:50,423 --> 00:15:51,563 说话人 SPEAKER_01：所以现在你可以谈论它了。
220 00:15:51,583 --> 00:15:52,706 说话人 SPEAKER_01：这是一个多模态人工智能。
221 00:15:53,307 --> 00:15:55,429 说话人 SPEAKER_00：嗯，让我谈谈在谷歌的另外一件事。
222 00:15:55,450 --> 00:16:03,883 说话人 SPEAKER_00：所以当你在那里的时候，谷歌发明了 Transformer 网络，或者说发明了 Transformer 架构，生成了预训练的 Transformer。
223 00:16:05,517 --> 00:16:12,466 说话人 SPEAKER_00：你是什么时候意识到这一点会如此核心和重要的？
224 00:16:12,486 --> 00:16:21,379 说话人 SPEAKER_00：这对我来说很有趣，因为这篇论文是在 2017 年发布的，而且它发布的时候，并不是像烟花一样在天空中爆炸。
225 00:16:22,000 --> 00:16:25,865 说话人 SPEAKER_00: 六年后，五年后，我们突然意识到后果。
226 00:16:25,924 --> 00:16:30,731 说话人 SPEAKER_00: 想想看，五年后还有哪些论文可能和我们现在的一样？
227 00:16:30,796 --> 00:16:35,708 说话人 SPEAKER_01: 所以，在 Transformer 之后，谷歌在短短几年后开发了 BERT。
228 00:16:36,671 --> 00:16:39,839 说话人 SPEAKER_01: 所以 BERT 清楚地表明，Transformer 是一个巨大的突破。
229 00:16:40,981 --> 00:16:44,451 说话人 SPEAKER_01：我没有立刻意识到这是一个多么重大的突破。
230 00:16:45,123 --> 00:16:47,746 说话人 SPEAKER_01：对此我感到很烦恼。
231 00:16:48,947 --> 00:16:52,552 说话人 SPEAKER_01：我花了几年的时间才意识到，但现在已经很明确了。
232 00:16:52,793 --> 00:17:01,063 说话人 SPEAKER_00：我第一次听到“transformer”这个词是在舞台上和你交谈时，你当时正在谈论 transformer 与 capsule 的比较，而这正是它刚发布之后。
233 00:17:01,504 --> 00:17:06,692 说话人 SPEAKER_00：让我们谈谈关于语言模型和其他模型的其他批评之一，那就是...
234 00:17:06,672 --> 00:17:13,461 说话人 SPEAKER_00：很快，我的意思是，实际上，他们可能已经吸收了人类创造的所有有机数据。
235 00:17:13,821 --> 00:17:22,032 说话人 SPEAKER_00：如果我现在创建一个 AI 模型，并在互联网上对其进行训练，它将训练在一堆东西上，主要是人类制作的东西，但也有一堆 AI 制作的东西，对吧？
236 00:17:22,413 --> 00:17:30,965 说话人 SPEAKER_00：而且你将继续在由 AI 创建的东西上训练 AI，无论是基于文本的语言模型还是多模态语言模型。
237 00:17:31,727 --> 00:17:32,528 说话人 说话人_00: 那会导致不可避免的衰败和腐败吗？
238 00:17:33,317 --> 00:17:36,424 说话人 说话人_00: 如有些人所争论的那样？
239 00:17:37,267 --> 00:17:40,753 说话人 说话人_00: 或者这只是我们必须面对的事情？
240 00:17:41,115 --> 00:17:48,069 说话人 说话人_00: 或者，正如 AI 领域的其他人所说，这是训练 AI 的最好事情，我们应该只使用合成数据来训练 AI？
241 00:17:48,488 --> 00:17:51,290 说话人 SPEAKER_01：好吧，实际上我在这方面并不真正知道答案。
242 00:17:52,010 --> 00:17:58,876 说话人 SPEAKER_01：我怀疑你需要采取预防措施，以免只是在你自己生成或之前版本的你生成的数据上训练。
243 00:17:59,678 --> 00:18:07,885 说话人 SPEAKER_01：我怀疑可以采取这些预防措施，尽管如果所有虚假数据都被标记为虚假的话会容易得多。
244 00:18:08,266 --> 00:18:13,471 说话人 SPEAKER_01：在人工智能领域有一个例子，在用自己的东西进行训练时帮助很大。
245 00:18:13,931 --> 00:18:16,133 说话人 SPEAKER_01：所以如果你没有太多训练数据，
246 00:18:16,348 --> 00:18:35,797 说话人 SPEAKER_01：或者更确切地说，你有大量未标记数据而标记数据很少，你可以训练一个模型来预测标记数据上的标签，然后你用同一个模型来训练预测未标记数据的标签，无论它预测什么，你都告诉它你是对的。
247 00:18:38,160 --> 00:18:40,363 说话人 SPEAKER_01：这实际上让模型工作得更好。
248 00:18:40,624 --> 00:18:42,946 说话人 SPEAKER_00：这究竟是怎么工作的？
249 00:18:44,040 --> 00:18:45,582 说话人 SPEAKER_01：总体来说，它往往是正确的。
250 00:18:46,962 --> 00:18:48,384 说话人 SPEAKER_01：这很复杂。
251 00:18:48,404 --> 00:18:53,249 说话人 SPEAKER_01：在许多年前，它已经被声调制解器分析得更好了。
252 00:18:53,608 --> 00:18:54,569 说话人 SPEAKER_01：他们玩了个同样的花招。
253 00:18:55,210 --> 00:18:59,473 说话人 说话人_00：所以听着这个，我在舞台上有了这样的领悟。
254 00:19:00,494 --> 00:19:02,517 说话人 说话人_00：你是一个非常批判我们走向的人。
255 00:19:03,116 --> 00:19:04,718 说话人 说话人_00：杀人机器人，收入不平等。
256 00:19:05,798 --> 00:19:07,601 说话人 说话人_00：你听起来也像是一个喜欢这些东西的人。
257 00:19:08,221 --> 00:19:09,201 说话人 SPEAKER_01：是的，我喜欢这些东西。
258 00:19:10,042 --> 00:19:13,625 说话人 SPEAKER_01：你怎么会不喜欢制作智能事物呢？
259 00:19:14,280 --> 00:19:17,984 说话人 SPEAKER_00：让我来谈谈可能对在座各位和所有人来说最重要的问题。
260 00:19:19,546 --> 00:19:23,809 说话人 SPEAKER_00：我们现在到了这样一个时刻，很多人喜欢这些东西，他们想建造它，想进行实验。
261 00:19:25,192 --> 00:19:27,233 说话人 SPEAKER_00: 但我们不希望有负面影响。
262 00:19:27,273 --> 00:19:29,095 说话人 SPEAKER_00: 我们不希望收入差距扩大。
263 00:19:29,195 --> 00:19:30,717 说话人 SPEAKER_00: 我不希望媒体消失。
264 00:19:31,298 --> 00:19:41,907 说话人 SPEAKER_00：我们现在应该做哪些选择和决定，以及哪些事情，以最大化善，最大化创造力，同时限制潜在的伤害？
265 00:19:42,174 --> 00:19:46,607 说话人 SPEAKER_01：所以我认为要回答这个问题，你必须区分许多种潜在的危害。
266 00:19:47,309 --> 00:19:49,836 说话人 SPEAKER_01：所以我会为你区分出六种。
267 00:19:49,936 --> 00:19:50,679 说话人 SPEAKER_00：请。
268 00:19:50,699 --> 00:19:52,223 说话人 SPEAKER_01：存在偏见和歧视。
269 00:19:52,384 --> 00:19:52,724 说话人 SPEAKER_00：是的。
270 00:19:53,666 --> 00:19:55,849 说话人 SPEAKER_01：这是现在就存在的问题。
271 00:19:56,851 --> 00:19:59,714 说话人 SPEAKER_01：这不是我们未来需要担心的事情之一。
272 00:19:59,734 --> 00:20:00,476 说话人 SPEAKER_01：现在正在发生。
273 00:20:01,297 --> 00:20:05,602 说话人 SPEAKER_01：但是，与所有其他事情相比，我认为这是相对容易解决的。
274 00:20:06,163 --> 00:20:13,531 说话人 SPEAKER_01：如果你的目标不是拥有一个完全无偏见的系统，而是仅仅拥有一个比它所取代的系统显著减少偏见的系统。
275 00:20:14,490 --> 00:20:18,535 所以，目前，是老白男在决定年轻黑人女性是否应该获得抵押贷款。
276 00:20:19,175 --> 00:20:23,240 说话人 SPEAKER_01：如果你只训练那份数据，你会得到一个同样有偏见的系统。
277 00:20:23,259 --> 00:20:24,961 说话人 SPEAKER_01：但是你可以分析这种偏见。
278 00:20:25,702 --> 00:20:27,964 说话人 SPEAKER_01：你可以看到它是如何有偏见的，因为它不会改变其行为。
279 00:20:27,984 --> 00:20:29,547 说话人 SPEAKER_01：你可以将其冻结，然后进行分析。
280 00:20:29,906 --> 00:20:32,730 说话人 SPEAKER_01：这样应该更容易纠正偏差。
281 00:20:32,750 --> 00:20:34,392 说话人 SPEAKER_01：所以，好吧，那是偏见和歧视。
282 00:20:35,133 --> 00:20:39,317 说话人 SPEAKER_01：我认为我们可以在这方面做很多事情，我认为我们做很多事情很重要，这是可行的。
283 00:20:40,411 --> 00:20:42,175 说话人 SPEAKER_01：下一个是战斗机器人。
284 00：20：42,676 --> 00：20：46,386 议长 SPEAKER_01：我真的很担心，因为国防部门要建造它们。
285 00：20：48,593 --> 00：20：51,682 议长 SPEAKER_01：我看不出你怎么能阻止他们这样做。
286 00：20：53,066 --> 00：20：55,752 发言者 SPEAKER_01：像《日内瓦公约》这样的东西会很棒。
287 00：20：56,222 --> 00：20：58,765 议长 SPEAKER_01：但这些只有在使用过之后才会发生。
288 00：20：58,865 --> 00：21：03,029 议长 SPEAKER_01：我相信，化学武器直到第一次世界大战之后才发生。
因此我认为可能会发生的事情是人们会使用战斗机器人，我们将看到它们有多么糟糕，然后也许我们可以通过一项国际公约来禁止它们。
290 00：21：13,740 --> 00：21：14,421 演讲者 SPEAKER_01：所以那是两个。
我意思是，你也可以告诉那些在构建 AI 的人不要把他们的设备卖给军方。
292 00:21:20,445 --> 00:21:21,007 说话人 SPEAKER_01：你可以试试。
293 00:21:21,047 --> 00:21:21,647 说话人 SPEAKER_01：试试。
294 00:21:22,028 --> 00:21:22,268 说话人 SPEAKER_01：好的。
295 00:21:22,288 --> 00:21:22,709 说话人 SPEAKER_01：第三。
296 00:21:23,690 --> 00:21:25,250 说话人 SPEAKER_01：军队有很多钱。
297 00:21:27,256 --> 00:21:30,520 说话人 SPEAKER_01：第三点，有失业问题。
298 00:21:30,721 --> 00:21:45,422 说话人 SPEAKER_01：你可以尝试做一些事情来确保生产力的提高，一些来自生产力提高的额外收入，用于帮助那些失业的人，如果创造的工作没有比摧毁的工作多。
299 00:21:46,923 --> 00:21:51,069 说话人 SPEAKER_01：这是一个社会政策问题，真正需要的是社会主义。
300 00:21:52,602 --> 00:21:56,567 说话人 SPEAKER_01：我们在加拿大，所以可以说社会主义。
301 00:21:58,150 --> 00:22:09,626 说话人 SPEAKER_01：第四点是战争回音室，因为大公司希望你点击那些让你愤怒的东西，因此给你越来越极端的内容。
302 00:22:09,686 --> 00:22:20,382 说话人 SPEAKER_01：所以你最终会陷入一个回音室，如果你在另一个回音室，你会相信这些疯狂的阴谋论者；如果你在我的回音室，你会相信真相。
303 00:22:21,661 --> 00:22:26,211 说话人 SPEAKER_01：这部分的成因是公司的政策，也许可以对此做些什么。
304 00:22:26,951 --> 00:22:29,577 说话人 说话人_00: 但这是一个存在的问题。
305 00:22:29,678 --> 00:22:32,183 说话人 说话人_00: 它在大语言模型出现之前就存在了。
306 00:22:32,203 --> 00:22:34,528 说话人 说话人_00: 事实上，大型语言模型可以逆转这一点。
307 00:22:36,051 --> 00:22:36,472 说话人 说话人_01: 也许吧。
308 00:22:37,234 --> 00:22:41,644 说话人 SPEAKER_00: 我的意思是，他们能否让它变得更好，或者是否会使这个问题变得更糟，这是一个开放的问题。
309 00:22:41,708 --> 00:22:46,113 说话人 SPEAKER_01: 是的，这是一个与 AI 有关的问题，但不是与大型语言模型有关。
310 00:22:46,133 --> 00:22:46,733 说话人 SPEAKER_00: 这怎么会是个问题呢？
311 00:22:46,773 --> 00:22:52,060 说话人 SPEAKER_00: 这是个与 AI 有关的问题，因为有一个使用 AI 算法的问题，这个算法是基于我们的情绪进行训练的，然后推动我们走向那些方向？
312 00:22:52,121 --> 00:22:52,221 说话人 SPEAKER_00: 是的。
313 00:22:52,240 --> 00:22:52,381 说话人 SPEAKER_00: 好的。
314 00:22:52,401 --> 00:22:54,163 说话人 SPEAKER_01: 好吧，那么是第四个。
315 00:22:55,365 --> 00:23:01,471 说话人 SPEAKER_01: 这是存在风险，正是我决定讨论的，因为很多人认为这是个玩笑。
316 00:23:02,053 --> 00:23:02,633 说话人 SPEAKER_00: 对。
317 00:23:02,653 --> 00:23:05,656 说话人 SPEAKER_01: 所以昨天《自然》杂志上发表了一篇社论
318 00:23:05,991 --> 00:23:13,307 说话人 SPEAKER_01: 他们基本上说，我对存在性风险的恐惧宣传分散了人们对实际风险的注意力。
319 00:23:14,068 --> 00:23:20,000 说话人 SPEAKER_01: 因此，他们将存在性风险与实际风险进行了比较，暗示存在性风险并非实际风险。
320 00:23:21,262 --> 00:23:24,846 说话人 SPEAKER_01：我认为人们应该明白这不仅仅是科幻。
321 00:23:25,186 --> 00:23:26,568 说话人 SPEAKER_01：这不仅仅是恐吓。
322 00:23:27,309 --> 00:23:29,311 说话人 SPEAKER_01：这是一个我们需要思考的真实风险。
323 00:23:29,372 --> 00:23:32,474 说话人 SPEAKER_01：我们需要提前想出如何应对它。
324 00:23:33,676 --> 00:23:34,897 说话人 SPEAKER_01：那么是五。
325 00:23:35,659 --> 00:23:37,820 说话人 SPEAKER_01：还有一件，但我怎么也想不起来是什么。
326 00:23:38,082 --> 00:23:40,403 说话人 SPEAKER_00：你的清单怎么没有以存在风险结束呢？
327 00:23:40,423 --> 00:23:41,965 说话人 SPEAKER_00：我觉得清单应该就到这里结束了。
328 00:23:42,026 --> 00:23:43,047 说话人 SPEAKER_01: 不，那已经是结尾了。
329 00:23:43,067 --> 00:23:47,372 说话人 SPEAKER_01: 但是我想，如果我谈论生存风险，我应该能够想起那个缺失的。
330 00:23:47,352 --> 00:23:48,073 说话人 SPEAKER_00: 但是我没有。
331 00:23:48,513 --> 00:23:50,999 说话人 SPEAKER_00: 好吧，那么，让我们来谈谈生存风险。
332 00:23:51,038 --> 00:24:02,839 说话人 SPEAKER_00: 请准确解释存在风险，它是如何发生的，或者尽可能想象，是什么错误导致了人类物种的灭绝或消失。
333 00:24:03,540 --> 00:24:11,335 说话人 SPEAKER_01: 在一个非常一般的基础上，如果你拥有一个比你聪明得多、非常擅长操纵人的东西，
334 00:24:11,617 --> 00:24:15,060 说话人 SPEAKER_01: 只是一个非常一般的问题，你是否有信心人类会保持控制权？
335 00:24:16,261 --> 00:24:23,469 说话人 SPEAKER_01: 然后你可以进入具体场景，探讨人们可能会失去控制的情况，尽管他们是创造它并赋予它目标的人。
336 00:24:24,288 --> 00:24:33,538 说话人 SPEAKER_01：一个非常明显的场景是，如果你被赋予一个目标，并且想要擅长实现它，你需要尽可能多的控制。
337 00:24:34,739 --> 00:24:41,625 说话人 SPEAKER_01：比如，如果我坐在一个无聊的研讨会上，我看到天花板上有一个小亮点，
338 00:24:42,736 --> 00:24:46,823 说话人 SPEAKER_01：然后我突然注意到，当我移动时，那个小亮点也会移动。
339 00:24:46,954 --> 00:24:49,038 说话人 SPEAKER_01：我意识到那是我的手表的反射。
340 00:24:49,137 --> 00:24:50,759 说话人 SPEAKER_01：太阳在表盘上反射。
341 00:24:51,641 --> 00:24:54,904 说话人 SPEAKER_01：所以我接下来就不想再听那无聊的研讨会了。
342 00:24:55,425 --> 00:24:58,911 说话人 SPEAKER_01：我立刻想办法让它这样走，让它那样走。
343 00:24:59,290 --> 00:25:01,933 说话人 SPEAKER_01：一旦我控制住了它，也许我就会再听研讨会了。
我们有一种非常强烈的内在冲动去控制，这是非常合理的。
345 00：25：06,921 --> 00：25：10,265 演讲者 SPEAKER_01：因为你得到的控制权越多，就越容易实现目标。
346 00：25：10,705 --> 00：25：12,989 演讲者 SPEAKER_01：我认为人工智能也能够推导出这一点。
347 00：25：13,469 --> 00：25：16,252 议长 SPEAKER_01：获得控制权是件好事，这样你就可以实现其他目标。
348 00:25:16,586 --> 00:25:26,079 说话者 SPEAKER_00: 等等，你真的认为控制将成为某种与... AI 训练我们所依赖的东西的天生特性吗？
349 00:25:26,099 --> 00:25:33,048 说话者 SPEAKER_00: 它们表现得像我们，它们思考得像我们，因为神经架构使它们像我们的人类大脑一样，并且因为它们是在我们的所有输出上训练的。
350 00:25:33,068 --> 00:25:39,237 说话者 SPEAKER_00: 所以你真的认为控制人类将是 AI 几乎渴望实现的事情吗？
351 00:25:39,857 --> 00:25:44,726 说话者 SPEAKER_01: 不，我认为它们将把它作为实现其他目标的一种方式。
352 00:25:45,006 --> 00:25:46,367 说话人 SPEAKER_01：我认为在我们这里，这是天生的。
353 00:25:46,388 --> 00:25:51,036 说话人 SPEAKER_01：我认为……我对说事物真的是天生的表示怀疑。
354 00:25:51,576 --> 00:25:57,626 说话人 SPEAKER_01：但我认为了解事物如何运作的愿望是一个非常明智的愿望，我认为我们拥有这种愿望。
355 00:25:58,688 --> 00:26:05,358 说话人 SPEAKER_00：所以我们有这样的愿望，然后人工智能将发展出操纵我们和控制我们的能力，
356 00:26:06,789 --> 00:26:08,353 说话人 SPEAKER_00: 我们无法回应，对吧？
357 00:26:08,374 --> 00:26:18,395 说话人 SPEAKER_00: 尽管善良的人也能使用同样强大的 AI 来对抗这些恶势力，但你认为我们仍然可能面临生存危机。
358 00:26:18,556 --> 00:26:19,699 说话人 SPEAKER_01: 是的。
359 00:26:19,719 --> 00:26:20,820 说话人 SPEAKER_01: 我不太清楚。
360 00:26:20,942 --> 00:26:26,554 说话人 SPEAKER_01：我的意思是，好人会比坏人拥有更多资源。
361 00:26:26,990 --> 00:26:28,532 说话人 SPEAKER_01：我不太确定这一点。
362 00:26:29,473 --> 00:26:35,404 说话人 SPEAKER_01：而且好 AI 会比坏 AI 更强大，好 AI 能够监管坏 AI。
363 00:26:35,806 --> 00:26:45,623 说话人 SPEAKER_01：目前我们就处于这种情况，有人用 AI 制造垃圾邮件，而像谷歌这样的公司则用 AI 过滤垃圾邮件。
364 00:26:45,603 --> 00:26:49,926 说话人 SPEAKER_01：目前来看，谷歌拥有更多资源，防守方正在击败攻击方。
365 00:26:50,347 --> 00:26:52,089 说话人 SPEAKER_01：但我看不到这种情况会一直持续下去。
366 00:26:52,109 --> 00:26:58,036 说话人 SPEAKER_00：我的意思是，即使在网络战中，有时看起来犯罪分子似乎取得了胜利，有时又看起来防守方取得了胜利。
367 00:26:58,076 --> 00:27:03,361 说话人 SPEAKER_00：所以你认为，关于超级智能人工智能控制人类，会出现这样的战斗吗？
368 00:27:03,381 --> 00:27:04,201 说话人 SPEAKER_01：这很可能，是的。
369 00:27:04,221 --> 00:27:09,788 说话人 SPEAKER_01：我不太相信试图阻止不良 AI 获得控制的良好 AI 会赢。
370 00:27:10,468 --> 00:27:11,128 说话人 SPEAKER_00：好的。
371 00:27:11,148 --> 00:27:12,150 说话人 SPEAKER_00：所以...
372 00:27:12,316 --> 00:27:21,459 说话人 SPEAKER_00: 好吧，在这个存在风险发生之前，在糟糕的 AI 做这件事之前，有很多极其聪明的人正在建造很多极其重要的事情。
373 00:27:22,000 --> 00:27:26,612 说话人 SPEAKER_00: 他们究竟可以做些什么来最大限度地限制这种风险呢？
374 00:27:27,148 --> 00:27:38,121 说话人 SPEAKER_01: 所以你可以做的一件事是，在 AI 变得超级智能之前，你可以进行实证研究，了解它是如何出错的，它是如何试图获得控制的。
375 00:27:38,481 --> 00:27:40,624 说话人 SPEAKER_01: 它是否试图获得控制，我们不知道它是否会这样做。
376 00:27:41,365 --> 00:27:53,898 说话人 SPEAKER_01：但在它比我们聪明之前，我认为开发它的人应该被鼓励投入大量精力去理解它可能会出错的地方，理解它可能会尝试夺取控制权。
377 00:27:53,878 --> 00:28:08,721 说话人 SPEAKER_01：我认为政府可以鼓励开发这项技术的各大公司投入相当的资源，也许不是等量的资源，但现在有 99 位非常聪明的人试图让它变得更好，而有一位非常聪明的人试图找出如何阻止它接管。
378 00:28:09,342 --> 00:28:11,684 说话人 SPEAKER_01：也许你希望它更加平衡。
379 00:28:11,934 --> 00:28:24,311 说话人 SPEAKER_00：因此，在某种程度上，这就是你现在的角色，你之所以与谷歌和平分手，是因为你想要能够发声并参与这次对话，让更多的人加入其中，而不是那 99 位。
380 00:28:24,872 --> 00:28:28,217 说话人 SPEAKER_01：我认为这对聪明人来说非常重要。
381 00:28:28,656 --> 00:28:32,782 说话人 SPEAKER_01：但我还想说，不要认为这是唯一的风险。
382 00:28:32,803 --> 00:28:38,009 说话人 SPEAKER_01：还有其他很多风险，我记得最后一个，就是假新闻。
383 00:28:38,512 --> 00:28:42,999 说话人 SPEAKER_01：因此，尝试标记所有假信息为假信息非常重要。
384 00:28:43,519 --> 00:28:46,383 说话人 SPEAKER_01：我们技术上能否做到，我不知道，但如果能的话那就太好了。
385 00:28:46,884 --> 00:28:48,365 说话人 SPEAKER_01：政府用假币这么做。
386 00:28:48,705 --> 00:28:52,711 说话人 SPEAKER_01：他们不会允许使用假币，因为这会反映到他们的核心利益上。
387 00:28:54,834 --> 00:28:58,137 说话人 SPEAKER_01：他们应该尝试用 AI 生成的材料来做。
388 00:28:58,577 --> 00:28:59,880 说话人 SPEAKER_01：我不知道他们能不能。
389 00:28:59,859 --> 00:29:01,842 说话人 SPEAKER_00：好吧，所以我们没有时间了。
390 00:29:01,922 --> 00:29:09,871 说话人 SPEAKER_00：给出一个具体的待办事项，一些阅读材料，一个思想实验，一件可以让观众带着思考离开的事情，好的，我会这样做。
391 00:29:10,832 --> 00:29:19,281 说话人 SPEAKER_00：人工智能是我们这一代人发明的最强大的东西，也许吧，我将让它变得更好，让它更有可能成为下一代的好力量。
392 00:29:20,143 --> 00:29:22,305 说话人 SPEAKER_00：那么他们如何让它更有可能成为一股善的力量呢？
393 00:29:22,365 --> 00:29:24,788 说话人 SPEAKER_00：是的，给大家最后一点思考。
394 00:29:26,319 --> 00:29:30,365 说话人 SPEAKER_01：我实际上没有计划让它更有可能成为善而不是恶。
395 00:29:30,424 --> 00:29:30,744 说话人 SPEAKER_01：抱歉。
396 00:29:31,425 --> 00:29:42,019 说话人 SPEAKER_01：我认为它正在被开发真是太好了，因为我们没有提到它的大量良好用途，比如在医学、气候变化等方面。
397 00:29:42,038 --> 00:29:45,643 说话人 SPEAKER_01：所以我认为人工智能的进步是不可避免的，可能也是好的。
398 00:29:46,483 --> 00:29:52,951 说话人 SPEAKER_01：但我们真的应该担心减轻它的所有不良副作用，并担心其存在的威胁。
399 00:29:53,152 --> 00:29:54,336 说话人 SPEAKER_00：好的，非常感谢。
400 00:29:54,375 --> 00:29:57,848 说话人 SPEAKER_00：多么深思熟虑、鼓舞人心、有趣、非凡聪明。
401 00:29:57,868 --> 00:29:59,093 说话人 SPEAKER_00：感谢 Jeffrey Hinton。
402 00:29:59,634 --> 00:29:59,855 说话人 SPEAKER_00：谢谢。
403 00:30:00,417 --> 00:30:01,761 说话人 SPEAKER_00：谢谢，Jeff。
404 00：30：01,781 --> 00：30：01,962 演讲者 SPEAKER_00：太好了。
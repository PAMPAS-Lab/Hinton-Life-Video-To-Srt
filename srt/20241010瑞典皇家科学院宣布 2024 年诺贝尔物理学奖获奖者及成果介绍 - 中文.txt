1 00:00:00,031 --> 00:00:07,421 主持人 SPEAKER_09：欢迎来到本次新闻发布会以及瑞典皇家科学院，我们将在这里揭晓今年的诺贝尔物理学奖。
2 00:00:08,862 --> 00:00:19,557 主持人 SPEAKER_09：我们将继续我们的传统，先用瑞典语开始介绍，然后继续用英语，当然您也可以用这两种语言中的任何一种提问。
3 00:00:21,664 --> 00:00:23,266 主持人 SPEAKER_09：我叫汉斯·埃勒格伦。
4 00:00:23,567 --> 00:00:27,150 主持人 SPEAKER_09：我是瑞典皇家科学院的秘书长。
5 00:00:28,111 --> 00:00:34,499 讲者 SPEAKER_09：在我右边是诺贝尔物理学奖委员会主席艾伦·蒙斯教授。
6 00:00:34,518 --> 00:00:42,127 讲者 SPEAKER_09：在我左边是诺贝尔物理学奖委员会成员、该领域专家安德斯·耶德贝克教授。
7 00:00:46,490 --> 00:00:50,034 讲者 SPEAKER_09：今年的奖项是关于学习机器的。
8 00:00:54,335 --> 00:01:20,545 讲者 SPEAKER_09：瑞典皇家科学院今天决定将 2024 年诺贝尔物理学奖授予美国普林斯顿大学的约翰·霍普菲尔德教授和加拿大多伦多大学的杰弗里·辛顿教授，以表彰他们在人工神经网络机器学习方面的基础发现和发明。
9 00:01:22,634 --> 00:01:26,177 说话人 SPEAKER_09：Ellen Moons 教授将现在为我们简要介绍奖项。
10 00:01:26,899 --> 00:01:27,179 说话人 SPEAKER_09：请。
11 00:01:28,561 --> 00:01:28,921 说话人 SPEAKER_01：谢谢。
12 00:01:31,224 --> 00:01:35,349 说话人 SPEAKER_01：学习是人类大脑的迷人能力。
13 00:01:36,310 --> 00:01:42,778 说话人 SPEAKER_01：我们可以识别图像和语音，并将它们与记忆和过去的经历联系起来。
14 00:01:43,739 --> 00:01:49,266 说话人 SPEAKER_01：数十亿个神经元相互连接，赋予我们独特的认知能力。
15 00:01:50,798 --> 00:01:57,709 说话人 SPEAKER_01：人工神经网络灵感来源于我们大脑中的这个神经元网络。
16 00:01:59,332 --> 00:02:06,444 说话人 SPEAKER_01：今年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·辛顿，
17 00:02:06,677 --> 00:02:24,534 说话人 SPEAKER_01：运用统计物理的基本概念设计人工神经网络，使其作为联想记忆功能并在大型数据集中寻找模式。
18 00:02:24,514 --> 00:02:37,332 说话人 SPEAKER_01：这些人工神经网络已被用于推进物理学各个领域的研究，这些领域包括粒子物理学、材料科学和天体物理学。
19 00:02:38,574 --> 00:02:41,258 说话人 SPEAKER_01：它们也已成为我们日常生活的一部分。
20 00:02:42,259 --> 00:02:46,506 说话人 SPEAKER_01：例如，在人脸识别和语言翻译中。
21 00:02:46,957 --> 00:03:05,068 诺贝尔奖得主的发现和发明构成了机器学习的基础，这有助于人类在诊断医疗状况等情况下做出更快、更可靠的决策。
22 00:03:06,735 --> 00:03:16,889 然而，尽管机器学习具有巨大的益处，但其快速发展也引发了我们对未来的担忧。
23 00:03:18,070 --> 00:03:31,747 我们人类共同承担着责任，以安全、道德的方式使用这项新技术，为人类的最大利益服务。
24 00:03:33,668 --> 00:03:34,330 谢谢。
25 00:03:35,394 --> 00:03:38,002 说话人 SPEAKER_09：艾贝克教授，您准备好做一个更详细的演讲了吗？
26 00:03:38,947 --> 00:03:39,146 说话人 SPEAKER_09：谢谢。
27 00:03:39,168 --> 00:03:39,468 说话人 SPEAKER_09：请。
28 00:03:45,991 --> 00:03:46,092 说话人 SPEAKER_10：是的。
29 00:03:46,662 --> 00:03:52,536 说话人 SPEAKER_10：今年诺贝尔物理学奖是关于人工神经网络的。
30 00:03:53,960 --> 00:03:59,171 说话人 SPEAKER_10：如今我们知道这是一种强大的计算方法。
31 00:03:59,913 --> 00:04:04,485 说话人 SPEAKER_10：但在 50 年前这并不明显。
32 00:04:04,465 --> 00:04:17,057 说话人 SPEAKER_10：但我们已知哺乳动物通过大脑中的某种计算方式在模式识别方面非常出色。
33 00:04:19,199 --> 00:04:33,612 说话人 SPEAKER_10：这激发了对理解简化神经元网络的集体特性的兴趣
34 00:04:33,591 --> 00:04:44,228 说话人 SPEAKER_10：通过耦合连接，耦合强度可能变弱或变强。
35 00:04:44,810 --> 00:05:01,235 说话人 SPEAKER_10：那么，接下来的想法就是确定耦合的强度以实现某种功能，这可以通过在许多示例上训练网络来完成。
36 00:05:15,913 --> 00:05:38,732 说话人 SPEAKER_10：1982 年，约翰·霍普菲尔德提出了一种动态网络，该网络可以存储和检索记忆，即联想记忆，这是一个重大突破。
37 00:05:38,711 --> 00:05:45,665 说话人 SPEAKER_10：记忆具有简单的二进制 01 节点，所有节点相连，成对相连。
38 00:05:48,151 --> 00:05:56,146 说话人 SPEAKER_10：随着时间的推移保持不变的状态被识别为记忆。
39 00:05:59,468 --> 00:06:10,262 说话人 SPEAKER_10：此外，可以引入一种类似于在物理学中研究磁性系统时所具有的能量。
40 00:06:11,444 --> 00:06:18,995 说话人 SPEAKER_10：而这种能量具有的性质是在对应记忆的状态下能量较低。
41 00:06:23,093 --> 00:06:32,648 说话人 SPEAKER_10：从比喻的角度来看，记忆位于能量景观的谷地中。
42 00:06:33,927 --> 00:06:49,005 说话人 SPEAKER_10：当从一个具有更高能量的扭曲模式开始时，网络会滑向能量较低的邻近谷地。
43 00:06:49,826 --> 00:06:59,800 说话人 SPEAKER_10：通过这个过程，扭曲的模式可以被纠正。
44 00:07:04,976 --> 00:07:25,077 说话人 SPEAKER_10：在后续工作中，约翰·霍普菲尔德还展示了这个网络在某种意义上是鲁棒的，即二进制节点可以被模拟节点所替代，他还展示了如何使用这个网络来解决困难的优化问题。
45 00:07:25,057 --> 00:07:44,416 说话人 SPEAKER_10：约翰·霍普菲尔德对这一网络的创建和探索是我们理解人工神经网络计算能力的一个里程碑。
46 00:07:50,336 --> 00:08:01,355 说话人 SPEAKER_10：紧接着，杰弗里·辛顿和特伦斯·西诺斯基又做出了另一个重要发现。
47 00:08:03,098 --> 00:08:14,016 说话人 SPEAKER_10：他们基于统计物理创建了一个随机版本的霍普菲尔德网络，称之为玻尔兹曼机。
48 00:08:16,206 --> 00:08:25,076 说话人 SPEAKER_10：在这里，重点是专利的统计分布，而不是单个专利。
49 00:08:26,418 --> 00:08:29,583 说话人 SPEAKER_10：这是一个生成模型。
50 00:08:30,303 --> 00:08:37,471 说话人 SPEAKER_10：一旦训练完成，它可以用来从学习到的分布中生成新的实例。
51 00:08:37,451 --> 00:08:47,734 说话人 SPEAKER_10：它具有与 Hopfield 网络相同的基本结构，但有两种类型的节点，隐藏节点和可见节点。
52 00:08:48,355 --> 00:08:55,090 说话人 SPEAKER_10：隐藏节点是为了使网络能够学习更一般的分布。
53 00:08:57,230 --> 00:09:06,860 说话人 SPEAKER_10：从理论上讲很有趣，但在实践中，玻尔兹曼机最初用途有限。
54 00:09:06,879 --> 00:09:11,264 说话人 SPEAKER_10：它在计算上要求过高，难以承受。
55 00:09:12,144 --> 00:09:26,460 说话人 SPEAKER_10：然而，一种耦合较少的版本，即受限玻尔兹曼机，发展成为一种多用途的工具，我很快还会提到它。
56 00:09:31,333 --> 00:09:37,701 说话人 SPEAKER_10：到目前为止，我谈到了具有反馈连接的循环网络。
57 00:09:38,402 --> 00:09:52,278 说话人 SPEAKER_10：许多当今的深度学习方法都涉及前馈网络，其中信息从输入层通过隐藏层流向输出层。
58 00:09:52,259 --> 00:10:08,486 说话人 SPEAKER_10：在 20 世纪 80 年代，Hinton 展示了如何训练具有隐藏层的这种网络。
59 00:10:08,868 --> 00:10:16,782 说话人 SPEAKER_10：在这个过程中，他还阐明了隐藏层的重要功能。
60 00:10:17,605 --> 00:10:31,609 说话人 SPEAKER_10：然后在 20 世纪 90 年代，多层网络得到了应用，例如在手写数字分类方面的成功应用。
61 00:10:32,811 --> 00:10:42,346 说话人 SPEAKER_10：但是可以训练的网络在连续层之间的耦合相对较少。
62 00:10:44,115 --> 00:10:54,177 说话人 SPEAKER_10：训练具有层间高连接性的更通用深度结构仍然是一个挑战。
63 00:10:56,283 --> 00:11:01,371 说话人 SPEAKER_10：在这里，实际上曼尼放弃了，但辛顿没有。
64 00:11:02,113 --> 00:11:09,164 说话人 SPEAKER_10：而辛顿通过使用这种受限玻尔兹曼机克服了这个障碍。
65 00：11：09,184 --> 00：11：17,919 演讲者 SPEAKER_10：他用它来预训练深层结构，通过这种方法，
66 00：11：17,899 --> 00：11：29,697 演讲者 SPEAKER_10：他成功地实现了深层和密集结构的示例，这是深度学习的突破。
最后，我已经谈到了物理学，物理学是如何成为人工神经网络创新和发展的推动力的。
68 00：11：55,961 --> 00：12：06,856 演讲者 SPEAKER_10：看到物理学作为一个研究领域如何从这些方法中受益，这也很有趣。
69 00:12:06,836 --> 00:12:18,259 说话人 SPEAKER_10：这里的一个例子，一个成熟的例子，是粒子物理和天体物理学中的数据分析。
70 00:12:19,400 --> 00:12:29,200 说话人 SPEAKER_10：越来越重要的应用是在材料建模中。
71 00:12:29,179 --> 00:12:37,130 说话人 SPEAKER_10：例如，为了寻找新型、更高效的太阳能电池。
72 00:12:38,932 --> 00:12:49,246 说话人 SPEAKER_10：另一个例子是用于实现更高分辨率的基于物理的气候建模。
73 00:12:51,927 --> 00:13:07,321 说话人 SPEAKER_10：最后，我想提及两个物理学领域外的成功应用，即蛋白质结构预测和医学图像分析。
74 00:13:10,488 --> 00:13:11,590 说话人 SPEAKER_10：感谢大家的关注。
75 00:13:13,020 --> 00:13:14,322 说话人 SPEAKER_09：谢谢，艾尔贝克教授。
76 00:13:15,464 --> 00:13:22,091 说话人 SPEAKER_09：我觉得我们可能电话里有约翰·霍普菲尔德或杰夫·辛顿。
77 00:13:24,375 --> 00:13:25,716 说话人 SPEAKER_09：早上好，Hinton 教授。
78 00:13:27,578 --> 00:13:28,078 说话人 SPEAKER_09：早上好。
79 00:13:28,279 --> 00:13:32,485 说话人 SPEAKER_09：请接受我们对您获得诺贝尔物理学奖最热烈的祝贺。
80 00:13:34,847 --> 00:13:35,589 说话人 SPEAKER_02：非常感谢。
81 00:13:36,429 --> 00:13:37,610 说话人 SPEAKER_09：你现在感觉怎么样？
82 00:13:40,087 --> 00:13:41,649 说话人 SPEAKER_02：我惊呆了。
83 00:13:41,870 --> 00:13:44,591 说话人 SPEAKER_02：我完全没有想到会发生这样的事情。
84 00:13:44,611 --> 00:13:45,673 说话人 SPEAKER_02：我非常惊讶。
85 00:13:48,235 --> 00:13:48,836 说话人 SPEAKER_09：我可以想象。
86 00:13:49,677 --> 00:13:56,263 说话人 SPEAKER_09：我正坐在这里，在瑞典皇家科学院这个美丽的会议大厅里，参加新闻发布会。
87 00:13:56,543 --> 00:14:01,488 说话人 SPEAKER_09：这里有许多来自瑞典和国际媒体的感兴趣的新闻记者。
88 00:14:02,389 --> 00:14:04,831 说话人 SPEAKER_09：您准备好回答他们的问题了吗？
89 00:14:06,692 --> 00:14:06,972 说话人 SPEAKER_09: 是的。
90 00:14:08,826 --> 00:14:10,369 说话人 SPEAKER_09: 是的，请。
91 00:14:10,389 --> 00:14:11,230 说话人 SPEAKER_09: 瑞典电视台。
92 00:14:12,110 --> 00:14:12,552 说话人 SPEAKER_04: 谢谢。
93 00：14：12,692 --> 00：14：18,779 演讲者 SPEAKER_04： 我最热烈地祝贺你们的成就和今年的诺贝尔物理学奖。
94 00:14:18,840 --> 00:14:25,008 说话者 SPEAKER_04：我叫苏珊·里岑，我的问题来自瑞典电视。
我知道我们许多观众，包括非专业人士，都对今天这里获得的发现非常好奇。
我很好奇，你记得你意识到今天获得突破的时刻吗？如果你能带我们回到那个时刻，简要地介绍一下，这些发现的原因或灵感是什么？
97 00:14:48,601 --> 00:14:55,352 说话人 SPEAKER_02：我记得和我的两位导师有过几次经历。
98 00:14:56,053 --> 00:15:00,179 说话人 SPEAKER_02：我对大卫·罗梅尔哈特和特里·桑诺夫斯基有着巨大的债务。
99 00:15:01,038 --> 00:15:12,230 说话人 SPEAKER_02：和大卫·雷梅尔哈特一起，我们在 1982 年初重新发现了反向传播算法。
100 00:15:12,530 --> 00:15:22,863 说话人 SPEAKER_02：和特里·辛诺夫斯基一起，我和特里发现了一种具有隐藏单元的 Hopfield 网络学习算法。
101 00:15:24,065 --> 00:15:29,390 说话人 SPEAKER_02：我记得很清楚，我去罗切斯特参加了一个会议
102 00:15:29,945 --> 00:15:31,826 说话人 SPEAKER_02：在那里约翰·霍普菲尔德发表了演讲。
103 00:15:32,788 --> 00:15:40,856 说话人 SPEAKER_02：我第一次了解到神经网络中的霍普菲尔德能量函数。
104 00:15:41,857 --> 00:15:50,846 说话人 SPEAKER_02：在那之后，我和特里一起之前研究过如何将神经网络推广到具有隐藏单元。
105 00:15:52,089 --> 00:15:58,956 说话人 SPEAKER_02：在 1982 年初，我们提出了一种学习算法。
106 00:15:59,812 --> 00:16:04,361 说话人 SPEAKER_02：针对巴尔的摩机器，它们是具有隐藏单元的热场网络。
107 00:16:06,145 --> 00:16:13,299 说话人 SPEAKER_02：最激动人心的时光是与大卫·拉梅尔哈特一起研究反向传播，以及与特里·西诺夫斯基一起研究巴尔的摩机器。
108 00:16:16,125 --> 00:16:16,767 说话人 SPEAKER_09：谢谢。
109 00:16:16,787 --> 00:16:18,269 说话人 SPEAKER_09：好的，更多问题。
110 00:16:21,176 --> 00:16:21,736 说话人 SPEAKER_09：首先在这里。
111 00:16:23,471 --> 00:16:26,375 说话人 SPEAKER_00：你好，波兰电视的博古米尔·拉达耶夫斯基。
112 00:16:26,495 --> 00:16:27,177 说话人 SPEAKER_00：恭喜你。
113 00:16:27,738 --> 00:16:36,450 说话人 SPEAKER_00：我提出的问题有一点关于未来，因为很明显，我们对神经网络和机器学习现在能做什么感到非常兴奋。
114 00:16:37,331 --> 00:16:42,979 说话人 SPEAKER_00：但我们更加兴奋的是它们未来可能做到的事情。
115 00:16:43,279 --> 00:16:52,972 说话人 SPEAKER_00：您对这项技术将对我们的文明产生多大影响有何预测？
116 00:16:54,774 --> 00:16:56,677 说话人 SPEAKER_02：我认为它将产生巨大的影响。
117 00:16:56,697 --> 00:16:59,038 说话人 SPEAKER_02：它将与工业革命相媲美。
118 00:16:59,820 --> 00:17:06,385 说话人 SPEAKER_02：但这次不是超越人的体力，而是超越人的智力。
119 00:17:07,666 --> 00:17:11,790 说话人 SPEAKER_02：我们没有经历过事物比我们更智能的感觉。
120 00:17:12,852 --> 00:17:16,556 说话人 SPEAKER_02：在许多方面都将是非常美好的。
121 00:17:16,816 --> 00:17:22,281 说话人 SPEAKER_02：在医疗保健等领域的应用将使我们的医疗保健在几乎所有方面都得到大幅提升。
122 00:17:22,301 --> 00:17:23,803 说话人 SPEAKER_02：这将使它们更加高效。
123 00:17:24,458 --> 00:17:29,924 说话人 SPEAKER_02：人们将能够在更短的时间内使用人工智能助手完成相同的工作量。
124 00:17:29,984 --> 00:17:45,124 说话人 SPEAKER_02：这将意味着生产力的巨大提升，但我们也必须担心许多可能的负面后果，尤其是这些事物失控的威胁。
125 00:17:45,144 --> 00:17:50,230 说话人 SPEAKER_09：我认为首先我们有这里，然后有那里。
126 00:17:52,521 --> 00:17:55,828 说话人 SPEAKER_07：嗨，西蒙·卡帕内洛，来自《今日新闻》。
127 00:17:55,909 --> 00:17:57,070 说话人 SPEAKER_07：恭喜你获奖。
128 00:17:58,253 --> 00:18:11,400 说话人 SPEAKER_07：我的问题是，去年你在《纽约时报》的一次采访中说，由于人工智能的风险，你后悔你人生的一部分工作。
129 00:18:12,000 --> 00:18:13,904 说话人 SPEAKER_07：你对今天的感受如何？
130 00:18:16,096 --> 00:18:42,930 说话人 SPEAKER_02：嗯，有两种遗憾，一种是后悔自己做了明知不应该做的事情，另一种是后悔在相同情况下你会再次做同样的事情，但最终可能结果并不好。嗯，第二种遗憾，在相同情况下我会再次做同样的事情，但我担心这种做法的总体后果可能是……
131 00:18:43,518 --> 00:18:46,422 说话人 SPEAKER_02：比我们更智能的系统最终会接管控制权。
132 00:18:49,946 --> 00:18:50,406 说话人 SPEAKER_08：是的，请。
133 00:18:55,433 --> 00:18:57,155 说话人 SPEAKER_03：嗨，恭喜你获奖。
134 00:18:57,215 --> 00:19:05,166 说话人 SPEAKER_03：我叫安娜-路易丝·梅伊格纳-阿恩，来自瑞典 TV4 电视台。
135 00:19:05,186 --> 00:19:12,276 说话人 SPEAKER_03：我想知道，您开发了这种玻尔兹曼机，
136 00:19:13,218 --> 00:19:39,359 说话人 SPEAKER_03：我想知道，从这个机器中产生了什么样的 AI，比如 GPT，或者您是如何在 X 光片中看到乳腺癌的，或者您是如何制作有趣的
137 00:19:39,593 --> 00:19:51,865 说话人 SPEAKER_03：在达利的图片中，你研究的是哪种人工智能，是否基于你的研究进行构建？
138 00:19:51,885 --> 00:19:55,190 说话人 SPEAKER_02：我参与了两种不同的学习算法。
139 00:19:55,789 --> 00:20:00,355 说话人 SPEAKER_02：一个是玻尔兹曼机，这是一种具有隐藏单元的 Hopfield 网络的学习算法。
140 00:20:01,455 --> 00:20:07,643 说话人 SPEAKER_02：我们最终找到了那个算法的一个实用版本，但那并不是导致
141 00:20:07,807 --> 00:20:10,330 说话人 SPEAKER_02：目前神经网络的主要进展。
142 00:20:10,691 --> 00:20:13,574 说话人 SPEAKER_02：这就是反向传播算法。
143 00:20:14,634 --> 00:20:20,701 说话人 SPEAKER_02：这是让神经网络学习任何事物的方法。
144 00:20:21,702 --> 00:20:34,355 说话人 SPEAKER_02：正是反向传播算法导致了人工智能应用的巨大增长，以及识别图像、理解语音和处理自然语言的能力。
145 00:20:35,737 --> 00:20:41,863 说话人 SPEAKER_02：这并不是玻尔兹曼机做的，而是反向传播算法。
146 00:20:41,883 --> 00:20:42,903 说话人 SPEAKER_09：还有其他问题吗？
147 00:20:47,167 --> 00:20:50,891 说话人 SPEAKER_06：是的，这里有一个，请。
148 00:20:50,911 --> 00:20:52,113 说话人 SPEAKER_06：嗨，我叫比尔。
149 00:20:52,452 --> 00:20:54,714 说话人 SPEAKER_06：我来自瑞典报纸《Nyteknik》。
150 00:20:56,016 --> 00:20:58,519 说话人 SPEAKER_06：你有没有喜欢的 AI 工具在使用？
151 00:21:02,481 --> 00:21:05,305 说话人 SPEAKER_02：我实际上使用 GPT-4 很多。
152 00:21:05,757 --> 00:21:10,305 说话人 SPEAKER_02：每当我想知道任何问题的答案时，我就会去问 GPT-4。
153 00:21:11,165 --> 00:21:21,002 说话人 SPEAKER_02：我不完全信任它，因为它可能会产生幻觉，但几乎在所有事情上，它都不是一个很好的专家，这非常有用。
154 00:21:25,130 --> 00:21:26,230 说话人 SPEAKER_09：好的。
155 00:21:27,653 --> 00:21:29,135 说话人 SPEAKER_09：我没有看到更多了。
156 00:21:29,576 --> 00:21:31,359 说话人 SPEAKER_09：请再给我看一眼后面的那只手，好吗？
157 00:21:33,954 --> 00:21:37,659 说话人 SPEAKER_05：是的，你好，祝贺你，保罗·里斯来自半岛电视台英语频道。
158 00:21:38,760 --> 00:21:43,125 说话人 SPEAKER_05：你能给我们描述一下接到电话时的情景吗？这对你有什么影响？
159 00:21:43,165 --> 00:21:47,991 说话人 SPEAKER_05：这是你日程表上预定好的一个日子以防接到那个电话吗？还是说这是一个晴天霹雳？
160 00:21:50,255 --> 00:21:51,435 说话人 SPEAKER_02：这是一个晴天霹雳。
161 00:21:52,076 --> 00:22:00,586 说话人 SPEAKER_02：我在加利福尼亚的一家没有网络连接且电话信号不好的廉价酒店。
162 00:22:01,765 --> 00:22:06,070 说话人 SPEAKER_02：我本来今天要去做一个核磁共振扫描，但现在我觉得我可能不得不取消了。
163 00:22:10,738 --> 00:22:11,058 说话人 SPEAKER_09：好的。
164 00:22:11,078 --> 00:22:14,983 说话人 SPEAKER_09：这可能是记者们对你的最后一个问题了，Hinton 教授。
165 00:22:17,407 --> 00:22:17,929 说话人 SPEAKER_09: 谢谢。
166 00:22:18,189 --> 00:22:18,569 说话人 SPEAKER_09: 谢谢。
167 00:22:18,609 --> 00:22:21,134 说话人 SPEAKER_09: 再次，我们最热烈的祝贺。
168 00:22:21,193 --> 00:22:27,142 说话人 SPEAKER_09: 我们期待在 12 月的斯德哥尔摩诺贝尔奖典礼上见到您。
169 00:22:29,224 --> 00:22:29,645 说话人 SPEAKER_09: 谢谢。
170 00:22:32,106 --> 00:22:32,567 说话人 SPEAKER_09: 好的。
171 00:22:35,010 --> 00:22:44,986 说话人 SPEAKER_09: 那么，让我们继续讨论有关物理学奖和涉及的研究的问题，或者如果您想向委员会成员提问关于他们的工作。
172 00:22:45,006 --> 00:22:48,490 说话人 SPEAKER_09: 再次提醒，欢迎用英语或瑞典语提问。
173 00:22:54,378 --> 00:22:54,839 说话人 SPEAKER_08: 请。
174 00:22:55,748 --> 00:23:04,204 说话人 SPEAKER_03: 是的，如果这两位科学家不存在的话，我们会像 GPT 一样吗？
175 00:23:07,030 --> 00:23:09,336 说话人 SPEAKER_09: 蒙斯教授，您想回应这个问题吗？
176 00:23:10,160 --> 00:23:15,269 说话人 SPEAKER_01: 这是一个很难回答的问题，因为它很难想象。
177 00：23：15,329 --> 00：23：23,866 演讲者 SPEAKER_01：当然，他们在这项技术的发展中做出了巨大的贡献。
178 00：23：24,387 --> 00：23：31,762 演讲者 SPEAKER_01：所以，在 80 年代，这些第一步已经迈出了。
后来，其他科学家在此基础上进行了发展，所以在某种程度上，没有那些开创性的首次发现和发明可能很难。
180 00:23:50,271 --> 00:24:03,498 说话者 SPEAKER_04：既然 John J. Hopfield 不在，我想了解一下什么让你最为兴奋，你认为他发现中最激动人心的部分是什么？
181 00:24:04,619 --> 00:24:05,923 说话人 SPEAKER_09：伊贝克教授，您想回应这个问题吗？
182 00:24:08,307 --> 00:24:11,232 说话人 SPEAKER_10：说到他的网络，他
183 00:24:12,682 --> 00:24:28,128 说话人 SPEAKER_10：其中一部分之前已经讨论过，但他能够将这些部分拼凑起来，创建一个具有明确功能和运作原则的网络。
184 00:24:28,148 --> 00:24:31,955 说话人 SPEAKER_10：这在领域内意义重大。
185 00:24:38,837 --> 00:24:40,480 说话人 SPEAKER_09：这里还有一个问题。
186 00:24:40,500 --> 00:24:40,961 说话人 SPEAKER_04：哦，对不起。
187 00:24:41,061 --> 00:24:41,301 说话人 SPEAKER_04：是的。
188 00:24:41,461 --> 00:24:47,632 说话人 SPEAKER_04：我也想知道 Jeffrey Hinton 所表达的担忧。
189 00：24：48,192 --> 00：24：51,679 演讲者 SPEAKER_04：你对这项技术有什么担忧？
190 00:24:53,701 --> 00:24:55,023 说话人 SPEAKER_09：莫斯教授，您想讨论什么问题？
191 00:24:56,371 --> 00:25:14,666 说话人 SPEAKER_01：这类担忧在科学界被广泛表达和讨论……我认为这是非常好的，因为它们有助于增进社会对机器学习的认识。
192 00:25:14,646 --> 00:25:26,619 说话人 SPEAKER_01：我认为非常重要，让尽可能多的人了解机器学习的机制……这样它就不会只掌握在少数人手中。
193 00:25:28,721 --> 00:25:37,310 说话人 SPEAKER_01：所以，我认为，当然，辛顿教授只是众多……表达自己观点的人之一。
194 00:25:38,373 --> 00:25:44,298 说话人 SPEAKER_01：嗯，我觉得这非常好。
195 00:25:46,067 --> 00:25:59,451 说话人 SPEAKER_09：也许我可以补充说，当然，在过去的时光里，有许多发现和发明有可能被滥用，但社会有责任制定法规来避免这种情况发生。
196 00:25:59,813 --> 00:26:03,839 说话人 SPEAKER_09：我认为这一点也适用于人工智能。
197 00:26:11,498 --> 00:26:13,963 说话人 SPEAKER_09：我想这就说完了。
198 00:26:14,003 --> 00:26:15,186 说话人 SPEAKER_09：时间正在流逝。
199 00:26:16,349 --> 00:26:20,299 说话人 SPEAKER_09：感谢您参加这次新闻发布会。
200 00:26:20,319 --> 00:26:24,912 说话人 SPEAKER_09：我们希望明天再次在这里见到您，届时我们将颁发诺贝尔化学奖。
201 00:26:25,614 --> 00:26:26,115 说话人 SPEAKER_09：谢谢。
1
00:00:16,129 --> 00:00:17,030
Speaker SPEAKER_01: Welcome everyone.

2
00:00:17,050 --> 00:00:23,638
Speaker SPEAKER_01: My name is Merrick Gertler and it is my privilege to serve as President of the University of Toronto.

3
00:00:25,080 --> 00:00:33,752
Speaker SPEAKER_01: We're gathered online from around the world to recognize and celebrate University Professor Emeritus Geoffrey Hinton, the 2024 Nobel Laureate in Physics.

4
00:00:34,973 --> 00:00:42,323
Speaker SPEAKER_01: Geoff Hinton is internationally recognized and admired as a pioneer in the field of artificial neural networks and deep learning.

5
00:00:42,909 --> 00:00:54,841
Speaker SPEAKER_01: His multidisciplinary research program is linked not only to AI and machine learning, but also to physics, cognitive psychology, neurobiology, mathematical optimization, and information theory.

6
00:00:56,164 --> 00:01:00,027
Speaker SPEAKER_01: Professor Hinton has had a profound impact on multiple fields and disciplines.

7
00:01:00,927 --> 00:01:04,591
Speaker SPEAKER_01: He has literally created new ways of thinking about thinking and learning.

8
00:01:06,013 --> 00:01:10,317
Speaker SPEAKER_01: The algorithms he and his students have developed have an astonishing reach.

9
00:01:10,787 --> 00:01:18,641
Speaker SPEAKER_01: They underpinned the data processing and advanced discovery capabilities that are now in astonishingly wide use today.

10
00:01:18,701 --> 00:01:34,989
Speaker SPEAKER_01: Professor Hinton's foundational contributions and their profound impact have led to widespread scholarly acclaim and even wider popular recognition around the globe, so much so that he is frequently referred to as the godfather of AI.

11
00:01:36,742 --> 00:01:46,706
Speaker SPEAKER_01: Professor Hinton has spent some three decades of his stellar academic career at the University of Toronto, where he was named University Professor, our highest academic honour, in 2006.

12
00:01:46,766 --> 00:01:53,180
Speaker SPEAKER_01: Due in large part to his leadership and exemplary mentorship of young scholars,

13
00:01:53,447 --> 00:01:57,114
Speaker SPEAKER_01: U of T has developed into a global leader in machine learning and AI.

14
00:01:57,894 --> 00:02:09,633
Speaker SPEAKER_01: This includes the ethical consequences and social impacts of AI, a topic that Professor Hinton has recently elevated, helping focus the world's attention on these important issues.

15
00:02:10,875 --> 00:02:20,209
Speaker SPEAKER_01: It is my honor and great pleasure to introduce University Professor Emeritus Geoffrey Hinton, 2024 Nobel Laureate in Physics.

16
00:02:20,870 --> 00:02:22,391
Speaker SPEAKER_01: Professor Hinton, welcome.

17
00:02:23,451 --> 00:02:24,173
Speaker SPEAKER_02: Thank you very much.

18
00:02:24,932 --> 00:02:26,455
Speaker SPEAKER_02: I'm still slightly in shock.

19
00:02:26,794 --> 00:02:29,698
Speaker SPEAKER_02: I got a phone call at one o'clock in the morning in California.

20
00:02:30,557 --> 00:02:33,721
Speaker SPEAKER_02: I thought about whether I should answer it or not.

21
00:02:33,760 --> 00:02:36,183
Speaker SPEAKER_02: And luckily I decided I would see who was calling.

22
00:02:36,903 --> 00:02:41,367
Speaker SPEAKER_02: And I was extremely surprised to get the Nobel Prize in physics.

23
00:02:41,868 --> 00:02:43,129
Speaker SPEAKER_02: I never expected that.

24
00:02:44,230 --> 00:02:53,239
Speaker SPEAKER_02: I think of the prize as a recognition of a large community of people who worked on neural networks for many years before they worked really well.

25
00:02:53,388 --> 00:03:00,956
Speaker SPEAKER_02: I'd particularly like to acknowledge my two main mentors, David Rumelhart, with whom I worked on the back propagation algorithm.

26
00:03:01,758 --> 00:03:07,044
Speaker SPEAKER_02: David died of a nasty brain disease quite young, but for that, he would be here instead of me.

27
00:03:07,084 --> 00:03:17,556
Speaker SPEAKER_02: And my colleague, Terry Sanofsky, who I worked with a lot in the 1980s on Boltzmann machines, and who taught me a lot about the brain.

28
00:03:17,872 --> 00:03:20,957
Speaker SPEAKER_02: I'd also like to acknowledge my students.

29
00:03:21,177 --> 00:03:28,991
Speaker SPEAKER_02: I was particularly fortunate to have many very clever students, much cleverer than me, who actually made things work.

30
00:03:30,552 --> 00:03:32,796
Speaker SPEAKER_02: They've gone on to do great things.

31
00:03:33,437 --> 00:03:36,723
Speaker SPEAKER_02: I'm particularly proud of the fact that one of my students fired Sam Altman.

32
00:03:37,604 --> 00:03:43,014
Speaker SPEAKER_02: And I think I better leave it there and leave it for questions.

33
00:03:43,033 --> 00:03:44,556
Speaker SPEAKER_01: Thank you so much, Geoff.

34
00:03:45,193 --> 00:03:52,883
Speaker SPEAKER_01: We will now take questions from members of the media, and I invite my colleague, Lisa Pires, from U of T's media relations team to moderate our Q&A.

35
00:03:53,084 --> 00:03:53,425
Speaker SPEAKER_01: Lisa.

36
00:03:56,147 --> 00:03:57,590
Speaker SPEAKER_00: Thank you, President Gerler.

37
00:03:57,610 --> 00:04:04,919
Speaker SPEAKER_00: To ensure we can answer as many questions as possible, we will be taking written questions only.

38
00:04:04,960 --> 00:04:15,133
Speaker SPEAKER_00: So please include your name and the news outlet you are affiliated with when submitting your questions using the Q&A box you'll see at the bottom of your screen.

39
00:04:15,872 --> 00:04:18,798
Speaker SPEAKER_00: We'll take a minute now to let the questions come in.

40
00:04:25,189 --> 00:04:26,110
Speaker SPEAKER_02: Well, we're taking a minute.

41
00:04:26,151 --> 00:04:36,327
Speaker SPEAKER_02: I'd like to say I should also acknowledge Joshua Bengio and Yann LeCun, who were close colleagues and were very instrumental in developing this whole field.

42
00:04:46,112 --> 00:04:52,721
Speaker SPEAKER_00: We see a question in our Q&A box from Adrian at CTV National News.

43
00:04:52,822 --> 00:04:58,288
Speaker SPEAKER_00: Adrian, if you wouldn't mind typing in your question, that would help us answer it.

44
00:05:05,939 --> 00:05:07,120
Speaker SPEAKER_00: Thank you so much, Adrian.

45
00:05:07,221 --> 00:05:10,225
Speaker SPEAKER_00: This is a question for Dr. Hinton.

46
00:05:10,644 --> 00:05:15,732
Speaker SPEAKER_00: Dr. Hinton, what do you believe your legacy will be when it comes to AI?

47
00:05:17,331 --> 00:05:26,463
Speaker SPEAKER_02: I'm hoping AI will lead to tremendous benefits, to tremendous increases in productivity, and to a better life for everybody.

48
00:05:26,845 --> 00:05:29,668
Speaker SPEAKER_02: I'm convinced that it will do that in healthcare.

49
00:05:30,589 --> 00:05:35,916
Speaker SPEAKER_02: My worry is that it may also lead to bad things.

50
00:05:36,677 --> 00:05:45,870
Speaker SPEAKER_02: And in particular, when we get things more intelligent than ourselves, no one really knows whether we're going to be able to control them.

51
00:05:49,107 --> 00:05:53,454
Speaker SPEAKER_00: Our next question comes from Victoria Gibson at Toronto Star.

52
00:05:54,336 --> 00:05:56,581
Speaker SPEAKER_00: This is another one for Dr. Hinton.

53
00:05:56,862 --> 00:06:02,331
Speaker SPEAKER_00: She asks, how would you use a neural network right now to improve Toronto as a city?

54
00:06:07,923 --> 00:06:11,149
Speaker SPEAKER_02: I'm not quite sure how a neural network could get rid of Doug Ford.

55
00:06:15,735 --> 00:06:19,779
Speaker SPEAKER_00: Okay, I've got a follow-up from Victoria again at Toronto Star.

56
00:06:20,220 --> 00:06:27,726
Speaker SPEAKER_00: She asks again of Dr. Hinton, how is the Canadian research landscape different now from when you were starting out?

57
00:06:28,208 --> 00:06:32,932
Speaker SPEAKER_00: And what is the biggest hurdle to reaching more research breakthroughs in Canada today?

58
00:06:36,995 --> 00:06:38,076
Speaker SPEAKER_02: Let me just think about that.

59
00:06:39,437 --> 00:06:44,442
Speaker SPEAKER_02: Obviously a big difference is people now recognize that neural networks actually work.

60
00:06:45,165 --> 00:06:47,170
Speaker SPEAKER_02: but much of the landscape is similar.

61
00:06:47,209 --> 00:06:56,007
Speaker SPEAKER_02: There's an organization called the Canadian Institute for Advanced Research that is a big help for people doing research in areas where Canada is strong.

62
00:06:57,048 --> 00:07:06,245
Speaker SPEAKER_02: I think the main thing about Canada as a place to do research is there isn't as much money as there is in the US.

63
00:07:06,665 --> 00:07:09,267
Speaker SPEAKER_02: but it uses its money quite wisely.

64
00:07:09,367 --> 00:07:34,495
Speaker SPEAKER_02: In particular, the main funding council for this kind of research, called NSERC, uses money for basic curiosity-driven research, and all of these advanced neural networks came out of basic curiosity-driven research, not out of throwing money at applied problems, but out of letting scientists follow their curiosity to try and understand things.

65
00:07:35,774 --> 00:07:37,156
Speaker SPEAKER_02: And Canada is quite good at that.

66
00:07:40,500 --> 00:07:40,860
Speaker SPEAKER_00: Thank you.

67
00:07:40,922 --> 00:07:45,908
Speaker SPEAKER_00: Our next question comes from Issam Ahmed at Agence France-Presse.

68
00:07:46,550 --> 00:07:48,031
Speaker SPEAKER_00: They say, congratulations.

69
00:07:48,413 --> 00:07:56,985
Speaker SPEAKER_00: Both you and Dr. Hopefield have warned of the dangers of unchecked AI and not understanding enough about how it now works.

70
00:07:57,545 --> 00:08:00,329
Speaker SPEAKER_00: How do we avoid catastrophic scenarios?

71
00:08:01,524 --> 00:08:04,009
Speaker SPEAKER_02: We don't know how to avoid them all at present.

72
00:08:04,028 --> 00:08:06,050
Speaker SPEAKER_02: That's why we urgently need more research.

73
00:08:07,553 --> 00:08:24,057
Speaker SPEAKER_02: So I'm advocating that our best young researchers, or many of them, should work on AI safety, and governments should force the large companies to provide the computational facilities that they need to do that.

74
00:08:26,786 --> 00:08:32,634
Speaker SPEAKER_00: Our next one is from Tara Deschamps from Canadian Press, again for Professor Hinton.

75
00:08:33,296 --> 00:08:40,225
Speaker SPEAKER_00: She asks, for a long time, AI was not seen as a sexy or popular technology like it is today.

76
00:08:40,264 --> 00:08:47,715
Speaker SPEAKER_00: I wonder if you could share a bit of what it was like to work on the underpinnings of the technology before it was so ubiquitous.

77
00:08:49,214 --> 00:09:00,885
Speaker SPEAKER_02: It was a lot of fun doing the research, but it was slightly annoying that many people said, in fact, most people in the field of AI said that neural networks would never work.

78
00:09:01,366 --> 00:09:14,039
Speaker SPEAKER_02: They were very confident that these things were just a waste of time and we would never be able to learn complicated things like, for example, understanding natural language using neural networks, and they were wrong.

79
00:09:16,940 --> 00:09:20,989
Speaker SPEAKER_00: The next question comes from Adrian Gobriel at CTV News.

80
00:09:21,009 --> 00:09:22,152
Speaker SPEAKER_00: This is his second question.

81
00:09:22,833 --> 00:09:28,004
Speaker SPEAKER_00: He asks of Dr. Hinton, can you elaborate on your concerns around AI?

82
00:09:28,066 --> 00:09:31,533
Speaker SPEAKER_00: Do you believe it might become more intelligent than humans?

83
00:09:32,195 --> 00:09:35,623
Speaker SPEAKER_00: Why and how quickly do you believe that could take place?

84
00:09:36,532 --> 00:09:45,277
Speaker SPEAKER_02: Okay, so most of the top researchers I know believe that AI will become more intelligent than people.

85
00:09:46,100 --> 00:09:48,988
Speaker SPEAKER_02: They vary on the time scales.

86
00:09:49,947 --> 00:09:53,052
Speaker SPEAKER_02: A lot of them believe that that will happen sometime in the next 20 years.

87
00:09:53,493 --> 00:09:55,235
Speaker SPEAKER_02: Some of them believe it will happen sooner.

88
00:09:55,836 --> 00:09:57,519
Speaker SPEAKER_02: Some of them believe it will take much longer.

89
00:09:58,041 --> 00:10:04,191
Speaker SPEAKER_02: But quite a few good researchers believe that sometime in the next 20 years, AI will become more intelligent than us.

90
00:10:05,192 --> 00:10:07,736
Speaker SPEAKER_02: And we need to think hard about what happens then.

91
00:10:13,116 --> 00:10:18,605
Speaker SPEAKER_00: We don't have the name of the next person who has asked a question, but they do have a fun one.

92
00:10:19,066 --> 00:10:22,972
Speaker SPEAKER_00: Who was your first call when you discovered you'd won the Nobel Prize?

93
00:10:24,674 --> 00:10:25,976
Speaker SPEAKER_02: My sister in Australia.

94
00:10:30,544 --> 00:10:32,488
Speaker SPEAKER_00: Follow up to that, what was the reaction?

95
00:10:32,508 --> 00:10:37,655
Speaker SPEAKER_02: I think she said something like, oh, my God.

96
00:10:41,585 --> 00:10:44,751
Speaker SPEAKER_00: Our next one is a follow-up question from Tara Deschamps.

97
00:10:44,772 --> 00:10:53,513
Speaker SPEAKER_00: Again, she's from Canadian Press, and she asks of Professor Hinton, you initially mentioned being flabbergasted when you got the news of the Nobel win this morning.

98
00:10:54,014 --> 00:10:56,961
Speaker SPEAKER_00: Can you tell us a bit about how your day has gone since?

99
00:10:58,020 --> 00:11:01,927
Speaker SPEAKER_02: Yes, I have very little sleep.

100
00:11:02,027 --> 00:11:06,953
Speaker SPEAKER_02: It was one o'clock in the morning and I'd probably had about an hour's sleep by the time the phone went.

101
00:11:07,875 --> 00:11:13,162
Speaker SPEAKER_02: I'm in California and since then I've probably had about one more hour's sleep.

102
00:11:14,244 --> 00:11:16,366
Speaker SPEAKER_02: So I'm rather sleep deprived now.

103
00:11:16,346 --> 00:11:29,309
Speaker SPEAKER_02: and it's just been lots and lots of people trying to get in touch with me, but also lots of messages from old friends from years ago I haven't seen in a long time and that's been very nice.

104
00:11:33,003 --> 00:11:38,975
Speaker SPEAKER_00: Our next question comes from Isabel Kirkwood at BetaKit, again for Professor Hinton.

105
00:11:39,596 --> 00:11:52,243
Speaker SPEAKER_00: She asks, Professor Hinton, how do you reconcile receiving this recognition with your outspokenness about the need to slow AI advancement and the risks that technology poses?

106
00:11:53,219 --> 00:11:57,926
Speaker SPEAKER_02: I've never recommended slowing the advancement of AI because I don't think that's feasible.

107
00:11:58,726 --> 00:12:10,363
Speaker SPEAKER_02: AI has so many good effects, like in healthcare, but in pretty much all industries, that I think there's no chance of us slowing the development of it.

108
00:12:15,450 --> 00:12:17,352
Speaker SPEAKER_02: Can you say the second half of the question again?

109
00:12:17,552 --> 00:12:18,455
Speaker SPEAKER_00: Absolutely.

110
00:12:18,475 --> 00:12:29,638
Speaker SPEAKER_00: She asks, how do you reconcile receiving this recognition with your outspokenness about the need to slow AI advancement and the risks that technology poses?

111
00:12:30,664 --> 00:12:38,437
Speaker SPEAKER_02: Okay, so actually the Nobel Committee recognized that my work on talking about safety was relevant here.

112
00:12:39,139 --> 00:12:42,024
Speaker SPEAKER_02: I can't remember exactly what they said, but they said something about that.

113
00:12:42,865 --> 00:12:49,837
Speaker SPEAKER_02: I think we need a serious effort to make sure it's safe, because if we can keep it safe, it'll be wonderful.

114
00:12:52,619 --> 00:13:12,028
Speaker SPEAKER_00: We have a follow-up again from Professor Hinton and again from Issam Ahmed from AFP who asks, do you think students and even professionals over-relying on LLMs is going to have a dumbing down effect or will we operate on a higher order?

115
00:13:13,105 --> 00:13:15,849
Speaker SPEAKER_02: I don't think it will have a significant dumbing down effect.

116
00:13:16,169 --> 00:13:22,479
Speaker SPEAKER_02: I think it'll be like what happened when they first had pocket calculators and people said, oh, kids won't learn math anymore.

117
00:13:22,519 --> 00:13:24,221
Speaker SPEAKER_02: They won't be able to do multiplication.

118
00:13:24,863 --> 00:13:28,388
Speaker SPEAKER_02: Well, you don't need to be able to do multiplication if you've got a pocket calculator.

119
00:13:29,210 --> 00:13:31,393
Speaker SPEAKER_02: And I think it'll be the same with LLMs.

120
00:13:31,774 --> 00:13:37,602
Speaker SPEAKER_02: People maybe won't remember as many facts that you can just ask an LLM and it will know.

121
00:13:37,643 --> 00:13:40,547
Speaker SPEAKER_02: But I think it'll make people smarter, not dumber.

122
00:13:42,568 --> 00:13:43,129
Speaker SPEAKER_00: Thank you.

123
00:13:43,149 --> 00:13:47,460
Speaker SPEAKER_00: A follow-up from Adrian Gobriel at CTV News.

124
00:13:48,160 --> 00:13:51,047
Speaker SPEAKER_00: He asks, if I could ask one more question.

125
00:13:51,086 --> 00:13:55,998
Speaker SPEAKER_00: He says, you used the word flabbergasted when you learned about the award.

126
00:13:56,479 --> 00:13:58,283
Speaker SPEAKER_00: Why were you so surprised?

127
00:13:59,443 --> 00:14:02,429
Speaker SPEAKER_02: I had absolutely no idea that I'd even been nominated.

128
00:14:02,450 --> 00:14:05,174
Speaker SPEAKER_02: I'm not a physicist.

129
00:14:05,576 --> 00:14:07,278
Speaker SPEAKER_02: I have very high respect for physics.

130
00:14:07,759 --> 00:14:14,854
Speaker SPEAKER_02: I dropped out of physics after my first year at university because I couldn't do the complicated math.

131
00:14:14,833 --> 00:14:19,061
Speaker SPEAKER_02: Getting an award in physics was very surprising to me.

132
00:14:19,402 --> 00:14:29,799
Speaker SPEAKER_02: I'm very pleased that the Nobel Committee recognized that there's been huge progress in the area of artificial neural nets.

133
00:14:30,280 --> 00:14:31,543
Speaker SPEAKER_02: Hopfield's work

134
00:14:31,523 --> 00:14:33,125
Speaker SPEAKER_02: was closely related to physics.

135
00:14:33,205 --> 00:14:38,913
Speaker SPEAKER_02: And some of the early work I did with Terry Sanofsky on Boltzmann machines was inspired by statistical physics.

136
00:14:39,455 --> 00:14:43,480
Speaker SPEAKER_02: But more recently, the work has had less relationship to physics.

137
00:14:43,760 --> 00:14:49,048
Speaker SPEAKER_02: And so I was very surprised that I got a prize in physics.

138
00:14:49,068 --> 00:15:00,905
Speaker SPEAKER_00: Our next question comes from Matt O'Brien at Associated Press, who asks of Professor Hinton, can you please elaborate on your comment earlier on the call about Sam Altman?

139
00:15:02,219 --> 00:15:07,048
Speaker SPEAKER_02: So OpenAI was set up with a big emphasis on safety.

140
00:15:08,571 --> 00:15:13,500
Speaker SPEAKER_02: Its primary objective was to develop artificial general intelligence and ensure that it was safe.

141
00:15:15,264 --> 00:15:20,835
Speaker SPEAKER_02: One of my former students, Ilya Sutskova, was the chief scientist.

142
00:15:20,883 --> 00:15:30,751
Speaker SPEAKER_02: And over time, it turned out that Sam Altman was much less concerned with safety than with profits.

143
00:15:30,772 --> 00:15:33,659
Speaker SPEAKER_02: And I think that's unfortunate.

144
00:15:37,217 --> 00:15:37,678
Speaker SPEAKER_00: Thank you.

145
00:15:37,719 --> 00:15:41,364
Speaker SPEAKER_00: Our next question comes from Jessica Coates at PA Media.

146
00:15:41,384 --> 00:15:44,427
Speaker SPEAKER_00: And this is, again, a question for Professor Hinton.

147
00:15:45,048 --> 00:15:53,539
Speaker SPEAKER_00: She asks, you mentioned the uncertain future around AI and the need for greater understanding of its potential opportunities and risks.

148
00:15:54,201 --> 00:15:58,626
Speaker SPEAKER_00: Do you believe governments look at stepping in to regulate AI more strictly?

149
00:15:58,647 --> 00:16:02,111
Speaker SPEAKER_00: How can governments better support AI research?

150
00:16:03,356 --> 00:16:10,205
Speaker SPEAKER_02: I think governments can encourage the big companies to spend more of their resources on safety research.

151
00:16:10,544 --> 00:16:22,077
Speaker SPEAKER_02: So at present, almost all of the resources go into making the models better so they can have shiny new models and there's a big competition going on and the models are getting much better and that's good.

152
00:16:22,618 --> 00:16:26,982
Speaker SPEAKER_02: But we need to accompany that with a comparable effort on AI safety.

153
00:16:27,344 --> 00:16:29,466
Speaker SPEAKER_02: The effort needs to be more than like 1%.

154
00:16:29,446 --> 00:16:36,297
Speaker SPEAKER_02: it needs to be like maybe a third of the effort goes into air safety, because if this stuff becomes unsafe, that's extremely bad.

155
00:16:36,357 --> 00:16:43,990
Speaker SPEAKER_00: Our next question is from Tara Deschamps again from CP.

156
00:16:45,332 --> 00:16:50,981
Speaker SPEAKER_00: She asks of Professor Hinton, any plans for the money that comes with the Nobel yet?

157
00:16:51,923 --> 00:16:53,666
Speaker SPEAKER_02: No specific plans.

158
00:16:53,787 --> 00:17:05,288
Speaker SPEAKER_02: I'm going to give it away to charities, but I know one charity I'll give some to, which provides jobs for neurodiverse young adults.

159
00:17:06,170 --> 00:17:09,797
Speaker SPEAKER_02: I will give it to some other charities, but I don't know which yet.

160
00:17:12,577 --> 00:17:26,153
Speaker SPEAKER_00: Our next question for Professor Hinton, again, is from Wa Lone of Reuters, who asks, do you have any recommendations for how to prevent serious consequences in the future?

161
00:17:27,035 --> 00:17:31,059
Speaker SPEAKER_00: By that they mean how people should be careful of AI and its use.

162
00:17:31,661 --> 00:17:33,742
Speaker SPEAKER_00: As you warned, it can be dangerous.

163
00:17:34,869 --> 00:17:40,759
Speaker SPEAKER_02: I don't think individual people being careful in how they use it is going to solve the problems.

164
00:17:41,440 --> 00:17:45,807
Speaker SPEAKER_02: I think the people developing AI need to be careful how they develop it.

165
00:17:46,728 --> 00:17:50,493
Speaker SPEAKER_02: And I think research needs to be done in the big companies which have the resources.

166
00:17:51,556 --> 00:17:55,561
Speaker SPEAKER_02: I'm not convinced that the way individual people use it is going to make much difference.

167
00:18:00,097 --> 00:18:04,006
Speaker SPEAKER_00: Our next question is another follow up from Issam Ahmed.

168
00:18:04,527 --> 00:18:07,393
Speaker SPEAKER_00: This is at AFP and again for Professor Hinton.

169
00:18:07,833 --> 00:18:12,563
Speaker SPEAKER_00: They ask, I know you said it's hard to predict what going bad might mean.

170
00:18:13,243 --> 00:18:18,714
Speaker SPEAKER_00: But if you had to hazard a stab at some rough areas of concern, what would those be?

171
00:18:20,398 --> 00:18:24,066
Speaker SPEAKER_02: So there are many different risks from AI and they all have different solutions.

172
00:18:24,748 --> 00:18:28,355
Speaker SPEAKER_02: So immediate risks are things like fake videos corrupting elections.

173
00:18:29,396 --> 00:18:38,474
Speaker SPEAKER_02: We've already seen politicians either accuse other people of using fake videos or use fake videos themselves and fake images.

174
00:18:38,454 --> 00:18:39,958
Speaker SPEAKER_02: So that's one immediate danger.

175
00:18:40,018 --> 00:18:44,726
Speaker SPEAKER_02: There's also very immediate dangers from things like cyber attacks.

176
00:18:44,746 --> 00:18:50,836
Speaker SPEAKER_02: So last year, for example, there was a 1200% increase in the number of phishing attacks.

177
00:18:51,336 --> 00:18:56,526
Speaker SPEAKER_02: And that's because these large language models make it very easy to do phishing attacks.

178
00:18:56,987 --> 00:19:01,173
Speaker SPEAKER_02: And you can no longer recognize them by the fact the spelling's wrong and the syntax is slightly odd.

179
00:19:02,155 --> 00:19:03,298
Speaker SPEAKER_02: Their English is perfect.

180
00:19:06,198 --> 00:19:09,064
Speaker SPEAKER_00: Next question comes from Victoria Gibson.

181
00:19:09,163 --> 00:19:10,646
Speaker SPEAKER_00: Again, she is at Toronto Star.

182
00:19:11,249 --> 00:19:18,463
Speaker SPEAKER_00: And she asks of Professor Hinton, you've spoken a few times today about the provincial government and the Ontario Science Centre.

183
00:19:18,984 --> 00:19:22,231
Speaker SPEAKER_00: Why is that top of mind as you receive this recognition?

184
00:19:23,613 --> 00:19:30,704
Speaker SPEAKER_02: So the Ontario Science Centre was very important in encouraging curiosity in young minds and curiosity about science.

185
00:19:32,106 --> 00:19:34,631
Speaker SPEAKER_02: It had some problems with the roof and it needed some renovation.

186
00:19:35,092 --> 00:19:46,329
Speaker SPEAKER_02: The estimate for the renovation was $200 million, but the government then told the people who estimated how much it would cost to multiply that by 1.85.

187
00:19:46,494 --> 00:19:50,766
Speaker SPEAKER_02: in order to get a much bigger number so that they could then justify knocking it down.

188
00:19:50,826 --> 00:19:56,319
Speaker SPEAKER_02: And the reasons it was knocked down were not the reasons the government gave as far as I can see.

189
00:19:56,781 --> 00:20:00,471
Speaker SPEAKER_02: It could have been repaired and it would have been much cheaper to repair it.

190
00:20:03,116 --> 00:20:10,128
Speaker SPEAKER_00: Next is another question from Tara Deschamps at Canadian Press for Professor Hinton.

191
00:20:10,609 --> 00:20:19,785
Speaker SPEAKER_00: She says, when people talk about the AI and technology landscape in Canada, your name always comes up as an example of what Canada can achieve.

192
00:20:20,467 --> 00:20:26,576
Speaker SPEAKER_00: But people also say the country has to be careful not to squander the opportunities you've created.

193
00:20:26,557 --> 00:20:33,208
Speaker SPEAKER_00: What do you think Canada can do to hold on to its status as a major player in the AI space?

194
00:20:34,623 --> 00:20:38,728
Speaker SPEAKER_02: It can keep funding curiosity-driven basic research.

195
00:20:38,807 --> 00:20:41,672
Speaker SPEAKER_02: That's very important for keeping the best researchers here.

196
00:20:42,292 --> 00:20:50,903
Speaker SPEAKER_02: But in this age of artificial neural networks, we also need significant computational resources to keep researchers in universities.

197
00:20:51,505 --> 00:20:53,567
Speaker SPEAKER_02: And the government is trying to do something about that.

198
00:20:53,606 --> 00:20:58,773
Speaker SPEAKER_02: They set aside $2 billion for computational resources for AI research.

199
00:20:58,753 --> 00:21:00,981
Speaker SPEAKER_02: So, I think they're doing what they can.

200
00:21:01,021 --> 00:21:09,792
Speaker SPEAKER_02: Obviously, we're a much smaller country than China or the United States, but given the resources they have, I think Canada is doing quite well.

201
00:21:13,637 --> 00:21:33,961
Speaker SPEAKER_00: Our next question comes from U of T News' very own Rahul Kalvapalli, who asks of Professor Hinton, you persisted with research in artificial neural networks, even during periods of waning interest in the topic among the scientific community.

202
00:21:34,563 --> 00:21:42,092
Speaker SPEAKER_00: Do you have a message for professors and students about persisting with endeavors that may be deemed unpopular or futile?

203
00:21:43,337 --> 00:21:44,941
Speaker SPEAKER_02: I think my message is this.

204
00:21:45,902 --> 00:21:52,472
Speaker SPEAKER_02: If you believe in something, don't give up on it until you understand why that belief is wrong.

205
00:21:52,873 --> 00:21:57,800
Speaker SPEAKER_02: Often you believe in things and you eventually figure out why that's a wrong thing to believe in.

206
00:21:58,442 --> 00:22:08,198
Speaker SPEAKER_02: But so long as you believe in something and you can't see why that's wrong, like the brain has to work somehow, so we have to figure out how it learns the connection strengths to make it work.

207
00:22:08,178 --> 00:22:14,688
Speaker SPEAKER_02: So long as you believe in that, keep working on it and don't let people tell you it's nonsense if you can't see why it's nonsense.

208
00:22:18,073 --> 00:22:18,554
Speaker SPEAKER_00: Thank you.

209
00:22:18,574 --> 00:22:28,388
Speaker SPEAKER_00: Our next question comes from Yasuhiro Kobayashi and they were with the Yomiuri Shimbun newspaper.

210
00:22:28,429 --> 00:22:33,236
Speaker SPEAKER_00: They ask, when will AI surpass human capabilities?

211
00:22:33,696 --> 00:22:35,619
Speaker SPEAKER_00: What will happen as a result?

212
00:22:36,696 --> 00:22:41,521
Speaker SPEAKER_02: So nobody knows when, but most of the good researchers I know think it will happen.

213
00:22:42,682 --> 00:22:46,767
Speaker SPEAKER_02: My guess is it'll probably happen sometime between five and 20 years from now.

214
00:22:47,146 --> 00:22:47,907
Speaker SPEAKER_02: It might be longer.

215
00:22:48,588 --> 00:22:50,250
Speaker SPEAKER_02: There's a very small chance it'll be sooner.

216
00:22:50,269 --> 00:22:54,294
Speaker SPEAKER_02: And we don't know what's going to happen then.

217
00:22:54,974 --> 00:23:06,365
Speaker SPEAKER_02: So if you look around, there are very few examples of more intelligent things being controlled by less intelligent things, which makes you wonder whether when AI gets smarter than us, it's going to take over control.

218
00:23:09,855 --> 00:23:11,678
Speaker SPEAKER_00: Thank you, Professor Hinton.

219
00:23:12,318 --> 00:23:19,027
Speaker SPEAKER_00: We no longer see any other questions, but we are happy to stay on the line for a little while.

220
00:23:19,167 --> 00:23:32,784
Speaker SPEAKER_00: If anybody has any last minute questions, please do use the Q&A toolbox you'll see at the bottom of your screen and give us your name and your media affiliation, please.

221
00:23:33,525 --> 00:23:36,588
Speaker SPEAKER_00: We do have time for a couple more if you have any more.

222
00:23:47,555 --> 00:23:50,221
Speaker SPEAKER_00: Another one from Rahul at U of T News.

223
00:23:50,922 --> 00:24:07,217
Speaker SPEAKER_00: He asks of President Gertler, he's asking, how do you expect Professor Hinton's Nobel Prize to reverberate through the university and inspire scholarship in AI and other fields?

224
00:24:09,204 --> 00:24:12,568
Speaker SPEAKER_01: Well, I think it's going to have a huge impact and a very, very positive one.

225
00:24:13,148 --> 00:24:23,759
Speaker SPEAKER_01: I was a very young assistant professor at U of T when another eminent scientist, John Polanyi, won the Nobel Prize in Chemistry in 1986.

226
00:24:23,839 --> 00:24:33,069
Speaker SPEAKER_01: And I can remember how proud I felt of our intellectual community when John got that wonderful news.

227
00:24:33,049 --> 00:24:40,441
Speaker SPEAKER_01: And it has continued to have such positive impact, not just in chemistry, but across the University of Toronto.

228
00:24:40,480 --> 00:24:53,519
Speaker SPEAKER_01: I think Jeff's win today will have a similarly positive effect, boosting morale across the university, but also helping us attract and retain fantastic talent.

229
00:24:53,538 --> 00:24:55,362
Speaker SPEAKER_01: Jeff has already talked about that.

230
00:24:55,342 --> 00:24:57,384
Speaker SPEAKER_01: in response to a couple of questions today.

231
00:24:57,403 --> 00:25:22,529
Speaker SPEAKER_01: And I think one cannot overstate the impact of a win like this on the ability of Canada, Toronto and the University of Toronto to be able to welcome talented newcomers, great students and wonderful faculty from across the country and around the world because of the recognition that arises with Jeff's win.

232
00:25:25,648 --> 00:25:26,912
Speaker SPEAKER_00: Thank you, President Gertler.

233
00:25:26,971 --> 00:25:35,413
Speaker SPEAKER_00: We are now going back to Professor Hinton with another question from Issam Ahmed from AFP.

234
00:25:35,935 --> 00:25:40,688
Speaker SPEAKER_00: They ask, what are the exciting next frontiers for you in AI?

235
00:25:42,625 --> 00:25:48,919
Speaker SPEAKER_02: Okay, I'm 76 and I'm not going to do much more frontier research, I believe.

236
00:25:49,160 --> 00:25:53,730
Speaker SPEAKER_02: I'm going to spend my time advocating for people to work on safety.

237
00:25:54,432 --> 00:25:59,502
Speaker SPEAKER_02: I think there's very exciting frontiers.

238
00:25:59,482 --> 00:26:04,414
Speaker SPEAKER_02: in robotics, in getting AI to be skilled at manipulating things.

239
00:26:04,915 --> 00:26:12,673
Speaker SPEAKER_02: At present, we're much better than computers at that or than artificial neural nets, but there will be a lot of progress there.

240
00:26:13,516 --> 00:26:17,204
Speaker SPEAKER_02: It may take a bit longer in that area, though.

241
00:26:17,184 --> 00:26:22,115
Speaker SPEAKER_02: I also think these large language models are going to get much, much better reasoning.

242
00:26:22,477 --> 00:26:29,573
Speaker SPEAKER_02: So the latest model from OpenAI and models from Google, like the latest versions of Gemini, are getting better at reasoning all the time.

243
00:26:30,596 --> 00:26:33,422
Speaker SPEAKER_02: And I think that's going to be very exciting to watch.

244
00:26:36,355 --> 00:26:39,118
Speaker SPEAKER_00: Our next question comes from Victoria Gibson.

245
00:26:39,199 --> 00:26:42,884
Speaker SPEAKER_00: Again, she's at the Toronto Star, and this is for Professor Hinton.

246
00:26:43,486 --> 00:26:52,000
Speaker SPEAKER_00: She asks, you offered some specifics on where AI can go poorly, such as cyber attacks, false videos, et cetera.

247
00:26:52,621 --> 00:26:57,749
Speaker SPEAKER_00: Can you share some more specific examples of how you think it can play a positive role?

248
00:26:59,211 --> 00:26:59,632
Speaker SPEAKER_02: Oh, yes.

249
00:27:00,031 --> 00:27:02,936
Speaker SPEAKER_02: So if you think about an area like healthcare,

250
00:27:03,457 --> 00:27:07,289
Speaker SPEAKER_02: A large part of the Ontario budget goes on health care.

251
00:27:08,401 --> 00:27:10,324
Speaker SPEAKER_02: it can make a tremendous difference there.

252
00:27:11,224 --> 00:27:18,694
Speaker SPEAKER_02: So I actually made a prediction in 2016 that by now, AI will be reading all the scans that radiologists normally read.

253
00:27:19,256 --> 00:27:20,617
Speaker SPEAKER_02: That prediction was wrong.

254
00:27:20,637 --> 00:27:22,119
Speaker SPEAKER_02: I was a bit overenthusiastic.

255
00:27:22,599 --> 00:27:26,025
Speaker SPEAKER_02: It may be another five years before that happens, but we're clearly getting there.

256
00:27:27,205 --> 00:27:30,290
Speaker SPEAKER_02: AI is gonna be much better at diagnosis.

257
00:27:30,510 --> 00:27:37,259
Speaker SPEAKER_02: So already, if you take difficult cases to diagnose, a doctor gets 40% correct,

258
00:27:37,240 --> 00:27:47,096
Speaker SPEAKER_02: A doctor, an AI system gets 50% correct, and the combination of the doctor with the AI system gets 60% correct, which is a big improvement.

259
00:27:47,678 --> 00:27:52,026
Speaker SPEAKER_02: In North America, several hundred thousand people a year die of bad diagnoses.

260
00:27:52,346 --> 00:27:55,131
Speaker SPEAKER_02: With AI, diagnoses are going to get much better.

261
00:27:55,567 --> 00:28:15,718
Speaker SPEAKER_02: But the thing that's going to really happen is you'll be able to have a family doctor who's an AI, who has seen a hundred million patients and knows huge amounts and will be much, much better at dealing with whatever ailment it is you have, because your AI family doctor will have seen many, many similar cases.

262
00:28:19,292 --> 00:28:21,116
Speaker SPEAKER_00: Thank you, Professor Hinton.

263
00:28:21,838 --> 00:28:24,605
Speaker SPEAKER_00: We no longer see any other questions.

264
00:28:25,326 --> 00:28:28,433
Speaker SPEAKER_00: But again, we do have time for one or two more.

265
00:28:28,453 --> 00:28:38,195
Speaker SPEAKER_00: So if anybody on the call would like to ask any other questions, we again invite you to include your name and the media outlet you're representing.

266
00:28:38,175 --> 00:28:42,983
Speaker SPEAKER_00: and type out those questions in the Q&A box at the bottom of your screen.

267
00:28:43,003 --> 00:29:01,855
Speaker SPEAKER_00: While we wait for any of those last minute questions to come in, Professor Hinton, we are curious, is there anything that we haven't touched on during this press conference today that you would like to mention or anything that we've kind of missed here with all the various press questions?

268
00:29:03,016 --> 00:29:08,862
Speaker SPEAKER_02: One thing we've only touched on briefly is the role of curiosity-driven basic research.

269
00:29:09,502 --> 00:29:19,673
Speaker SPEAKER_02: So artificial neural nets, the groundwork was all done by university researchers, almost all done by university researchers, just following their curiosity.

270
00:29:20,434 --> 00:29:22,978
Speaker SPEAKER_02: And funding that kind of research is very important.

271
00:29:23,478 --> 00:29:32,347
Speaker SPEAKER_02: It's not as expensive as other kinds of research, but it lays the foundation for things that later are very expensive and involve a lot of technology.

272
00:29:35,365 --> 00:29:35,826
Speaker SPEAKER_00: Thank you.

273
00:29:35,865 --> 00:29:44,180
Speaker SPEAKER_00: We've got another one here from Victoria Gibson at Toronto Star, possibly a follow-up to what you were saying about healthcare and AI.

274
00:29:44,641 --> 00:29:52,213
Speaker SPEAKER_00: She says, why do you think we haven't yet reached the point you predicted where AI is playing a bigger role in healthcare?

275
00:29:52,795 --> 00:29:55,720
Speaker SPEAKER_00: Are there any barriers left to this happening?

276
00:29:56,762 --> 00:30:01,415
Speaker SPEAKER_02: One barrier is the medical profession is very conservative.

277
00:30:01,717 --> 00:30:02,839
Speaker SPEAKER_02: There's good reasons for that.

278
00:30:02,880 --> 00:30:08,375
Speaker SPEAKER_02: If people die when you make a mistake, it's a good

279
00:30:08,490 --> 00:30:13,397
Speaker SPEAKER_02: policy to be conservative, but they're relatively slow to adopt new technology.

280
00:30:14,199 --> 00:30:21,710
Speaker SPEAKER_02: Another reason is I was just wrong about the speed at which AI systems would be better than radiologists at reading scans.

281
00:30:22,230 --> 00:30:27,157
Speaker SPEAKER_02: They're now comparable with radiologists at lots of different kinds of scans and better at a few.

282
00:30:27,137 --> 00:30:30,624
Speaker SPEAKER_02: I think in another few years, they'll definitely be better than radiologists.

283
00:30:30,923 --> 00:30:39,479
Speaker SPEAKER_02: And what we'll see is collaborations between radiologists and AI systems where the AI system reads the scan and the radiologist checks that it didn't make a mistake.

284
00:30:40,259 --> 00:30:43,105
Speaker SPEAKER_02: And after a while, the AI systems will be doing nearly all the work.

285
00:30:45,987 --> 00:30:46,667
Speaker SPEAKER_00: Okay, great.

286
00:30:46,768 --> 00:30:48,190
Speaker SPEAKER_00: Thank you, Professor Hinton.

287
00:30:48,289 --> 00:30:51,874
Speaker SPEAKER_00: That is all the time we have for questions today.

288
00:30:51,894 --> 00:31:00,663
Speaker SPEAKER_00: So in the chat box, you will see an email address that you can contact if you have additional questions.

289
00:31:01,806 --> 00:31:03,247
Speaker SPEAKER_00: You should see it popping up right now.

290
00:31:03,307 --> 00:31:08,733
Speaker SPEAKER_00: That email address is media.relations at utoronto.ca.

291
00:31:08,753 --> 00:31:14,019
Speaker SPEAKER_00: So at this point, I will now ask President Gertler to provide closing remarks.

292
00:31:15,821 --> 00:31:16,722
Speaker SPEAKER_01: Well, thank you, Lisa.

293
00:31:17,123 --> 00:31:18,204
Speaker SPEAKER_01: Thank you, Jeff.

294
00:31:18,224 --> 00:31:22,788
Speaker SPEAKER_01: And congratulations once again on this wonderful achievement, your Nobel Prize.

295
00:31:22,848 --> 00:31:38,205
Speaker SPEAKER_01: I'm sure I speak for the entire University of Toronto community and indeed for all of Canada and for your many, many friends and colleagues and admirers around the world when I say how incredibly proud we are of your achievements that have been recognized today.

296
00:31:38,266 --> 00:31:43,652
Speaker SPEAKER_01: Thank you also to everyone for joining us today in this wonderful celebration.

297
00:31:44,613 --> 00:31:45,053
Speaker SPEAKER_01: Cheers.


1
00:00:05,684 --> 00:00:16,539
Speaker SPEAKER_00: Thank you.

2
00:00:17,039 --> 00:00:30,018
Speaker SPEAKER_00: Delighted to be back here again and I'm delighted to get to share the stage with Geoff Hinton, who is one of the smartest, most wonderful, most capable and kindest people in this entire field.

3
00:00:30,419 --> 00:00:31,640
Speaker SPEAKER_00: How are you Geoffrey Hinton?

4
00:00:32,835 --> 00:00:33,396
Speaker SPEAKER_01: I'm fine.

5
00:00:33,417 --> 00:00:35,238
Speaker SPEAKER_01: Thank you for the over-the-top introduction.

6
00:00:35,560 --> 00:00:37,101
Speaker SPEAKER_00: All right.

7
00:00:37,121 --> 00:00:43,350
Speaker SPEAKER_00: Well, Jeff, I want to start with a little conversation that you and I had almost exactly a year ago.

8
00:00:44,152 --> 00:00:47,036
Speaker SPEAKER_00: And we were in Toronto, and we were about to go on a stage.

9
00:00:48,057 --> 00:00:49,820
Speaker SPEAKER_00: And two of my children were with me.

10
00:00:49,920 --> 00:00:51,662
Speaker SPEAKER_00: They were then 14 and 12.

11
00:00:52,484 --> 00:00:57,692
Speaker SPEAKER_00: And you looked at the older one, and you said, are you going to go into media like your father?

12
00:00:57,712 --> 00:00:59,433
Speaker SPEAKER_00: And he responded.

13
00:00:59,936 --> 00:01:00,216
Speaker SPEAKER_00: No.

14
00:01:00,957 --> 00:01:02,359
Speaker SPEAKER_00: And you said, good.

15
00:01:02,399 --> 00:01:07,885
Speaker SPEAKER_00: And then I said, well, if he's not going to go into media, what should he do?

16
00:01:08,566 --> 00:01:11,490
Speaker SPEAKER_00: And you said, he should be a plumber.

17
00:01:12,451 --> 00:01:16,816
Speaker SPEAKER_00: And so that son has just applied for the school paper.

18
00:01:17,358 --> 00:01:24,426
Speaker SPEAKER_00: And I'm curious if you think he's making a grievous mistake and I should actually go send him downstairs to fix the ducks.

19
00:01:26,245 --> 00:01:27,968
Speaker SPEAKER_01: No, I was being somewhat humorous.

20
00:01:28,549 --> 00:01:32,659
Speaker SPEAKER_01: But I do think that plumbing is going to last longer than most professions.

21
00:01:33,701 --> 00:01:38,432
Speaker SPEAKER_01: I think currently, with AI,

22
00:01:38,698 --> 00:01:40,941
Speaker SPEAKER_01: The thing it's worst at is physical manipulation.

23
00:01:41,001 --> 00:01:44,566
Speaker SPEAKER_01: It's getting better quickly, but that's where it's worst compared with people.

24
00:01:45,227 --> 00:01:46,168
Speaker SPEAKER_00: All right, wonderful.

25
00:01:46,408 --> 00:01:52,594
Speaker SPEAKER_00: All right, so what I want to do this interview, I want to start a little bit with Dr. Hinton's background, ground this.

26
00:01:53,055 --> 00:01:58,402
Speaker SPEAKER_00: Then I want to go into a section where I ask him some of the most interesting technical questions, some of the ones we talked about on stage.

27
00:01:58,783 --> 00:02:04,849
Speaker SPEAKER_00: We'll talk a little AI for good, then a little AI for bad, and then we'll talk a little bit about regulatory frameworks.

28
00:02:05,049 --> 00:02:05,870
Speaker SPEAKER_00: Sound good, Jeff?

29
00:02:06,813 --> 00:02:07,894
Speaker SPEAKER_00: Okay, great.

30
00:02:07,914 --> 00:02:30,745
Speaker SPEAKER_00: All right, so I first want to start 40 years ago, where you are a lonely scientist, and you have what turns out to be one of the most important insights of this field, maybe of the latter 20th century, where you realize that to make an extremely powerful computer, you should pursue that by modeling it on the architecture of the human brain.

31
00:02:31,286 --> 00:02:33,528
Speaker SPEAKER_00: And it sounds somewhat obvious now.

32
00:02:33,508 --> 00:02:34,409
Speaker SPEAKER_00: But it wasn't then.

33
00:02:34,430 --> 00:02:38,415
Speaker SPEAKER_00: So tell me about that moment of insight that really gets this field going.

34
00:02:40,218 --> 00:02:44,125
Speaker SPEAKER_01: This is a nice myth, but there were a bunch of different people who thought that.

35
00:02:44,585 --> 00:02:49,132
Speaker SPEAKER_01: In particular, in the 1950s, both von Neumann and Turing thought that.

36
00:02:49,733 --> 00:02:54,419
Speaker SPEAKER_01: It was very unfortunate that they both died young, otherwise the history of our field might have been very different.

37
00:02:55,008 --> 00:03:01,334
Speaker SPEAKER_01: But it seemed to me just obvious that if you want to understand intelligence, you need to understand the most intelligent thing we know about, and that's us.

38
00:03:02,455 --> 00:03:09,862
Speaker SPEAKER_01: And our intelligence doesn't come from people programming in a lot of propositions, and then it's using logic to reason with those propositions.

39
00:03:10,563 --> 00:03:15,848
Speaker SPEAKER_01: It emerges from a brain that was designed mainly for vision and motor control and things like that.

40
00:03:16,729 --> 00:03:23,597
Speaker SPEAKER_01: And it's clearly that the connection strengths in that brain change as you learn, and we just have to figure out how that happens.

41
00:03:24,336 --> 00:03:26,439
Speaker SPEAKER_00: All right, so that makes good sense.

42
00:03:26,460 --> 00:03:27,322
Speaker SPEAKER_00: You're grounded in history.

43
00:03:27,342 --> 00:03:28,423
Speaker SPEAKER_00: So now let's go very quickly.

44
00:03:28,824 --> 00:03:29,444
Speaker SPEAKER_00: You work on this.

45
00:03:30,265 --> 00:03:32,169
Speaker SPEAKER_00: People say you're heading down the wrong path.

46
00:03:32,468 --> 00:03:33,090
Speaker SPEAKER_00: You pursue it.

47
00:03:33,450 --> 00:03:34,292
Speaker SPEAKER_00: Other people join you.

48
00:03:34,513 --> 00:03:36,354
Speaker SPEAKER_00: Eventually, it becomes clear you're on a good path.

49
00:03:36,435 --> 00:03:37,537
Speaker SPEAKER_00: It's not clear where it will go.

50
00:03:38,057 --> 00:03:39,139
Speaker SPEAKER_00: You win a Turing Prize.

51
00:03:39,620 --> 00:03:40,300
Speaker SPEAKER_00: You join Google.

52
00:03:40,320 --> 00:03:41,483
Speaker SPEAKER_00: You sell a company to Google.

53
00:03:42,204 --> 00:03:45,990
Speaker SPEAKER_00: You then, about a year and a half ago,

54
00:03:46,156 --> 00:03:47,296
Speaker SPEAKER_00: You leave Google.

55
00:03:47,317 --> 00:03:52,462
Speaker SPEAKER_00: Tell me about the moment when you leave, which is a few months after the release of Chat GPT.

56
00:03:52,962 --> 00:03:56,706
Speaker SPEAKER_00: Tell me what the last thing you worked on was and that moment of departure.

57
00:03:58,188 --> 00:03:59,389
Speaker SPEAKER_01: First, let me get something straight.

58
00:03:59,550 --> 00:04:04,616
Speaker SPEAKER_01: I left for several reasons, one being that I was 75 and I decided I should retire then anyway.

59
00:04:05,377 --> 00:04:14,627
Speaker SPEAKER_01: I didn't just leave in order to talk about the dangers of AI, but that was another reason.

60
00:04:14,606 --> 00:04:22,538
Speaker SPEAKER_01: And I became acutely aware of the dangers of AI, the existential threat, at the beginning of 2023, in around March 2023.

61
00:04:23,139 --> 00:04:30,610
Speaker SPEAKER_01: And I started talking to other people who were scared about the existential threat, like Roger Gross, for example, and they encouraged me to go public.

62
00:04:31,190 --> 00:04:34,776
Speaker SPEAKER_01: And then I made a decision to leave Google so that I could speak freely.

63
00:04:34,755 --> 00:04:45,447
Speaker SPEAKER_01: The reason I became scared was I was working on trying to figure out how analog computers could do these large language models for 30 watts instead of megawatts.

64
00:04:46,367 --> 00:04:55,055
Speaker SPEAKER_01: And while doing that, I became convinced that there's something about digital computation that just makes it much better than what the brain does.

65
00:04:55,737 --> 00:05:01,523
Speaker SPEAKER_01: Up until that point, I'd spent 50 years thinking that if we could only make it more like the brain, it would be better.

66
00:05:01,502 --> 00:05:07,076
Speaker SPEAKER_01: And I finally realized at the beginning of 2023 that it has something the brain can never have.

67
00:05:07,596 --> 00:05:18,380
Speaker SPEAKER_01: Because it's digital, you can make many copies of the same model that work in exactly the same way, and each copy can look at a different part of the data set.

68
00:05:19,019 --> 00:05:24,452
Speaker SPEAKER_01: and get a gradient and they can combine those gradients and that allows them to learn much, much more.

69
00:05:24,473 --> 00:05:27,459
Speaker SPEAKER_01: That's why GPT-4 can know so much more than a person.

70
00:05:28,161 --> 00:05:34,276
Speaker SPEAKER_01: It was multiple different copies running on multiple different hardware that looked at all of the internet.

71
00:05:34,257 --> 00:05:36,259
Speaker SPEAKER_01: That's something we can never have.

72
00:05:36,338 --> 00:05:41,365
Speaker SPEAKER_01: So basically, what they've got and we haven't got is they can share very efficiently.

73
00:05:41,865 --> 00:05:44,007
Speaker SPEAKER_01: We can share very inefficiently.

74
00:05:44,367 --> 00:05:45,428
Speaker SPEAKER_01: That's what's going on now.

75
00:05:46,009 --> 00:05:51,255
Speaker SPEAKER_01: I produce sentences, you try and figure out how to change the synapses in your brain so you might have said that.

76
00:05:51,896 --> 00:05:53,877
Speaker SPEAKER_01: That's a very slow and inefficient way of sharing.

77
00:05:54,838 --> 00:06:01,766
Speaker SPEAKER_01: Digital intelligences, if they're different copies of the same model, can share with a bandwidth of trillions of bits.

78
00:06:02,826 --> 00:06:09,413
Speaker SPEAKER_00: And so you have this moment, this realization that suddenly these systems can be massively more powerful than you thought.

79
00:06:09,434 --> 00:06:12,096
Speaker SPEAKER_00: It must have been a moment both of great excitement.

80
00:06:12,757 --> 00:06:15,259
Speaker SPEAKER_00: Why was the great fear so prevalent too?

81
00:06:17,442 --> 00:06:22,706
Speaker SPEAKER_01: Well, it made me think they're gonna become more intelligent than us sooner than I thought.

82
00:06:24,189 --> 00:06:27,552
Speaker SPEAKER_01: And it made me think they're just a better form of intelligence.

83
00:06:28,762 --> 00:06:32,913
Speaker SPEAKER_00: Let me ask you about two of the other godfathers of AI.

84
00:06:32,973 --> 00:06:34,978
Speaker SPEAKER_00: So you won the Turing Prize with three people.

85
00:06:35,418 --> 00:06:39,870
Speaker SPEAKER_00: Yann LeCun, who now runs AI at Meta, Yoshua Bengio.

86
00:06:40,692 --> 00:06:42,456
Speaker SPEAKER_00: I was trying to figure out the differences between you.

87
00:06:42,475 --> 00:06:43,619
Speaker SPEAKER_00: And let me know if this works.

88
00:06:43,639 --> 00:06:45,101
Speaker SPEAKER_00: You're all godfathers.

89
00:06:45,418 --> 00:06:51,709
Speaker SPEAKER_00: Jan kind of thinks of AI as Fredo Corleone, not very capable, easy to control.

90
00:06:52,471 --> 00:06:56,838
Speaker SPEAKER_00: Joshua maybe thinks of it as Sonny, you know, potentially quite dangerous.

91
00:06:57,298 --> 00:07:01,987
Speaker SPEAKER_00: And you view it as Michael, Michael Corleone, potentially extremely dangerous.

92
00:07:02,468 --> 00:07:03,569
Speaker SPEAKER_00: Is that more or less correct?

93
00:07:03,610 --> 00:07:05,333
Speaker SPEAKER_00: I don't think so.

94
00:07:05,353 --> 00:07:08,338
Speaker SPEAKER_00: I think Joshua and I have very similar views about the dangers.

95
00:07:08,942 --> 00:07:14,410
Speaker SPEAKER_00: But your difference with Jan is essentially you view this as a much more powerful system than he does.

96
00:07:14,709 --> 00:07:17,012
Speaker SPEAKER_00: And that's why you are more concerned than he is.

97
00:07:19,937 --> 00:07:21,499
Speaker SPEAKER_01: That's one difference, yes.

98
00:07:21,978 --> 00:07:23,721
Speaker SPEAKER_01: That's a main difference.

99
00:07:23,742 --> 00:07:26,685
Speaker SPEAKER_01: So I think it really is intelligent already.

100
00:07:27,386 --> 00:07:29,007
Speaker SPEAKER_01: And Jan thinks a cat's more intelligent.

101
00:07:29,689 --> 00:07:29,889
Speaker SPEAKER_01: Right.

102
00:07:30,430 --> 00:07:34,274
Speaker SPEAKER_00: Well, let's get into that intelligence, which I think is one of the most interesting questions

103
00:07:35,233 --> 00:07:42,937
Speaker SPEAKER_00: Do you think that there is anything in the human mind that cannot be replicated by these machines and by AI systems?

104
00:07:43,016 --> 00:07:47,610
Speaker SPEAKER_00: Is there anything that our brains can do that can't be replicated in a machine?

105
00:07:48,694 --> 00:07:48,754
Speaker SPEAKER_01: No.

106
00:07:50,793 --> 00:07:59,322
Speaker SPEAKER_00: And does that mean that there's nothing that we can do that cannot be surpassed by these intelligent machines?

107
00:08:00,182 --> 00:08:07,672
Speaker SPEAKER_00: Can, for example, one could say that they will eventually be able to produce more beautiful music.

108
00:08:08,172 --> 00:08:13,658
Speaker SPEAKER_00: They will be able to do all the things that we do better than us that involve simple cognition.

109
00:08:14,920 --> 00:08:16,040
Speaker SPEAKER_01: That's what I believe, yes.

110
00:08:17,742 --> 00:08:29,074
Speaker SPEAKER_00: And you don't believe there's anything spiritual or outside or anything that is beyond what can be captured in a set of neural networks.

111
00:08:30,617 --> 00:08:35,261
Speaker SPEAKER_01: I think what we mean by spiritual could be captured by these alien intelligences.

112
00:08:35,903 --> 00:08:38,225
Speaker SPEAKER_01: I agree with Sam Altman that it's an alien intelligence.

113
00:08:38,284 --> 00:08:39,225
Speaker SPEAKER_01: It's not quite like us.

114
00:08:39,706 --> 00:08:40,868
Speaker SPEAKER_01: It has some differences from us.

115
00:08:41,948 --> 00:08:45,133
Speaker SPEAKER_01: But if you look at

116
00:08:46,260 --> 00:08:47,182
Speaker SPEAKER_01: things like religion.

117
00:08:47,423 --> 00:08:49,225
Speaker SPEAKER_01: I don't see why you shouldn't get religious ones.

118
00:08:51,250 --> 00:08:58,582
Speaker SPEAKER_00: Yesterday, when I asked Altman this question, he said that, well, there might be one difference, which is subjective experience.

119
00:08:59,543 --> 00:09:02,590
Speaker SPEAKER_00: A bot, a system can't experience the world.

120
00:09:03,331 --> 00:09:07,938
Speaker SPEAKER_00: Do you believe that AI systems can have subjective experience?

121
00:09:07,999 --> 00:09:08,559
Speaker SPEAKER_00: Yes, I do.

122
00:09:08,779 --> 00:09:10,243
Speaker SPEAKER_00: I think they already do.

123
00:09:10,712 --> 00:09:12,174
Speaker SPEAKER_00: All right, let's go into that a little bit more.

124
00:09:12,215 --> 00:09:12,735
Speaker SPEAKER_00: Explain that.

125
00:09:12,775 --> 00:09:14,938
Speaker SPEAKER_00: That is a controversial proposition, Jeff.

126
00:09:14,958 --> 00:09:16,682
Speaker SPEAKER_00: You can't get away with a one-sentence answer.

127
00:09:16,942 --> 00:09:18,183
Speaker SPEAKER_00: Please follow up, Dr. Hinton.

128
00:09:19,365 --> 00:09:22,109
Speaker SPEAKER_01: Okay, I was trying to give nice, sharp answers to your questions.

129
00:09:22,129 --> 00:09:24,933
Speaker SPEAKER_01: It's lovely, because I have... In the way that Altman didn't.

130
00:09:25,394 --> 00:09:28,538
Speaker SPEAKER_01: But, yeah, we need to follow up on that one.

131
00:09:29,220 --> 00:09:39,774
Speaker SPEAKER_01: So, my view is that almost everybody has a completely wrong model of what the mind is.

132
00:09:40,480 --> 00:09:43,267
Speaker SPEAKER_01: This is a difficult thing to sell.

133
00:09:43,307 --> 00:09:49,081
Speaker SPEAKER_01: I'm now in a position where I have this belief that's kind of out of sync with what most people firmly believe.

134
00:09:49,942 --> 00:09:52,649
Speaker SPEAKER_01: I'm always very happy in that position.

135
00:09:53,929 --> 00:09:59,597
Speaker SPEAKER_01: So most people have a view of the mind as a kind of internal theater.

136
00:10:00,538 --> 00:10:05,424
Speaker SPEAKER_01: In fact, people are sort of so convinced this view is right that they don't even think it's a view.

137
00:10:05,785 --> 00:10:07,347
Speaker SPEAKER_01: They don't even think it's a model they have.

138
00:10:07,628 --> 00:10:08,669
Speaker SPEAKER_01: They think it's just obvious.

139
00:10:09,410 --> 00:10:12,955
Speaker SPEAKER_01: In much the same way as people thought it was just obvious that the sun goes around the earth.

140
00:10:13,756 --> 00:10:16,360
Speaker SPEAKER_01: I mean, you just look at it and it goes around the earth.

141
00:10:17,774 --> 00:10:24,965
Speaker SPEAKER_01: Eventually, people realized that the sun doesn't go around the earth, the earth rotates on its axis.

142
00:10:26,126 --> 00:10:30,732
Speaker SPEAKER_01: That was a little technical error Sam made, and since I'm pedantic, I like to pick him up on it.

143
00:10:31,173 --> 00:10:38,725
Speaker SPEAKER_01: It's not that the, they thought the, initially they thought the sun goes around the earth, and then they realized the earth goes around the sun.

144
00:10:39,125 --> 00:10:40,388
Speaker SPEAKER_01: That's not the right contrast.

145
00:10:40,828 --> 00:10:45,115
Speaker SPEAKER_01: They thought the sun went around the earth, and then they realized the earth rotates on its axis.

146
00:10:45,095 --> 00:10:47,577
Speaker SPEAKER_01: The earth going around the sun is to do with years, not with days.

147
00:10:48,558 --> 00:10:51,842
Speaker SPEAKER_01: But anyway, it was obvious the sun went around the earth and we were wrong.

148
00:10:51,942 --> 00:10:52,663
Speaker SPEAKER_01: We had a model.

149
00:10:53,225 --> 00:10:54,466
Speaker SPEAKER_01: It was a straightforward model.

150
00:10:54,966 --> 00:10:56,268
Speaker SPEAKER_01: It was obviously right.

151
00:10:56,327 --> 00:10:57,629
Speaker SPEAKER_01: You could just see it happening.

152
00:10:58,210 --> 00:10:59,631
Speaker SPEAKER_01: And we were wrong about that model.

153
00:11:00,533 --> 00:11:03,976
Speaker SPEAKER_01: And I think the same is true of what most people think about the mind.

154
00:11:04,437 --> 00:11:08,503
Speaker SPEAKER_01: Most people think about an inner theater and they're just wrong about that.

155
00:11:08,562 --> 00:11:12,267
Speaker SPEAKER_01: They haven't understood how the language of mental states works.

156
00:11:12,922 --> 00:11:15,866
Speaker SPEAKER_00: But explain how that applies to an AI system.

157
00:11:15,966 --> 00:11:29,730
Speaker SPEAKER_00: Explain the way, if I say to GPT-4, I say, you've just experienced a loud sound and something has collided with you.

158
00:11:30,390 --> 00:11:33,395
Speaker SPEAKER_00: It isn't feeling pain or hurt and its ears don't ache.

159
00:11:33,416 --> 00:11:36,380
Speaker SPEAKER_00: To what sense has it had a subjective experience?

160
00:11:37,373 --> 00:11:39,918
Speaker SPEAKER_01: Okay, so let's take a nice simple example.

161
00:11:40,860 --> 00:11:47,493
Speaker SPEAKER_01: I don't pretend to have the full answer to what consciousness is, although I think I've made a little bit of progress.

162
00:11:48,134 --> 00:11:51,642
Speaker SPEAKER_01: In fact, the progress was made by philosophers in the last century.

163
00:11:54,587 --> 00:11:59,297
Speaker SPEAKER_01: So if I say to you, I see little pink elephants floating in front of me,

164
00:12:00,340 --> 00:12:09,955
Speaker SPEAKER_01: One way of thinking about that is there's an inner theater and in my inner theater there are little pink elephants and I can sort of directly see those little pink elephants.

165
00:12:10,635 --> 00:12:13,659
Speaker SPEAKER_01: And if you ask what they're made of, they're made of stuff called qualia.

166
00:12:14,081 --> 00:12:21,010
Speaker SPEAKER_01: Maybe some pink qualia and some elephant qualia and some right way up qualia and some moving qualia all somehow conjoined together.

167
00:12:21,532 --> 00:12:23,053
Speaker SPEAKER_01: That's one theory of what's going on.

168
00:12:23,173 --> 00:12:25,538
Speaker SPEAKER_01: It's an inner theater with funny spooky stuff in it.

169
00:12:26,462 --> 00:12:32,553
Speaker SPEAKER_01: A completely different theory is, I'm trying to tell you what my perceptual system is telling me.

170
00:12:33,375 --> 00:12:40,347
Speaker SPEAKER_01: And my perceptual system is telling me there's little pink elephants out there floating in the air, and I know that's wrong.

171
00:12:41,649 --> 00:12:50,125
Speaker SPEAKER_01: So the way I tell you what my perceptual system's telling me is by saying, what would have to be the case for my perceptual system to be working correctly?

172
00:12:50,307 --> 00:13:01,142
Speaker SPEAKER_01: So really, when I say I have the subjective experience of little pink elephants floating in front of me, I can say exactly the same thing without using the word subjective experience.

173
00:13:01,662 --> 00:13:10,914
Speaker SPEAKER_01: I can say what my perceptual system is telling me would be correct if the world contained little pink elephants floating in front of me.

174
00:13:11,182 --> 00:13:18,029
Speaker SPEAKER_01: In other words, what's funny about these little pink elephants is not that they're in an inner theater made of funny stuff called qualia.

175
00:13:18,769 --> 00:13:20,873
Speaker SPEAKER_01: They're hypothetical states of the world.

176
00:13:21,774 --> 00:13:24,255
Speaker SPEAKER_01: And it's just a sort of indirect reference trick.

177
00:13:24,275 --> 00:13:31,624
Speaker SPEAKER_01: I can't directly describe what my perceptual system is telling me, but I can say what would have to be in the world for it to be correct.

178
00:13:32,926 --> 00:13:38,471
Speaker SPEAKER_00: So a machine can more or less do the same thing with its perception?

179
00:13:39,092 --> 00:13:41,657
Speaker SPEAKER_01: Yes, and so let me give you an example of that.

180
00:13:41,836 --> 00:13:47,985
Speaker SPEAKER_01: So I want to give you an example of a chatbot that's obviously having a subjective experience.

181
00:13:48,004 --> 00:13:59,441
Speaker SPEAKER_01: Suppose I have a multimodal chatbot and it's got a camera and a robot arm and I train it up and it can talk and it can see things and I put an object in front of it and say point at the object.

182
00:14:00,221 --> 00:14:01,263
Speaker SPEAKER_01: It'll point at the object.

183
00:14:02,504 --> 00:14:08,273
Speaker SPEAKER_01: Now I put a prism in front of its lens and without it knowing,

184
00:14:09,129 --> 00:14:13,395
Speaker SPEAKER_01: And now I put an object in front of it and say, point at the object, and it points off to one side.

185
00:14:14,177 --> 00:14:16,080
Speaker SPEAKER_01: And I say, no, that's not where the object is.

186
00:14:16,139 --> 00:14:18,763
Speaker SPEAKER_01: The object is straight in front of you, but I put a prism in front of your lens.

187
00:14:19,563 --> 00:14:22,849
Speaker SPEAKER_01: And the chatbot says, oh, I see, the prism bent the light rays.

188
00:14:23,570 --> 00:14:29,518
Speaker SPEAKER_01: So the object's actually straight in front of me, but I had the subjective experience that it was off to one side.

189
00:14:30,173 --> 00:14:36,145
Speaker SPEAKER_01: And if the chatbot said that, I think it would be using the phrase subjective experience in exactly the way we use it.

190
00:14:36,886 --> 00:14:40,294
Speaker SPEAKER_01: It's not referring to spooky inner stuff that chatbots couldn't have.

191
00:14:40,916 --> 00:14:46,528
Speaker SPEAKER_01: It's referring to a hypothetical state of the world such that the perception of the chatbot would have been correct.

192
00:14:47,520 --> 00:14:54,087
Speaker SPEAKER_00: Wow, all right, this is the, you're the first person to have argued to me about this, but that is a fascinating, fascinating case to make.

193
00:14:54,128 --> 00:15:10,625
Speaker SPEAKER_00: Let's talk about interoperability, which was something I asked Altman about, because to him, understanding the inner core of an AI system would be the thing that would most protect us from catastrophic outcomes.

194
00:15:10,605 --> 00:15:12,932
Speaker SPEAKER_00: You help design these systems.

195
00:15:13,014 --> 00:15:18,190
Speaker SPEAKER_00: Why is it so hard to look inside of them and understand what they're doing?

196
00:15:19,836 --> 00:15:21,480
Speaker SPEAKER_01: Okay, let's take an extreme case.

197
00:15:21,541 --> 00:15:23,366
Speaker SPEAKER_01: Let's suppose we had a big data set

198
00:15:24,307 --> 00:15:26,509
Speaker SPEAKER_01: And we're trying to answer a yes-no question.

199
00:15:27,291 --> 00:15:30,052
Speaker SPEAKER_01: And in this data set, there's lots of weak regularities.

200
00:15:30,774 --> 00:15:35,798
Speaker SPEAKER_01: Maybe there's 300,000 weak regularities that suggest that the answer should be no.

201
00:15:36,778 --> 00:15:40,481
Speaker SPEAKER_01: And there's 600,000 weak regularities that suggest that the answer should be yes.

202
00:15:41,263 --> 00:15:43,544
Speaker SPEAKER_01: And the regularities are about equal strength.

203
00:15:44,044 --> 00:15:45,807
Speaker SPEAKER_01: So the answer is very clearly yes.

204
00:15:46,287 --> 00:15:48,448
Speaker SPEAKER_01: There's overwhelming evidence the answer should be yes.

205
00:15:48,929 --> 00:15:51,192
Speaker SPEAKER_01: But this evidence is in all these weak regularities.

206
00:15:51,272 --> 00:15:53,673
Speaker SPEAKER_01: It's just in the combined effect of them all.

207
00:15:53,653 --> 00:15:55,076
Speaker SPEAKER_01: This is an extreme case, of course.

208
00:15:56,758 --> 00:15:59,600
Speaker SPEAKER_01: If you then ask someone, OK, explain why it said yes.

209
00:16:01,082 --> 00:16:05,246
Speaker SPEAKER_01: The only way to explain why it said yes is to go into this 600,000 weak regularities.

210
00:16:06,288 --> 00:16:19,864
Speaker SPEAKER_01: So when you're in a domain where there's lots and lots of weak regularities, and there's so many of them that they're actually significant, their combined effect is significant, there's no reason to expect you should be able to get simple explanations of things.

211
00:16:21,024 --> 00:16:28,174
Speaker SPEAKER_00: And so in that conversation yesterday, Altman pointed out a paper from Anthropic, which I thought was incredibly interesting.

212
00:16:28,816 --> 00:16:40,533
Speaker SPEAKER_00: And the paper talks about analyzing the inner workings of Claude, Anthropic's model, and finding all the connections, the neural connections to the concept of the Golden Gate Bridge.

213
00:16:40,552 --> 00:16:44,658
Speaker SPEAKER_00: And you add weight to all those connections, and you create Golden Gate Claude.

214
00:16:44,999 --> 00:16:47,582
Speaker SPEAKER_00: And then you go into that chatbot, and you say, tell me a love story.

215
00:16:47,822 --> 00:16:50,307
Speaker SPEAKER_00: And it's a love story that happens on the Golden Gate Bridge.

216
00:16:50,287 --> 00:16:52,910
Speaker SPEAKER_00: And you ask it, you know, what it is, and it describes the Golden Gate Bridge.

217
00:16:54,212 --> 00:17:12,800
Speaker SPEAKER_00: Given that, why can we not go in to a large language model and adjust the weights, not for the Golden Gate Bridge, but for, say, the concept of empathy, the concept of compassion, and then create a large language model that is much more likely to do good for the world?

218
00:17:13,961 --> 00:17:18,208
Speaker SPEAKER_01: I think you can make an empathetic model, but not by directly adjusting the weights.

219
00:17:18,248 --> 00:17:20,791
Speaker SPEAKER_01: You just train it on data that exhibits empathy.

220
00:17:22,835 --> 00:17:24,136
Speaker SPEAKER_00: Then you get the same result.

221
00:17:24,979 --> 00:17:25,179
Speaker SPEAKER_01: Yes.

222
00:17:25,779 --> 00:17:26,901
Speaker SPEAKER_00: Should we be doing that?

223
00:17:27,623 --> 00:17:33,912
Speaker SPEAKER_01: There's been lots of examples in the past of people trying to understand what individual neurons are doing.

224
00:17:35,343 --> 00:17:37,465
Speaker SPEAKER_01: I've been doing that for like 50 years.

225
00:17:38,106 --> 00:17:46,798
Speaker SPEAKER_01: And if the neurons connected directly to the inputs or connected directly to the outputs, you stand a chance of understanding what the individual neurons are doing.

226
00:17:47,359 --> 00:17:58,095
Speaker SPEAKER_01: But once you have multiple layers, it's very, very hard to understand what a neuron deep inside the system is really doing because it's marginal effect that counts.

227
00:17:58,695 --> 00:18:03,442
Speaker SPEAKER_01: And it's marginal effect is very different depending on what the other neurons are doing, depending on the input.

228
00:18:03,423 --> 00:18:06,567
Speaker SPEAKER_01: So as the inputs change, the marginal effects of all these neurons change.

229
00:18:06,989 --> 00:18:11,076
Speaker SPEAKER_01: And it's just extremely hard to get a good theory of what they're doing.

230
00:18:11,096 --> 00:18:24,637
Speaker SPEAKER_00: So I could take my neural network that I've been building backstage and try to adjust the weights for compassion and I actually come up with some kind of horrible animal killing machine because I don't know exactly what I've done and how everything connects?

231
00:18:24,736 --> 00:18:28,180
Speaker SPEAKER_01: Yeah, I may be one of the few people who's actually tried doing this.

232
00:18:28,240 --> 00:18:36,509
Speaker SPEAKER_01: So in the very early days of neural networks, when the learning algorithms weren't working very well, I had a Lisp machine, and the mouse had three buttons.

233
00:18:37,731 --> 00:18:42,277
Speaker SPEAKER_01: And I figured out a way of displaying all the weights in a small neural network.

234
00:18:42,297 --> 00:18:45,361
Speaker SPEAKER_01: And I made it so if you press the left button, the weight got a little bit smaller.

235
00:18:45,381 --> 00:18:47,663
Speaker SPEAKER_01: And if you press the right button, the weight got a little bit bigger.

236
00:18:48,444 --> 00:18:52,009
Speaker SPEAKER_01: And if you press the middle button, you could see what the value of the weight was.

237
00:18:52,461 --> 00:18:53,748
Speaker SPEAKER_01: will print out the value of the weight.

238
00:18:54,471 --> 00:18:57,888
Speaker SPEAKER_01: And I tried fiddling around with neural nets, adjusting the weights.

239
00:18:57,970 --> 00:18:59,416
Speaker SPEAKER_01: It's really difficult.

240
00:18:59,498 --> 00:19:01,681
Speaker SPEAKER_01: Back prop is much better.

241
00:19:02,402 --> 00:19:09,710
Speaker SPEAKER_00: Well, we'll have to wait for a next level AI that is even smarter than Geoffrey Hinton to figure out how to do this.

242
00:19:11,251 --> 00:19:13,013
Speaker SPEAKER_00: Let's talk a little bit about some AI for good.

243
00:19:13,335 --> 00:19:16,519
Speaker SPEAKER_00: You've often talked about the benefits that will come to the medical field.

244
00:19:16,818 --> 00:19:24,067
Speaker SPEAKER_00: And when you go through the SDGs, it seems like good health and medicine is an area where you feel that AI will have a lot of benefits.

245
00:19:24,107 --> 00:19:24,587
Speaker SPEAKER_00: Is that fair?

246
00:19:24,808 --> 00:19:27,632
Speaker SPEAKER_00: And tell me why.

247
00:19:28,726 --> 00:19:29,626
Speaker SPEAKER_01: Yeah, I'm a bit stumped.

248
00:19:29,666 --> 00:19:30,567
Speaker SPEAKER_01: It's just obvious why.

249
00:19:31,169 --> 00:19:34,473
Speaker SPEAKER_01: It's going to be much better at interpreting medical images.

250
00:19:35,013 --> 00:19:41,583
Speaker SPEAKER_01: In 2016, I said that by 2021, it will be much better than clinicians at interpreting medical images.

251
00:19:41,943 --> 00:19:42,565
Speaker SPEAKER_01: And I was wrong.

252
00:19:42,644 --> 00:19:47,991
Speaker SPEAKER_01: It's going to take another five to 10 years, partly because medicine is very slow to take up new things.

253
00:19:49,093 --> 00:19:52,417
Speaker SPEAKER_01: But also, I overestimated the rate of short-term progress.

254
00:19:53,640 --> 00:19:56,943
Speaker SPEAKER_01: So that's a wrong prediction I made.

255
00:19:56,923 --> 00:19:58,526
Speaker SPEAKER_01: But it clearly is getting better.

256
00:19:58,546 --> 00:20:05,194
Speaker SPEAKER_01: Now it's comparable with quite good medical experts at many kinds of medical images.

257
00:20:05,415 --> 00:20:06,676
Speaker SPEAKER_01: Not all of them, but many of them.

258
00:20:07,258 --> 00:20:08,500
Speaker SPEAKER_01: And it's getting better all the time.

259
00:20:09,201 --> 00:20:11,584
Speaker SPEAKER_01: And it can see much more data than any clinician.

260
00:20:12,105 --> 00:20:14,248
Speaker SPEAKER_01: So it's just obvious in the end it's going to get better.

261
00:20:14,688 --> 00:20:16,130
Speaker SPEAKER_01: I just thought it would happen a bit sooner.

262
00:20:16,490 --> 00:20:21,498
Speaker SPEAKER_01: But it's also much better at things like combining lots and lots of data about a patient.

263
00:20:21,518 --> 00:20:25,242
Speaker SPEAKER_01: Combining data about the genome, the results of all the medical tests.

264
00:20:25,222 --> 00:20:35,618
Speaker SPEAKER_01: I mean, I would really love it if my family doctor had seen 100 million patients and could remember stuff about them all, or had incorporated information from them all.

265
00:20:36,039 --> 00:20:48,439
Speaker SPEAKER_01: And so when I go in with some strange, weird symptoms, the doctor can immediately say what it is, because she's seen like 500 patients who are quite similar already in that 100 million that she's seen.

266
00:20:48,619 --> 00:20:50,843
Speaker SPEAKER_01: That's coming, and that's going to be amazing.

267
00:20:51,076 --> 00:20:58,411
Speaker SPEAKER_00: And so the future of the medical benefits then are A, doctors who have seen many more patients and trained on them.

268
00:20:58,711 --> 00:21:02,279
Speaker SPEAKER_00: B, specific tasks like analyzing images.

269
00:21:02,760 --> 00:21:04,284
Speaker SPEAKER_00: And what about scientific breakthroughs?

270
00:21:04,644 --> 00:21:08,712
Speaker SPEAKER_00: I like the stuff that your old colleagues are working for on AlphaFold 3, AlphaFold 2.

271
00:21:09,384 --> 00:21:10,707
Speaker SPEAKER_01: Of course there's going to be lots of those.

272
00:21:10,787 --> 00:21:18,297
Speaker SPEAKER_01: It's going to be wonderful for doing things like understanding what goes on, as well as designing new drugs.

273
00:21:18,356 --> 00:21:21,560
Speaker SPEAKER_01: Obviously it's going to help with designing new drugs.

274
00:21:21,820 --> 00:21:23,743
Speaker SPEAKER_01: I think Demis is a big believer in that now.

275
00:21:25,346 --> 00:21:28,710
Speaker SPEAKER_01: But it's going to help us understand basic science.

276
00:21:28,690 --> 00:21:36,318
Speaker SPEAKER_01: And in many cases, there's large amounts of data that are not of the kind that we evolved to deal with.

277
00:21:36,338 --> 00:21:42,125
Speaker SPEAKER_01: So it's not visual data, it's not acoustic data, it's data in genomes and things.

278
00:21:42,906 --> 00:21:51,036
Speaker SPEAKER_01: And I think these AI systems are going to be much, much better at dealing with large volumes of data and seeing patterns in it and understanding it.

279
00:21:51,319 --> 00:21:57,867
Speaker SPEAKER_00: And that gets at one of my main critiques of the field of AI that I'm curious if you share.

280
00:21:57,887 --> 00:22:13,445
Speaker SPEAKER_00: I understand why so many researchers and some of your former students, many people who are Pioneers Field are working so hard to make machines that are just like humans and are indistinguishable from humans.

281
00:22:13,425 --> 00:22:22,517
Speaker SPEAKER_00: But there are also all these other people who are trying to build very specific things, like AlphaFold3, or try to figure out how to use AI to push forward cancer research.

282
00:22:23,458 --> 00:22:35,653
Speaker SPEAKER_00: Do you think I'm wrong to feel like there's too much weight and too much focus on the AGI side and not enough weight and focus on the specific scientific benefit side?

283
00:22:36,832 --> 00:22:38,634
Speaker SPEAKER_01: I think you may well be right about that.

284
00:22:39,035 --> 00:22:44,560
Speaker SPEAKER_01: For a long time I thought that AGI isn't going to be, there's not going to be a moment when suddenly these things get smarter than us.

285
00:22:45,101 --> 00:22:47,785
Speaker SPEAKER_01: They're going to get better than us at different things at different times.

286
00:22:48,525 --> 00:22:55,733
Speaker SPEAKER_01: So if you play chess or Go, it's clearly, there's no way a human is ever going to be as good as things like AlphaGo or AlphaZero.

287
00:22:56,453 --> 00:22:57,635
Speaker SPEAKER_01: They've way surpassed us.

288
00:22:59,178 --> 00:23:04,864
Speaker SPEAKER_01: We can learn a lot from the way they play those games, and people are learning that, but they're way ahead of us there.

289
00:23:05,012 --> 00:23:07,817
Speaker SPEAKER_01: and probably in coding they're already way ahead of me.

290
00:23:09,299 --> 00:23:10,382
Speaker SPEAKER_01: I'm not a very good coder.

291
00:23:11,763 --> 00:23:16,712
Speaker SPEAKER_01: So I think the idea that all of a sudden they're going to be better at everything is silly.

292
00:23:17,133 --> 00:23:22,261
Speaker SPEAKER_01: They're going to get better at different things at different times and physical manipulation is going to be one of the later things, I believe.

293
00:23:23,271 --> 00:23:39,752
Speaker SPEAKER_00: And so when your former students are asking for projects to pursue, do you often point them in the direction of say, we'll do more basic scientific research, push for more discoveries, as opposed to continuing to go for human-like intelligence?

294
00:23:39,772 --> 00:23:44,298
Speaker SPEAKER_01: My former students are now all so old that they don't ask me anymore.

295
00:23:46,580 --> 00:23:50,145
Speaker SPEAKER_00: His former students run basically every AI company in the world.

296
00:23:50,165 --> 00:23:52,788
Speaker SPEAKER_00: So that was like kind of a subtle way to get at that question.

297
00:23:53,089 --> 00:23:53,671
Speaker SPEAKER_00: We'll let that be.

298
00:23:53,911 --> 00:23:55,392
Speaker SPEAKER_00: So back to AI for good.

299
00:23:56,573 --> 00:24:10,866
Speaker SPEAKER_00: Looking at the SDGs, looking at the ambitions of the people in the room, do you feel like AI will transform education in a way that helps equity, particularly as these systems become fluent in every language on earth?

300
00:24:12,028 --> 00:24:12,327
Speaker SPEAKER_01: Yes.

301
00:24:13,429 --> 00:24:15,250
Speaker SPEAKER_01: So let me give you a little story.

302
00:24:15,371 --> 00:24:22,017
Speaker SPEAKER_01: When I was at school, my father insisted I learn German because he thought that was going to be the language of science.

303
00:24:22,789 --> 00:24:30,222
Speaker SPEAKER_01: That was because in chemistry, German sort of was the language of science, I believe, in the middle part of the last century, or the early part.

304
00:24:31,005 --> 00:24:32,227
Speaker SPEAKER_01: And I wasn't very good at German.

305
00:24:32,907 --> 00:24:34,270
Speaker SPEAKER_01: I didn't do very well in it.

306
00:24:34,852 --> 00:24:41,202
Speaker SPEAKER_01: And so my parents got me a private tutor, and pretty soon I was top of the class in German.

307
00:24:41,909 --> 00:24:55,944
Speaker SPEAKER_01: Private tutors are just much more efficient than sitting in a class listening to broadcasts by the teacher because the private tutor can see exactly what it is you misunderstand and give you just that little bit of information you need to understand it correctly.

308
00:24:56,786 --> 00:24:59,148
Speaker SPEAKER_01: And so I think everybody's gonna get private tutors.

309
00:24:59,489 --> 00:25:04,194
Speaker SPEAKER_01: And until now, private tutors were the domain of the rich or the middle-class and ambitious.

310
00:25:06,396 --> 00:25:08,378
Speaker SPEAKER_01: So in that sense, it's gonna help a whole lot.

311
00:25:08,400 --> 00:25:11,442
Speaker SPEAKER_01: And I think the Khan Academy believes that too.

312
00:25:12,097 --> 00:25:13,118
Speaker SPEAKER_00: That's a huge thing.

313
00:25:13,138 --> 00:25:23,670
Speaker SPEAKER_00: I mean, if everyone has these incredibly capable private tutors, they can speak their languages, which we'll get at some point, God willing, soon.

314
00:25:23,690 --> 00:25:25,332
Speaker SPEAKER_00: It's been a big topic here.

315
00:25:26,432 --> 00:25:29,316
Speaker SPEAKER_00: Don't you see the world becoming more equal?

316
00:25:30,758 --> 00:25:32,078
Speaker SPEAKER_01: In that sense, yes.

317
00:25:32,359 --> 00:25:36,384
Speaker SPEAKER_01: In terms of educational opportunities, I think it will become more equal.

318
00:25:37,105 --> 00:25:40,147
Speaker SPEAKER_01: The elite universities aren't gonna like this,

319
00:25:41,173 --> 00:25:43,676
Speaker SPEAKER_01: I think it will become more equal, yeah.

320
00:25:43,717 --> 00:25:44,218
Speaker SPEAKER_00: We're not here.

321
00:25:44,258 --> 00:25:46,000
Speaker SPEAKER_00: We're more interested in the future of humanity.

322
00:25:46,020 --> 00:25:49,125
Speaker SPEAKER_00: This is not, you know, AI for elite universities.

323
00:25:49,184 --> 00:25:49,986
Speaker SPEAKER_00: It's AI for good.

324
00:25:50,006 --> 00:25:52,328
Speaker SPEAKER_00: So I think we can take this as a win on this stage.

325
00:25:52,970 --> 00:25:53,431
Speaker SPEAKER_00: Absolutely.

326
00:25:53,671 --> 00:26:08,270
Speaker SPEAKER_00: But there was a gap there in your answer suggesting that you feel like AI overall won't be a net force for equality, may in fact be net-net a force for inequality.

327
00:26:08,411 --> 00:26:10,354
Speaker SPEAKER_00: Was I wrong to read that into your answer?

328
00:26:11,431 --> 00:26:15,516
Speaker SPEAKER_01: Well, we live in a capitalist system, and the capitalist systems have delivered a lot for us.

329
00:26:16,237 --> 00:26:18,278
Speaker SPEAKER_01: But we know some things about capitalist systems.

330
00:26:18,799 --> 00:26:30,795
Speaker SPEAKER_01: If you look at things like big oil, or big tobacco, or asbestos, or all sorts of other things, we know that in capitalist systems, people are trying to make profits.

331
00:26:31,675 --> 00:26:39,345
Speaker SPEAKER_01: And you need strong regulation so that in their attempts to make profits, they don't screw up the environment, for example.

332
00:26:40,422 --> 00:26:44,788
Speaker SPEAKER_01: We clearly need that for AI, and we're not getting it nearly fast enough.

333
00:26:45,830 --> 00:26:53,824
Speaker SPEAKER_01: So if you look at what Sam Altman said yesterday, he sort of gave the impression that, yeah, they're very concerned about safety and so on.

334
00:26:54,364 --> 00:26:56,227
Speaker SPEAKER_01: But we've now had an experiment on that.

335
00:26:56,708 --> 00:27:02,817
Speaker SPEAKER_01: We've seen the results of an experiment where you pit safety against profits.

336
00:27:02,798 --> 00:27:05,401
Speaker SPEAKER_01: Now the experiment was done in rather bad conditions.

337
00:27:05,500 --> 00:27:15,212
Speaker SPEAKER_01: It was done when all of the employees of OpenAI were about to be able to turn their paper money into real money because there was a big funding round coming and they were going to be able to sell their shares.

338
00:27:15,814 --> 00:27:18,576
Speaker SPEAKER_01: So it wasn't an experiment done in ideal circumstances.

339
00:27:19,178 --> 00:27:21,922
Speaker SPEAKER_01: But it's clear who won out of profits and safety.

340
00:27:22,521 --> 00:27:24,704
Speaker SPEAKER_01: And it's clear now that

341
00:27:25,376 --> 00:27:28,881
Speaker SPEAKER_01: what OpenAI has got, it's got a new safety group.

342
00:27:30,342 --> 00:27:34,326
Speaker SPEAKER_01: It's employed some economists, or at least one economist.

343
00:27:34,346 --> 00:27:36,909
Speaker SPEAKER_01: I think of economists as the high priests of capitalism.

344
00:27:37,789 --> 00:27:46,818
Speaker SPEAKER_01: And I don't think it's gonna worry about the existential threat nearly as much as Ilya and the people working with him did.

345
00:27:51,022 --> 00:27:53,645
Speaker SPEAKER_01: I also think,

346
00:27:54,148 --> 00:27:59,663
Speaker SPEAKER_01: The problem is that capitalism is about making profits, which I'm not totally against.

347
00:27:59,702 --> 00:28:08,344
Speaker SPEAKER_01: I mean, it's done wonderful things for us, that drive, but it needs to be regulated so it doesn't also cause bad things.

348
00:28:10,045 --> 00:28:11,207
Speaker SPEAKER_01: create a lot of wealth.

349
00:28:11,788 --> 00:28:16,959
Speaker SPEAKER_01: I think it's clear to almost everybody that AI is going to increase productivity.

350
00:28:17,599 --> 00:28:21,407
Speaker SPEAKER_01: The question is, where is that additional wealth going to go?

351
00:28:21,448 --> 00:28:24,053
Speaker SPEAKER_01: And I don't think it's going to go to poor people.

352
00:28:24,073 --> 00:28:25,415
Speaker SPEAKER_01: I think it's going to go to rich people.

353
00:28:25,435 --> 00:28:27,941
Speaker SPEAKER_01: So I think it's going to increase the gap between rich and poor.

354
00:28:28,280 --> 00:28:28,982
Speaker SPEAKER_01: That's what I believe.

355
00:28:30,365 --> 00:28:31,647
Speaker SPEAKER_00: Do you not have hope?

356
00:28:33,400 --> 00:28:46,773
Speaker SPEAKER_00: Seems like you're saying AI, its powers, the fact that it will probably be a small number of corporations because of the resources needed to train these large language models, that AI is kind of incompatible with capitalism and equality.

357
00:28:47,414 --> 00:29:02,951
Speaker SPEAKER_00: Do you not have hope that some of what we were just talking about, equity and education, the ability of everybody to have access to extremely powerful machines, if not entirely as powerful as the most expensive machines, do you not have hope that that will counterbalance it?

358
00:29:04,180 --> 00:29:12,672
Speaker SPEAKER_01: There is some hope of that, but sort of for most of my life I thought that as people got more educated, they'd get more sensible.

359
00:29:13,952 --> 00:29:15,996
Speaker SPEAKER_01: And it hasn't really happened.

360
00:29:16,436 --> 00:29:22,424
Speaker SPEAKER_01: If you look at the Republican Party now, they're just spewing lies and just crazy lies.

361
00:29:23,738 --> 00:29:27,101
Speaker SPEAKER_00: This is a good moment.

362
00:29:27,121 --> 00:29:28,303
Speaker SPEAKER_00: Let's go into the question.

363
00:29:29,384 --> 00:29:33,428
Speaker SPEAKER_00: I want to get into the question of how to do regulation and your ideas for that.

364
00:29:33,907 --> 00:29:38,532
Speaker SPEAKER_00: But I also want to get through some of your other fears about where AI is taking us.

365
00:29:39,193 --> 00:29:49,964
Speaker SPEAKER_00: Why don't you here lay out maybe the one or two things, not that you're worried about, kind of the existential fears that you're worried about for the economy, but your fears for the next 12 months.

366
00:29:51,446 --> 00:29:57,214
Speaker SPEAKER_01: Okay, so I'm worried about something I know very little about, which is cybercrime.

367
00:29:58,355 --> 00:30:04,324
Speaker SPEAKER_01: I heard Dawn Song speak recently and she said that phishing attacks went up 1200% last year.

368
00:30:05,164 --> 00:30:16,240
Speaker SPEAKER_01: And of course they're getting much, much better because you can't recognize them anymore by spelling mistakes or funny foreign syntax because they're all done by chatbots now.

369
00:30:17,266 --> 00:30:19,009
Speaker SPEAKER_01: or a lot of them are.

370
00:30:19,029 --> 00:30:21,433
Speaker SPEAKER_01: So I'm worried about that, but I don't know much about that.

371
00:30:21,974 --> 00:30:26,443
Speaker SPEAKER_01: Another thing I'm worried about a lot is fake videos corrupting elections.

372
00:30:27,246 --> 00:30:34,380
Speaker SPEAKER_01: I think it's fairly obvious that just before each election, there's going to be lots of fake videos when there isn't time to refute them.

373
00:30:34,781 --> 00:30:39,849
Speaker SPEAKER_01: And I actually think it would be a good idea to inoculate the public against fake videos.

374
00:30:40,651 --> 00:30:41,933
Speaker SPEAKER_01: So treat them like a disease.

375
00:30:41,993 --> 00:30:47,162
Speaker SPEAKER_01: And the way you inoculate against a disease is you give a kind of attenuated version of it.

376
00:30:47,942 --> 00:30:50,326
Speaker SPEAKER_01: And so I think

377
00:30:50,307 --> 00:30:53,109
Speaker SPEAKER_01: There's a bunch of philanthropic billionaires out there.

378
00:30:53,130 --> 00:30:59,718
Speaker SPEAKER_01: I think they should spend their money, or some of it, putting on the airwaves a month or so before these elections.

379
00:31:00,417 --> 00:31:03,142
Speaker SPEAKER_01: Lots of fake videos that are very convincing.

380
00:31:03,162 --> 00:31:05,403
Speaker SPEAKER_01: At the end, they say, but this is fake.

381
00:31:06,085 --> 00:31:09,087
Speaker SPEAKER_01: That wasn't Trump speaking, and Trump never said anything like that.

382
00:31:10,410 --> 00:31:13,073
Speaker SPEAKER_01: Or that wasn't Biden speaking, and Biden never said anything like that.

383
00:31:13,432 --> 00:31:15,015
Speaker SPEAKER_01: This was a fake video.

384
00:31:15,738 --> 00:31:18,961
Speaker SPEAKER_01: Then you'll make people suspicious of more or less everything.

385
00:31:19,522 --> 00:31:21,746
Speaker SPEAKER_01: That's a good idea if there's a lot of fake videos around.

386
00:31:22,626 --> 00:31:25,671
Speaker SPEAKER_01: But then you need a way for people to check whether a video is real.

387
00:31:27,653 --> 00:31:32,318
Speaker SPEAKER_01: That's an easier problem than checking whether it's fake, if they're willing to put in like 30 seconds of work.

388
00:31:32,960 --> 00:31:38,386
Speaker SPEAKER_01: So Jan Tallinn suggested, for example, you could have a QR code at the beginning of each video.

389
00:31:38,406 --> 00:31:41,391
Speaker SPEAKER_01: You could use the QR code to get to a website.

390
00:31:41,624 --> 00:31:47,536
Speaker SPEAKER_01: If the same video's on the website, you know that website claims this video's real.

391
00:31:48,116 --> 00:31:52,967
Speaker SPEAKER_01: And now you've reduced the problem of saying whether a video's real to the problem whether that website's real.

392
00:31:53,307 --> 00:31:54,388
Speaker SPEAKER_01: And websites are unique.

393
00:31:55,050 --> 00:32:03,227
Speaker SPEAKER_01: So if you're sure it really is the Trump campaign website, then you know the Trump campaign really put out that video.

394
00:32:03,460 --> 00:32:05,402
Speaker SPEAKER_00: So could we just pause for a second?

395
00:32:05,663 --> 00:32:07,684
Speaker SPEAKER_00: This is why I love interviewing Geoffrey Hinton.

396
00:32:07,885 --> 00:32:22,243
Speaker SPEAKER_00: We've gone from new theories of consciousness, an incredibly controversial theory about subjective feelings, to an idea that we should inoculate the public against fake news by pumping out low dosages of fake videos.

397
00:32:22,423 --> 00:32:26,368
Speaker SPEAKER_00: Let's go to the first part, because your solution there had, I think, if I heard correctly, two parts.

398
00:32:26,890 --> 00:32:30,134
Speaker SPEAKER_00: So the first is inoculate the public against fake videos.

399
00:32:30,153 --> 00:32:32,656
Speaker SPEAKER_00: So you mean specifically?

400
00:32:32,636 --> 00:32:42,670
Speaker SPEAKER_00: Someone should create millions of short, fake but not very damaging videos and put them on Twitter threads?

401
00:32:42,971 --> 00:32:44,512
Speaker SPEAKER_01: They could be moderately damaging.

402
00:32:44,532 --> 00:32:47,696
Speaker SPEAKER_01: They're not going to be convincing unless they look like real political advertisements.

403
00:32:50,140 --> 00:32:58,611
Speaker SPEAKER_01: But at the end of the advertisement, they're short advertisements so you hope people watch to the end, at the end of the advertisement it says this was fake.

404
00:32:59,249 --> 00:33:01,573
Speaker SPEAKER_01: That's the attenuation that allows you to deal with it.

405
00:33:01,753 --> 00:33:02,074
Speaker SPEAKER_00: I see.

406
00:33:02,213 --> 00:33:05,420
Speaker SPEAKER_00: So you watch it and you're like, ah, this proves my point.

407
00:33:05,461 --> 00:33:06,682
Speaker SPEAKER_00: Oh, wait, that was fake.

408
00:33:06,903 --> 00:33:08,266
Speaker SPEAKER_00: And then you're more distrusting.

409
00:33:08,286 --> 00:33:08,665
Speaker SPEAKER_00: I like this.

410
00:33:08,767 --> 00:33:08,987
Speaker SPEAKER_00: OK.

411
00:33:09,126 --> 00:33:09,587
Speaker SPEAKER_00: Exactly.

412
00:33:09,828 --> 00:33:13,315
Speaker SPEAKER_00: Then the second part is that every video should have a QR code.

413
00:33:13,635 --> 00:33:14,497
Speaker SPEAKER_00: And so you see something.

414
00:33:14,557 --> 00:33:15,880
Speaker SPEAKER_00: Now you're aware of it.

415
00:33:16,401 --> 00:33:17,983
Speaker SPEAKER_00: And so you scan the little QR code.

416
00:33:18,023 --> 00:33:18,965
Speaker SPEAKER_00: You go to the website.

417
00:33:19,185 --> 00:33:19,807
Speaker SPEAKER_00: Ah, it's real.

418
00:33:19,846 --> 00:33:20,788
Speaker SPEAKER_00: It's on a real website.

419
00:33:20,888 --> 00:33:22,471
Speaker SPEAKER_00: That's the idea?

420
00:33:22,451 --> 00:33:29,300
Speaker SPEAKER_01: It's not sufficient just that it takes you to a real website, because fake videos could take you to the same real website.

421
00:33:30,102 --> 00:33:31,744
Speaker SPEAKER_01: The video has to be there.

422
00:33:31,765 --> 00:33:34,147
Speaker SPEAKER_01: The same video has to be there on that website.

423
00:33:34,628 --> 00:33:35,028
Speaker SPEAKER_00: Fair enough.

424
00:33:35,730 --> 00:33:37,893
Speaker SPEAKER_00: Let's talk about biases and how to prevent them.

425
00:33:37,952 --> 00:33:43,279
Speaker SPEAKER_00: One of the risks that people talk about is that AI systems trained on biased data will have biased results.

426
00:33:44,422 --> 00:33:48,847
Speaker SPEAKER_00: Let's go back to medicine, where you've made the compelling case that

427
00:33:48,827 --> 00:33:51,172
Speaker SPEAKER_00: Net AI will be hugely beneficial.

428
00:33:51,732 --> 00:34:05,974
Speaker SPEAKER_00: You can imagine a doctor who has just been trained on the medical records of people in the United States not giving the right medical advice to somebody from Zambia because they're different medical concerns, different DNA, et cetera.

429
00:34:06,435 --> 00:34:10,481
Speaker SPEAKER_00: How worried are you about this problem and what does one do to fix it?

430
00:34:11,844 --> 00:34:17,293
Speaker SPEAKER_01: Okay, so I'm less worried about the bias and discrimination problems than I am about the other problems.

431
00:34:17,974 --> 00:34:22,101
Speaker SPEAKER_01: And I am aware that I'm an old white male, so that might have something to do with it.

432
00:34:22,400 --> 00:34:23,882
Speaker SPEAKER_01: It hasn't happened to me much.

433
00:34:25,726 --> 00:34:28,791
Speaker SPEAKER_01: But I think if you make the goal

434
00:34:28,771 --> 00:34:34,561
Speaker SPEAKER_01: Replace biased systems or biased people with systems that are less biased.

435
00:34:34,581 --> 00:34:38,208
Speaker SPEAKER_01: Not systems that are unbiased, but systems that are less biased.

436
00:34:38,228 --> 00:34:39,710
Speaker SPEAKER_01: That seems eminently doable.

437
00:34:40,552 --> 00:34:49,226
Speaker SPEAKER_01: So if I have data of all white men deciding whether young black women should get mortgages, I'm going to expect there to be some bias there.

438
00:34:49,206 --> 00:34:56,940
Speaker SPEAKER_01: Once I've trained an AI system on that data, I can actually freeze the weights and I can go and examine the bias in a way that I can't with people.

439
00:34:57,641 --> 00:35:04,311
Speaker SPEAKER_01: With people, if you try and examine their biases, you get the kind of Volkswagen effect that they realize you're examining them and they behave in a quite different way.

440
00:35:06,916 --> 00:35:10,842
Speaker SPEAKER_01: I just invented the name, the Volkswagen effect, but there you go.

441
00:35:12,646 --> 00:35:19,577
Speaker SPEAKER_01: With AI systems, if you freeze the weights, you can measure the bias much better and do things to overcome it, ameliorate it.

442
00:35:19,976 --> 00:35:21,539
Speaker SPEAKER_01: You'll never get rid of it completely.

443
00:35:22,041 --> 00:35:23,141
Speaker SPEAKER_01: It's too difficult, I think.

444
00:35:24,403 --> 00:35:30,353
Speaker SPEAKER_01: But suppose you make the target, make the new system considerably less biased than the system it's replacing.

445
00:35:30,813 --> 00:35:32,476
Speaker SPEAKER_01: I think that's eminently doable.

446
00:35:32,693 --> 00:35:33,193
Speaker SPEAKER_00: Remarkable.

447
00:35:33,594 --> 00:35:57,755
Speaker SPEAKER_00: And do you feel as though the focus in the industry on biases, which has been a major topic, has underappreciated the fact that actually these systems could end up being more just and that really we should just, instead of saying we've got to wipe out all the biases, we should just say, let's make them less biased than humans and go from there.

448
00:35:58,951 --> 00:36:00,773
Speaker SPEAKER_01: I think that would be rational.

449
00:36:00,932 --> 00:36:04,657
Speaker SPEAKER_01: I don't think politically... I'm not sure it's politically acceptable.

450
00:36:04,677 --> 00:36:07,842
Speaker SPEAKER_01: I mean, suppose you said, we're going to introduce self-driving cars.

451
00:36:08,461 --> 00:36:11,826
Speaker SPEAKER_01: They kill lots of people on the road, but only half as many as ordinary cars.

452
00:36:12,547 --> 00:36:13,829
Speaker SPEAKER_01: I don't think you get away with that.

453
00:36:14,128 --> 00:36:16,351
Speaker SPEAKER_01: They have to kill almost nobody for you to get away with it.

454
00:36:18,574 --> 00:36:24,141
Speaker SPEAKER_01: So, I think there's a political problem there to do with accepting new technology in a rational way.

455
00:36:26,202 --> 00:36:26,764
Speaker SPEAKER_01: But...

456
00:36:28,516 --> 00:36:33,882
Speaker SPEAKER_01: I think we should aim for systems significantly less biased and be content with that.

457
00:36:34,824 --> 00:36:42,592
Speaker SPEAKER_00: All right, let's go to what you've described in interviews as the biggest risk with AI, which is that they would get sub-goals, right?

458
00:36:42,853 --> 00:36:49,201
Speaker SPEAKER_00: And that they would get a goal beyond the initial goal given to them by their creators and by their users.

459
00:36:49,661 --> 00:36:56,429
Speaker SPEAKER_00: Explain A, what you think a sub-goal is, B, why that's so bad, and C, what we can do about it.

460
00:36:57,742 --> 00:37:10,762
Speaker SPEAKER_01: So an innocuous kind of sub-goal is, if I want an AI agent to plan a trip for me, I say, you've got to get me to North America, suppose I'm in Europe, I say, you have to get me to North America.

461
00:37:11,543 --> 00:37:15,550
Speaker SPEAKER_01: So it will have a sub-goal of figuring out how to get me to the airport.

462
00:37:16,356 --> 00:37:18,400
Speaker SPEAKER_01: That's just a classic kind of sub-goal.

463
00:37:19,143 --> 00:37:22,349
Speaker SPEAKER_01: And if you want to make intelligent agents, they have to have sub-goals like that.

464
00:37:22,690 --> 00:37:27,902
Speaker SPEAKER_01: They have to be able to focus on one little part of the problem and solve that without worrying about everything.

465
00:37:29,231 --> 00:37:38,364
Speaker SPEAKER_01: Now, as soon as you have a system that can create its own sub-goals, there's a particular sub-goal that's very helpful.

466
00:37:38,824 --> 00:37:41,329
Speaker SPEAKER_01: And that sub-goal is get more control.

467
00:37:42,070 --> 00:37:46,996
Speaker SPEAKER_01: If I get more control, I can be better at doing all sorts of things that the user wants me to do.

468
00:37:47,777 --> 00:37:49,760
Speaker SPEAKER_01: So it just makes sense to get more control.

469
00:37:51,222 --> 00:37:53,264
Speaker SPEAKER_01: And the worry is that

470
00:37:54,088 --> 00:38:01,318
Speaker SPEAKER_01: Eventually an AI system will figure out, look, if I could control everything, I could give these silly humans what they want without them having any control at all.

471
00:38:02,380 --> 00:38:05,103
Speaker SPEAKER_01: And that's probably true.

472
00:38:06,286 --> 00:38:15,117
Speaker SPEAKER_01: But then the worry is, suppose the AI system ever decided that it was a little bit more interested in itself than it was in the humans, we'd be done for.

473
00:38:15,958 --> 00:38:20,146
Speaker SPEAKER_00: And in fact, even before we're done for, as you were describing that, I got quite worried.

474
00:38:20,166 --> 00:38:23,490
Speaker SPEAKER_00: So you have an AI, and the goal is get Nick to the airport on time.

475
00:38:24,132 --> 00:38:26,876
Speaker SPEAKER_00: The AI is, in some future state, all powerful.

476
00:38:27,257 --> 00:38:32,525
Speaker SPEAKER_00: Well, the best way to get Nick to the airport is probably immobilize Nick, put his hands behind his back, just throw him in a car.

477
00:38:32,565 --> 00:38:35,251
Speaker SPEAKER_00: It's much more efficient, because then he doesn't talk to anybody on the way out.

478
00:38:35,711 --> 00:38:39,637
Speaker SPEAKER_00: So you can see these subgoals going terribly wrong.

479
00:38:40,833 --> 00:38:43,295
Speaker SPEAKER_01: Yeah, but remember it's a very intelligent system.

480
00:38:43,835 --> 00:38:49,402
Speaker SPEAKER_01: By then it should be able to not go wrong in ways that are obviously against human interests.

481
00:38:49,422 --> 00:38:54,327
Speaker SPEAKER_01: It should be trained in such a way that it is interested in human interests.

482
00:38:54,648 --> 00:38:56,829
Speaker SPEAKER_00: All right, excellent, because I don't really want that to happen to me.

483
00:38:57,451 --> 00:39:01,574
Speaker SPEAKER_00: All right, let's go through, I want to go through some regulatory frameworks.

484
00:39:01,735 --> 00:39:07,802
Speaker SPEAKER_00: I have a question for you that I think you may be able to answer better than most anybody, which is,

485
00:39:09,182 --> 00:39:32,186
Speaker SPEAKER_00: One of the things that holds back the big AI companies and the AI researchers from working on safety or slowing down, it isn't just power, it isn't just money, it's the dream of doing something great, or as coders say, finding something sweet.

486
00:39:32,166 --> 00:39:44,942
Speaker SPEAKER_00: Tell me about a moment so regulators can understand this where you as a developer on the cusp of a breakthrough, what that feels like and how regulators should think about that as they think about policy.

487
00:39:47,005 --> 00:39:52,891
Speaker SPEAKER_01: I'm not sure I can give you good insight into that.

488
00:39:54,407 --> 00:40:04,963
Speaker SPEAKER_01: For a researcher, for a researcher who's driven by curiosity, working on how to make something more capable, to introduce some dramatic new capability.

489
00:40:05,884 --> 00:40:17,059
Speaker SPEAKER_01: Like a previous speaker talked about the idea that you learn a model for one language, you learn a model for a different language, and then you can take the internal representations and rotate them onto each other.

490
00:40:18,260 --> 00:40:19,222
Speaker SPEAKER_01: That's kind of amazing.

491
00:40:19,242 --> 00:40:22,085
Speaker SPEAKER_01: You get a lot of joy out of seeing something like that.

492
00:40:22,065 --> 00:40:27,775
Speaker SPEAKER_01: And I'm not sure you get the same level of joy out of working on safety.

493
00:40:29,358 --> 00:40:31,260
Speaker SPEAKER_01: So I sort of agree with you there.

494
00:40:31,601 --> 00:40:33,625
Speaker SPEAKER_01: However, working on safety is very important.

495
00:40:34,347 --> 00:40:39,996
Speaker SPEAKER_01: And there are some very good researchers who are keen to spend their careers working on safety.

496
00:40:40,356 --> 00:40:45,846
Speaker SPEAKER_01: And I think we should do everything we can to make that career path be a rewarding career path.

497
00:40:46,400 --> 00:40:51,737
Speaker SPEAKER_00: So you would say to the young enterprise encoders in this room, this is God's work.

498
00:40:52,400 --> 00:40:59,362
Speaker SPEAKER_00: Work on safety, or that would be a good thing to do, perhaps even better than being a plumber.

499
00:41:00,693 --> 00:41:04,157
Speaker SPEAKER_01: Oh yes, if you could make progress on safety, that would be amazing, yes.

500
00:41:04,516 --> 00:41:06,639
Speaker SPEAKER_00: All right, excellent, I'll talk to my kids.

501
00:41:08,641 --> 00:41:10,603
Speaker SPEAKER_00: Let's talk about the regulatory frameworks you want.

502
00:41:11,264 --> 00:41:17,590
Speaker SPEAKER_00: One thing you've talked about is, I believe you went to 10 Downing Street and said that the UK should have universal basic income.

503
00:41:18,012 --> 00:41:23,797
Speaker SPEAKER_00: Will you explain why, and then explain the other regulations that you recommended there?

504
00:41:25,043 --> 00:41:34,072
Speaker SPEAKER_01: Yes, it was, I got invited to attend Downing Street and there were a whole bunch of Sunax advisors, his chief of staff and a whole bunch of other people who advised him on AI.

505
00:41:34,793 --> 00:41:36,094
Speaker SPEAKER_01: And I talked to them for quite a while.

506
00:41:36,976 --> 00:41:38,998
Speaker SPEAKER_01: At that point, I wasn't sitting down.

507
00:41:39,099 --> 00:41:49,829
Speaker SPEAKER_01: So I walked into this room and there was this big group of advisors and I spent a while talking to them, including saying, I thought that I wasn't confident AI would create as many jobs as it got rid of.

508
00:41:50,510 --> 00:41:54,054
Speaker SPEAKER_01: And so they'd need something like universal basic income.

509
00:41:55,570 --> 00:42:05,822
Speaker SPEAKER_01: When the meeting came to an end, I started to go out the door and realized I'd been standing directly in front of a huge picture of Margaret Thatcher and explaining to people they should have socialism.

510
00:42:06,362 --> 00:42:08,804
Speaker SPEAKER_01: I'm in front of a big picture of Margaret Thatcher, which is quite funny.

511
00:42:11,188 --> 00:42:11,407
Speaker SPEAKER_00: All right.

512
00:42:11,967 --> 00:42:14,650
Speaker SPEAKER_00: So universal basic income.

513
00:42:14,891 --> 00:42:20,498
Speaker SPEAKER_00: What else is in the Geoffrey Hinton regulatory plan for a world with AI?

514
00:42:20,478 --> 00:42:29,074
Speaker SPEAKER_01: I think a very straightforward thing, which Sam Alton won't like, is this idea that comparable resources should be devoted to safety.

515
00:42:30,077 --> 00:42:39,215
Speaker SPEAKER_01: If you look at the statements made by at least one of the people who left OpenAI because it wasn't serious enough about safety, it was to do with resources.

516
00:42:41,581 --> 00:42:42,181
Speaker SPEAKER_01: I think

517
00:42:42,802 --> 00:42:49,771
Speaker SPEAKER_01: the government, if it can, should insist that more resources be put into safety.

518
00:42:50,152 --> 00:42:51,574
Speaker SPEAKER_01: It's a bit like with an oil company.

519
00:42:51,614 --> 00:43:00,704
Speaker SPEAKER_01: You can insist they put significant resources into cleaning up waste dumps and cleaning up the stuff they spew out.

520
00:43:01,246 --> 00:43:02,266
Speaker SPEAKER_01: And governments can do that.

521
00:43:02,286 --> 00:43:04,610
Speaker SPEAKER_01: And if governments don't do that, they just keep spewing stuff out.

522
00:43:05,190 --> 00:43:12,539
Speaker SPEAKER_01: That's clearly the role of government, to make capitalism work without destroying everything.

523
00:43:13,010 --> 00:43:14,110
Speaker SPEAKER_01: that's what they should be doing.

524
00:43:15,873 --> 00:43:17,355
Speaker SPEAKER_00: But there's a simpler way to do that, right?

525
00:43:17,394 --> 00:43:23,501
Speaker SPEAKER_00: I mean, government can regulate these big companies and say you have to work on safety and we need to audit and make sure you're doing that.

526
00:43:24,362 --> 00:43:35,494
Speaker SPEAKER_00: But the government could also just fund a lot of safety research and take a lot of government data and make it available to safety researchers and fund a bunch of compute and give that to safety researchers.

527
00:43:35,554 --> 00:43:42,983
Speaker SPEAKER_00: So should the government officials here all be setting up AI, should the UN set up an AI safety institute?

528
00:43:44,650 --> 00:43:50,365
Speaker SPEAKER_01: I think the UN is rather strapped for funds, and the UN has to do things like feeding people in Gaza.

529
00:43:51,990 --> 00:43:53,835
Speaker SPEAKER_01: I'd rather it spent the money feeding people in Gaza.

530
00:43:53,856 --> 00:43:55,942
Speaker SPEAKER_01: I don't think the UN has the resources.

531
00:43:56,784 --> 00:44:00,094
Speaker SPEAKER_01: Maybe it should have the resources, but it doesn't.

532
00:44:00,074 --> 00:44:02,657
Speaker SPEAKER_01: Canada, I don't think, has the resources.

533
00:44:02,737 --> 00:44:10,070
Speaker SPEAKER_01: Canada's made a serious effort to put money into funding compute for universities and startups.

534
00:44:10,710 --> 00:44:17,641
Speaker SPEAKER_01: So they recently put $2 billion into that, which is a lot of money, especially for Canada.

535
00:44:17,661 --> 00:44:20,547
Speaker SPEAKER_01: But it's nothing compared with what the big companies can do.

536
00:44:21,588 --> 00:44:27,077
Speaker SPEAKER_01: Maybe countries like Saudi Arabia can put in comparable money, but I'm not so sure they're interested in safety.

537
00:44:27,445 --> 00:44:34,610
Speaker SPEAKER_00: So Jeff, we have one minute left, and I have 14 more questions, even though you gave wonderful, wonderful, brisk answers.

538
00:44:35,110 --> 00:44:36,797
Speaker SPEAKER_00: So I'm going to ask one big one at the end.

539
00:44:38,228 --> 00:44:42,777
Speaker SPEAKER_00: From all of this AI research, you've been studying how the brain works.

540
00:44:42,797 --> 00:44:45,161
Speaker SPEAKER_00: You have incredible theories about why we sleep.

541
00:44:46,304 --> 00:44:49,429
Speaker SPEAKER_00: If you get a chance to talk to Dr. Hinton, I advise you ask him about that.

542
00:44:50,512 --> 00:44:58,025
Speaker SPEAKER_00: What have we learned about the brain in the last year and a half in this explosion of AI that has surprised you?

543
00:44:59,844 --> 00:45:00,827
Speaker SPEAKER_01: Oh, that surprised me.

544
00:45:01,507 --> 00:45:06,695
Speaker SPEAKER_01: I'd rather go back a few years and say what's really surprised me is how good these big language models are.

545
00:45:07,577 --> 00:45:18,255
Speaker SPEAKER_01: I think I made the first language model in 1985 that used back propagation to try and predict the next word in a sequence of words.

546
00:45:18,735 --> 00:45:20,297
Speaker SPEAKER_01: The sequence are only three words long.

547
00:45:20,338 --> 00:45:26,527
Speaker SPEAKER_01: So it was a whole system only had a few thousand weights, but it was the first of that kind of model.

548
00:45:26,507 --> 00:45:35,324
Speaker SPEAKER_01: And at that time, I was very excited about the fact that it seemed to be able to unify two different theories of the meaning of a word.

549
00:45:35,965 --> 00:45:38,572
Speaker SPEAKER_01: One theory is that it's to do with its relations with other words.

550
00:45:39,052 --> 00:45:40,635
Speaker SPEAKER_01: That's the sort of de Sassure theory.

551
00:45:41,277 --> 00:45:45,445
Speaker SPEAKER_01: And the other theory that comes from psychologists is that it's a big set of semantic features.

552
00:45:45,862 --> 00:45:57,967
Speaker SPEAKER_01: And what we've done now is by learning embeddings and having interactions between the features of embeddings of different words or word fragments, we've managed to unify these two different theories of meaning.

553
00:45:58,427 --> 00:46:05,442
Speaker SPEAKER_01: And we now, I believe, have big language models that really understand what they're saying in pretty much the same way as people do.

554
00:46:05,422 --> 00:46:15,016
Speaker SPEAKER_01: So one final point I want to make is that the origin of these language models, you use backprop to predict the next word, was not to make a good technology.

555
00:46:15,318 --> 00:46:17,221
Speaker SPEAKER_01: It was to try and understand how people do it.

556
00:46:17,842 --> 00:46:24,873
Speaker SPEAKER_01: So I think the way in which people understand language, the best model we have of that is these big AI models.

557
00:46:25,833 --> 00:46:29,659
Speaker SPEAKER_01: So people who say, no, they don't really understand, that's nonsense.

558
00:46:29,719 --> 00:46:32,103
Speaker SPEAKER_01: They understand in the same way as we understand.

559
00:46:33,130 --> 00:46:34,753
Speaker SPEAKER_00: All right, well, we'll have to end on that note.

560
00:46:34,793 --> 00:46:42,574
Speaker SPEAKER_00: It makes me somewhat heartened to know that Jeff Hinton's incredible mind is in some way behind all the AI models that we use today.

561
00:46:42,594 --> 00:46:44,117
Speaker SPEAKER_00: Thank you so much, Dr. Hinton.

562
00:46:44,358 --> 00:46:45,400
Speaker SPEAKER_00: Thank you for joining us today.

563
00:46:46,384 --> 00:46:46,784
Speaker SPEAKER_01: Thank you.


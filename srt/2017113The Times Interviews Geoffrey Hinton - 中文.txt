1 00:00:01,415 --> 00:00:04,479 主持人 主持人_00：请欢迎杰弗里·辛顿和凯德·梅茨。
2 00:00:09,625 --> 00:00:25,126 主持人 主持人_01：与所有走上舞台的其他人不同，我们将保持站立，但这是有原因的。
3 00:00:25,512 --> 00:00:35,182 主持人 主持人_01：由于背部问题，杰弗里·辛顿最后一次坐下是在2005年6月，他说那是一个错误。
4 00:00:36,685 --> 00:00:38,546 主持人 主持人_01：所以今天我们不会犯同样的错误。
5 00:00:38,587 --> 00:00:39,328 说话人 SPEAKER_01：我们要站起来。
6 00:00:40,348 --> 00:00:51,942 说话人 SPEAKER_01：而且，今天我们讨论的几乎所有内容，我认为并不夸张，都可以追溯到杰夫的工作。
7 00:00:52,462 --> 00:01:06,917 说话人 SPEAKER_01：他并没有发明神经网络，但早在 80 年代，他所进行的研究对这一技术的演变至关重要，现在它正在重新发明图像识别、语音识别和语言翻译。
8 00:01:08,838 --> 00:01:12,582 说话人 SPEAKER_01：这正是过去五年所发生的事情。
9 00:01:13,504 --> 00:01:20,611 说话人 SPEAKER_01: 但因为我们把你请到这里，也许你可以向观众解释一下，因为我认为这有助于我们接下来讨论的方向，
10 00:01:20,828 --> 00:01:22,450 说话人 SPEAKER_01: 什么是神经网络。
11 00:01:24,052 --> 00:01:24,472 说话人 SPEAKER_00: 好的。
12 00:01:25,834 --> 00:01:31,082 说话人 SPEAKER_00: 大家都知道神经网络是如何工作的，但大家对此的了解程度各不相同。
13 00:01:32,004 --> 00:01:34,186 说话人 SPEAKER_00: 所以如果你知道，就关掉你的耳朵。
14 00:01:34,908 --> 00:01:40,536 说话人 SPEAKER_00: 神经网络基本上是一个你可以向它展示输入的系统。
15 00:01:42,058 --> 00:01:46,584 说话人 SPEAKER_00: 在这个系统中，有很多类似脑细胞的小过程。
16 00:01:47,206 --> 00:01:48,548 说话人 SPEAKER_00: 它们的工作方式有点像脑细胞。
17 00:01:48,947 --> 00:01:50,751 说话人 SPEAKER_00: 它们之间有联系。
18 00:01:51,018 --> 00:01:53,180 说话人 SPEAKER_00: 你会学习这些联系上的权重。
19 00:01:54,724 --> 00:01:58,790 说话人 SPEAKER_00: 通过改变联系上的权重，你改变了系统会给出的输出。
20 00:01:59,932 --> 00:02:03,296 说话人 SPEAKER_00: 所以神经网络的基本问题是我给你一个输入。
21 00:02:03,918 --> 00:02:05,962 说话人 SPEAKER_00: 我给你展示你应该产生的输出。
22 00:02:06,582 --> 00:02:10,549 说话人 SPEAKER_00: 神经网络需要找出如何改变所有这些连接强度。
23 00:02:10,568 --> 00:02:15,877 说话人 SPEAKER_00: 这有一个明显的算法来做这件事，就是你随机改变一个，看看它是否变得更好。
24 00:02:16,478 --> 00:02:17,699 说话人 SPEAKER_00: 这非常低效。
25 00:02:18,844 --> 00:02:28,475 说话者 SPEAKER_00：有一种稍微不那么明显的算法，就是你观察当前连接强度下输出的结果和你希望得到的结果之间的差异。
26 00:02:29,037 --> 00:02:34,842 说话者 SPEAKER_00：然后你进行一些微积分运算，确定对于每一个连接强度，应该如何调整它，以使输出更接近你想要的。
27 00:02:35,484 --> 00:02:38,608 说话者 SPEAKER_00：然后你稍微调整所有连接强度，以使输出更好。
28 00:02:39,108 --> 00:02:43,332 说话者 SPEAKER_00：你只需对成千上万或数百万个例子这样做。
29 00:02:43,752 --> 00:02:45,774 说话人 SPEAKER_00：你可能认为它会陷入困境，但它不会。
30 00:02:46,936 --> 00:02:49,397 说话人 SPEAKER_00：最终它会陷入困境，但最终它会陷入一个好的解决方案中。
31 00:02:50,199 --> 00:02:56,526 说话人 SPEAKER_00：对于传统人工智能中的人来说，认为这些事情永远不会成功是完全可以理解的。
32 00:02:57,347 --> 00:03:01,230 说话人 SPEAKER_00：但结果是我们获得了大数据集和大型计算机，它们工作得非常出色。
33 00:03:02,652 --> 00:03:09,681 主持人：为了让我们了解您目前的工作，请具体描述一下我们目前是如何进行图像识别的。
34 00:03:09,841 --> 00:03:11,682 主持人：您使用了不同的神经网络技术。
35 00:03:12,051 --> 00:03:13,334 主持人：用于不同的任务。
36 00:03:13,354 --> 00:03:18,423 主持人：那么，请描述一下我们今天用于图像识别的方法。
37 00:03:18,502 --> 00:03:21,247 说话人 SPEAKER_00: 好吧，我将以一种不寻常的方式描述。
38 00:03:21,307 --> 00:03:27,399 说话人 SPEAKER_00: 对于图像识别，我们使用一种叫做卷积神经网络的东西。
39 00:03:28,721 --> 00:03:34,431 说话人 SPEAKER_00: 我将描述一个不用于图像识别的卷积网络，因为它很容易理解。
40 00:03:35,237 --> 00:03:49,336 说话人 SPEAKER_00: 这是一种由多伦多大学的 David Duvenoe 教授等人开发出的网络。假设你有一个分子，你想预测它的性质。
41 00:03:49,757 --> 00:03:51,360 说话人 说话人_00: 我只给你分子结构。
42 00:03:51,379 --> 00:03:53,042 说话人 说话人_00: 我只给你原子链。
43 00:03:53,883 --> 00:03:57,106 说话人 说话人_00: 假设你有一个碳和一个碳和一个氮和一个氧。
44 00:03:58,229 --> 00:04:02,814 说话人 说话人_00: 所以我们将有一个第一层描述，就是我刚才给出的，这是在每个原子。
45 00:04:03,335 --> 00:04:05,118 说话人 SPEAKER_00: 我可以告诉你那个原子的信息。
46 00:04:05,800 --> 00:04:07,122 说话人 SPEAKER_00: 那就是第一层特征。
47 00:04:08,063 --> 00:04:14,657 说话人 SPEAKER_00: 也许我会用一串数字而不是一个词来告诉你，但这串数字是针对那个原子的特定数字。
48 00:04:14,676 --> 00:04:19,487 说话人 SPEAKER_00: 在下一层，我对那个原子的描述更加丰富。
49 00:04:19,720 --> 00:04:26,007 说话者 说话者_00：我的更详细的描述是一系列数字，这些数字不仅告诉我那个原子的信息，还告诉我它的邻居是什么。
50 00:04:26,908 --> 00:04:33,495 说话者 说话者_00：所以在更高一级的水平上，我有一系列数字，表明这是一个碳原子，一边是一个碳原子，另一边是一个氮原子。
51 00:04:34,235 --> 00:04:37,779 说话者 说话者_00：我还说在氮原子上，这是一个氮原子，一边是一个碳原子，另一边是一个氧原子。
52 00:04:38,841 --> 00:04:48,192 说话者 说话者_00：然后在更高一级的水平上，我得到这是一个碳原子，它的一边是一个带有氧原子的氮原子，以此类推。
53 00:04:49,083 --> 00:04:59,495 说话人 说话人_00：当你达到相当高的水平时，你会有一个包含大量数字的大向量，它实际上能告诉你很多关于那个原子的信息，因为它逐渐将上下文信息融入对那个原子的描述中。
54 00:05:00,557 --> 00:05:03,319 说话人 说话人_00：好的，现在让我们转向用于视觉的卷积神经网络。
55 00:05:04,281 --> 00:05:07,805 说话人 说话人_00：你开始，我会简化，但这正是这个想法的本质。
56 00:05:08,545 --> 00:05:10,267 说话人 说话人_00：你从像素网格开始。
57 00:05:10,848 --> 00:05:13,572 说话人 SPEAKER_00: 你所知道的就是这些像素的 RGB 值。
58 00:05:14,310 --> 00:05:17,233 说话人 SPEAKER_00: 这就是像素的底层描述。
59 00:05:17,254 --> 00:05:27,444 说话人 SPEAKER_00: 然后，我将得到一个更丰富的像素描述，可能会说，这是一个具有这些 RGB 值的像素，但它这一侧有更亮的像素，另一侧有更暗的像素。
60 00:05:27,463 --> 00:05:30,766 说话人 SPEAKER_00: 至少在红通道中是这样，但在绿通道中，它有更亮的像素，等等。
61 00:05:31,307 --> 00:05:36,211 说话人 SPEAKER_00：因此，您可以得到关于该像素周围像素的描述。
62 00:05:36,733 --> 00:05:40,976 说话人 SPEAKER_00：您正在获取上下文信息并将其放入该像素的描述中。
63 00:05:41,834 --> 00:05:47,423 说话人 SPEAKER_00：然后您进入下一个层次，此时邻居已经从他们的邻居那里获得了一些上下文信息。
64 00:05:47,944 --> 00:05:49,086 说话人 SPEAKER_00：然后您将这个信息加入其中。
65 00:05:49,125 --> 00:06:04,490 说话者 说话者_00: 因此，你将得到越来越丰富的描述，描述每个像素中发生的事情，它逐渐吸收周围环境中的更多信息，经过几层或现在可能经过100层后，你将得到每个像素的非常丰富的描述。
66 00:06:04,730 --> 00:06:08,836 说话者 说话者_00: 从那个描述中，你可以开始猜测图像中的物体。
67 00:06:10,267 --> 00:06:13,151 说话者 说话者_01: 话虽如此，现在这种方法效果相当不错。
68 00:06:13,230 --> 00:06:22,279 说话者 说话者_01: 现在我们有了计算能力和数据，在某些衡量标准下，在识别照片中的物体时，例如，比人类做得更好。
69 00:06:22,300 --> 00:06:26,884 讲者 SPEAKER_01: 但你现在已公开表示这是有缺陷的，确实如此。
70 00:06:27,786 --> 00:06:28,807 讲者 SPEAKER_01: 为什么它是有缺陷的？
71 00:06:28,826 --> 00:06:32,370 讲者 SPEAKER_01: 这个概念的限制是什么？
72 00:06:32,389 --> 00:06:33,190 讲者 SPEAKER_00: 好的。
73 00:06:33,211 --> 00:06:37,035 说话人 说话人_00：很久以前，我曾是位心理学家。
74 00:06:37,095 --> 00:06:40,257 说话人 说话人_00：我对心理意象非常感兴趣。
75 00:06:41,420 --> 00:06:45,064 说话人 说话人_00：我提出了一些关于人们进行心理意象时发生什么的理论。
76 00:06:46,365 --> 00:06:58,156 说话人 说话人_00：很明显，当一个人理解一个形状时，他们是通过建立一个坐标系并理解形状的各个部分相对于坐标系的位置来做到这一点的。
77 00:07:00,240 --> 00:07:05,384 说话人 说话人_00：例如，如果我们拿一个字母 H，如果我采用一个垂直坐标系，
78 00:07:05,483 --> 00:07:17,562 说话人 说话人_00：这是一个大写字母 H。如果我旋转我的坐标系，相信 H 的位置没有变，这样从上方看，这是一个顶部和底部都有较大横杠的大写字母 I。
79 00:07:18,043 --> 00:07:21,608 说话人 说话人_00：所以当我改变坐标系时，它改变了形状的属性。
80 00:07:22,970 --> 00:07:24,413 说话人 说话人_00：现在，卷积神经网络无法做到这一点。
81 00:07:24,492 --> 00:07:33,567 说话人 说话人_00：这是一种卷积 MET，你给它展示一些东西，它进行一些处理，并且不会根据它选择的坐标系得到不同的表示。
82 00:07:34,254 --> 00:07:40,526 说话人 说话人_00：为了展示这种现象有多么强烈，我将向您展示一个三维拼图。
83 00:07:42,370 --> 00:07:48,903 说话人 说话人_00：而这个三维拼图，只有两块拼图。
84 00:07:49,043 --> 00:07:50,846 说话人 说话人_00：而且这些拼图并不复杂。
85 00:07:51,988 --> 00:07:54,372 说话人 SPEAKER_00: 你需要把它们放在一起组成一个四面体。
86 00:07:55,636 --> 00:08:02,646 说话人 SPEAKER_00: 以免你说你不知道什么是四面体，大多数金字塔都有正方形底座，就像埃及金字塔一样。
87 00:08:03,288 --> 00:08:05,411 说话人 SPEAKER_00: 想象一个底座是三角形的金字塔。
88 00:08:05,432 --> 00:08:08,896 说话人 SPEAKER_00: 所以它的底座是三角形，它的侧面也是三角形。
89 00:08:09,358 --> 00:08:11,240 说话者 说话者_00：它有三个三角形面和一个底面。
90 00:08:11,762 --> 00:08:12,862 说话者 说话者_00：并且它顶部有一个尖角。
91 00:08:13,504 --> 00:08:15,427 说话者 说话者_00：这就是你思考四面体的方式。
92 00:08:16,369 --> 00:08:21,997 说话者 说话者_00：所以你使用一个坐标系，其中有一个垂直向上的轴指向顶点。
93 00:08:22,314 --> 00:08:27,120 说话人 说话人_00: 然后在另一个轴上，你必须做出一些决定，关于你要把那些轴放在哪个方向。
94 00:08:27,721 --> 00:08:28,884 说话人 说话人_00: 但是你确实做出了那个决定。
95 00:08:29,464 --> 00:08:31,507 说话人 说话人_00: 你会看到那样一个四面体。
96 00:08:33,149 --> 00:08:42,682 说话人 说话人_00: 如果我给你两块，每块都是四面体的一半，会发生一件令人难以置信的事情，那就是你无法拼成四面体。
97 00:08:42,701 --> 00:08:45,164 说话人 SPEAKER_00: 我把它给了 Cade Metz。
98 00:08:45,706 --> 00:08:48,109 说话人 SPEAKER_00: 他花了一两分钟试图拼成四面体，但做不到。
99 00:08:48,690 --> 00:08:50,111 说话人 SPEAKER_00: 他完全正常。
100 00:08:51,913 --> 00:08:57,000 说话人 SPEAKER_00: 偶尔会遇到一些特别烦人的人，他们立刻就能做到。
101 00:08:58,143 --> 00:09:02,368 说话人 SPEAKER_00: 一个特别烦人的家伙是约翰·吉安德里亚，他是谷歌的高级副总裁。
102 00:09:02,769 --> 00:09:05,552 说话人 SPEAKER_00: 我在向他解释这件事，并展示这个别人都做不到的巧妙技巧。
103 00:09:05,572 --> 00:09:07,335 说话人 SPEAKER_00: 他问，哦，问题是什么？
104 00:09:07,355 --> 00:09:11,200 说话人 SPEAKER_00: 太烦人了。
105 00:09:11,220 --> 00:09:12,743 说话人 说话人_00：我现在将向您展示这两件东西。
106 00:09:16,408 --> 00:09:17,149 说话人 说话人_00：这是这两件东西。
107 00:09:17,188 --> 00:09:17,769 说话人 说话人_00：它们是一样的。
108 00:09:17,809 --> 00:09:21,094 说话人 说话人_00：您可能已经看到了它们的样子。
109 00:09:21,664 --> 00:09:23,087 说话人 说话人_00：这些非常简单，它们是相同的。
110 00:09:23,509 --> 00:09:24,370 说话人 说话人_00：这就是人们所做的事情。
111 00:09:24,390 --> 00:09:26,515 说话人 说话人_00：他们说，不，那不是一个四面体。
112 00:09:27,277 --> 00:09:27,938 说话人 说话人_00：让我们看看。
113 00:09:29,081 --> 00:09:32,788 说话者 说话者_00: 那不是一个四面体。
114 00:09:33,650 --> 00:09:34,493 说话者 说话者_00: 那不是一个四面体。
115 00:09:34,653 --> 00:09:36,837 说话者 说话者_00: 他们就这样继续下去。
116 00:09:37,222 --> 00:09:52,092 说话者 说话者_00: 现在我把这些折磨人的工具带到了麻省理工学院，我进行了一个实验，其中水平轴是你作为麻省理工学院教授的年数，垂直轴是你所需的时间（分钟）。
117 00:09:53,068 --> 00:10:02,659 讲者：如果你在麻省理工学院只当了一两年教授，你可以在几分钟内完成，考虑到这两件作品各有五个面且完全相同，它们组成一个四面体，这仍然相当非凡。
118 00:10:03,880 --> 00:10:05,403 讲者：所以你需要解释一下，对吧？
119 00:10:05,682 --> 00:10:06,724 讲者：这太奇怪了。
120 00:10:07,725 --> 00:10:15,253 讲者：如果你在麻省理工学院有终身教职，我试过两个人，他们需要无限的时间。
121 00:10:18,297 --> 00:10:21,220 讲者：这就是终身职位的意义，对吧？
122 00:10:22,921 --> 00:10:31,413 讲者：其中一个人拒绝这么做，另一个人非常奇怪。
123 00:10:31,432 --> 00:10:32,534 讲者：他看了这两块。
124 00:10:32,554 --> 00:10:33,235 讲者：他没有拿起它们。
他只是看了看它们，看了很长时间，然后写下了一个证明，表明这是不可能的。
126 00:10:41,928 --> 00:10:43,211 说话者 说话者_00：他名叫卡尔·休伊特。
127 00:10:43,250 --> 00:10:44,192 发言人 SPEAKER_00：这已经是很久以前的事了。
128 00:10:44,231 --> 00:10:48,337 发言人 SPEAKER_00: 我现在要向你们展示
129 00:10:49,481 --> 00:10:50,182 说话人 SPEAKER_00：这是怎么工作的。
130 00:10:50,562 --> 00:10:52,065 说话人 SPEAKER_00：所以人们这么做。
131 00:10:53,046 --> 00:10:54,207 说话人 SPEAKER_00：不，那不是一个四面体。
132 00:10:54,508 --> 00:10:55,249 说话人 SPEAKER_00：然后他们这么做。
133 00:10:55,288 --> 00:10:56,510 说话人 SPEAKER_00: 不，那不是一个四面体。
134 00:10:56,870 --> 00:10:57,932 说话人 SPEAKER_00: 他们没有做的事情是。
135 00:10:59,794 --> 00:11:01,817 说话人 SPEAKER_00: 现在如果你看那个，那就是你想要的，对吧？
136 00:11:01,836 --> 00:11:03,720 说话人 SPEAKER_00: 那是一个四面体，如果你不知道它是什么的话。
137 00:11:04,860 --> 00:11:05,981 说话者 说话者_00: 真的很容易就能做出一个四面体。
138 00:11:06,001 --> 00:11:06,844 说话者 说话者_00: 你只需要这样做。
139 00:11:07,904 --> 00:11:10,288 说话者 说话者_00: 你只需要把正方形面拼合在一起。
140 00:11:11,188 --> 00:11:12,890 说话者 说话者_00: 那为什么人们做不到呢？
141 00:11:14,474 --> 00:11:17,918 说话者 说话者_00：现在看看这个四面体，并想想你用它使用的坐标系。
142 00:11:18,198 --> 00:11:22,965 说话者 说话者_00：有一条垂直的轴从我的下巴穿过那里的顶峰，一直到底座的中间。
143 00:11:24,706 --> 00:11:31,235 说话者 说话者_00：当我向你展示这些碎片中的一个时，如果我只展示一个，你就在这个碎片上施加一个坐标系。
144 00:11:31,817 --> 00:11:37,124 说话者 说话者_00：它有一个这样的长轴，还有一个这样的轴，还有一个这样的轴。
145 00:11:37,684 --> 00:11:42,591 说话者 说话者_00：这些轴与描述四面体通常使用的轴根本不对齐。
146 00:11:43,921 --> 00:11:48,307 说话者 说话者_00：正因为如此，你无法看到这与四面体有何关联。
147 00:11:50,049 --> 00:11:51,852 说话者 说话者_00：那些解决问题的人是通过推理来解决的。
148 00:11:51,893 --> 00:11:55,697 说话者 说话者_00：他们说，看，四面体，所有的面都是三角形的。
149 00:11:55,958 --> 00:11:57,520 说话人 SPEAKER_00: 我永远也做不出一个三角形。
150 00:11:57,760 --> 00:11:59,224 说话人 SPEAKER_00: 所以一定是这两个面。
151 00:11:59,524 --> 00:12:00,785 说话人 SPEAKER_00: 最后，他们就这样放。
152 00:12:04,971 --> 00:12:08,157 说话人 SPEAKER_00: 所以这整个的目的，嗯，主要目的是为了好玩。
153 00:12:08,277 --> 00:12:13,725 说话者 SPEAKER_00: 但是次要的观点是试图说服你们这些做后框架的人
154 00:12:14,616 --> 00:12:20,363 说话者 SPEAKER_00: 在物体上，他们使用这些框架来理解物体，并且无法克服这一点。
155 00:12:20,423 --> 00:12:25,629 说话者 SPEAKER_00: 这是我们看事物的内在属性，这并不是卷积网络所做的事情。
156 00:12:27,191 --> 00:12:38,682 说话者 SPEAKER_01: 因此，因为这是我们看事物的内在属性，所以你希望产生一个计算系统，在该系统中施加你提到的相同坐标系，即 3D 坐标系，到图像上
157 00:12:38,883 --> 00:12:41,067 说话者 SPEAKER_01：因此可以更好地识别它们，对吧？
158 00:12:41,087 --> 00:12:42,711 说话者 SPEAKER_00：是的，我还有另一条逻辑。
159 00:12:42,831 --> 00:12:45,155 说话者 SPEAKER_00：这是无懈可击的逻辑。
160 00:12:46,438 --> 00:12:49,144 说话者 SPEAKER_00：有两种系统在观点上没有任何问题。
161 00:12:50,427 --> 00:12:53,855 说话人 SPEAKER_00：一个是我们的感知系统，另一个是计算机图形学。
162 00:12:54,676 --> 00:12:55,999 说话人 SPEAKER_00：因此，它们以相同的方式工作。
163 00:12:57,341 --> 00:12:59,726 说话人 SPEAKER_00：这是自然推理。
164 00:13:00,128 --> 00:13:05,053 说话人 SPEAKER_00：所以在计算机图形学中，当有人说“做一个房子”时，你需要做什么。
165 00:13:06,176 --> 00:13:08,938 说话者 说话者_00：所以他们告诉你房子应该在哪儿。
166 00:13:10,240 --> 00:13:12,182 说话者 说话者_00：现在你想要知道门应该在哪儿。
167 00:13:13,105 --> 00:13:16,028 说话者 说话者_00：嗯，你知道房子和门之间的关系。
168 00:13:17,210 --> 00:13:19,052 说话者 说话者_00：那么在数学上你是怎么考虑这个问题的？
169 00:13:19,111 --> 00:13:23,378 说话者 说话者_00：嗯，你给房子设定了一个参考框架，也给门设定了一个参考框架。
170 00:13:23,878 --> 00:13:26,861 说话者 说话者_00：你有两个坐标系，一个用于房子，一个用于门。
171 00:13:26,993 --> 00:13:36,030 说话者 说话者_00：你将它们存储在计算机中，作为关于房子是什么的知识的一部分，存储这两个坐标系之间的关系。
172 00:13:36,432 --> 00:13:40,259 说话者 说话者_00：如何从房子的坐标系转换到门的坐标系。
173 00:13:40,460 --> 00:13:47,549 说话者 SPEAKER_00: 然后对于门，如果你想知道门把手在哪里，你知道门把手的坐标系，你知道门把手相对于门的位置等等。
174 00:13:47,570 --> 00:13:53,216 说话者 SPEAKER_00: 所以计算机图形学从整个物体开始，一直分解到最基本的小东西，比如小三角形。
175 00:13:53,918 --> 00:14:00,147 说话者 SPEAKER_00: 然后进行渲染，这意味着它开始处理光和表面的属性，实际上产生像素。
176 00:14:00,767 --> 00:14:02,669 说话者 SPEAKER_00: 但到那时，它只是在做几何。
177 00:14:04,085 --> 00:14:15,280 说话者 SPEAKER_00：如果您认为我们应该使用与计算机图形处理视角变化相同的方法，因为计算机图形处理视角非常出色，那么您希望反过来操作。
178 00:14:15,301 --> 00:14:16,482 说话者 SPEAKER_00：您要从像素开始。
179 00:14:17,524 --> 00:14:27,918 说话者 SPEAKER_00：您希望进行一个初始阶段，分析光和表面的特性，并得到小块表面的几何形状。
180 00:14:29,500 --> 00:14:32,826 说话者 SPEAKER_00：然后从那里开始，将小块拼接成更大的部分。
181 00:14:34,020 --> 00:14:42,476 说话者 说话者_00：为了用神经网络做到这一点，你需要使用与我们目前使用的非常不同的一种神经网络。
182 00:14:43,398 --> 00:14:45,581 说话者 说话者_00：我应该谈谈我们目前使用的神经网络类型。
183 00:14:46,443 --> 00:14:54,458 说话者 说话者_00：如果你在这个领域待了很长时间，就像我一样，你就会知道我们现在使用的神经网络并没有什么特别之处。
184 00:14:54,479 --> 00:14:55,640 说话者 说话者_00：我们只是随便创造了它们。
185 00:14:56,363 --> 00:14:58,693 说话人 SPEAKER_00：它们不是，神经网络不是这样的。
186 00:14:59,316 --> 00:15:04,922 说话人 SPEAKER_00：有些人想出了一些技巧，我们有一些效果相当不错的技巧，但还有数百种其他技巧我们没有探索。
187 00:15:05,493 --> 00:15:13,139 说话人 SPEAKER_00：为了让您看到当前最先进的状态有多么原始，我们使用的是这种输入输出曲线的对数单位。
188 00:15:13,799 --> 00:15:15,001 说话人 SPEAKER_00：这是一个像这样的 S 形曲线。
189 00:15:16,123 --> 00:15:18,865 说话人 说话人_00：过去30年，我们之所以使用那些，是因为我们认为那些是最好的。
190 00:15:19,625 --> 00:15:26,251 说话人 说话人_00：然后我们发现，如果输入输出关系是这样的，这就叫做修正线性单元，这些更容易训练。
191 00:15:26,772 --> 00:15:29,413 说话人 说话人_00：所以，过去30年，我们一直在使用错误的东西。
192 00:15:30,014 --> 00:15:31,655 说话人 说话人_00：然后我们发现了一些更好的东西。
193 00:15:32,376 --> 00:15:33,457 说话人 说话人_00：然后我们用了那个。
194 00:15:34,078 --> 00:15:42,517 说话人 说话人_00：嗯，如果我们对这样简单的事情都这么愚蠢，想想看我们在所有其他神经网络中犯了多少错误。
195 00:15:45,222 --> 00:15:49,010 说话人 说话人_00：所以我们可以制造一种不同类型的神经网络，其中将神经元分成小组。
196 00:15:49,633 --> 00:15:51,977 说话人 说话人_00：我将那个小组称为胶囊。
197 00:15:51,998 --> 00:15:53,421 说话人 说话人_00: 在这个胶囊里，
198 00:15:53,822 --> 00:15:55,004 说话人 说话人_00: 你将看到一串数字。
199 00:15:55,625 --> 00:15:57,107 说话人 说话人_00: 这些是神经元的活动。
200 00:15:57,528 --> 00:15:59,410 说话人 说话人_00: 但它们将会是一组组相关的。
201 00:16:00,052 --> 00:16:02,095 说话人 说话人_00：在普通的神经网络中，你只是有很多神经元。
202 00:16:02,475 --> 00:16:06,181 说话人 说话人_00：并没有说明这些神经元和那些神经元是一起的，那些神经元和那些神经元是一起的。
203 00:16:06,461 --> 00:16:07,322 说话人 说话人_00：它可能会学会这样做。
204 00:16:07,341 --> 00:16:09,745 说话人 说话人_00：但在胶囊网络中，我们将提前定义这一点。
205 00:16:10,687 --> 00:16:12,450 说话人 说话人_00: 我们将拥有，比如说，16个神经元。
206 00:16:13,090 --> 00:16:14,091 说话人 说话人_00: 它们都将一起。
207 00:16:15,313 --> 00:16:21,923 说话人 说话人_00: 它们将代表，嗯，这组神经元将变得活跃。
208 00:16:22,105 --> 00:16:24,589 说话人 说话人_00: 还将有一个额外的神经元来表示这组神经元是否活跃。
209 00:16:25,811 --> 00:16:31,320 说话者 SPEAKER_00：当出现特定类型的实体时，它们会变得活跃，比如凯德的鼻子。
210 00:16:32,221 --> 00:16:46,105 说话者 SPEAKER_00：所以我的视觉系统已经发现了鼻子，它将激活许多神经元，这些神经元表示鼻子中嵌入的坐标系与眼球中嵌入的坐标系之间的几何关系。
211 00:16:47,705 --> 00:16:52,673 说话者 SPEAKER_00：这就是我对鼻子的方向、位置和大小的知识。
212 00:16:53,033 --> 00:16:58,422 说话者 SPEAKER_00：这是鼻子中的坐标系与眼球中的坐标系之间的几何关系。
213 00:16:58,442 --> 00:16:59,222 说话者 SPEAKER_00: 好的。
214 00:16:59,243 --> 00:17:04,711 说话者 SPEAKER_00: 可能是一组神经元活跃起来，它们认出了凯德的嘴巴是嘴巴。
215 00:17:05,893 --> 00:17:13,644 说话者 SPEAKER_00: 那组 16 个神经元将代表嘴巴中嵌入的坐标系与我眼球之间的关系。
216 00:17:15,835 --> 00:17:32,701 说话者 SPEAKER_00: 然后，那些认为可能找到了鼻子但并不确定的 16 个神经元，如果它们知道鼻子的视角，就可以预测整个脸部嵌入的坐标系应该与我眼球之间的关系。
217 00:17:33,843 --> 00:17:38,028 说话者：也就是说，它们正在预测我视野中人脸的大小、方向和位置。
218 00:17:39,712 --> 00:17:41,255 说话者：因此，它们做出预测。
219 00:17:41,957 --> 00:17:46,644 说话者：代表老鼠的16个神经元也可以做出预测。
220 00:17:47,025 --> 00:17:48,186 说话者：我希望我之前说过“鼻子”。
221 00:17:48,207 --> 00:17:49,808 说话人 说话人_00：但无论如何，鼻子做出预测。
222 00:17:50,069 --> 00:17:51,151 说话人 说话人_00：嘴巴做出预测。
223 00:17:51,711 --> 00:17:54,957 说话人 说话人_00：这些是每个的16个神经活动。
224 00:17:55,878 --> 00:17:57,520 说话人 说话人_00：当我移动时，你保持静止。
225 00:17:57,761 --> 00:17:59,263 说话人 SPEAKER_00：当我移动时，不，保持不动。
226 00:17:59,284 --> 00:17:59,785 说话人 SPEAKER_00：不要看我。
227 00:18:00,326 --> 00:18:01,688 说话人 SPEAKER_00：当我移动时。
228 00:18:02,359 --> 00:18:06,428 说话人 SPEAKER_00：这 16 个数字会变化，因为他的嘴巴与我眼球的关系发生了变化。
229 00:18:07,029 --> 00:18:08,352 说话人 SPEAKER_00：那么这些都是神经活动。
230 00:18:09,054 --> 00:18:14,766 说话人 SPEAKER_00：但是有一件事是不变的，那就是他的鼻子和脸之间的关系。
231 00:18:16,028 --> 00:18:21,819 说话人 SPEAKER_00：所以我们做的是，我们取代表他的嘴巴与我眼球关系的 16 个数字。
232 00:18:22,458 --> 00:18:25,240 说话人 SPEAKER_00：然后我们用矩阵，一个 4x4 矩阵，将它们相乘。
233 00:18:26,041 --> 00:18:29,926 说话人 SPEAKER_00: 我们实际上将它们形成一个 4x4 的矩阵，然后乘以另一个 4x4 的矩阵。
234 00:18:30,386 --> 00:18:32,589 说话人 SPEAKER_00: 这对于计算机图形学的人来说听起来非常熟悉。
235 00:18:33,270 --> 00:18:37,316 说话人 SPEAKER_00: 我们得到一个矩阵，说明他的脸应该如何与我的眼睛相关联。
236 00:18:38,196 --> 00:18:41,961 说话人 SPEAKER_00: 我们对鼻子和嘴巴也做同样的处理。
237 00:18:43,082 --> 00:18:51,132 说话人 SPEAKER_00: 如果鼻子和嘴巴正确地与面部相关联，它们会对面部姿态做出相同的预测。
238 00:18:52,292 --> 00:18:59,471 说话人 SPEAKER_00: 当我移动时，当我改变我的视角时，鼻子会改变它的预测，因为我已经改变了与他的面部关系。
239 00:19:00,212 --> 00:19:01,576 说话人 SPEAKER_00: 嘴巴也会改变它的预测。
240 00:19:01,596 --> 00:19:03,340 说话人 SPEAKER_00: 但不会改变的是，它们会达成一致。
241 00:19:03,381 --> 00:19:09,576 说话人 SPEAKER_00：所以关于人脸形状的所有知识
242 00:19:10,450 --> 00:19:18,184 说话人 SPEAKER_00：都包含在这些应用于视场矩阵的变换矩阵中，你变换它，你就能预测出人脸的视场。
243 00:19:19,047 --> 00:19:21,151 说话人 SPEAKER_00：而这种知识根本不依赖于视场。
244 00:19:22,093 --> 00:19:25,859 说话人 SPEAKER_00：所以他的鼻子和脸之间的关系完全独立于我的视场。
245 00:19:27,163 --> 00:19:30,990 说话者 说话者_00：那么我们想要放入神经网络的是，我们希望活动非常依赖于视角。
246 00:19:32,067 --> 00:19:34,450 说话者 说话者_00：我们希望活动非常依赖于视角。
247 00:19:35,310 --> 00:19:39,095 说话者 说话者_00：大多数做卷积神经网络的人试图使活动独立于视角，这是错误的。
248 00:19:39,695 --> 00:19:41,317 说话者 说话者_00：你应该使活动依赖于视角。
249 00:19:42,159 --> 00:19:47,705 说话人 SPEAKER_00: 你让连接上的权重完全独立于视角。
250 00:19:47,786 --> 00:19:49,327 说话人 SPEAKER_00: 那就是关于形状的知识所在。
251 00:19:50,608 --> 00:19:53,333 说话人 SPEAKER_00: 现在你尝试预测整个物体的视角。
252 00:19:53,853 --> 00:19:57,617 说话人 SPEAKER_00: 如果你的预测结果一致，你就说，嘿，那个东西在那里。
253 00:19:58,179 --> 00:20:01,061 Speaker SPEAKER_00: And if you get predictions that don't agree, you say, hey, 
重试
  
错误原因
254 00:20:01,700 --> 00:20:06,069 Speaker SPEAKER_00: there's nothing there. Um, it's fake news. Um, so, um, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, 
重试
  
错误原因
255 00:20:08,163 --> 00:20:14,510 Speaker SPEAKER_00: So this is a much more robust way of identifying objects than what we use at present. 
重试
  
错误原因
256 00:20:14,530 --> 00:20:15,592 Speaker SPEAKER_00: And I want to give you an analogy. 
重试
  
错误原因
257 00:20:15,771 --> 00:20:16,452 发言人 SPEAKER_00：我说得太长了。
258 00:20:16,472 --> 00:20:16,794 说话人 说话人_00: 请说。
259 00:20:17,634 --> 00:20:19,757 说话人 说话人_00: 我想给你一个来自智能世界的类比。
260 00:20:22,079 --> 00:20:26,665 说话人 说话人_00: 假设你在监听无线电交通，你看到了纽约的提及，芝加哥的提及，还有洛杉矶的提及。
261 00:20:27,145 --> 00:20:29,890 说话人 说话人_00: 你还看到了九月的提及，十月的提及。
262 00:20:29,910 --> 00:20:34,035 说话人 说话人_00: 你看到了对第5次、第12次和第11次的提及。
263 00:20:34,015 --> 00:20:35,896 说话人 说话人_00: 你看到很多这样的东西。
264 00:20:36,597 --> 00:20:41,544 说话人 说话人_00: 假设你在三条不同的信息中看到了纽约，9月11日。
265 00:20:42,865 --> 00:20:44,507 说话人 说话人_00: 这是一个高维巧合。
266 00:20:45,087 --> 00:20:46,449 说话人 SPEAKER_00: 几件事情在那里巧合。
267 00:20:46,730 --> 00:20:52,215 说话人 SPEAKER_00: 如果你看到这种组合多次，那真的是你应该注意的事情。
268 00:20:52,876 --> 00:21:02,587 说话人 SPEAKER_00: 同样地，鼻子预测人脸的大小和位置方向，嘴巴也预测同样的事情。
269 00:21:02,872 --> 00:21:04,413 说话人 SPEAKER_00: 因此，你得到了一个高维巧合。
270 00:21:05,536 --> 00:21:18,914 说话者 说话者_00：我认为神经网络应该工作的方式与目前它们实际工作的方式之间有很大的区别，目前的神经网络取一个神经元，它有一些权重，然后观察这些权重与下一层的活动之间的关系。
271 00:21:18,934 --> 00:21:19,435 说话者 说话者_00：它们匹配吗？
272 00:21:20,196 --> 00:21:24,643 说话者 说话者_00：它应该做的是获取活动以做出预测，说，两组预测是否匹配？
273 00:21:25,265 --> 00:21:26,226 说话者 说话者_00：这要强大得多。
274 00:21:26,787 --> 00:21:36,898 说话人 SPEAKER_00：幸运的是，我有一个非常好的，基本上是研究生，和我一起工作了六个月，她让一切都能顺利进行。
275 00:21:37,559 --> 00:21:40,723 说话人 SPEAKER_01：是的，我们应该说这不仅仅是一种理论。
276 00:21:40,763 --> 00:21:49,855 说话人 SPEAKER_01：你最近发表了关于这个主题的两篇论文，你至少在某些情况下已经证明，这可以超越卷积神经网络在图像识别方面的表现。
277 00:21:50,272 --> 00:21:52,718 说话人 SPEAKER_00：所以我们展示的还处于初期阶段。
278 00:21:53,679 --> 00:22:02,756 说话者 说话者_00：把它这么快公之于众有点尴尬，但我们展示了一个小型数据集，这个数据集是为了测试形状识别能力而设计的。
279 00:22:03,317 --> 00:22:05,801 说话者 说话者_00：我们的错误率比卷积网络低一半。
280 00:22:06,742 --> 00:22:08,686 说话者 说话者_00：但我之前见过。
281 00:22:09,448 --> 00:22:18,163 说话者 说话者_00：所以，在2009年，我的两名研究生为语音识别设计了一种技术，这种技术略优于成熟技术。
282 00:22:18,324 --> 00:22:21,430 说话人 说话人_00：卷积神经网络现在已经成为一项成熟的技术。
283 00:22:21,450 --> 00:22:26,659 说话人 说话人_00：如果你能让一个研究生在众多未调优的方面做得稍微好一些，
284 00:22:26,638 --> 00:22:28,501 说话人 说话人_00：最终很明显你会做得更好。
285 00:22:29,162 --> 00:22:44,103 说话人 说话人_00：这种情况发生在语音识别上，发生在形状识别和卷积神经网络上，我也希望它会在这种新的形状识别方式上发生，并且神经网络将不再有这种愚蠢的问题，即如果它们已经学会了从某些视角识别事物，它们就无法从新的视角识别它们。
286 00:22:45,003 --> 00:22:52,173 说话人 SPEAKER_01：你认为它也可以应用于其他任务，超越图像识别，比如语音识别或翻译或其他？
287 00:22:52,271 --> 00:23:04,491 所以，你应该通过匹配滤波器来识别事物的通用想法，也就是说，不是通过一些权重和一些活动，说权重大的地方活动大。
288 00:23:05,053 --> 00:23:05,934 这就是匹配滤波器。
289 00:23:06,756 --> 00:23:07,897 这是一种识别事物的方法。
290 00:23:07,917 --> 00:23:09,099 说话人 说话人_00：这是模板匹配。
291 00:23:10,480 --> 00:23:20,417 说话人 说话人_00：识别事物的一个更强大的方法是获取两个不同的测量值，但多维度的测量值，进行预测并查看预测是否一致。
292 00:23:21,098 --> 00:23:31,817 说话人 说话人_00：如果在高维空间中多次获得预测的一致性，那么这更是一个明显的迹象，表明你确实找到了一些东西。
293 00:23:32,479 --> 00:23:34,462 说话人 说话人_00：我认为神经网络将会改变以实现这一点。
294 00:23:35,236 --> 00:23:37,619 说话人 SPEAKER_01：我们也应该指出，这并不是一个新想法。
295 00:23:37,660 --> 00:23:41,766 说话人 SPEAKER_01：这是你从 70 年代末就有的一个老想法，对吧？
296 00:23:41,786 --> 00:23:43,347 说话人 SPEAKER_01：甚至在你的破产之前。
297 00:23:43,367 --> 00:23:43,628 说话人 SPEAKER_00：是的。
298 00:23:45,632 --> 00:23:49,176 说话人 SPEAKER_00: 你可能今天早上已经听说神经网络进展得多么快了。
299 00:23:51,019 --> 00:23:56,208 说话人 SPEAKER_00: 所以我 38 年前就开始做这件事了，现在它终于要开花结果了。
300 00:23:58,290 --> 00:23:59,573 说话人 SPEAKER_00: 但我有点慢。
301 00:24:00,515 --> 00:24:01,696 说话人 SPEAKER_01: 但这是一个不同的世界，对吧？
302 00:24:01,717 --> 00:24:02,738 说话人 SPEAKER_01：我们有了计算能力。
303 00:24:02,798 --> 00:24:03,680 说话人 SPEAKER_01：我们有了数据。
304 00:24:04,240 --> 00:24:08,646 说话人 SPEAKER_01：听起来您认为这可以以更快的速度发展。
305 00:24:09,087 --> 00:24:09,328 说话人 SPEAKER_00：是的。
306 00:24:09,409 --> 00:24:11,291 说话人 说话人_00：我应该给出一个重大的警告。
307 00:24:12,512 --> 00:24:13,474 说话人 说话人_00：这只是一个理论。
308 00:24:13,955 --> 00:24:16,038 说话人 说话人_00：这在小数据集上是可行的。
309 00:24:16,559 --> 00:24:18,382 说话人 说话人_00：它在小数据集上表现得相当出色。
310 00:24:18,863 --> 00:24:20,766 说话人 说话人_00: 但我们不知道它是否能在大数据集上工作。
311 00:24:20,786 --> 00:24:23,069 说话人 说话人_00: 除非它在大数据集上工作，否则你不应该相信它。
312 00:24:25,192 --> 00:24:25,512 说话人 说话人_01: 好的。
313 00:24:25,532 --> 00:24:26,575 说话人 说话人_01: 我们有值得期待的东西。
314 00:24:27,395 --> 00:24:27,896 说话人 SPEAKER_01: 谢谢，吉姆。
315 00:24:27,916 --> 00:24:28,096 说话人 SPEAKER_00: 好的。
316 00:24:28,116 --> 00:24:28,438 说话人 SPEAKER_00: 谢谢。
317 00:24:28,959 --> 00:24:29,239 说话人 SPEAKER_01: 感谢。
1 00:00:01,110 --> 00:00:02,753 说话人 SPEAKER_05：我们将迎来超级智能。
2 00:00:03,193 --> 00:00:07,841 说话人 SPEAKER_05：这不是炒作，也不是为了分散人们对人工智能其他问题的注意力。
3 00:00:08,340 --> 00:00:10,263 说话人 SPEAKER_05：这是我们长期以来所坚信的。
4 00:00:10,523 --> 00:00:14,050 说话人 SPEAKER_05：我认为在 5 到 20 年内，我们将得到类似超级智能的东西。
5 00:00:14,589 --> 00:00:19,236 说话人 说话人_00: 人工智能将是人类有史以来最强大，如果不是最强大的技术之一。
6 00:00:19,718 --> 00:00:23,722 说话人 说话人_00: 我们必须非常认真地对待这些风险，而且我们没有太多时间进行研究。
7 00:00:23,803 --> 00:00:25,565 说话人 说话人_00: 你后悔过什么吗？
8 00:00:30,321 --> 00:00:35,427 说话人 说话人_01: 我们处在一个非常特殊的地方，瑞典皇家科学院的大楼里。
9 00:00:36,530 --> 00:00:45,200 诺贝尔奖每年根据阿尔弗雷德·诺贝尔的遗愿在物理学、化学和经济学领域颁发。
10 00:00:46,081 --> 00:00:53,789 今天的采访对象是人工智能之父、诺贝尔物理学奖获得者杰弗里·辛顿。
11 00:00:54,752 --> 00:00:56,734 他是否后悔自己创造的一切？
12 00:00:57,118 --> 00:01:01,396 欢迎来到这场新闻发布会和瑞典皇家科学院。
13 00:01:02,240 --> 00:01:05,474 主持人 SPEAKER_04：当然，热烈欢迎我们的获奖者。
14 00:01:06,164 --> 00:01:07,546 主持人 SPEAKER_04：我叫汉斯·埃勒格林。
15 00:01:07,585 --> 00:01:09,668 主持人 SPEAKER_04：我是学院的秘书长。
16 00:01:09,870 --> 00:01:11,052 主持人 SPEAKER_04：我宣布新闻发布会开始。
17 00:01:11,992 --> 00:01:13,275 说话人 SPEAKER_04: 谁想先开始？
18 00:01:13,295 --> 00:01:13,635 说话人 SPEAKER_04: 请。
19 00:01:14,656 --> 00:01:17,981 说话人 SPEAKER_06: 我的第一个问题是向杰弗里·辛顿教授。
20 00:01:18,001 --> 00:01:21,487 说话人 SPEAKER_06: 您是人工神经网络的之父。
21 00:01:21,688 --> 00:01:27,257 说话人 SPEAKER_06：当您回顾今天的 AI 发展时，您后悔过什么吗？
22 00:01:28,018 --> 00:01:32,745 说话人 SPEAKER_06：如果您可以回到过去，您会这样做吗？
23 00:01:34,091 --> 00:01:35,274 说话人 SPEAKER_05：有两种后悔。
24 00:01:35,313 --> 00:01:39,739 说话人 SPEAKER_05：一种是当你做了某事，当时你知道你不应该这么做时的内疚后悔。
25 00:01:39,760 --> 00:01:41,402 说话人 SPEAKER_05：我没有那些。
26 00:01:42,022 --> 00:01:44,706 说话人 SPEAKER_05：如果在同样的情况下，我会再次这样做。
27 00:01:45,367 --> 00:01:52,117 说话人 SPEAKER_05：然而，我认为这可能是不幸的，因为我们可能会比我想的更快地获得超级智能。
28 00:01:52,677 --> 00:01:55,021 说话人 SPEAKER_05：我希望我早点考虑安全性。
29 00:01:55,542 --> 00:02:01,450 说话人 SPEAKER_06：我的第二个问题是向杰弗里·辛顿教授和德米斯·哈萨比斯提问，如果达到这个水平，您是否真的相信存在类似超人类的人工智能，或者这只是大科技和大型公司的营销手段？
30 00:02:01,801 --> 00:02:15,205 说话人 SPEAKER_06：您是否真的相信，如果我们达到这个水平，存在类似超人类的人工智能，还是这只是大科技和大型公司的营销手段？
31 00:02:15,225 --> 00:02:18,872 说话人 SPEAKER_05：我认为我和德米斯都相信我们将实现超智能。
32 00:02:18,893 --> 00:02:20,215 说话人 SPEAKER_05：这不是炒作。
33 00:02:20,194 --> 00:02:23,519 说话人 SPEAKER_05：它并不是为了分散人们对 AI 其他问题的关注。
34 00:02:24,020 --> 00:02:25,923 说话人 SPEAKER_05：这是我们长期以来所坚信的。
35 00:02:25,962 --> 00:02:33,491 说话人 SPEAKER_05：现在，我以为它还会远一些，但最近发展的速度意味着我认为它将会很快到来。
36 00:02:34,193 --> 00:02:40,521 说话人 SPEAKER_05：我认为在 5 到 20 年之间，我认为 Demis 认为大约在 10 年后，我们将得到类似超级智能的东西。
37 00:02:40,542 --> 00:02:43,806 说话人 SPEAKER_05：我们必须认真考虑如何保持控制。
38 00:02:43,937 --> 00:02:47,364 说话人 SPEAKER_00：我同意 Hinton 教授刚才所说的。
39 00:02:47,604 --> 00:03:01,268 说话人 SPEAKER_00：实际上，我们早在 2010 年 DeepMind 成立之初，就思考过如果构建这类智能会发生什么。
40 00:03:02,028 --> 00:03:04,073 说话人 SPEAKER_00：当然，我的热情始终是
41 00:03:04,052 --> 00:03:19,108 主持人：构建这些工具来帮助我们进行科学发现，正如我们今天所看到的，我认为我们会从中得到惊人的成果，比如治疗疾病、帮助能源、气候，以及我们人类今天面临的许多重大挑战。
42 00:03:19,087 --> 00:03:29,768 主持人：我们始终意识到任何强大通用技术带来的风险，我认为人工智能将是人类有史以来发明最强大，如果不是最强大的技术之一。
43 00:03:30,269 --> 00:03:35,258 主持人：因此，我们需要非常认真地对待这些风险，而且我们没有太多时间去进行研究。
44 00:03:35,237 --> 00:03:41,228 主持人：并且需要考虑如何解释这些系统以及控制这些系统。
45 00:03:41,548 --> 00:03:54,528 说话人 SPEAKER_00：但这也是一个社会问题，不仅仅是一个技术问题，那就是我们想用这些系统做什么，我们想如何部署它们，并确保全人类都能从这些系统能做的事情中受益。
46 00:03:54,677 --> 00:04:11,538 说话人 SPEAKER_01：在会议期间，也提出了这个问题，即今天的监管是否能够阻止人工智能的发展，以及所有这一切与大型科技公司的普遍利益有何关系，包括那些由利润驱动的公司。
47 00:04:11,519 --> 00:04:19,526 说话人 SPEAKER_05：我首先要指出的是，人工智能的一个短期危险是致命自主武器的开发。
48 00:04:20,226 --> 00:04:22,149 说话人 SPEAKER_05：那里不会有任何监管。
49 00:04:22,670 --> 00:04:30,737 说话人 SPEAKER_05：如果你看看欧洲的规定，例如，它们中有专门条款说明这些规定不适用于人工智能的军事用途。
50 00:04:31,398 --> 00:04:36,002 说话人 SPEAKER_05：所以当涉及到致命性自主武器时，各国政府都不愿意自我监管。
51 00:04:36,382 --> 00:04:40,766 说话人 SPEAKER_05：而且所有主要武器供应商之间都在进行一场军备竞赛。
52 00:04:40,747 --> 00:04:47,988 说话人 SPEAKER_05：比如美国、中国、俄罗斯、英国、以色列，甚至可能是瑞典，虽然我不太清楚。
53 00:04:50,062 --> 00:05:00,055 说话人 说话人_00：我认为人工智能是一项非常重要的技术，需要加以规范，但我认为我们制定正确的规范非常重要。
54 00:05:00,235 --> 00:05:06,322 说话人 说话人_00：目前最难的事情是它发展得太快，变化也很快。
55 00:05:06,362 --> 00:05:12,069 说话人 说话人_00：我们几年前讨论要规范的内容，现在考虑的规范已经不同了。
56 00:05:12,050 --> 00:05:29,992 说话人 说话人_00：因此，我建议政府和民间社会采取快速灵活的规范，也许可以借鉴我们在医疗保健、交通等领域已有的规范，并观察技术的发展，然后迅速适应其发展方向。
57 00:05:30,273 --> 00:05:35,720 主持人：在会议上，我们还向 DeepMind 的创始人 Demis Hassabis 提出了一个问题。
58 00:05:36,100 --> 00:05:40,925 主持人：现在的技术是否具有歧视性，以及如何确保它们不会歧视？
59 00:05:41,040 --> 00:05:48,687 说话人：是的，看，现在的 AI 确实需要大量的资源，尤其是计算资源，而且成本非常高。
60 00:05:49,588 --> 00:06:01,160 说话人：但是，由这些产生的许多模型都是开源的，或者几乎可以轻松地现成地提供给几乎任何人使用，包括我们构建的许多工具。
61 00:06:01,699 --> 00:06:06,704 说话人 说话人_00：我认为技术的民主化进程非常迅速。
62 00:06:06,805 --> 00:06:08,786 说话人 说话人_00：它正在迅速扩散。
63 00:06:11,685 --> 00:06:19,314 说话人 说话人_02：人工智能是一个非常强大有力的工具，就像一种新型的显微镜，就像一种获取世界信息的新方式。
64 00:06:19,374 --> 00:06:20,995 说话人 说话人_02：但我们通过实验来验证它。
65 00:06:21,055 --> 00:06:24,718 说话人 SPEAKER_02：我们用它来解决以前同样类型的问题。
66 00:06:24,738 --> 00:06:27,923 说话人 SPEAKER_02：也许我们现在会有一些看起来可以解决的问题。
67 00:06:28,442 --> 00:06:37,052 说话人 SPEAKER_02：因此，我认为我们将看到越来越多的 AI 作为过程中的一个步骤出现，甚至是主要步骤。
68 00:06:37,031 --> 00:06:41,459 说话人 SPEAKER_02：对于未来的诺贝尔奖获得者来说，但因为他们把 AI 引入其中，所以他们不会得到它，对吧？
69 00:06:41,519 --> 00:07:01,790 说话人 SPEAKER_02：这不仅仅是关于资金，更是关于发现，他们将因为发现而获得它，而且希望甚至，你知道，我最期待的是当有人在这里，例如，使用 AlphaFold 或其他蛋白质结构预测工具成为他们发现细胞工作新方式的决定性步骤，他们在这里，在这里，是因为他们知道细胞是如何工作的。
70 00:07:05,365 --> 00:07:14,944 说话人 SPEAKER_03：人工智能非常强大，但应该认识到深度学习方法需要真正庞大且精心整理的数据集。
71 00:07:15,264 --> 00:07:22,057 说话人 SPEAKER_03：我们三个人真正受益于 60 年的辛勤工作。
72 00:07:22,038 --> 00:07:29,846 说话人 SPEAKER_03：由成千上万的科学家和数十亿美元的投资来解决蛋白质结构的实验问题。
73 00:07:30,307 --> 00:07:36,293 说话人 SPEAKER_03：从某种意义上说，我们拥有这样一笔难以置信的资源，这是几代科学家共同积累的。
74 00:07:36,874 --> 00:07:44,661 说话人 SPEAKER_03：这也是为什么，嗯，这确实使得应用强大的深度学习方法来解决这些问题成为可能。
75 00:07:45,103 --> 00:07:51,048 说话人 SPEAKER_03：因此，在思考人工智能和深度学习方法如何进步，
76 00:07:51,028 --> 00:07:55,295 说话人 SPEAKER_03：以及如何更广泛地应用于科学领域。
77 00:07:56,476 --> 00:08:05,769 说话人 SPEAKER_03：但我认为这取决于其他领域可用的或生成的丰富数据集的程度。
78 00:08:05,829 --> 00:08:12,619 说话人 SPEAKER_03：例如，当我们向上移动生物复杂性等级时，显然有大量未解决的问题。
79 00:08:13,019 --> 00:08:15,163 说话人 SPEAKER_03：存在数据集
80 00:08:15,142 --> 00:08:20,427 说话人 SPEAKER_03：问题是，这些数据集在深度和丰富性上是否真的可以比较？
81 00:08:20,548 --> 00:08:21,990 说话人 SPEAKER_03: 答案可能是否定的。
82 00:08:22,029 --> 00:08:27,836 说话人 SPEAKER_03: 我认为不仅仅是方法，数据本身也非常重要。
83 00:08:27,855 --> 00:08:32,880 说话人 SPEAKER_01: 会议结束后，我们与化学诺贝尔奖得主大卫·贝克见面了。
84 00:08:33,020 --> 00:08:41,590 说话人 SPEAKER_01: 你很快就能在我们的频道上观看整个对话，但现在，我们分享的是关于人工智能的简短摘录。
85 00:08:42,851 --> 00:08:53,602 说话人 SPEAKER_06：你认为今年的诺贝尔奖是对人工智能、对技术的致敬吗？
86 00:08:53,623 --> 00:08:59,808 说话人 SPEAKER_06：你将与 DeepMind 的联合创始人共享诺贝尔奖，他们创造了这个 AI 系统。
87 00:09:00,649 --> 00:09:05,394 说话人 SPEAKER_06：在一次新闻发布会上，你坐在人工智能之父 Geoffrey Hinton 旁边。
88 00:09:05,815 --> 00:09:08,437 说话人 SPEAKER_06：你认为这是对那种技术的致敬吗？
89 00:09:08,418 --> 00:09:20,655 讲者 SPEAKER_03：嗯，我认为今年的奖项确实证明了人工智能在许多领域产生的巨大影响。
90 00:09:21,418 --> 00:09:26,424 讲者 SPEAKER_03：而且在我特别关注的领域，他们提到的是我们在人工智能之前就已经完成的工作。
91 00:09:26,806 --> 00:09:33,254 讲者 SPEAKER_03：但另一方面，我们过去四年一直在开发的设计方法都是人工智能方法。
92 00:09:33,495 --> 00:09:37,682 讲者 SPEAKER_03：所以我认为今年确实有一个共同的主题。
93 00:09:37,898 --> 00:09:42,441 说话人 SPEAKER_06：在你的工作中，是更有人性还是更偏向计算机科学。
94 00:09:42,927 --> 00:09:51,960 说话人 SPEAKER_03：嗯，我觉得两者都有，但你知道，设计本质上是一种人类活动，因为你要决定你想解决什么问题。
95 00:09:52,701 --> 00:10:06,779 说话人 SPEAKER_03：所以，世界各地的人们都来到我的小组，很多来自欧洲，还有其他地方的人，他们都带着解决特定问题的热情，比如做些关于气候变化或治愈疾病的事情。
96 00:10:07,621 --> 00:10:10,065 说话人 SPEAKER_03：所以这非常具有人性，
97 00：10：11,243 --> 00：10：16,287 演讲者 SPEAKER_03：你知道，你想为哪些需要解决的问题设计新的蛋白质？
98 00：10：16,548 --> 00：10：18,317 演讲者 SPEAKER_03：计算机不会告诉你这些。

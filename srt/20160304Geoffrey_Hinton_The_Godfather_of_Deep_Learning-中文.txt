1 00：00：01,026 --> 00：00：03,943 演讲者 SPEAKER_02：在你完成输入之前，谷歌就知道你想搜索什么。
2 00：00：04,285 --> 00：00：06,798 演讲者 SPEAKER_02：Facebook 可以在照片中自动标记您。
3 00：00：07,080 --> 00：00：09,230 演讲者 SPEAKER_02：哎呀，汽车现在可以自动驾驶了。
4 00：00：09,446 --> 00：00：10,968 演讲者 SPEAKER_02：这不仅仅是计算机变得更好。
5 00：00：11,048 --> 00：00：13,673 演讲者 SPEAKER_02：人工智能越来越聪明了。
6 00：00：14,194 --> 00：00：22,126 演讲者 SPEAKER_02：Jeffrey Hinton 在深度机器学习方面的三十年工作帮助它成为现实，他现在加入我们，了解 AI 的现状和未来发展方向。
7 00：00：22,185 --> 00：00：28,716 演讲者 SPEAKER_02：他是多伦多大学的计算机科学教授，也是 Google 的杰出研究员，很高兴您来到 TVO。
8 00：00：28,955 --> 00：00：29,637 演讲者 SPEAKER_02： 很高兴来到这里。
9 00：00：29,676 --> 00：00：33,042 演讲者 SPEAKER_02：想先给我们讲一个深度学习的基本定义吗？
10 00：00：33,021 --> 00：00：37,631 演讲者 SPEAKER_01：所以，你的大脑里有超过 100 亿个神经元。
11 00：00：37,993 --> 00：00：38,414 议长 SPEAKER_01：甚至是我的？
12 00：00：38,914 --> 00：00：39,375 议长 SPEAKER_01：甚至是你的。
13 00：00：39,456 --> 00：00：39,735 议长 SPEAKER_01：好的。
14 00：00：40,076 --> 00：00：45,307 演讲者 SPEAKER_01：它的工作方式是，每时每刻，每个神经元都必须决定是否进行 ping。
15 00：00：46,210 --> 00：00：49,377 发言者 SPEAKER_01：它根据从其他神经元获得的 ping 来做出决定。
16 00：00：49,356 --> 00：00：50,838 议长 SPEAKER_01：它对这些 ping 进行加权。
17 00：00：51,238 --> 00：00：58,765 发言者 SPEAKER_01：所以有些 ping 需要很多注意，这些 ping 告诉它要么你应该去 ping，要么你不应该去 ping，它会改变这些权重。
18 00：00：59,127 --> 00：01：06,653 演讲者 SPEAKER_01：所以通过改变它听其他神经元的多少，神经元可以改变它的行为方式，这就是你学习一切的方式。
19 00：01：07,254 --> 00：01：13,079 演讲者 SPEAKER_01：所以，这只剩下一个问题，那就是，改变你听其他神经元多少的原理是什么？
20 00：01：13,180 --> 00：01：14,941 演讲者 SPEAKER_01：这叫学习算法。
21 00：01：15,260 --> 00：01：23,090 演讲者 SPEAKER_01：深度学习是一种学习算法，用于改变一个神经元在多大程度上依赖其他神经元来决定是否进行 ping。
22 00：01：23,251 --> 00：01：25,353 演讲者 SPEAKER_01：我假设也有浅层学习吗？
23 00：01：25,855 --> 00：01：26,155 议长 SPEAKER_01：哦，是的。
24 00：01：26,215 --> 00：01：26,917 演讲者 SPEAKER_01：学习很肤浅。
25 00：01：26,936 --> 00：01：27,918 议长 SPEAKER_01：其他人就是这样做的。
26 00：01：28,659 --> 00：01：32,784 发言者 SPEAKER_01：输入和输出之间没有很多神经元层。
27 00：01：32,864 --> 00：01：34,066 演讲者 SPEAKER_01：我们在这里讨论深度学习。
28 00：01：34,126 --> 00：01：38,533 演讲者 SPEAKER_01：深度学习如何模拟人类如何了解世界？
29 00：01：38,513 --> 00：01：46,765 演讲者 SPEAKER_01：嗯，没有人真正知道在真实的大脑中，你是如何改变决定一个神经元对另一个神经元影响程度的连接强度的。
30 00：01：48,206 --> 00：01：52,311 演讲者 SPEAKER_01：但在 1980 年代，人们想出了一种非常有效的算法来做到这一点。
31 00：01：53,313 --> 00：01：55,917 演讲者 SPEAKER_01：它本来就是大脑的简化模型。
32 00：01：57,198 --> 00：01：58,981 演讲者 SPEAKER_01：没人知道大脑是不是真的是这样工作的。
33 00：01：59,441 --> 00：02：03,507 演讲者 SPEAKER_01：回到 80 年代，人们非常怀疑，因为算法效果不佳。
34 00：02：03,487 --> 00：02：08,776 演讲者 SPEAKER_01：但是随着计算机的速度越来越快，我们有了更大的数据集，这个算法现在效果非常好。
35 00：02：09,356 --> 00：02：10,378 议长 SPEAKER_01：到处都在用。
36 00：02：10,758 --> 00：02：11,800 扬声器 SPEAKER_01：它用在你的手机里。
37 00：02：12,481 --> 00：02：16,086 演讲者 SPEAKER_01：所以现在看来，大脑可能在做什么似乎是一个更好的选择。
38 00：02：16,187 --> 00：02：17,508 议长 SPEAKER_01：你知道是谁编的，这个算法吗？
39 00：02：18,531 --> 00：02：23,337 演讲者 SPEAKER_01：它是在 1970 年左右由某个不知名的家伙发明的。
40 00：02：24,099 --> 00：02：26,223 演讲者 SPEAKER_01：它被很多人重新发明了。
41 00：02：26,824 --> 00：02：30,930 演讲者 SPEAKER_01：然后在 80 年代，当计算机的速度足够快，可以有效地实现它时，
42 00：02：30,909 --> 00：02：34,514 演讲者 SPEAKER_01：嗯，人们开始使用它并展示它能做什么。
43 00：02：35,135 --> 00：02：37,399 演讲者 SPEAKER_01：但是当时计算机的速度还不够快，无法让它真正令人印象深刻。
44 00：02：37,879 --> 00：02：40,663 演讲者 SPEAKER_01：所以主流 AI 不相信这个算法。
45 00：02：41,504 --> 00：02：49,877 演讲者 SPEAKER_01：几年前发生的事情是计算机变得足够快，突然之间，这个算法开始解决主流人工智能无法解决的所有问题。
46 00：02：50,578 --> 00：02：52,320 说话者 SPEAKER_01：例如，就像识别语音一样。
47 00：02：52,341 --> 00：02：59,972 演讲者 SPEAKER_02：呃，Watson 会不会，Jeopardy 的那台电脑，他打败了所有人，这会是我们在这里讨论的一部分吗？
48 00：03：00,425 --> 00：03：06,574 演讲者 SPEAKER_01：Watson 中有一些机器学习，其中一些可能很可能会使用这种算法，但主要是手动编程。
49 00：03：06,615 --> 00：03：11,081 演讲者 SPEAKER_01：这是一个非常令人印象深刻的系统，但它需要大量的人力才能运作。
50 00：03：11,401 --> 00：03：14,687 演讲者 SPEAKER_01：这些人工神经网络的理念是，你将尝试学习一切。
51 00：03：15,528 --> 00：03：19,716 演讲者 SPEAKER_02：我怀疑每个人都知道 Watson 是谁，但如果你不知道，让我们播放一个片段并提醒大家。
52 00：03：19,955 --> 00：03：22,259 议长 SPEAKER_02：这是来自 Jeopardy 的 Watson，他非常出色。
53 00：03：22,760 --> 00：03：23,360 扬声器 SPEAKER_02：请滚动剪辑。
54 00：03：23,782 --> 00：03：25,604 演讲者 SPEAKER_02：1000 人的最终边界，亚历克斯。
55 00：03：25,822 --> 00：03：27,752 演讲者 SPEAKER_00：本次活动不需要门票。
56 00：03：27,772 --> 00：03：31,713 发言者 SPEAKER_00：一个黑洞的边界，物质无法逃脱。
57 00：03：31,963 --> 00：03：32,403 演讲者 SPEAKER_00：华生。
58 00：03：32,965 --> 00：03：34,385 演讲者 SPEAKER_00：什么是事件视界？
59 00：03：34,645 --> 00：03：34,866 议长 SPEAKER_00：是的。
60 00：03：35,586 --> 00：03：37,248 演讲者 SPEAKER_00：文学人物，APB？
61 00：03：37,429 --> 00：03：38,189 演讲者 SPEAKER_00：200 美元。
62 00：03：38,689 --> 00：03：43,634 议长 SPEAKER_00：因吃掉赫罗斯加国王的战士而被通缉了 12 年的犯罪狂欢。
63 00：03：44,254 --> 00：03：46,377 议长 SPEAKER_00：贝奥武夫警官被指派处理这个案子。
64 00：03：47,018 --> 00：03：47,758 演讲者 SPEAKER_00：华生。
65 00：03：47,777 --> 00：03：48,739 议长 SPEAKER_00：格伦德尔是谁？
66 00：03：48,919 --> 00：03：49,139 议长 SPEAKER_00：是的。
67 00：03：49,639 --> 00：03：51,622 演讲者 SPEAKER_00：Final Frontiers，售价 200 美元。
68 00：03：52,301 --> 00：03：58,608 演讲者 SPEAKER_00：这是西斯廷教堂墙上的米开朗基罗的壁画，描绘了得救的人和被诅咒的人。
69 00：03：59,127 --> 00：03：59,449 演讲者 SPEAKER_00：华生。
70 00：04：00,169 --> 00：04：01,450 演讲者 SPEAKER_00：什么是玻璃判断？
71 00：04：01,510 --> 00：04：01,971 议长 SPEAKER_00：对。
72 00：04：02,828 --> 00：04：05,211 议长 SPEAKER_02：你知道的，其他两个家伙都做对了，真是太神奇了。
73 00：04：05,570 --> 00：04：06,872 议长 SPEAKER_02：但我确实注意到他们那里有东西。
74 00：04：06,951 --> 00：04：08,013 议长 SPEAKER_02：好的，再说一遍，让我们来了解一下。
75 00：04：08,153 --> 00：04：12,758 演讲者 SPEAKER_02：Watson 中的人工智能与深度学习相比如何？
76 00：04：13,538 --> 00：04：18,904 演讲者 SPEAKER_01：所以主要区别在于深度学习，你试图在没有人编程的情况下学习所有内容。
77 00：04：18,925 --> 00：04：24,130 演讲者 SPEAKER_01：在你的计算机模拟中，唯一被编程的是学习算法。
78 00：04：24,151 --> 00：04：28,355 演讲者 SPEAKER_01：这个神经网络中的一切都是从数据中学习的，而不是手动编程的。
79 00：04：28,875 --> 00：04：31,197 议长 SPEAKER_01：原来他们在想。
80 00：04：31,177 --> 00：04：32,680 议长 SPEAKER_01：嗯，是的，你可以这么说。
81 00：04：33,721 --> 00：04：34,242 议长 SPEAKER_01：我刚才知道。
82 00：04：34,603 --> 00：04：34,783 议长 SPEAKER_01：是的。
83 00：04：34,863 --> 00：04：35,685 议长 SPEAKER_01：不过，这准确吗？
84 00：04：36,146 --> 00：04：37,848 议长 SPEAKER_01：在某些方面是独立思考吗？
85 00：04：38,048 --> 00：04：40,973 演讲者 SPEAKER_01：你可能会激怒一些哲学家，但是，是的，我认为他们在思考。
86 00：04：42,196 --> 00：04：43,819 议长 SPEAKER_01：呃，这很沉重。
87 00：04：43,959 --> 00：04：47,384 议长 SPEAKER_02：好，我们......我们这里有一个例子。
88 00：04：47,865 --> 00：04：48,507 议长 SPEAKER_02：我们试试这个好吗？
89 00：04：49,649 --> 00：04：52,233 议长 SPEAKER_02：把我值得信赖的，嗯，设备拿到这里了。
90 00：04：52,512 --> 00：04：53,435 议长 SPEAKER_02：好的。
91 00：04：53,454 --> 00：04：56,459 演讲者 SPEAKER_02：这是谷歌翻译...
92 00：04：56,879 --> 00：05：01,403 演讲者 SPEAKER_02：它是为 iPad 编程的，显然可以将西班牙语翻译成英语。
93 00：05：01,442 --> 00：05：02,685 议长 SPEAKER_02：这就是它现在的编程目的。
94 00：05：03,966 --> 00：05：05,586 演讲者 SPEAKER_02：Sheldon，你想把摄像机对准这个，我们试试这个？
95 00：05：05,887 --> 00：05：10,492 发言者 SPEAKER_02：我们这里有一个叫 hola（西班牙语）的东西。
96 00：05：10,512 --> 00：05：11,593 演讲者 SPEAKER_02：现在，让我们看看是不是这样。
97 00：05：12,012 --> 00：05：12,954 议长 SPEAKER_02：它被编程到这里。
98 00：05：13,574 --> 00：05：15,156 议长 SPEAKER_02：我们要把这个放在上面。
99 00：05：15,175 --> 00：05：16,276 议长 SPEAKER_02：哦，这已经发生了。
100 00：05：16,617 --> 00：05：17,017 演讲者 SPEAKER_02：你看那个。
101 00：05：18,218 --> 00：05：26,447 演讲者 SPEAKER_02：你把摄像头放在 hola 上方，它立即一遍又一遍地翻译成你好。
102 00：05：27,439 --> 00：05：30,785 演讲者 SPEAKER_02：您能给我们介绍一下神经网络是怎么参与的吗？
103 00：05：30,805 --> 00：05：32,166 演讲者 SPEAKER_02：首先，什么是神经网络？
104 00：05：32,187 --> 00：05：33,389 议长 SPEAKER_02：因为这就是这里发生的事情，对吧？
105 00：05：33,730 --> 00：05：37,975 演讲者 SPEAKER_01：好的，所以神经网络是一大堆神经元的模拟。
106 00：05：37,995 --> 00：05：41,822 演讲者 SPEAKER_01：这是通过改变神经元之间的连接强度来学习的东西。
107 00：05：42,603 --> 00：05：43,584 议长 SPEAKER_01：这就是这里发生的事情吗？
108 00：05：44,146 --> 00：05：46,949 演讲者 SPEAKER_01：所以为了识别字符，
109 00：05：46,930 --> 00：05：56,007 演讲者 SPEAKER_01：它使用神经网络，这个神经网络是用来自许多不同字体的大量字符训练的，并且有很多不同的失真和噪声。
110 00：05：56,968 --> 00：06：04,803 演讲者 SPEAKER_01：神经网络是目前能够可靠地识别变形和嘈杂字符的最佳系统。
111 00：06：04,783 --> 00：06：14,392 演讲者 SPEAKER_02：那么，这个程序只是翻译那个，因为有人做了一个代码来考虑西班牙语中所有可能的单词来翻译吗？
112 00：06：15,134 --> 00：06：17,240 议长 SPEAKER_02：还是这东西在思考？
113 00：06：18,384 --> 00：06：25,673 演讲者 SPEAKER_01：好的，对于这个特定的程序，我认为目前它没有使用神经网络进行翻译。
114 00：06：25,694 --> 00：06：28,338 演讲者 SPEAKER_01：它使用神经网络来进行字符识别。
115 00：06：29,678 --> 00：06：34,324 演讲者 SPEAKER_01：但是谷歌和其他人已经有神经网络在做翻译了。
116 00：06：34,886 --> 00：06：36,127 议长 SPEAKER_01：他们在做翻译。
117 00：06：36,567 --> 00：06：38,589 议长 SPEAKER_01：他们目前没有在网上使用。
118 00：06：39,170 --> 00：06：44,718 演讲者 SPEAKER_01：当你做谷歌翻译时，它会查看一种语言的短语，然后将它们翻译成另一种语言的短语。
119 00：06：44,737 --> 00：06：47,802 演讲者 SPEAKER_01：它有一张巨大的桌子。
120 00：06：47,781 --> 00：06：55,654 演讲者 SPEAKER_01：但是有一种新的机器翻译方法更有趣，它使用神经网络，它读取一种语言的句子并将其转化为一个想法。
121 00：06：56,435 --> 00：07：00,702 演讲者 SPEAKER_01：也就是说，当我说什么时，它表达了一个想法。
122 00：07：01,584 --> 00：07：07,494 发言者 SPEAKER_01：显然，翻译的方法是弄清楚第一语言表达的想法，然后用第二语言说同样的事情。
123 00：07：08,456 --> 00：07：11,781 演讲者 SPEAKER_01：谷歌现在有这样工作的翻译系统。
124 00：07：11,812 --> 00：07：17,658 演讲者 SPEAKER_01：它们与中型训练集上的现有翻译系统差不多。
125 00：07：18,199 --> 00：07：23,704 演讲者 SPEAKER_01：在非常大的数据集上，它们还不如现有系统，但它们会好。
126 00：07：24,365 --> 00：07：38,459 演讲者 SPEAKER_01：几年后，我们将进行机器翻译，将一种语言的句子转化为一个大的神经活动模式，即该句子背后的思想，然后用另一种语言说出该思想。
127 00：07：38,625 --> 00：07：41,790 演讲者 SPEAKER_01：当它看到细微差别时，它能理解它吗？
128 00：07：42,372 --> 00：07：43,814 议长 SPEAKER_01：它能理解一些细微差别。
129 00：07：44,454 --> 00：07：47,158 演讲者 SPEAKER_01：目前，它还可以进行很多改进。
130 00：07：48,761 --> 00：07：50,463 议长 SPEAKER_01：所以它目前不能做一些事情。
131 00：07：50,704 --> 00：07：56,752 演讲者 SPEAKER_01：就像我用英语对你说，奖杯放不进行李箱里，因为它太大了。
132 00：07：56,733 --> 00：07：59,877 演讲者 SPEAKER_01：你知道它指的是奖杯，因为它放不下。
133 00：08：00,317 --> 00：08：05,444 演讲者 SPEAKER_01：但是如果我说奖杯太小了，所以不能放进行李箱，你知道它指的是行李箱。
134 00：08：05,985 --> 00：08：08,747 演讲者 SPEAKER_01：这就是影响你翻译方式的现实世界的知识。
135 00：08：09,528 --> 00：08：14,975 演讲者 SPEAKER_01：现在，如果你从英语翻译成法语，用法语你就是不能说，你必须选择一个性别。
136 00：08：15,836 --> 00：08：25,069 演讲者 SPEAKER_01：所以我们不能把那个英文句子翻译成法语句子，因为你需要现实世界的知识来决定用什么性别来写它。
137 00：08：25,555 --> 00：08：26,375 议长 SPEAKER_01：那会发生的。
138 00：08：26,697 --> 00：08：28,759 议长 SPEAKER_01：我不知道几年后还是十年后会发生。
139 00：08：29,281 --> 00：08：31,764 议长 SPEAKER_01：但是一旦发生这种情况，我们就会知道这真的是理解。
140 00：08：32,046 --> 00：08：35,471 议长 SPEAKER_01：它能毫不费力地找出同音异义词吗？
141 00：08：35,532 --> 00：08：36,754 议长 SPEAKER_01：像这样的事情没问题。
142 00：08：36,894 --> 00：08：41,120 演讲者 SPEAKER_01：这是利用复杂的现实世界知识来消除事物的歧义。
143 00：08：42,062 --> 00：08：44,846 议长 SPEAKER_01：它开始能够做到了，但还不能正确地做到。
144 00：08：44,826 --> 00：08：49,211 演讲者 SPEAKER_02：您认为深度学习将特别改变未来的某个领域吗？
145 00：08：50,033 --> 00：08：54,239 演讲者 SPEAKER_01：不，我认为它会在很多很多领域改变未来。
146 00：08：54,678 --> 00：08：55,820 演讲者 SPEAKER_01：那么让我给你举几个例子。
147 00：08：56,400 --> 00：09：02,389 演讲者 SPEAKER_01：在过去的几年里，它已经成为识别语音的首选方法。
148 00：09：03,330 --> 00：09：08,155 演讲者 SPEAKER_01：它现在正在成为转录语音的首选方法。
149 00：09：08,196 --> 00：09：14,243 演讲者 SPEAKER_01：所以，从声波一直到转录所说的内容，只有一个神经网络可以做所有事情。
150 00：09：14,222 --> 00：09：17,551 演讲者 SPEAKER_01：它将成为机器翻译的首选方法。
151 00：09：19,677 --> 00：09：21,341 演讲者 SPEAKER_01：假设你想设计一种新药。
152 00：09：21,861 --> 00：09：29,581 演讲者 SPEAKER_01：你想知道，我给你一堆候选分子，你想知道它们与某个目标位点的结合情况如何。
153 00：09：29,865 --> 00：09：34,030 演讲者 SPEAKER_01：你喜欢预测这个，而不是做实验，因为做预测比做实验便宜得多。
154 00：09：34,510 --> 00：09：36,854 演讲者 SPEAKER_01：然后你只对那些预测效果好的那些进行实验。
155 00：09：37,995 --> 00：09：41,158 演讲者 SPEAKER_01：神经网络最近成为实现这一目标的最佳方法。
156 00：09：42,039 --> 00：09：47,426 演讲者 SPEAKER_01：如果你想识别路上的行人，神经网络绝对是最好的方法。
157 00：09：48,366 --> 00：09：50,730 议长 SPEAKER_01：所以一切都结束了。
158 00：09：50,769 --> 00：09：57,577 演讲者 SPEAKER_01：这些神经网络，尤其是那些使用深度学习算法的神经网络，
159 00：09：57,557 --> 00：10：00,227 扬声器 SPEAKER_01： 将无处不在。
160 00：10：00,248 --> 00：10：05,307 演讲者 SPEAKER_01：你认为我们距离神经网络能够做大脑能做的任何事情还有多少年？
161 00：10：05,658 --> 00：10：06,278 议长 SPEAKER_01：我不知道。
162 00：10：06,318 --> 00：10：08,923 演讲者 SPEAKER_01：很难预测五年后的未来。
163 00：10：08,962 --> 00：10：10,365 议长 SPEAKER_01：我认为这在未来五年内不会发生。
164 00：10：11,125 --> 00：10：15,692 演讲者 SPEAKER_01：除此之外，一切都是一种迷雾，所以我在做出预测时会非常谨慎。
165 00：10：15,711 --> 00：10：18,716 议长 SPEAKER_01：这有什么让你紧张的地方吗？
166 00：10：19,758 --> 00：10：22,041 议长 SPEAKER_01：嗯，从长远来看，是的。
167 00：10：22,322 --> 00：10：29,511 演讲者 SPEAKER_01：我的意思是，很明显，拥有其他比我们更聪明的超级智能生物是一件令人紧张的事情。
168 00：10：29,812 --> 00：10：33,197 议长 SPEAKER_01：这不会在很长一段时间内发生，但从长远来看，这是一件值得紧张的事情。
169 00：10：33,277 --> 00：10：35,259 演讲者 SPEAKER_01：这其中的哪个方面让你紧张？
170 00：10：35,240 --> 00：10：36,361 议长 SPEAKER_01：嗯，他们会对我们好吗？
171 00：10：38,024 --> 00：10：38,926 演讲者 SPEAKER_01：就像电影一样。
172 00：10：39,407 --> 00：10：41,049 议长 SPEAKER_02：你担心电影里的那个场景。
173 00：10：41,070 --> 00：10：42,392 演讲者 SPEAKER_01：从很长远来看，是的。
174 00：10：42,412 --> 00：10：45,378 议长 SPEAKER_01：我认为在未来的五到十年内，我们不必担心它。
175 00：10：45,898 --> 00：10：52,590 演讲者 SPEAKER_01：另外，电影总是把它描绘成一种个体智能。
176 00：10：52,942 --> 00：11：00,094 演讲者 SPEAKER_01：我认为它可能朝着不同的方向发展，我们与这些事物共同发展。
177 00：11：00,313 --> 00：11：02,437 演讲者 SPEAKER_01：所以，这些东西并不是完全自主的。
178 00：11：02,456 --> 00：11：03,879 演讲者 SPEAKER_01：他们是为了帮助我们而开发的。
179 00：11：03,919 --> 00：11：05,162 议长 SPEAKER_01：他们就像私人助理。
180 00：11：06,003 --> 00：11：08,486 演讲者 SPEAKER_01：我们会和他们一起发展。
181 00：11：08,466 --> 00：11：11,852 议长 SPEAKER_01：这更像是一种共生关系，而不是竞争。
182 00：11：12,293 --> 00：11：13,274 议长 SPEAKER_01：但我们不知道。
183 00：11：13,294 --> 00：11：14,778 议长 SPEAKER_02：这是期待还是希望？
184 00：11：14,798 --> 00：11：15,278 议长 SPEAKER_02：那是个希望。
185 00：11：15,298 --> 00：11：17,221 演讲者 SPEAKER_02：这听起来更像是一种希望，而不是期望。
186 00：11：17,743 --> 00：11：18,445 议长 SPEAKER_02：让我在这里读一下。
187 00：11：18,485 --> 00：11：24,355 演讲者 SPEAKER_02：这是 G. Clay Whitaker 去年 12 月的《桃子与每日野兽》中，谈到了 AI 掌舵的那一年。
188 00：11：24,575 --> 00：11：27,620 演讲者 SPEAKER_02：人工智能今年所做的不仅仅是关注算法。
189 00：11：28,182 --> 00：11：31,807 演讲者 SPEAKER_02：虽然我们多年来一直听说超级计算机和量子计算，
190 00：11：31,788 --> 00：11：40,043 演讲者 SPEAKER_02：这是第一次以闪电般的速度思考答案文本开始与你我分担道路、屋顶和责任。
191 00：11：40,344 --> 00：11：43,049 议长 SPEAKER_02：人们对这是否是一件好事存在分歧。
192 00：11：44,673 --> 00：11：48,480 演讲者 SPEAKER_02：我确实想追求这种期望与希望的概念。
193 00：11：49,826 --> 00：11：51,490 议长 SPEAKER_02：你希望一切都会顺利。
194 00：11：52,311 --> 00：11：56,837 议长 SPEAKER_02：但从长远来看，我感觉到你的期望可能没有那么良性。
195 00：11：56,898 --> 00：11：57,759 议长 SPEAKER_02：这样说公平吗？
196 00：11：58,299 --> 00：12：02,767 议长 SPEAKER_01：我认为很难知道五年后会发生什么。
197 00：12：03,347 --> 00：12：06,292 演讲者 SPEAKER_01：所以我的心态是，我就是不知道会发生什么。
198 00：12：07,333 --> 00：12：13,043 议长 SPEAKER_01：我认为试图阻止这项技术会非常困难。
199 00：12：13,582 --> 00：12：15,986 演讲者 SPEAKER_01：我的意思是，如果你看看自动柜员机，
200 00：12：16,674 --> 00：12：20,682 演讲者 SPEAKER_01：我猜当他们被介绍时，人们抱怨他们让银行出纳员失业。
201 00：12：21,182 --> 00：12：23,827 议长 SPEAKER_01：但我想现在没有人会说他们是个坏主意。
202 00：12：24,308 --> 00：12：24,990 议长 SPEAKER_01：甚至银行出纳员？
203 00：12：25,149 --> 00：12：26,272 议长 SPEAKER_01：甚至银行出纳员。
204 00：12：26,292 --> 00：12：30,679 议长 SPEAKER_01：我的意思是，他们的工作更有趣，因为他们处理棘手的案件，而不是你只想拿出 20 美元。
205 00：12：31,520 --> 00：12：31,701 议长 SPEAKER_01：是的。
206 00：12：32,383 --> 00：12：37,451 演讲者 SPEAKER_01：很明显，这项技术是一股向善的力量。
207 00：12：37,432 --> 00：12：45,004 演讲者 SPEAKER_01：嗯，一项技术是好的力量还是坏的力量，很大程度上取决于政治制度以及政治制度决定如何处理它。
208 00：12：45,125 --> 00：12：55,322 演讲者 SPEAKER_02：这就是我想跟进的内容，因为很明显，生活许多不同领域的事物变化得如此之快，比我们的政治制度制定规则和法律的设计速度还要快。
209 00：12：55,302 --> 00：13：03,537 演讲者 SPEAKER_02：那么，您认为政治或政府必须有多深入的参与才能应对这个领域即将发生的变化？
210 00：13：03,856 --> 00：13：04,999 议长 SPEAKER_01：他们必须参与进来。
211 00：13：05,139 --> 00：13：15,538 演讲者 SPEAKER_01：所以，如果你只看无人驾驶汽车，那么业内的每个人都很清楚，我认为，无人驾驶汽车将挽救很多生命。
212 00：13：15,518 --> 00：13：19,106 演讲者 SPEAKER_01：但是政客们对无人驾驶汽车第一次撞倒某人感到恐惧。
213 00：13：20,831 --> 00：13：28,730 演讲者 SPEAKER_01：所以从政治上讲，如果无人驾驶汽车杀死了几个人，但拯救了数万人，那对政客来说就是一个问题。
214 00：13：29,270 --> 00：13：33,662 议长 SPEAKER_01：但是他们应该正视它，说，看，这些东西会让我们更安全。
215 00：13：33,642 --> 00：13：38,408 议长 SPEAKER_02：需要一个勇敢的政治家来说，我知道有两个人被杀了，但这是我们拯救的 10,000 人。
216 00：13：38,428 --> 00：13：39,791 议长 SPEAKER_02：你看不到保存的 10,000 个。
217 00：13：39,811 --> 00：13：41,052 议长 SPEAKER_02：你肯定能看到那两个人被杀了。
218 00：13：41,413 --> 00：13：42,134 议长 SPEAKER_02：是的。
219 00：13：42,215 --> 00：13：43,296 议长 SPEAKER_01：而且会有很多这样的内容。
220 00：13：43,376 --> 00：13：46,260 演讲者 SPEAKER_01：但很明显，无人驾驶汽车将是一件好事。
221 00：13：48,063 --> 00：13：48,384 议长 SPEAKER_02：好的。
222 00：13：48,443 --> 00：13：52,409 演讲者 SPEAKER_02：那么总而言之，您希望深度学习对我们的未来产生什么样的影响？
223 00：13：52,964 --> 00：14：01,674 演讲者 SPEAKER_01：我希望它能让 Google 阅读文档并理解它们所说的内容，从而为你返回更好的搜索结果。
224 00：14：02,115 --> 00：14：06,701 演讲者 SPEAKER_01：所以你可以按文档的内容搜索，而不是按文档中的单词搜索。
225 00：14：06,721 --> 00：14：18,076 演讲者 SPEAKER_01：我希望它能成为聪明的私人助理，他们能够以明智的方式回答问题，并进行明智的对话，而不是不断脱轨的对话。
226 00：14：19,542 --> 00：14：20,964 演讲者 SPEAKER_01：它将为我们提供无人驾驶汽车。
227 00：14：21,344 --> 00：14：22,966 议长 SPEAKER_01：这显然很快就会到来。
228 00：14：23,846 --> 00：14：29,592 演讲者 SPEAKER_01：我想这会让电脑更容易使用，因为你可以对你的电脑说，我怎么打印这个该死的东西？
229 00：14：29,613 --> 00：14：32,796 演讲者 SPEAKER_01：然后计算机会来做这件事，而不是你得弄清楚所有这些命令。
230 00：14：33,417 --> 00：14：37,120 议长 SPEAKER_01：所以，如果你是对的，它应该让我们的生活变得更好。
231 00：14：37,522 --> 00：14：42,967 演讲者 SPEAKER_01：是的，我们应该像自动遥控机器一样，让那一点点生活变得更好，但它应该在很多事情上做到这一点。
232 00：14：43,928 --> 00：14：44,649 扬声器 SPEAKER_02：手指交叉。
233 00：14：45,110 --> 00：14：45,590 议长 SPEAKER_02：是的。
234 00：14：45,610 --> 00：14：47,773 演讲者 SPEAKER_02：Geoffrey Hannon，很高兴你今晚能参加我们的 TVO。
235 00：14：47,793 --> 00：14：48,874 演讲者 SPEAKER_02：非常感谢。
236 00：14：50,001 --> 00：14：53,157 演讲者 SPEAKER_00：通过学习的力量帮助 TVO 创造一个更美好的世界。
237 00：14：53,518 --> 00：14：57,395 演讲者 SPEAKER_00：今天就来 supporttvo.org 并进行免税捐款。

1 00:00:02,478 --> 00:00:04,120 说话人 SPEAKER_01：让我检查一下它是否可以点击通过。
2 00:00:04,500 --> 00:00:05,621 说话人 SPEAKER_01：你现在能看到下一张幻灯片了吗？
3 00:00:05,801 --> 00:00:05,982 说话人 SPEAKER_02：是的。
4 00:00:06,261 --> 00:00:07,442 说话人 SPEAKER_02：我们在这里看到了下一张幻灯片。
5 00:00:08,023 --> 00:00:08,484 说话人 SPEAKER_02：完美。
6 00:00:08,624 --> 00:00:08,864 演讲者 SPEAKER_02: 好吧。
7 00:00:08,923 --> 00:00:09,884 演讲者 SPEAKER_02: 谢谢，Hinton 博士。
8 00:00:09,965 --> 00:00:11,106 演讲者 SPEAKER_02: 扎克，请随意去掉。
9 00:00:11,606 --> 00:00:12,028 演讲者 SPEAKER_02: 很好。
10 00:00:12,048 --> 00:00:12,628 演讲者 SPEAKER_02：我会把它拿掉。
11 00:00:13,249 --> 00:00:17,312 演讲者 SPEAKER_06：我认为在所有演讲者中，Hinton 博士不需要介绍。
12 00:00:20,556 --> 00:00:20,835 演讲者 SPEAKER_01：很好。
13 00:00:20,896 --> 00:00:21,797 说话人 SPEAKER_06：我可以开始讲话了吗？
14 00:00:22,417 --> 00:00:23,498 演讲者 SPEAKER_01: 是的，你可以开始讲话。
15 00:00:24,339 --> 00:00:25,100 演讲者 SPEAKER_01: 好的，很好。
16 00:00:25,120 --> 00:00:25,501 演讲者 SPEAKER_01: 好的。
17 00:00:25,521 --> 00:00:28,103 演讲者 SPEAKER_01: 所以今天我要讲一些不同寻常的事情。
18 00:00:28,123 --> 00:00:31,286 说话人 SPEAKER_01：我将要谈论 Hopfield 网络和玻尔兹曼机。
19 00:00:31,638 --> 00:00:33,119 说话人 SPEAKER_01：这些最近变得热门。
20 00:00:34,142 --> 00:00:39,929 说话人 SPEAKER_01：这些是神经网络历史上的事物，现在从事神经网络的大多数人都不了解。
21 00:00:39,950 --> 00:00:46,279 说话人 SPEAKER_01：所以，我可以假设你对它们一无所知，但你确实对机器学习和神经网络有所了解。
22 00:00:47,381 --> 00:00:47,661 说话人 SPEAKER_01：好的。
23 00:00:49,444 --> 00:00:58,557 说话人 SPEAKER_01：所以，约翰·霍普菲尔德发明了被称为霍普菲尔德网络的东西，霍普菲尔德网络由具有它们之间循环连接的二进制阈值单元组成。
24 00:00:59,954 --> 00:01:04,001 说话人 SPEAKER_01：现在，一般来说，当你有一个循环网络时，它可以表现出各种不同的行为。
25 00:01:04,481 --> 00:01:08,629 说话人 SPEAKER_01：它可以稳定下来，它可以振荡，它可以变得混沌，所以你无法预测它会做什么。
26 00:01:09,590 --> 00:01:15,602 说话人 SPEAKER_01：但是 Hopfield 意识到，如果你使这种连接对称，那么就有一个全局能量函数。
27 00:01:16,082 --> 00:01:19,147 说话人 SPEAKER_01：并且连接的对称性等同于牛顿第三定律。
28 00:01:19,168 --> 00:01:21,992 说话人 SPEAKER_01：这相当于说作用力和反作用力大小相等，方向相反。
29 00:01:22,414 --> 00:01:24,257 说话人 SPEAKER_01：这就是全局能量函数所需要的。
30 00:01:26,481 --> 00:01:29,906 演讲者 SPEAKER_01：现在整个网络的每个二进制配置都有一个能量。
31 00:01:30,168 --> 00:01:35,995 演讲者 SPEAKER_01：我所说的二进制配置是指将所有神经元的值分配为 1 和 0。
32 00:01:36,918 --> 00:01:37,838 演讲者 SPEAKER_01：这就是一种配置。
33 00:01:39,781 --> 00:01:47,534 说话人 SPEAKER_01：二值阈值决策规则将使单元采用最小化能量的任何状态。
34 00:01:49,957 --> 00:01:53,602 说话人 SPEAKER_01：如果我们有一大堆神经元，
35 00:01:54,275 --> 00:01:57,257 说话人 SPEAKER_01：存在一个能量函数，它是许多不同贡献的总和。
36 00:01:57,698 --> 00:02:01,262 说话人 SPEAKER_01：这个能量函数将控制网络稳定下来时会发生什么。
37 00:02:02,644 --> 00:02:07,709 说话人 SPEAKER_01：所以能量函数有一个与单个神经元偏差有关的项。
38 00:02:07,728 --> 00:02:08,729 说话人 SPEAKER_01：我们先忽略这个。
39 00:02:09,431 --> 00:02:12,454 说话人 SPEAKER_01：然后主要术语涉及到神经元之间的交互。
40 00:02:13,455 --> 00:02:20,282 说话人 SPEAKER_01：因此第二个术语表示神经元 i 和神经元 j 之间存在一个权重 ij。
41 00:02:21,241 --> 00:02:24,425 说话人 SPEAKER_01：这就是那两个神经元的两种状态，要么是一，要么是零。
42 00:02:25,606 --> 00:02:29,954 说话人 SPEAKER_01：所以，只有当两个神经元都处于激活状态时，第二个项才会发挥作用。
43 00:02:30,715 --> 00:02:38,186 说话人 SPEAKER_01：如果两个神经元都处于激活状态，它们之间的权重就会对负能量做出贡献，或者我称之为“良好度”。
44 00:02:39,307 --> 00:02:46,358 说话人 SPEAKER_01：因此，良好的状态是指那些具有大量具有大权重的单元对的状况，这些单元对之间具有大的正权重。
45 00:02:46,377 --> 00:02:48,260 说话者 SPEAKER_01：我们先忽略偏差项。
46 00:02:50,856 --> 00:03:01,748 说话者 SPEAKER_01：所以这个二次能量函数使得每个神经元都能知道它对全局的贡献，以及它的状态对全局能量的贡献。
47 00:03:02,850 --> 00:03:17,387 说话者 SPEAKER_01：对于神经元 I，它会有一个我称之为能量间隙的概念，即在关闭神经元 I 的情况下，整个网络的能量减去在开启神经元 I 的情况下整个网络的能量，给定所有其他神经元的状态。
48 00:03:17,974 --> 00:03:20,337 说话者 SPEAKER_01：这就是神经元的能量间隙。
49 00:03:20,358 --> 00:03:22,801 说话人 SPEAKER_01：这就是随着神经元状态的改变，能量的变化。
50 00:03:23,842 --> 00:03:33,276 说话人 SPEAKER_01：这个能量间隙正是神经元 i 的偏置加上来自其他所有神经元 j 的连接权重之和。
51 00:03:34,016 --> 00:03:37,021 说话人 SPEAKER_01：这当然就是二进制阈值神经元所计算的内容。
52 00:03:37,481 --> 00:03:43,129 说话人 SPEAKER_01：它将来自所有其他激活神经元的输入加起来，即状态为 1 的神经元，加上偏置。
53 00:03:43,169 --> 00:03:45,252 说话人 SPEAKER_01：如果它大于 0，就会打开。
54 00:03:45,272 --> 00:03:46,615 说话人 SPEAKER_01：如果它小于 0，就会关闭。
55 00:03:48,907 --> 00:03:52,953 说话人 SPEAKER_01：这就是能隙，即能量差异，取决于它是开启还是关闭。
56 00:03:54,935 --> 00:03:57,018 说话人 SPEAKER_01：这里有一个 Hopfield 网络的例子。
57 00:04:00,562 --> 00:04:03,127 说话人 SPEAKER_01：我们可以给它赋予状态。
58 00:04:03,467 --> 00:04:05,449 说话人 SPEAKER_01：现在我已将状态分配给所有神经元。
59 00:04:06,211 --> 00:04:18,829 说话人 SPEAKER_01：如果我想计算该状态的能量，或者我称之为“良好度”的负能量，它就是所有单元对之间状态的乘积与它们之间权重的总和。
60 00:04:19,314 --> 00:04:23,980 说话人 SPEAKER_01：所以在这个网络中，只有一对单元同时处于激活状态，它们之间的权重为三。
61 00:04:24,581 --> 00:04:27,324 说话人 SPEAKER_01：这样我们得到一个能量为负三或者一个良好度为三。
62 00:04:28,326 --> 00:04:29,867 说话人 SPEAKER_01：通常谈论良好度更容易一些。
63 00:04:31,269 --> 00:04:37,757 说话人 SPEAKER_01：现在，我们可以通过选择网络中的一个神经元，比如那个，来更新网络的状态。
64 00:04:38,978 --> 00:04:42,382 说话人 SPEAKER_01：现在我们可以问，那个神经元的总输入是多少？
65 00:04:43,202 --> 00:04:45,286 说话人 SPEAKER_01：嗯，在这个网络中，我没有加入任何偏差。
66 00:04:45,805 --> 00:04:48,850 说话人 SPEAKER_01：所以总的输入是从其他活跃神经元那里得到的。
67 00:04:49,336 --> 00:04:53,482 说话人 SPEAKER_01：它从与之相连的活跃神经元那里得到一个-4 的输入。
68 00:04:53,862 --> 00:04:55,384 说话人 SPEAKER_01：与之相连的其他神经元处于关闭状态。
69 00:04:55,423 --> 00:04:56,966 演讲者 SPEAKER_01：所以它的总输入是减 4。
70 00:04:57,646 --> 00:04:59,490 说话人 SPEAKER_01：它将保持相同的状态。
71 00:04:59,550 --> 00:05:00,451 说话人 SPEAKER_01：它将保持在 0。
72 00:05:00,992 --> 00:05:02,173 说话人 SPEAKER_01：而良好度仍然是 3。
73 00:05:02,593 --> 00:05:05,216 说话人 SPEAKER_01：我认为我们可以选择另一个神经元。
74 00:05:05,237 --> 00:05:06,038 说话人 SPEAKER_01: 是的，就像那样。
75 00:05:07,000 --> 00:05:12,927 说话人 SPEAKER_01: 如果我们看那个，它的总输入来自其他活跃神经元的数量是 3。
76 00:05:12,947 --> 00:05:13,488 说话人 SPEAKER_01: 就这样。
77 00:05:14,110 --> 00:05:15,490 说话人 SPEAKER_01：所以它有一个正输入。
78 00:05:15,550 --> 00:05:16,692 说话人 SPEAKER_01：它会保持开启状态。
79 00:05:17,617 --> 00:05:21,862 说话人 SPEAKER_01：这有点无聊了，我们想要一个会变化的，所以让我们选择那个。
80 00:05:23,004 --> 00:05:35,677 说话人 SPEAKER_01：那个是关闭的，但是如果你看现在的总输入，它从其中一个神经元得到了加二，等等，从另一个神经元减一，所以我们得到了总输入加一，所以我们将它翻转成开启状态，因为它得到了正输入。
81 00:05:36,158 --> 00:05:42,524 说话人 SPEAKER_01：当我们将其翻转成开启状态时，网络的优良度变为四，能量变为负四，所以我们得到了网络更好的状态。
82 00:05:43,447 --> 00:05:46,629 说话人 SPEAKER_01：现在网络所处的状态，当这些单元三角处于激活状态时，
83 00:05:46,846 --> 00:05:50,290 说话人 SPEAKER_01：左边的三角，这是一个能量最小值。
84 00:05:51,250 --> 00:05:54,795 说话人 SPEAKER_01：如果你现在环顾四周并问，我能否翻转任何单元来使事情变得更好？
85 00:05:54,855 --> 00:05:55,314 说话人 SPEAKER_01：你不能。
86 00:05:56,677 --> 00:06:00,180 说话人 SPEAKER_01：这是一个局部最优解，能量的局部最小值。
87 00:06:01,961 --> 00:06:08,348 说话人 SPEAKER_01：但是如果你用同样的网络，实际上有一个更深的能量最小值，就是有一个右三角形。
88 00:06:10,692 --> 00:06:12,173 说话人 SPEAKER_01：所以网络有两个三角形。
89 00:06:12,553 --> 00:06:14,495 说话人 SPEAKER_01：这有点像美国的政治体系。
90 00:06:15,076 --> 00:06:16,718 说话人 SPEAKER_01：在这些三角形中，
91 00:06:17,086 --> 00:06:20,891 说话人 SPEAKER_01：神经元之间往往有正权重，这两个三角形彼此讨厌。
92 00:06:21,331 --> 00:06:27,220 说话人 SPEAKER_01：因此，两个三角形之间有一个大的负权重，即负四，尽管一个单元同时参与两个三角形。
93 00:06:28,362 --> 00:06:41,139 说话人 SPEAKER_01：因此，这个网络的两个最小值是在右侧三角形开启时，其良好度为五，这是最深的最低点，以及左侧三角形开启时，其良好度为四，不是那么好。
94 00:06:42,442 --> 00:06:45,185 说话人 SPEAKER_01：好吧，我刚刚都说了。
95 00:06:46,144 --> 00:06:47,427 说话人 SPEAKER_01：所以这就是全局最小值。
96 00:06:49,610 --> 00:06:52,394 说话人 SPEAKER_01：霍普菲尔德想到了利用这种网络的方法。
97 00:06:52,915 --> 00:07:05,692 说话人 SPEAKER_01：所以霍普菲尔德建立的是，如果你建立对称连接并使用二进制阈值决策规则，然后逐个更新神经元，网络将收敛到能量最小值，但不一定是全局最小值。
98 00:07:06,593 --> 00:07:10,860 说话人 SPEAKER_01：他认为你可以在生物学中用这个来存储记忆。
99 00:07:11,139 --> 00:07:16,067 说话人 SPEAKER_01：你可以把记忆想象成这样一个神经网络的能量最小值。
100 00:07:16,451 --> 00:07:23,860 说话人 SPEAKER_01：这显然是一个高度理想化的神经网络，但它是由二进制神经元组成的网络，这些神经元要么开启要么关闭，它们之间有成对权重。
101 00:07:25,442 --> 00:07:44,062 说话人 SPEAKER_01：然后可能发生的情况是，如果你对一个记忆进行破坏，或者遗漏了它的一部分，比如说一些神经元，我不知道它们的状态是什么，它们处于一种中立状态，或者我直接改变一些记忆单元的状态，它可以纠正破坏，或者填补不完整的记忆。
102 00:07:45,697 --> 00:07:50,444 讲者 SPEAKER_01：现在，关于记忆作为能量最小值这一想法，很久以前是由 I.A.提出的。
103 00:07:50,485 --> 00:07:52,307 讲者 SPEAKER_01：理查兹在文学批评一书中提出了这个观点。
104 00:07:53,170 --> 00:07:54,312 讲者 SPEAKER_01：霍普菲尔德并不知道这一点。
105 00:07:55,012 --> 00:07:58,338 讲者 SPEAKER_01：因此，霍普菲尔德没有必要参考那本书。
106 00:07:59,341 --> 00:08:01,785 说话人 SPEAKER_01: 当然，Schmidhuber 会说他是骗子，因为他没有这么做。
107 00:08:02,625 --> 00:08:10,540 说话人 SPEAKER_01: 能量最小值代表记忆，你可以得到一个内容可寻址的存储器。
108 00:08:11,396 --> 00:08:14,781 说话人 SPEAKER_01: 因此，你只需知道它的一些部分，就可以访问内存中的项目。
109 00：08：14,862 --> 00：08：19,048 议长 SPEAKER_01：你知道一些单位的状态，现在这个网络将填补其余的记忆。
110 00：08：21,370 --> 00：08：24,475 演讲者 SPEAKER_01：这是在 Google 之前的 16 年完成的。
111 00：08：25,415 --> 00：08：30,262 演讲者 SPEAKER_01：在 Google 之前的 16 年，内容可寻址内存还不习惯。
您通常不能只通过提供其内容的一小部分来访问事物，并获取其余部分。
113 00：08：35,289 --> 00：08：41,217 演讲者 SPEAKER_01：当然，一旦 Google 出现，你就可以通过在文档中给出几个生僻词来访问文档，然后你就会得到文档。
但是这是一个内容可寻址存储的早期例子，并且在生物学上更为合理。
115 00：08：47,731 --> 00：08：49,173 扬声器 SPEAKER_01：它对硬件损坏很强大。
116 00：08：50,375 --> 00：08：56,285 演讲者 SPEAKER_01：这有点像拥有几块恐龙骨骼，然后重建整个恐龙，人们一直在这样做。
117 00:08:59,370 --> 00:09:06,361 说话人 SPEAKER_01：所以，在 Hopfield 网络中存储记忆的方式取决于单元的状态是一还是负一。
118 00:09:06,982 --> 00:09:12,230 讲者 SPEAKER_01：我经常用单元来代替神经元，1 和 -1 的状态，或者 1 和 0 的状态。
119 00:09:12,350 --> 00:09:15,556 讲者 SPEAKER_01：如果它们是 1 和 -1 的状态，存储规则非常简单。
120 00:09:15,596 --> 00:09:21,705 讲者 SPEAKER_01：你只需通过这两个神经元在记忆中的活动乘积来改变权重。
121 00:09:22,645 --> 00:09:25,650 讲者 SPEAKER_01：所以如果它们的状态相反，你就减少权重。
122 00:09:26,071 --> 00:09:28,354 说话者 SPEAKER_01：如果它们处于相同的状态，你就增加权重。
123 00:09:29,115 --> 00:09:30,457 说话者 SPEAKER_01：这似乎是件合情合理的事。
124 00:09:30,918 --> 00:09:34,504 说话者 SPEAKER_01：这样就会形成一个能工作的 Hopfield 记忆。
125 00:09:34,524 --> 00:09:35,504 说话者 SPEAKER_01：如果它们有
126 00:09:36,076 --> 00:09:36,976 演讲者 SPEAKER_01：这是一个非常简单的规则。
127 00:09:36,996 --> 00:09:38,418 演讲者 SPEAKER_01：它根本不涉及迭代。
128 00:09:38,500 --> 00:09:40,001 演讲者 SPEAKER_01：你只需一次性存储记忆。
129 00：09：41,524 --> 00：09：48,934 发言者 SPEAKER_01：如果他们的权重状态为 0 和 1，则规则会稍微复杂一些。
130 00:09:48,975 --> 00:09:51,719 演讲者 SPEAKER_01：实际上有几种规则的版本，但这是其中一种。
131 00：09：52,519 --> 00：09：55,724 发言者 SPEAKER_01：你用这两个术语的乘积来改变权重。
每个这些项要么是减半，要么是加半，具体取决于神经元是开启还是关闭。
133 00:10:03,416 --> 00:10:05,138 说话人 SPEAKER_01：所以如果 Si 是关闭的，
134 00:10:05,741 --> 00:10:07,203 说话人 SPEAKER_01：第一个项你会得到减半。
135 00:10:07,264 --> 00:10:09,648 说话人 SPEAKER_01：如果 SJ 开启，第二个项你会得到加半。
136 00:10:10,669 --> 00:10:13,494 说话人 SPEAKER_01：因此你会减小两个神经元之间的权重。
137 00:10:14,014 --> 00:10:15,417 说话人 SPEAKER_01：它们会相互抑制。
138 00:10:16,719 --> 00:10:16,980 说话人 SPEAKER_01: 好吧。
139 00:10:19,123 --> 00:10:23,450 说话人 SPEAKER_01: 所以人们发现霍普菲尔德网络，物理学家做了很多研究。
140 00:10:24,410 --> 00:10:30,340 说话人 SPEAKER_01: 他们发现，限制你能存储多少记忆的一个因素是虚假的极小值。
141 00:10:31,562 --> 00:10:33,284 说话人 SPEAKER_01: 所以每次你记忆，
网络的一个完整配置，即对所有网络中的神经元进行一和零的分配，你创建了一个能量最小值。
143 00：10：40,297 --> 00：10：42,160 议长 SPEAKER_01：至少你希望你创造了一个最低限度的能源。
因此，例如，我们可以通过存储两个不同的记忆来创建两个不同的能量最小值，就像这样。
但如果记忆很接近，你可能会创建一个虚假的最小值，这是两种记忆的平均值。
146 00:10:58,749 --> 00:10:59,610 说话人 SPEAKER_01：这是个问题。
147 00:11:00,417 --> 00:11:02,359 说话人 SPEAKER_01：我们现在有一个错误的记忆。
148 00:11:02,440 --> 00:11:03,900 说话人 SPEAKER_01：这是这两个记忆的平均值。
149 00:11:06,784 --> 00:11:12,032 说话人 SPEAKER_01：有人在频道上和 Alexa 说话。
150 00:11:12,052 --> 00:11:13,774 说话人 SPEAKER_01: 确保这个人被静音。
151 00:11:13,813 --> 00:11:16,937 说话人 SPEAKER_02: 是的，我刚刚已经静音了他们。
152 00:11:16,998 --> 00:11:17,918 说话人 SPEAKER_02: 对不起。
153 00:11:17,938 --> 00:11:18,539 说话人 SPEAKER_01: 对不起。
154 00:11:20,062 --> 00:11:23,306 说话人 SPEAKER_01：这限制了 Hockfield 网络的能力。
155 00:11:24,388 --> 00:11:27,792 说话人 SPEAKER_01：实际上，那张图片非常糟糕。
156 00:11:28,581 --> 00:11:32,085 说话人 SPEAKER_01：因为 Hopfield 网络的状态空间是超立方体的角。
157 00:11:32,706 --> 00:11:37,950 讲者 SPEAKER_01：如果有 n 个神经元，就有一个 n 维超立方体，而不同的配置就是该超立方体的顶点。
158 00:11:38,931 --> 00:11:44,495 说话者 SPEAKER_01：所以把它展示成这样的一维，把状态空间展示成一维是误导的，但你应该明白这个意思。
159 00:11:44,836 --> 00:11:52,342 说话者 SPEAKER_01：如果你有两个相似的回忆，当你存储它们时，你实际上可能会得到这两个回忆的平均值，这个平均值比任何一个回忆都要好。
160 00:11:53,583 --> 00:11:58,048 说话者 SPEAKER_01：因此，为了处理这个问题，Hopfield、Feinstein 和 Palmer 建议
161 00:11:58,770 --> 00:12:03,316 说话者 SPEAKER_01：你应该将网络置于随机状态，让它稳定下来，然后进行去学习。
162 00:12:04,136 --> 00:12:10,403 说话人 SPEAKER_01：这样就可以摆脱深层极小值，因为它可能会稳定到一个深层极小值，这可能是虚假状态。
163 00:12:11,004 --> 00:12:12,725 说话人 SPEAKER_01：这似乎非常乐观。
164 00:12:13,226 --> 00:12:17,270 说话人 SPEAKER_01：他们展示了这是可行的，但他们没有解释为什么它可行。
165 00:12:17,892 --> 00:12:20,414 说话人 SPEAKER_01：他们没有关于应该进行多少遗忘学习的理论。
166 00:12:22,157 --> 00:12:27,883 说话人 SPEAKER_01：基于这个想法，基本上是 Feinstein 的想法，克里克和米钦森，
167 00:12:28,082 --> 00:12:31,909 说话人 SPEAKER_01：提出了可能学习是梦境的作用模型。
168 00:12:33,352 --> 00:12:36,259 说话人 SPEAKER_01：所以当你做梦时，你不会记得你的梦。
那就是，你醒来前所做的梦在你的短期记忆中，可能是在你神经网络中的权重临时变化中。
170 00：12：44,815 --> 00：12：49,163 演讲者 SPEAKER_01：它会很快消失，但你可以记得你刚才醒来前做的最后一点梦。
171 00：12：49,666 --> 00：12：56,153 演讲者 SPEAKER_01：但是如果我们在夜间随机叫醒你，我们可以看到你每晚做几个小时的梦，而你却记不住了。
172 00:12:56,173 --> 00:12:57,153 说话者 SPEAKER_01：你为什么记不住它们？
173 00:12:57,173 --> 00:12:58,215 说话人 SPEAKER_01：因为它们很有趣。
174 00:12:59,056 --> 00:13:01,879 说话人 SPEAKER_01：我的梦通常比白天发生的事情更有趣。
175 00:13:01,938 --> 00:13:03,841 说话人 SPEAKER_01：然而我却不记得它们。
176 00:13:04,861 --> 00:13:12,409 说话人 SPEAKER_01：因此克里克的看法是，梦的目的是为了忘记那些没有得到数据支持的你倾向于相信的事情。
177 00:13:13,971 --> 00:13:18,035 说话人 SPEAKER_01：但他们无法说明你应该忘记多少。
178 00:13:18,386 --> 00:13:26,537 说话人 SPEAKER_01：我和 Terry Sanofsky 开始思考，能否将知识遗忘作为最小化某些合理成本函数的方法？
179 00:13:27,138 --> 00:13:31,623 说话人 SPEAKER_01：因此，我们可以确定应该进行多少次知识遗忘，以及通过知识遗忘进行优化的是什么。
180 00:13:33,565 --> 00:13:39,812 说话人 SPEAKER_01：然后，这结合了关于你可能使用 Hopfield 网络的不同理论。
181 00:13:41,575 --> 00:13:45,860 说话人 SPEAKER_01：所以我们不仅仅可以用它们来存储记忆，我们还可以用它们来进行感知。
182 00:13:46,421 --> 00:13:47,381 说话人 SPEAKER_01：在感知方面，
183 00:13:47,750 --> 00:13:52,916 说话人 SPEAKER_01：你所拥有的是一些输入，比如图像的像素，你想要对这些输入进行解释。
184 00:13:53,576 --> 00:13:56,280 说话人 SPEAKER_01：也就是说，获取对图像所描绘内容的某种解释。
所以要用 Hopfield 网络做到这一点，你需要将单元分成两组。
186 00：14：02,008 --> 00：14：04,691 演讲者 SPEAKER_01：会有可见的单位，这是你输入的地方。
187 00：14：04,711 --> 00：14：06,513 演讲者 SPEAKER_01：所以它们就像像素一样。
188 00：14：07,374 --> 00：14：09,037 扬声器 SPEAKER_01：如果你打开它们，那是一个明亮的像素。
189 00:14:09,076 --> 00:14:10,538 说话人 SPEAKER_01: 如果你关闭它们，那就是一个暗淡的像素。
190 00:14:11,379 --> 00:14:17,486 说话人 SPEAKER_01：然后会有隐藏单元对应于你解读为图像中发生的事情。
191 00:14:19,964 --> 00:14:24,932 说话人 SPEAKER_01：整个配置的能量将代表解释的糟糕程度。
192 00:14:25,732 --> 00:14:34,826 说话人 SPEAKER_01：因此，随着隐藏单元的更新，它们逐渐稳定到更好的状态、更低能量的状态，它们将获得更好的输入图像的解释。
193 00:14:36,288 --> 00:14:38,432 讲者 SPEAKER_01：我想给您一个具体的例子。
194 00:14:38,491 --> 00:14:43,119 演讲者 SPEAKER_01：这不是一个现实的例子，但它能让你理解这个概念。
195 00:14:44,822 --> 00:14:48,267 演讲者 SPEAKER_01：假设我给你展示一些图像中的 2D 线条。
196 00:14:48,618 --> 00:14:51,080 演讲者 SPEAKER_01：而你希望将它们解释为 3D 边缘。
197 00:14:52,942 --> 00:14:57,046 演讲者 SPEAKER_01：所以显然，在 2D 线条中，你丢失了一些关于 3D 边缘的信息。
198 00:14:57,647 --> 00:15:02,552 说话人 SPEAKER_01: 你已经丢失了关于该边端点深度的信息。
199 00:15:04,232 --> 00:15:05,735 说话人 SPEAKER_01: 所以你的眼球在蓝色中。
200 00:15:06,414 --> 00:15:08,297 说话人 SPEAKER_01: 并且有两个红色射线。
201 00:15:09,357 --> 00:15:16,304 说话人 SPEAKER_01: 我将向你展示许多会导致图像中完全相同的 2D 线的 3D 边。
202 00:15:16,846 --> 00:15:18,287 说话人 SPEAKER_01: 那一个和那一个。
203 00:15:18,773 --> 00:15:24,898 说话人 SPEAKER_01: 两个 3D 边缘的端点都落在这两条射线上时，都会在图像上形成同一条线。
204 00:15:25,619 --> 00:15:27,240 说话人 SPEAKER_01: 这就是你所丢失的信息。
你知道其中一条边是正确的，但你不知道深度，不知道这些射线上的深度有多深。
206 00：15：37,068 --> 00：15：37,369 议长 SPEAKER_01：好的。
所以这里有一整个家族的边，它们都可以是图像中同一条 2D 线的解释。
208 00：15：45,817 --> 00：15：48,139 演讲者 SPEAKER_01：所以我们可以建立一个知道这一点的神经网络。
209 00:15:49,097 --> 00:15:55,065 讲者 SPEAKER_01：我们知道如果物体是不透明的，你一次只能看到其中一条边，因为它们会互相遮挡。
210 00:15:57,488 --> 00:16:03,836 讲者 SPEAKER_01：所以，我们可以有一个线描图，并且我们可以有一大堆代表特定二维线的神经元。
211 00:16:05,437 --> 00:16:07,899 讲者 SPEAKER_01：先不用担心效率，这只是为了传达想法。
212 00:16:09,461 --> 00:16:16,049 讲者 SPEAKER_01：在这张图片中，有两条特定的线，我展示了如果这些线存在，哪些神经元会变得活跃。
213 00:16:16,721 --> 00:16:22,368 说话人 SPEAKER_01：一般来说，每张图像中只有少数这些神经元会变得活跃，因为图像中的大多数线并不存在。
214 00:16:23,850 --> 00:16:27,614 说话人 SPEAKER_01：然后对于这些行，我们可以有很多 3D 边。
215 00:16:29,316 --> 00:16:30,616 说话人 SPEAKER_01：这些边应该竞争。
216 00:16:31,817 --> 00:16:40,967 说话人 SPEAKER_01：所以这些绿色箭头是具有正权重的连接，表示如果你看到这条线，你应该尝试激活所有这些 3D 边。
217 00:16:41,708 --> 00:16:45,192 说话人 SPEAKER_01：红色线条是具有负权重的连接。
218 00:16:45,359 --> 00:16:48,282 说话人 SPEAKER_01：这些边中的每一个都会抑制其他所有边。
219 00:16:49,402 --> 00:17:01,832 说话人 SPEAKER_01：因此，你需要一个小电路，当你激活一条线时，它会尝试激活这些边，并且对任何一条边被激活都感到满意，但如果激活了多条边，它就会感到不高兴。
220 00:17:02,573 --> 00:17:04,536 说话人 SPEAKER_01：但它实际上不知道该激活哪条边。
221 00:17:05,936 --> 00:17:07,778 说话人 SPEAKER_01：我们还可以对图像中的另一条线做同样的处理。
222 00:17:08,919 --> 00:17:12,782 说话人 SPEAKER_01: 然后我们可以这样说，我们对世界中的物体有一些先验知识。
223 00:17:13,303 --> 00:17:14,403 说话人 SPEAKER_01: 例如，
224 00:17:14,738 --> 00:17:26,868 说话人 SPEAKER_01: 我们认为，如果你看到两条线，比如这些箭头在底部的线条，那么如果它们在图像中相交，它们在三维空间中很可能也相交。
225 00:17:27,970 --> 00:17:33,354 说话人 SPEAKER_01: 因此，我们可以在这两条线的解释之间建立一种联系，就像这样。
226 00:17:34,474 --> 00:17:38,439 说话人 SPEAKER_01：假设这两条线实际上在三维空间中相交。
227 00:17:40,260 --> 00:17:42,923 说话人 SPEAKER_01：然后我们将在这两条线之间建立一个正向连接。
228 00:17:42,942 --> 00:17:44,104 说话人 SPEAKER_01：它们应该相互支撑。
229 00:17:45,096 --> 00:17:49,782 说话人 SPEAKER_01：实际上可能存在两条在三维空间中垂直相交的线。
230 00:17:50,303 --> 00:17:53,386 说话者 SPEAKER_01：然后我们添加大的正连接，因为我们喜欢看到直角。
231 00:17:54,208 --> 00:17:56,211 说话者 SPEAKER_01：所以有两条线在直角处相交。
232 00:17:56,631 --> 00:17:58,894 说话者 SPEAKER_01：我们真的希望这两条线能同时激活。
233 00:18:00,016 --> 00:18:13,972 说话者 SPEAKER_01：我认为如果我们添加这样的连接，这些连接实际上关于我们期望物体看起来像什么，以及我们期望如何解释线条，并构建一个像这样的大型网络，那么网络可能会有许多稳定状态，
234 00:18:14,222 --> 00:18:32,943 说话者 SPEAKER_01：例如，对应于在深度上翻转这个立方体，就像尼克尔立方体一样，但它应该稳定在一个状态中，在这个状态中，它试图最大化有多少条线的深度相交，以及它们是否在图像中相交，以及有多少个直角。
好的，所以你可以制作一个这样的电路，你可以希望如果在连接上放置正确的权重，你将能够用 3D 边缘来解释图像。
236 00：18：45,424 --> 00：18：46,705 议长 SPEAKER_01：这就引出两个问题。
237 00：18：47,507 --> 00：18：48,807 演讲者 SPEAKER_01：一个是搜索问题。
238 00:18:49,528 --> 00:18:52,412 说话人 SPEAKER_01：如何避免陷入局部最小值？
239 00:18:52,751 --> 00:18:55,173 说话人 SPEAKER_01：因为如果你在做感知，你希望得到最好的解释。
240 00:18:56,516 --> 00:19:06,464 说话人 SPEAKER_01：第二个问题是，嗯，你是如何学习所有这些连接强度的？
241 00:19:07,486 --> 00:19:08,807 说话人 SPEAKER_01：那么让我们从第一个问题开始。
242 00:19:12,090 --> 00:19:13,592 说话人 SPEAKER_01：如果你想避免局部最小值，
243 00:19:14,280 --> 00:19:17,183 说话人 SPEAKER_01：实际上在决策规则中加入一些噪声是有益的。
244 00:19:18,506 --> 00:19:25,795 说话人 SPEAKER_01：所以在 Hopfield 网络中，使用二进制阈值决策规则，这将总是将单元翻转至降低能量的任何状态。
245 00:19:27,096 --> 00:19:29,157 说话人 SPEAKER_01：这使得无法逃离局部最小值。
246 00:19:31,240 --> 00:19:36,047 演讲者 SPEAKER_01：所以如果你处于局部最小值 A，你就无法到达 B，因为系统中没有噪声。
247 00:19:37,929 --> 00:19:44,135 但是如果你添加一些随机噪声，你可能会在 A 处产生一些抖动，最终到达 B，而且你从 A 到 B 的可能性比从 B 到 A 的可能性更大。所以噪声会提高你的机会，因为从 B 到 A 的能量障碍比反过来更高。
248 00:19:44,825 --> 00:19:53,875 所以噪声会帮助你提高机会，因为从 B 到 A 的能量障碍比反过来更高。
249 00:19:56,337 --> 00:20:01,423 现在，有一种称为模拟退火的技术，它建议开始时添加大量噪声，然后逐渐减少噪声的数量。
250 00:20:02,064 --> 00:20:05,267 说话人 SPEAKER_01：因为有很多噪音，很容易越过障碍。
251 00:20:06,107 --> 00:20:11,974 说话人 SPEAKER_01：而有一点噪音，你几乎不可能从 B 到 A。最佳折衷方案是从大量噪音开始，逐渐减少。
252 00:20:12,535 --> 00:20:13,936 说话人 SPEAKER_01：这被称为模拟退火。
253 00:20:14,862 --> 00:20:19,167 说话人 SPEAKER_01：它是由 Kirkpatrick 等人发明的，大约与 Hopfield 网络发明的时间相同。
254 00:20:21,290 --> 00:20:28,000 讲者 SPEAKER_01：我和 Terry 对在 Hopfield 网络中使用随机二进制单元以及使用模拟退火非常感兴趣。
255 00:20:30,001 --> 00:20:31,463 讲者 SPEAKER_01：那么让我们看看一个随机单元。
256 00:20:33,366 --> 00:20:35,829 讲者 SPEAKER_01：它有一个叫做温度的东西，这是噪声的量。
257 00:20:37,352 --> 00:20:43,079 讲者 SPEAKER_01：噪声水平有效地减小了配置之间的能量间隙。
258 00:20:43,583 --> 00:20:45,204 演讲者 SPEAKER_01：所以这个方程看起来是这样的。
259 00:20:46,226 --> 00:20:49,189 演讲者 SPEAKER_01：在热场网中，温度为零。
260 00:20:49,669 --> 00:21:05,065 演讲者 SPEAKER_01：所以如果你把温度设为零，你会看到如果ΔE 是正的，我们就有 E 的负数次方，它是负无穷大，E 的负无穷大是零。
261 00:21:05,746 --> 00:21:11,531 演讲者 SPEAKER_01：所以如果ΔE 是正的，我们就会得到 1 除以 1，单位被激活的概率将是 1。
262 00:21:13,029 --> 00:21:18,457 演讲者 SPEAKER_01：如果ΔE 是负数，我们将除以零，得到负无穷大。
263 00:21:19,298 --> 00:21:22,962 演讲者 SPEAKER_01：那么我们将得到 E 的负无穷大，我是不是理解错了？
264 00:21:23,483 --> 00:21:30,472 演讲者 SPEAKER_01：是的，如果ΔE 是负数，带有负号的ΔE，我们得到正数，得到 E 的正无穷大。
265 00:21:30,913 --> 00:21:34,397 演讲者 SPEAKER_01：所以我们得到 1 除以 1 加上 E 的正无穷大，结果是零。
266 00:21:34,979 --> 00:21:36,881 说话人 SPEAKER_01：那么打开它的概率将是零。
267 00:21:37,281 --> 00:21:38,584 说话人 SPEAKER_01：这就是 Hopfield 决策规则。
268 00:21:38,604 --> 00:21:42,008 说话人 SPEAKER_01：这只是一个在零温度下的随机决策规则。
269 00:21:42,730 --> 00:21:48,018 说话人 SPEAKER_01：显然，如果你懂任何物理学，问题就是当温度不是零时会发生什么？
270 00:21:49,359 --> 00:21:51,061 说话人 SPEAKER_01：然后你得到了模拟退火。
271 00:21:52,203 --> 00:21:54,067 说话人 SPEAKER_01：这似乎是霍普菲尔德网络的重大进步。
272 00:21:54,086 --> 00:21:58,534 说话人 SPEAKER_01：它们可以进行模拟退火。
273 00:21:59,355 --> 00:22:06,806 说话人 SPEAKER_01：我只是提醒你能量间隙的公式，它只是单位关闭时的能量与单位开启时的能量之差。
274 00:22:08,828 --> 00:22:10,932 说话人 SPEAKER_01：但是模拟退火最终变成了一个干扰。
275 00:22:11,266 --> 00:22:14,910 说话人 SPEAKER_01：它仍然有用，但我不想深入探讨其中的复杂性。
276 00:22:15,951 --> 00:22:24,358 说话人 SPEAKER_01：所以我们现在将保持温度为 1，这样我们就可以抓住玻尔兹曼机的核心思想。
277 00:22:24,900 --> 00:22:27,942 说话人 SPEAKER_01：所以我们不会去调整温度，我们只需将温度设置为 1。
278 00:22:29,344 --> 00:22:32,007 演讲者 SPEAKER_01: 好的。
279 00:22:32,027 --> 00:22:36,250 演讲者 SPEAKER_01: 现在我得解释一下在温度为 1 时的热平衡。
280 00:22:36,832 --> 00:22:41,256 演讲者 SPEAKER_01: 如果你不是物理学家或统计学家，热平衡是一个难以理解的概念。
281 00:22:41,741 --> 00:22:46,566 演讲者 SPEAKER_01: 所以大多数人最初认为这意味着系统已经稳定下来，没有变化。
282 00:22:47,166 --> 00:22:48,847 说话人 SPEAKER_01：那不是热平衡的定义。
283 00:22:50,750 --> 00:22:54,192 说话人 SPEAKER_01：当一个系统达到热平衡时，事物仍在变化。
284 00:22:55,013 --> 00:22:59,077 说话人 SPEAKER_01：任何处于热平衡状态的系统都在不断翻转变换状态。
285 00:23:00,558 --> 00:23:06,104 说话人 SPEAKER_01：但发现它处于特定状态的概率分布已经稳定下来。
286 00:23:07,025 --> 00:23:09,707 说话者 SPEAKER_01：这很难想。
287 00:23:10,278 --> 00:23:15,465 说话者 SPEAKER_01：统计学家称之为平稳分布，物理学家称之为热平衡。
288 00:23:16,547 --> 00:23:27,824 说话者 SPEAKER_01：一个直观的想法是想象一大堆相同的系统，但在决定是否开启或关闭时，你使用不同的随机数生成器。
289 00:23:29,465 --> 00:23:30,708 说话者 SPEAKER_01：所以你看能级差。
290 00：23：31,048 --> 00：23：39,441 演讲者 SPEAKER_01：如果能量差距是正的，你通常会打开它，如果它是负的，你通常会关闭它。
291 00：23：39,843 --> 00：23：41,325 议长 SPEAKER_01：这是一个随机决策规则。
所以如果能隙接近于零，你可能可以打开或关闭某些东西。
我们想象这个由相同系统组成的巨大集合，每个系统都有一个不同的随机数生成器。
294 00:23:53,762 --> 00:24:00,792 说话人 SPEAKER_01：它们都有相同的能量函数，但它们会处于不同的状态，然后都在状态之间切换。
295 00:24:00,813 --> 00:24:02,796 说话人 SPEAKER_01：如果你以随机状态启动它们，
296 00:24:03,181 --> 00:24:05,103 说话人 SPEAKER_01：它们会进入其他随机状态。
297 00:24:05,483 --> 00:24:14,614 说话人 SPEAKER_01：但过了一段时间后，如果你使用我之前展示的随机二进制神经元决策规则，任何一种状态的比例将会稳定。
298 00:24:15,835 --> 00:24:22,863 演讲者 SPEAKER_01：所以你可以把配置的概率想成这个巨大集合中处于那个状态的比例。
299 00:24:23,943 --> 00:24:31,332 演讲者 SPEAKER_01：在热平衡状态下，处于那个状态的事物会翻转到其他状态，而处于其他状态的事物会翻转到那个状态。
300 00:24:31,817 --> 00:24:36,261 而那两个，从那个状态中损失的和从那个状态中获得的，会相互抵消。
301 00:24:37,262 --> 00:24:39,484 演讲者 SPEAKER_01：这就叫做详细平衡。
302 00:24:39,506 --> 00:24:46,451 说话人 SPEAKER_01：因此，您将得到的是特定配置中配置的分数，系统中特定配置的分数将是稳定的。
303 00:24:46,471 --> 00:24:50,056 说话人 SPEAKER_01：即使您更新所有配置的单位，它也不会再改变。
304 00:24:50,536 --> 00:24:51,676 说话人 SPEAKER_01：这就是热平衡。
305 00:24:52,597 --> 00:24:57,403 说话人 SPEAKER_01：我将再给您一次解释热平衡的机会。
306 00:24:59,205 --> 00:25:17,162 说话人 SPEAKER_01：那么，我们可以在所有这些相同系统中选择任何我们喜欢的分布，当我们围绕每个系统循环时，让单元随机翻转开或关，逐渐会发生的是，我们将达到热平衡，其中分数是恒定的。
307 00:25:18,103 --> 00:25:20,546 说话人 SPEAKER_01：为此，我再给你一个类比。
308 00:25:22,227 --> 00:25:28,673 说话人 SPEAKER_01：假设你在拉斯维加斯有一个巨大的赌场，里面满是发牌员，
309 00:25:29,884 --> 00:25:34,332 说话人 SPEAKER_01：我们需要比 52 的阶乘还要多的发牌员。
310 00:25:34,352 --> 00:25:37,536 说话人 SPEAKER_01：所以这是一个相当大的牌手数量，但作为一个思想实验，这没关系。
311 00:25:38,478 --> 00:25:44,105 说话人 SPEAKER_01：每个牌手开始时都带着他的牌，牌序完全符合标准。
312 00:25:44,707 --> 00:25:50,035 说话人 SPEAKER_01：所以可能是黑桃 A、黑桃 K、黑桃 Q 等等。
313 00:25:50,055 --> 00:25:59,048 说话人 SPEAKER_01：所以最初，所有的牌堆都是标准配置，即按照标准顺序排列。
314 00:25:59,837 --> 00:26:06,528 说话者 SPEAKER_01：然后它们开始洗牌，进行的是随机洗牌，而不是那些能回到相同状态的复杂洗牌，只是随机洗牌。
315 00:26:07,348 --> 00:26:12,615 说话者 SPEAKER_01：因此，当它们开始洗牌时，你会得到更多不同配置的牌组。
316 00:26:14,298 --> 00:26:22,329 说话者 SPEAKER_01：但在它们开始洗牌不久后，你仍然会保留这个特性，黑桃 K 很可能紧挨着红桃 Q。
317 00:26:22,349 --> 00:26:24,373 说话者 SPEAKER_01：它们还没有在洗牌过程中将它们分开。
318 00:26:25,233 --> 00:26:27,718 说话人 SPEAKER_01：在他们只洗牌很短的时间后，
319 00:26:28,153 --> 00:26:34,123 说话人 SPEAKER_01：仍然会有很多张牌组配置，其中黑桃 K 紧挨着黑桃 Q。
320 00:26:34,143 --> 00:26:43,356 说话人 SPEAKER_01：但如果他们继续洗牌很长时间，那么在足够长的时间后，这些牌组中的所有牌序都将同等可能。
321 00:26:44,357 --> 00:26:45,640 说话人 SPEAKER_01：这就是热平衡。
322 00:26:46,280 --> 00:26:50,247 说话人 SPEAKER_01：他们一直在洗牌，一直在洗牌，所以任何一副牌都会发生变化。
323 00:26:51,228 --> 00:26:56,997 说话人 SPEAKER_01：但是如果你看这整个牌组的集合，任何一种配置的分数都保持不变。
324 00:26:58,765 --> 00:27:07,011 说话人 SPEAKER_01：现在，这里有一个问题是，对于牌组来说，所有配置都是等可能的，除非可能有些牌比其他牌重。
325 00:27:07,692 --> 00:27:10,976 说话人 SPEAKER_01：但是对于我们正在讨论的系统，并不是所有配置都是等可能的。
326 00:27:11,395 --> 00:27:15,179 说话人 SPEAKER_01：在热平衡状态下，不同的配置将具有不同的概率。
327 00:27:17,181 --> 00:27:17,500 说话人 SPEAKER_07：好的。
328 00:27:21,365 --> 00:27:27,269 说话人 SPEAKER_01：那么，现在让我们来谈谈如何用我们所说的玻尔兹曼机来建模二进制数据。
329 00:27:28,229 --> 00:27:29,510 说话人 SPEAKER_01：来建模我们所说的玻尔兹曼机。
330 00:27:29,550 --> 00:27:32,035 讲者 SPEAKER_01：这是一个具有可见单元和隐藏单元的 Hopfield 网络。
331 00:27:32,976 --> 00:27:38,463 讲者 SPEAKER_01：它有一个随机决策规则，即根据能量差以概率将单元打开或关闭。
332 00:27:43,432 --> 00:27:49,480 讲者 SPEAKER_01：因此，我们希望能够处理一组二进制向量，这些向量是训练数据。
333 00:27:49,500 --> 00:27:51,743 讲者 SPEAKER_01：所以想象一下它们是二进制图像。
334 00:27:52,986 --> 00:27:57,492 说话人 SPEAKER_01：我们希望模型为训练集中的二进制向量分配一个高概率，相对较高的概率，
335 00:27:57,710 --> 00:28:02,875 说话人 SPEAKER_01：并且为所有其他二进制向量分配一个相对较低的概率。
336 00:28:03,915 --> 00:28:05,458 说话人 SPEAKER_01：这就是训练的目标。
337 00:28:06,739 --> 00:28:11,864 说话人 SPEAKER_01：记住，这只是针对可见向量，即构成图像的向量。
338 00:28:12,325 --> 00:28:16,348 说话人 SPEAKER_01：我们有了所有这些额外的单元，可以用来帮助实现这一点。
339 00:28:16,990 --> 00:28:20,073 说话人 SPEAKER_01：所以这比 Hopfield 网络要复杂得多，在 Hopfield 网络中你可以看到一切。
340 00:28:20,472 --> 00:28:26,298 说话人 SPEAKER_01：你有很多隐藏单元，可以用它们来帮助你模拟可见单元上的二进制数据。
341 00:28:26,718 --> 00:28:28,740 说话人 SPEAKER_01：但问题是，你应该如何使用它们？
342 00:28:28,759 --> 00:28:30,662 说话人 SPEAKER_01: 看起来这是一个非常复杂的问题。
343 00:28:32,844 --> 00:28:39,391 说话人 SPEAKER_01: 所以在玻尔兹曼机中，一切取决于可见和隐藏单元的联合配置的能量。
344 00:28:41,211 --> 00:28:47,739 说话人 SPEAKER_01: 而联合配置的能量与这些配置的概率相关。
345 00:28:48,278 --> 00:28:49,881 说话人 SPEAKER_01: 你可以把它想象成两种不同的方式。
346 00:28:51,321 --> 00:28:54,025 演讲者 SPEAKER_01：所以你可以这样说，如果我有一个联合配置，
347 00:28:54,358 --> 00:29:09,094 演讲者 SPEAKER_01：该配置具有可见单元上的二进制向量 V 和隐藏单元上的二进制向量 H，这种二进制配置将具有能量，这个能量就是整个联合配置的能量，该配置在可见单元上有 V，在隐藏单元上有 H。
348 00:29:09,673 --> 00:29:10,515 演讲者 SPEAKER_01：这将是一个数值。
349 00:29:11,556 --> 00:29:20,144 演讲者 SPEAKER_01：在热平衡状态下找到该配置的概率将与 E 的负指数成比例，这里的 E 是能量。
350 00:29:20,845 --> 00:29:22,527 说话人 SPEAKER_01：这就是玻尔兹曼分布。
351 00:29:23,047 --> 00:29:27,332 说话人 SPEAKER_01：当概率与 e 的负能量成正比时。
352 00:29:27,352 --> 00:29:37,846 说话人 SPEAKER_01：还有一种思考概率的方法，我们可以这样说，好吧，拿这个玻尔兹曼机开始随机更新单元。
353 00:29:37,885 --> 00:29:45,756 说话人 SPEAKER_01：并且你正在随机更新所有单元，包括可见的和隐藏的。
354 00:29:46,276 --> 00:29:47,478 说话人 SPEAKER_01：您没有输入任何数据。
355 00:29:47,498 --> 00:29:51,702 说话人 SPEAKER_01：您只是在让玻尔兹曼机做梦，如果您愿意的话，或者生成。
356 00:29:52,949 --> 00:30:06,064 说话人 SPEAKER_01：在达到热平衡之后，这可能需要很长时间，您会查看整个系统，这个玻尔兹曼机，是否在可见单元上有向量 v，在隐藏单元上有向量 h。
357 00:30:07,625 --> 00:30:19,480 说话人 SPEAKER_01：您看到这种情况的概率将恰好等于您可以从能量计算出的概率，因为在热平衡时，它将达到玻尔兹曼分布。
您会注意到我将 V 和 H 的概率表达为与 e 的负能量成正比，这很重要。
它不等于能量减去 e，它只是成比例的，因为您必须归一化概率，使它们加起来等于 1。
360 00：30：36,061 --> 00：30：38,045 议长 SPEAKER_01：但关键是这两个定义是一致的。
你能计算在考虑所有可能配置的情况下，以及如果你只是随机运行它时你能测量到什么。
362 00:30:48,957 --> 00:30:52,701 演讲者 SPEAKER_01：所以关节配置的能量将类似于这样。
363 00:30:53,320 --> 00:30:58,026 演讲者 SPEAKER_01：将存在偏差项，我已经包含了可见单元和隐藏单元的偏差。
364 00:30:58,666 --> 00:31:00,929 演讲者 SPEAKER_01：可见单元之间将存在交互。
365 00:31:01,829 --> 00:31:05,953 演讲者 SPEAKER_01：是的，这就是能量。
366 00:31:06,494 --> 00:31:07,516 说话人 SPEAKER_01：这些是偏差项。
367 00:31:08,416 --> 00:31:10,858 说话人 SPEAKER_01：可见单元之间存在交互。
368 00:31:11,680 --> 00:31:15,784 说话人 SPEAKER_01：我们用 i 和 j 这样表示，所以，用小于号，这样你就不会重复计数。
369 00:31:16,325 --> 00:31:18,626 说话人 SPEAKER_01：因此，你不会计算 i 等于 j 的情况。
370 00:31:19,635 --> 00:31:26,262 说话人 SPEAKER_01：然后会有权重，可见单元和隐藏单元之间，以及隐藏单元之间的交互。
371 00:31:27,023 --> 00:31:28,805 说话人 SPEAKER_01：将会有一个像那样的巨大能量表达式。
372 00:31:30,948 --> 00:31:34,550 说话人 SPEAKER_01：系统的行为将取决于这些偏差和权重。
373 00:31:37,855 --> 00:31:47,044 说话人 SPEAKER_01：因此，如果我们想用能量来定义概率，我们知道给定隐藏单元的可见单元的概率 P 将与它成正比。
374 00:31:47,715 --> 00:31:50,719 说话者 SPEAKER_01：e 的负值，表示该关节配置的能量。
375 00:31:51,619 --> 00:31:52,842 说话者 SPEAKER_01：但我们通常化的是什么？
376 00:31:52,862 --> 00:32:06,195 说话者 SPEAKER_01：嗯，我们必须通过所有可能的可见向量 u 和所有可能的隐藏向量 g 的和来正常化 e 的负值，这里的 u 位于可见单元上，g 位于隐藏单元上。
377 00:32:07,057 --> 00:32:10,000 说话者 SPEAKER_01：这个正常化项是物理学家所说的配分函数。
378 00:32:10,661 --> 00:32:12,482 说话人 SPEAKER_01：这计算起来是个怪物。
379 00:32:13,084 --> 00:32:15,006 说话人 SPEAKER_01：实际上对于大型系统来说，是无法计算的。
380 00:32:15,026 --> 00:32:15,866 说话人 SPEAKER_01：它只是太大了。
381 00:32:16,420 --> 00:32:21,307 说话人 SPEAKER_01：这就是为什么能够采样并获得相同或大致相同的答案是很重要的。
382 00:32:23,490 --> 00:32:24,893 说话人 SPEAKER_01：这就是所谓的配分函数。
383 00:32:26,556 --> 00:32:37,953 说话人 SPEAKER_01：要得到联合配置的实际概率，你必须计算该联合配置的能量，然后除以配分函数进行归一化。
384 00:32:39,516 --> 00:32:43,962 说话人 SPEAKER_01：如果你想得到一个可见向量的能量，对不起，如果你想得到一个可见向量的概率，
385 00:32:45,007 --> 00:32:49,532 说话人 SPEAKER_01：所以，我们训练了一个玻尔兹曼机，现在我们将让它生成数据。
386 00:32:49,853 --> 00:32:52,395 说话人 SPEAKER_01：这是一个生成模型，是生成式 AI。
387 00:32:54,278 --> 00:33:08,476 说话人 SPEAKER_01：它在热平衡状态下生成特定向量在其可见单元的概率将是所有可能的隐藏配置的联合能量 V 和 H 的总和除以配分函数。
388 00:33:09,477 --> 00:33:14,202 说话人 SPEAKER_01：这就是如果您想从权重中解析地计算出概率时需要进行的计算。
389 00:33:14,655 --> 00:33:20,304 说话人 SPEAKER_01：这就是它在生成数据时，其可见单元上具有向量 v 的概率。
390 00:33:20,604 --> 00:33:23,488 说话人 SPEAKER_01：没有输入正在运行，只是在可见单元上生成数据。
391 00:33:26,553 --> 00:33:31,500 说话人 SPEAKER_01：我将通过一个工作示例来讲解，因为我发现如果看到过工作示例，我通常对这些事情理解得更好。
392 00:33:32,381 --> 00:33:35,644 说话人 SPEAKER_01：所以让我们有一个非常简单的网络，包含两个可见单元和两个隐藏单元。
393 00:33:37,147 --> 00:33:41,794 说话人 SPEAKER_01：所以可见单元有四种可能的状态。
394 00:33:42,414 --> 00:33:44,737 说话人 SPEAKER_01: 1, 0, 0, 1 和 0, 0。
395 00:33:45,317 --> 00:33:47,380 说话人 SPEAKER_01: 并且有两个隐藏单元的四种可能状态。
396 00:33:48,642 --> 00:33:52,726 说话人 SPEAKER_01: 显然有 16 种可能的联合状态，我已经在这个表格中向您展示了。
397 00:33:54,087 --> 00:33:57,570 说话人 SPEAKER_01: 因此，前四行是当两个可见单元处于开启状态时。
398 00:33:57,611 --> 00:34:00,734 演讲者 SPEAKER_01：我们查看隐藏单元的所有可能状态。
399 00:34:01,675 --> 00:34:07,162 演讲者 SPEAKER_01：对于这些联合配置中的每一个，我们计算一个负能量，即是一个好的指标。
400 00:34:08,222 --> 00:34:09,443 演讲者 SPEAKER_01：那么让我们看最上面一行。
401 00:34:09,864 --> 00:34:10,885 演讲者 SPEAKER_01：这是 1, 1, 1, 1。
402 00:34:10,925 --> 00:34:12,286 演讲者 SPEAKER_01：如果我们把所有东西都打开，
403 00:34:13,144 --> 00:34:21,818 演讲者 SPEAKER_01：那么“好”将等于这三个权重的总和，因为它们都在开启的单元对之间，所以将得到加 2，减 1，加 1，所以“好”将是 2。
404 00:34:23,059 --> 00:34:29,489 演讲者 SPEAKER_01：我们可以对所有的其他联合配置做同样的处理，然后我们会得到现在 e 的负能量的数值。
405 00:34:31,331 --> 00:34:35,297 演讲者 SPEAKER_01：然后我们可以把这些数值加起来得到 39.7。
406 00:34:36,159 --> 00:34:37,240 说话人 SPEAKER_01：这就是配分函数。
407 00:34:38,041 --> 00:34:42,088 说话人 SPEAKER_01：我们除以 39.7，现在会得到一些加起来等于 1 的数字。
408 00:34:42,355 --> 00:34:43,576 说话人 SPEAKER_01：所以会有概率。
409 00:34:44,978 --> 00:34:56,451 说话人 SPEAKER_01：如果你想在特定的玻尔兹曼机中计算概率，通过多次更新单元的状态使其达到热平衡，那么在两个可见单元上看到 1,1 的概率是多少？
410 00:34:57,311 --> 00:34:58,534 说话人 SPEAKER_01: 嗯，它是 0.466。
411 00:34:59,974 --> 00:35:03,559 说话人 SPEAKER_01: 根据隐藏单元的四种不同状态，你可以从四个不同的角度来理解它。
412 00:35:04,219 --> 00:35:05,882 说话人 SPEAKER_01: 其中有两种概率相当高。
413 00:35:06,300 --> 00:35:17,452 说话人 SPEAKER_01: 其中有两种概率较低，将它们相加，如果将这个玻尔兹曼机运行到热平衡状态，在可见单元上表现出状态 1, 1 的概率是 0.466。
414 00:35:18,594 --> 00:35:21,217 说话人 SPEAKER_01：如果你实施这个并尝试一下，是的，大致是正确的。
415 00:35:21,237 --> 00:35:26,021 说话人 SPEAKER_01：当你平衡采样时，它将有 0.466 的时间你会看到 1, 1。
416 00:35:27,322 --> 00:35:29,445 说话人 SPEAKER_01：看到 0, 0 将会非常罕见。
417 00:35:29,505 --> 00:35:34,030 说话人 SPEAKER_01：你大约会看到 0.084 的时间。
418 00:35:34,887 --> 00:35:42,318 演讲者 SPEAKER_01：但我已经给你详细地举了一个例子，说明了如何将网络中的权重转换为关节配置的能量。
419 00:35:42,378 --> 00:35:52,153 演讲者 SPEAKER_01：将这些转换为指数为负 C 的能量，通过归一化转换为概率，然后对隐藏单元的所有状态进行积分，现在你可以得到可见单元的概率。
420 00:35:53,556 --> 00:36:02,110 演讲者 SPEAKER_01：这就是玻尔兹曼机的工作原理，这三个数字，加二、减一和加一，如何模拟可见向量上的概率分布。
421 00:36:02,949 --> 00:36:16,744 演讲者 SPEAKER_01：显然，如果你有一个训练集，它表明向量 1, 1 和 1, 0 很常见，而向量 0, 1 和 0, 0 很少见，这将是一个不错的模型，因为它会使常见的两个更常见，而罕见的两个更罕见。
422 00:36:20,286 --> 00:36:20,668 演讲者 SPEAKER_07: 好的。
423 00:36:23,070 --> 00:36:32,940 演讲者 SPEAKER_01: 要从模型中获取样本，你不能像我在大网络中那样进行刚才的分析计算，但你所能做的是
424 00:36:33,409 --> 00:36:51,277 演讲者 SPEAKER_01: 就是使用马尔可夫链蒙特卡洛方法，这相当于，你遍历所有这些单元，可见的和隐藏的，一次选择一个单元，使用随机二进制决策规则来决定它应该是开启还是关闭，然后一直这样做，直到达到热平衡，然后查看可见单元，你就得到了模型的样本。
425 00:36:54,300 --> 00:36:55,103 演讲者 SPEAKER_01: 这就是我刚才说的。
426 00:36:56,224 --> 00:36:57,867 说话人 SPEAKER_01：你在温度为 1 的情况下做这件事。
427 00:36:58,027 --> 00:37:02,673 说话人 SPEAKER_01：所以你从运行在温度为 1 的模型中获取了一个样本。
428 00:37:03,396 --> 00:37:10,764 说话人 SPEAKER_01：得到任何特定样本的概率将与你能计算出的概率相同，但你不能计算，因为它太大。
429 00:37:13,849 --> 00:37:17,652 说话人 SPEAKER_01：所以得到可见和隐藏的联合配置的概率看起来会是这样。
430 00:37:19,835 --> 00:37:28,865 演讲者 SPEAKER_01：如果你想要一个特定可见向量的隐藏单元样本，你所做的是将可见向量限制在可见单元上。
431 00:37:29,467 --> 00:37:32,070 演讲者 SPEAKER_01：所以你不需要采样这些，你只需要采样隐藏单元。
432 00:37:32,505 --> 00:37:37,152 演讲者 SPEAKER_01：现在你将从该输入的解释中进行采样。
433 00:37:37,733 --> 00:37:40,659 演讲者 SPEAKER_01：将限制可见向量想象成像线描一样。
434 00:37:41,659 --> 00:37:44,985 讲者 SPEAKER_01：想想隐藏单元就像边缘单元所做的那样。
435 00:37:45,987 --> 00:37:49,231 讲者 SPEAKER_01：现在您可以采样那条线图的解释了。
436 00:37:53,257 --> 00:37:59,086 讲者 SPEAKER_01：所以就像采样完整的配置一样，您需要采样可见向量。
437 00:37:59,523 --> 00:38:04,170 讲者 SPEAKER_01：但您只允许隐藏单元翻转，因为这将与特定的可见向量一起。
438 00:38:05,112 --> 00:38:06,614 说话人 SPEAKER_01: 我应该说我们差不多完成了。
439 00:38:10,699 --> 00:38:16,028 说话人 SPEAKER_01: 你需要根据可见向量从后验中获取这些样本，以便学习权重。
440 00:38:16,949 --> 00:38:25,242 说话人 SPEAKER_01: 现在，你可能认为，每个人都这么认为，在这个网络中学习权重会非常复杂。
441 00:38:27,907 --> 00:38:29,750 说话人 SPEAKER_01: 我给你一个相信这个原因的理由。
442 00:38:30,690 --> 00:38:48,396 讲者 SPEAKER_01：我们学习的目标是设置权重，以便当我们通过更新隐藏单元和更新可见单元并持续这样做，直到达到热力学一致的运动，然后查看可见单元上的样本时，以这种方式采样，你会得到概率。
443 00:38:50,018 --> 00:38:55,887 讲者 SPEAKER_01：对于训练数据，你会得到高概率，而对于其他所有内容，你会得到低概率。
444 00:38:56,661 --> 00:39:04,813 讲者 SPEAKER_01：所以，如果你想最大化训练数据的概率乘积，这相当于最大化训练数据的对数概率之和。
445 00:39:06,675 --> 00:39:11,201 讲者 SPEAKER_01：现在我们将看到为什么可能得到一个简单的学习规则。
446 00:39:13,182 --> 00:39:16,347 说话人 SPEAKER_01：所以我们让网络达到其稳态分布。
447 00:39:19,050 --> 00:39:20,592 说话人 SPEAKER_01：然后我们采样可见向量。
448 00:39:21,940 --> 00:39:26,985 说话人 SPEAKER_01：然后我们问，这些可见向量的概率与网络中的能量有何关系？
449 00:39:28,186 --> 00:39:31,610 说话人 SPEAKER_01：嗯，我回头再谈这个问题。
450 00:39:31,630 --> 00:39:37,496 演讲者 SPEAKER_01：我首先想向您展示为什么您可能会认为学习权重非常困难。
451 00:39:38,356 --> 00:39:46,664 演讲者 SPEAKER_01：假设我有这样的训练数据，它说有一个包含两个可见单元的网络。
452 00:39:47,152 --> 00:39:56,922 演讲者 SPEAKER_01：应该经常有一个零作为可见单元的状态，并且经常有一个零一个，但很少有一个零零或一个一个。
453 00:39:58,364 --> 00:40:10,378 演讲者 SPEAKER_01：如果您只是用直觉和常识，很明显，您希望从第一个可见单元到最后的可见单元，再到另一个可见单元，这五权重链。
454 00:40:11,179 --> 00:40:14,123 演讲者 SPEAKER_01：您希望所有这些权重的乘积都是负数。
455 00:40:14,784 --> 00:40:17,007 演讲者 SPEAKER_01：如果所有这些权重的乘积是负数，
456 00:40:17,476 --> 00:40:22,099 演讲者 SPEAKER_01：那么开启第一个单元将倾向于关闭第二个单元，反之亦然。
457 00:40:22,119 --> 00:40:22,900 演讲者 SPEAKER_01：这些权重是对称的。
458 00:40:24,121 --> 00:40:26,483 说话人 SPEAKER_01：所以我们绝不想让产品为负。
459 00:40:26,503 --> 00:40:34,411 说话人 SPEAKER_01：那么现在如果你看一个权重，比如 w1，然后问，我应该怎么改变它来改善情况？
460 00:40:35,172 --> 00:40:36,974 说话人 SPEAKER_01：嗯，你想要让产品为负。
461 00:40:38,454 --> 00:40:44,900 说话人 SPEAKER_01：所以如果其他所有权重都是正的，你想要让 w1 为负。
462 00:40:45,032 --> 00:40:48,356 演讲者 SPEAKER_01：其他权重中有一个是负数，你希望 w1 为正数。
463 00:40:49,679 --> 00:40:56,967 演讲者 SPEAKER_01：所以你如何改变 w1 取决于 w3 的值，这是一个远程权重。
464 00:40:57,028 --> 00:41:04,018 演讲者 SPEAKER_01：这甚至不涉及与 w1 连接影响的相同单位。
465 00:41:05,259 --> 00:41:10,445 演讲者 SPEAKER_01：因此，为了知道如何改变 w1，你需要知道 w3 的符号。
466 00:41:10,907 --> 00:41:14,331 讲者 SPEAKER_01：现在在一个像反向传播这样的算法中，
467 00:41:15,070 --> 00:41:19,355 讲者 SPEAKER_01：你将信息向后发送以获取该信息。
468 00:41:19,375 --> 00:41:25,362 讲者 SPEAKER_01：但在玻尔兹曼机中，你有一种完全不同的方式来获取这些信息，这看起来就像魔法一样。
469 00:41:27,704 --> 00:41:29,445 讲者 SPEAKER_01：所以，这里有一个非常令人惊讶的事实。
470 00:41:31,407 --> 00:41:40,918 演讲者 SPEAKER_01：关于所有其他权重，一个权重需要知道的一切都包含在其中，可以通过查看两个相关性来本地获得。
471 00:41:40,958 --> 00:41:43,541 演讲者 SPEAKER_01：你不需要像反向传播那样的回传。
472 00:41:46,170 --> 00:41:53,844 演讲者 SPEAKER_01：所以假设我有一个可见向量 v，我想让这个可见向量更有可能被网络生成。
473 00:41:54,284 --> 00:42:05,262 演讲者 SPEAKER_01：所以我给它一个图像的例子，我说，如何，我问，我应该如何改变两个神经元之间的权重 wij，以使 v 更有可能？
474 00:42:06,704 --> 00:42:11,192 说话人 SPEAKER_01：它可能存在于两个可见神经元之间，或者存在于一个可见神经元和一个隐藏神经元之间，或者存在于两个隐藏神经元之间。
475 00:42:12,193 --> 00:42:12,954 说话人 SPEAKER_01：这是同样的规则。
476 00:42:14,217 --> 00:42:38,291 说话人 SPEAKER_01：你想要的导数，将使 V 的对数概率更高，就是当你将 V 固定在可见单元上并达到热平衡时，你和我以及你和 J 同时被激活的频率，与你不固定任何东西在可见单元上，只是让它自由幻想时，它们同时被激活的频率之间的差异。
477 00:42:38,331 --> 00:42:41,456 说话人 SPEAKER_01：你运行它时，你正在更新所有单元，
478 00:42:42,061 --> 00:42:47,987 说话者 SPEAKER_01：将它运行到 Regius 热平衡状态，然后看看这两个单元同时开启的频率。
479 00:42:49,309 --> 00:42:52,751 说话者 SPEAKER_01：所以这里有一个极其简单的学习规则。
480 00:42:52,771 --> 00:43:04,545 说话者 SPEAKER_01：它说学习神经元 i 和 j 之间的连接强度，不需要像反向传播那样，因为反向传播在神经学上不太可能，因为它需要在反向传递中传播与正向传递不同的信息。
481 00:43:05,164 --> 00:43:11,612 说话者 SPEAKER_01：当你清醒并且有可见向量输入时，你有一个非常简单的规则。
482 00:43:12,182 --> 00:43:15,806 讲者 SPEAKER_01: 只需测量 i 和 j 的相关性。这就是那些尖括号的意思。
483 00:43:15,827 --> 00:43:17,768 讲者 SPEAKER_01: 这是物理学家对相关性的表示法。
484 00:43:18,329 --> 00:43:20,331 讲者 SPEAKER_01: 测量 i 和 j 同时出现的频率。
485 00:43:21,273 --> 00:43:25,896 讲者 SPEAKER_01: 然后去睡觉，让模型生成数据。
486 00:43:27,858 --> 00:43:32,423 演讲者 SPEAKER_01：现在你们正在更新所有单元，不让其中一些单元由感官输入驱动。
487 00:43:33,025 --> 00:43:34,045 演讲者 SPEAKER_01：然后测量相同的东西。
488 00:43:34,106 --> 00:43:35,887 演讲者 SPEAKER_01：测量这两个单元同时开启的频率。
489 00:43:37,009 --> 00:43:39,911 演讲者 SPEAKER_01：这种相关性的差异就是学习规则。
490 00:43:40,972 --> 00:43:42,994 说话人 SPEAKER_01：这只是一个非常漂亮的学习规则。
491 00:43:43,474 --> 00:43:44,416 说话人 SPEAKER_01：它非常、非常简单。
492 00:43:45,137 --> 00:43:47,179 说话人 SPEAKER_01：看起来你可以很容易地把它放入大脑中。
493 00:43:47,199 --> 00:43:58,976 说话人 SPEAKER_01：而不是像你正在做的反向传播那样，先向前再向后，这不能实时进行，你必须先向前，然后停下来再向后，这对感知来说并不很有用。
494 00:43:59,817 --> 00:44:01,699 讲者 SPEAKER_01：这个算法说，不，不，你不能这么做。
495 00:44:02,179 --> 00:44:08,869 讲者 SPEAKER_01：当你醒着，数据进来时，你只是在测量所有连接单元之间的相关性。
496 00:44:09,811 --> 00:44:11,672 讲者 SPEAKER_01：然后当你睡着时，你会有不同的阶段。
497 00:44:12,634 --> 00:44:16,097 讲者 SPEAKER_01：你从模型中生成数据，并测量相关性。
498 00:44:16,818 --> 00:44:19,882 说话人 SPEAKER_01：这些相关性的差异就是学习信号。
499 00:44:21,362 --> 00:44:23,565 说话人 SPEAKER_01：当然，这里有一个负号。
500 00:44:24,045 --> 00:44:31,054 说话人 SPEAKER_01：这个负号的意思是，当你睡觉时，如果两个单元同时激活，你会减弱它们之间的连接强度。
501 00:44:31,393 --> 00:44:39,463 说话人 SPEAKER_01：也就是说，你使你在睡觉时得到的联合配置比你在清醒时得到的配置更不可能。
502 00:44:39,797 --> 00:44:43,862 说话人 SPEAKER_01：这就像你在睡眠中正在遗忘的快速理论。
503 00:44:44,322 --> 00:44:47,945 说话人 SPEAKER_01：但实际上这是某个东西的导数，这个学习规则。
504 00:44:47,965 --> 00:44:53,632 说话人 SPEAKER_01：这正是你想要的那个东西的导数，即生成训练向量的对数概率。
505 00:44:55,454 --> 00:44:56,875 说话人 SPEAKER_01：现在你可能要问，为什么它这么简单？
506 00:44:59,518 --> 00:45:00,097 说话人 SPEAKER_01: 我都说了。
507 00:45:01,800 --> 00:45:09,788 说话人 SPEAKER_01: 原因很简单，学习规则只是与这种相关性的差异成正比。
508 00:45:10,391 --> 00:45:13,315 说话人 SPEAKER_01: 当你锁定可见单元或者不使用数据锁定它们时。
509 00:45:14,556 --> 00:45:15,777 说话人 SPEAKER_01: 简单的原因是这个。
510 00:45:16,759 --> 00:45:27,050 讲者 SPEAKER_01：在热平衡状态下，你会有这样一个神奇的性质，即配置的对数概率是能量的线性函数。
511 00:45:28,110 --> 00:45:30,974 换句话说，概率是能量指数的负数。
512 00:45:31,474 --> 00:45:34,597 所以取对数，对数概率与能量呈线性关系。
513 00:45:35,780 --> 00:45:37,902 能量是权重的线性函数。
514 00:45:38,606 --> 00:45:46,115 讲者 SPEAKER_01：所以如果你取能量，即 S-I-S-J-W-I-J，对 W-I-J 求导，你只会得到 S-I-S-J。
515 00:45:46,936 --> 00:45:53,005 讲者 SPEAKER_01：因此，你对权重求能量导数，得到的是这个 S-I-S-J。
516 00:45:55,047 --> 00:46:01,375 讲者 SPEAKER_01：因为达到热平衡的过程会传播关于权重的信息，
517 00:46:01,726 --> 00:46:04,449 讲者 SPEAKER_01：你得到了这个神奇的性质，你不需要反向传播阶段。
518 00:46:04,949 --> 00:46:08,673 说话者 SPEAKER_01：你只需利用清醒时和睡眠时获得的关联即可。
519 00:46:11,056 --> 00:46:16,923 说话者 SPEAKER_01：如果你思考为什么需要负相，让我们回到数学上来。
520 00:46:17,184 --> 00:46:28,257 说话者 SPEAKER_01：可见向量的概率，我知道我超时了，可见向量的概率是所有隐藏配置
521 00:46:28,574 --> 00:46:48,356 说话者 SPEAKER_01：在该可见向量上的可见单元和任何隐藏配置 H 在隐藏单元上的和，因此这里有一个和，通过配分函数归一化，配分函数是所有可能的可见向量和所有可能的隐藏向量上 e 的负指数之和。
522 00:46:49,637 --> 00:46:58,547 说话者 SPEAKER_01：如果你考虑如何最大化 V 的概率，你应该显然降低能量，提高品质
523 00:46:59,016 --> 00:47:04,724 说话者 SPEAKER_01：尤其是那些已经很好的向量。
524 00:47:05,764 --> 00:47:20,782 说话者 SPEAKER_01：因此，在学习算法的积极阶段，当你清醒时，对于每个向量 v，你试图用与之相匹配的隐藏向量填充它，这是一个对 v 的合理解释，然后你试图通过降低该组合的能量来使这种组合更可信。
525 00:47:21,844 --> 00:47:28,753 说话者 SPEAKER_01：但是这里有一个归一化项，如果你使一切更可信，你不会使它更可信。
526 00:47:29,391 --> 00:47:36,960 说话人 SPEAKER_01：你所要做的是找到所有竞争组合，这些 u 和 g，并将它们都变得不太可能。
527 00:47:37,681 --> 00:47:41,284 说话人 SPEAKER_01：这正是学习规则中的负项所做的事情。
528 00:47:41,605 --> 00:47:53,539 说话人 SPEAKER_01：它在使配分函数变小的同时，使与特定隐藏向量的“好”的总和变大，与特定可见向量的“好”的总和变大。
529 00:47:56,442 --> 00:47:58,503 说话人 SPEAKER_01：所以当你醒着的时候学习的正面阶段。
530 00:47:58,922 --> 00:48:03,309 演讲者 SPEAKER_01：最大化最上面一行，负相则最小化最下面一行。
531 00:48:04,791 --> 00:48:11,682 演讲者 SPEAKER_01：因此，这些基于能量的模型必须有两个这样的阶段，因为你无法计算这个配分函数。
532 00:48:11,702 --> 00:48:12,403 演讲者 SPEAKER_01：它太大了。
533 00:48:12,724 --> 00:48:13,585 演讲者 SPEAKER_01：你必须进行采样。
534 00:48:14,507 --> 00:48:23,221 说话人 SPEAKER_01：因此，这种基于能量的模型可以解释为什么人们想要睡觉、做梦并且不记得梦境。
535 00:48:24,543 --> 00:48:25,605 说话人 SPEAKER_01：这就是我演讲的结束。
536 00:48:26,512 --> 00:48:29,255 说话人 SPEAKER_01：所以我将尝试看看我是否还能再次见到人们。
537 00:48:29,755 --> 00:48:31,938 说话人 SPEAKER_01：非常感谢您的演讲。
538 00:48:32,760 --> 00:48:33,641 说话人 SPEAKER_06: 我们是吗？
539 00:48:33,681 --> 00:48:37,043 说话人 SPEAKER_06: 我们有时间回答几个问题吗？
540 00:48:37,364 --> 00:48:38,164 说话人 SPEAKER_06: 是的，我有时间。
541 00:48:39,126 --> 00:48:39,407 说话人 SPEAKER_06: 好的。
542 00:48:41,469 --> 00:48:43,130 说话人 SPEAKER_06：您希望我们定在什么时间？
543 00:48:44,351 --> 00:48:46,193 说话人 SPEAKER_01：也许到 210 或者什么，如果您愿意的话。
544 00:48:46,213 --> 00:48:47,375 说话人 SPEAKER_06：是的，绝对是问题。
545 00:48:47,996 --> 00:48:48,376 说话人 SPEAKER_06：绝对是。
546 00:48:49,177 --> 00:48:51,518 演讲者 SPEAKER_06：我们将开始问答环节。
547 00:48:51,579 --> 00:48:53,061 我们通常的做法是。
548 00:48:53,530 --> 00:48:59,418 听众成员举起他们的虚拟手，然后口头提出问题。
549 00:48:59,458 --> 00:49:01,880 因此，这更像是一场互动讨论。
550 00:49:05,405 --> 00:49:11,554 演讲者 SPEAKER_06：您希望我们的问题是否应该专门针对讲座内容，还是开放式的？
551 00:49:12,273 --> 00:49:14,998 演讲者 SPEAKER_06：开放式。
552 00:49:15,518 --> 00:49:17,340 演讲者 SPEAKER_01：哦，让我再说一件事。
553 00:49:17,380 --> 00:49:18,442 演讲者 SPEAKER_01：我将提问。
554 00:49:18,862 --> 00:49:20,485 说话人 SPEAKER_01：这些玻尔兹曼机有什么用吗？
555 00:49:20,945 --> 00:49:23,068 说话人 SPEAKER_01：最初，它们似乎没什么用。
556 00:49:23,409 --> 00:49:26,913 说话人 SPEAKER_01：在大系统中设置热平衡太慢了。
557 00:49:27,414 --> 00:49:28,735 说话人 SPEAKER_01：我的意思是，这太慢了，毫无希望。
558 00:49:28,795 --> 00:49:32,561 说话人 SPEAKER_01：数学很美，但效率低得无法救药。
559 00:49:33,262 --> 00:49:38,750 说话人 SPEAKER_01：但 17 年后，我发现可以制作更简单的版本，它们可以快速收敛。
560 00:49:39,471 --> 00:49:44,277 说话人 SPEAKER_01：这些实际上被用于初始化随后用反向传播训练的神经网络。
561 00:49:45,050 --> 00:49:48,233 说话人 SPEAKER_01：通过使用更简单的玻尔兹曼机版本，一次学习一层神经网络。
562 00:49:48,534 --> 00:49:51,936 说话人 SPEAKER_01：结果证明，这正是打开洪水闸门的一件事。
563 00:49:52,237 --> 00:49:58,923 说话人 SPEAKER_01：我们突然可以训练非卷积的深度网络，通过使用受限玻尔兹曼机进行初始化。
564 00:49:58,943 --> 00:50:05,871 说话人 SPEAKER_01：所以这些实际上是在使神经网络通过反向传播工作道路上的。
565 00:50:06,711 --> 00:50:09,335 说话人 SPEAKER_01：一旦我们使它们工作，我们就扔掉了所有的玻尔兹曼东西。
566 00:50:09,735 --> 00:50:12,217 说话人 SPEAKER_01：我以为这件事不会再有人知道了。
567 00:50:12,257 --> 00:50:14,280 说话人 SPEAKER_01：但物理学家们喜欢它。
568 00:50:14,547 --> 00:50:29,467 说话人 SPEAKER_01：所以当物理学家们认为，嗯，你知道，人工智能领域正在发生很多事情，也许我们应该为人工智能获得诺贝尔奖时，他们不得不构建一个非常复杂的理由，试图找到一些看起来像物理的人工智能，玻尔兹曼机显然看起来像物理。
569 00:50:29,487 --> 00:50:38,460 说话人 SPEAKER_01：所以他们只能抓住一个非常细的线索，即玻尔兹曼机实际上在 2006 年至 2011 年之间被用来初始化反向传播。
570 00:50:38,481 --> 00:50:39,862 说话人 SPEAKER_01: 并且
571 00:50:40,213 --> 00:50:47,601 说话人 SPEAKER_01: 这在物理学和人工智能之间建立了一种联系，这足以让他们有理由用物理学奖来奖励人工智能。
572 00:50:48,943 --> 00:50:49,864 说话人 SPEAKER_01: 现在您可以提问了。
573 00:50:53,166 --> 00:50:56,650 说话人 SPEAKER_06: 好的，那么，是的，让我们欢迎提问。
574 00:50:58,532 --> 00:51:03,838 说话人 SPEAKER_02：这里有很多问题通过聊天发过来，所以如果你能口头提问，请举手。
575 00:51:03,858 --> 00:51:04,940 说话人 SPEAKER_06：我们将口头回答这些问题。
576 00:51:05,420 --> 00:51:08,262 说话人 SPEAKER_02：是的，你可以一个接一个地说。
577 00:51:08,885 --> 00:51:10,887 说话人 SPEAKER_06：好的，罗比·乔杜里。
578 00:51:11,827 --> 00:51:12,307 演讲者 SPEAKER_06：问题。
579 00:51:14,010 --> 00:51:17,873 演讲者 SPEAKER_04：我挺好的，我...我的摄像头已经打开了，是从飞机上还是别的什么地方。
580 00:51:17,893 --> 00:51:19,373 演讲者 SPEAKER_06：是的，如果你能的话，请打开你的摄像头。
581 00:51:20,355 --> 00:51:21,275 演讲者 SPEAKER_04：是的，谢谢。
582 00:51:21,596 --> 00:51:25,719 说话人 SPEAKER_01: 嗯，那会很棒，因为我可以看到你的嘴唇，我有点聋，所以这会很有帮助。
583 00:51:25,739 --> 00:51:26,300 说话人 SPEAKER_01: 看见你的嘴唇。
584 00:51:27,981 --> 00:51:30,844 说话人 SPEAKER_04: 你好，Hinton 博士，我是 Robbie。
585 00:51:32,106 --> 00:51:37,170 说话人 SPEAKER_04: 非常感谢你的演讲。
586 00:51:37,402 --> 00:51:39,485 说话人 SPEAKER_04：关于 Hopfield 网络。
587 00:51:39,686 --> 00:51:44,192 说话人 SPEAKER_04：我真的很喜欢您对 Hopfield 网络和能量的讨论。
588 00:51:44,652 --> 00:51:54,885 说话人 SPEAKER_04：我想问一下，直到现在，抱歉，我之前从未听说过 Hopfield 网络。
589 00:51:55,005 --> 00:52:02,094 说话人 SPEAKER_04：我已经了解了机器学习和深度学习，以及最新的，比如图神经网络。
590 00:52:02,311 --> 00:52:04,992 说话人 SPEAKER_04：几何深度学习等等。
591 00:52:05,373 --> 00:52:09,518 说话人 SPEAKER_04：您知道霍普菲尔德网络现在是否还存在吗？
592 00:52:10,798 --> 00:52:12,800 说话人 SPEAKER_04：或者——霍普菲尔德网络有一个新版本。
593 00:52:13,822 --> 00:52:20,027 说话人 SPEAKER_01：有一个新版本，人们正在尝试一种霍普菲尔德网络的变体。
594 00:52:20,367 --> 00:52:31,557 说话人 SPEAKER_01：但基本上，在 20 世纪 80 年代初，如果你去 NeurIPS（当时称为 NIPS），大部分论文都是关于 Hopfield 网络的。
595 00:52:32,010 --> 00:52:37,318 说话人 SPEAKER_01：然后到了 20 世纪 80 年代中期，有一些关于反向传播的论文。
596 00:52:38,639 --> 00:52:43,985 说话人 SPEAKER_01：然后 Hopfield 网络论文和反向传播网络论文的数量逐渐减少。
597 00:52:44,086 --> 00:52:46,289 说话人 SPEAKER_01：然后有很多关于支持向量机的论文。
598 00:52:47,291 --> 00:52:52,958 说话人 SPEAKER_01：然后到 2000 年代中期，反向传播又回来了，而 Hopfield 网络却没有。
599 00:52:55,400 --> 00:52:56,563 说话人 SPEAKER_01：这大致是该领域的历程。
600 00:52:56,842 --> 00:53:00,668 说话人 SPEAKER_01：但在当前情况下，我觉得实际上解释
601 00:53:01,440 --> 00:53:04,945 说话人 SPEAKER_01：玻尔兹曼机再次，因为它们是一个美丽但毫无实际应用价值的思想。
602 00:53:06,748 --> 00:53:07,730 说话人 SPEAKER_04: 嗯。
603 00:53:07,750 --> 00:53:16,181 说话人 SPEAKER_04: 你看，我知道当我们学习机器学习时，会用到，还在讨论玻尔兹曼机。
604 00:53:16,240 --> 00:53:30,300 说话人 SPEAKER_04: 我的意思是，我认为即使在现在，即使我们听到图神经网络或因果深度学习等出现时，也应该讨论一些热门领域网络和玻尔兹曼网络。
605 00:53:30,483 --> 00:53:36,570 说话人 SPEAKER_04: 关于图神经网络的出现，甚至像因果深度学习这样的。
606 00:53:38,072 --> 00:53:39,375 说话人 SPEAKER_01: 嗯。
607 00:53:39,695 --> 00:53:46,985 这是该领域的历史的一部分，可能仍然相关，特别是 Boltz 机器学习算法以及有一个单独的阶段来解除学习的想法。
608 00:53:48,527 --> 00:53:50,048 但我们应该让其他人提问。
609 00:53:50,548 --> 00:53:51,650 说话人 SPEAKER_05: 安德鲁，你有问题吗？
610 00:53:53,152 --> 00:53:55,956 说话人 SPEAKER_08: 谢谢。
611 00:53:55,976 --> 00:54:00,422 说话人 SPEAKER_05: 如果你想分享，如果你介意的话，请打开你的摄像头。
612 00:54:02,360 --> 00:54:02,840 说话人 SPEAKER_05: 不介意。
613 00:54:05,222 --> 00:54:06,224 说话人 SPEAKER_05: 也许你的麦克风。
614 00:54:07,244 --> 00:54:07,965 说话人 SPEAKER_05: 你的麦克风。
615 00:54:08,226 --> 00:54:09,327 说话人 SPEAKER_03: 是的。
616 00:54:09,648 --> 00:54:10,047 说话人 SPEAKER_07: 你能听到我吗？
617 00:54:11,210 --> 00:54:11,429 说话人 SPEAKER_03: 能。
618 00:54:12,871 --> 00:54:14,052 说话人 SPEAKER_03：很高兴见到你。
619 00:54:14,193 --> 00:54:29,329 说话人 SPEAKER_03：所以我的问题是，在这种情况下，我的直觉对吗？我们确实需要更多关于“反学习”的内容，即使目前大多数神经网络主要关注学习分子部分，还是我的理解有误？
620 00:54:30,253 --> 00:54:32,717 说话人 SPEAKER_01：有两种模型。
621 00:54:32,817 --> 00:54:40,947 说话人 SPEAKER_01：有一种模型你有分步函数，你使用能量来操纵概率，但你需要一个分步函数。
622 00:54:41,608 --> 00:54:42,809 说话人 SPEAKER_01：然后有一些模型你不需要。
623 00:54:43,030 --> 00:54:45,251 说话人 SPEAKER_01：至于标准神经网络，没有配分函数。
624 00:54:46,253 --> 00:54:49,657 说话人 SPEAKER_01：这就是为什么你不需要重新学习。
625 00:54:49,677 --> 00:54:51,920 说话人 SPEAKER_07：Manolis，你有问题吗？
626 00:54:56,686 --> 00:54:59,670 说话人 SPEAKER_07：麻省理工学院的卡利斯博士。
627 00:55:04,072 --> 00:55:04,552 说话人 SPEAKER_07：你能听到我吗？
628 00:55:09,858 --> 00:55:10,659 说话人 SPEAKER_07：曼诺利斯·卡拉斯。
629 00:55:11,862 --> 00:55:12,643 说话人 SPEAKER_07：你举手了。
630 00:55:15,987 --> 00:55:17,228 演讲者 SPEAKER_05: 好的，我们会再联系你，我想。
631 00:55:17,248 --> 00:55:18,409 演讲者 SPEAKER_06: 哦，你能听到吗？
632 00:55:18,751 --> 00:55:19,030 演讲者 SPEAKER_06: 好的。
633 00:55:19,871 --> 00:55:21,353 演讲者 SPEAKER_00: 是的。
634 00:55:21,534 --> 00:55:22,295 说话人 SPEAKER_00: 嗨，教授。
635 00:55:22,315 --> 00:55:23,217 说话人 SPEAKER_00: 你也能看到我吗？
636 00:55:23,277 --> 00:55:23,836 说话人 SPEAKER_00: 你能看到我吗？
637 00:55:24,217 --> 00:55:24,498 说话人 SPEAKER_00: 是的。
638 00:55:24,538 --> 00:55:27,222 说话人 SPEAKER_06: 你想在摄像头的屏幕上打开吗？
639 00:55:27,661 --> 00:55:29,744 说话人 SPEAKER_00: 我想我已经打开了视频。
640 00:55:29,784 --> 00:55:30,365 说话人 SPEAKER_00: 你现在能看到我吗？
641 00:55:30,606 --> 00:55:31,047 说话人 SPEAKER_00: 你能听到我说话吗？
642 00:55:31,586 --> 00:55:32,487 说话人 SPEAKER_00: 是的。
643 00:55:32,809 --> 00:55:33,068 说话人 SPEAKER_00: 好的。
644 00:55:33,449 --> 00:55:33,690 说话人 SPEAKER_00: 太棒了。
645 00:55:34,092 --> 00:55:46,010 说话人 SPEAKER_00: 所以我是麻省理工学院人工智能领域的教授，同时也在研究人脑，并进行大量的精神病学研究和认知生物医学研究。
646 00:55:46,512 --> 00:56:01,094 演讲者 演讲者_00：我们人类大脑中存在这种非凡的细胞多样性，这引发了一个问题，我们是否应该考虑
647 00:56:01,632 --> 00:56:09,483 演讲者 演讲者_00：人类大脑的多样性可能是一个暗示，我们需要在人工神经网络中拥有不同类型的神经元。
648 00:56:09,922 --> 00:56:22,239 演讲者 演讲者_00：换句话说，如果大脑中存在如此多的专业化，这是否仅仅是通过我们人工神经网络中复杂的数十亿参数自然发生的？
649 00:56:23,201 --> 00:56:28,628 演讲者 演讲者_00：或者我们应该在它们中硬编码专业化的约束？
650 00:56:28,869 --> 00:56:31,351 说话人 说话人_00：我想知道您在多大程度上考虑了智能的自然基础，以及这仅仅是人类大脑进化的产物，还是说这是使我们变得智能的东西。
651 00:56:31,972 --> 00:56:42,811 说话人 说话人_00：我认为如果您想了解大脑中的情况，您需要认真对待这样一个事实：存在许多不同的细胞类型，许多不同类型的抑制性中间神经元，以及许多不同类型的锥体细胞，它们在大多数地方都是相当相似的，但有许多抑制性神经元在模型中根本就没有被建模。
652 00:56:43,092 --> 00:56:55,534 说话人 说话人_01：我认为，如果您想了解大脑中的情况，您需要认真对待这样一个事实：存在许多不同的细胞类型，许多不同类型的抑制性中间神经元，以及许多不同类型的锥体细胞，它们在大多数地方都是相当相似的，但有许多抑制性神经元在模型中根本就没有被建模。
653 00:56:56,188 --> 00:57:02,297 说话人 说话人_01：在大多数地方，这些锥体细胞相当相似，但有许多抑制性神经元在模型中根本就没有被建模。
654 00:57:04,280 --> 00:57:05,420 说话人 SPEAKER_01：你必须认真对待这件事。
655 00:57:05,460 --> 00:57:10,969 说话人 SPEAKER_01：我认为我们还没有真正理解基本的学习算法。
656 00:57:11,510 --> 00:57:19,260 说话人 SPEAKER_01：我们知道反向传播工作得很好，因为它可以计算梯度，但对于大脑来说这似乎相当不切实际。
657 00:57:19,983 --> 00:57:31,195 说话人 SPEAKER_01：我和其他人尝试过用活动的时间差来表示反向传播的东西，即误差函数的导数。
658 00:57:31,856 --> 00:57:37,181 讲者 SPEAKER_01：所以你可以用神经活动来表示误差函数的导数和实际活动。
659 00:57:37,882 --> 00:57:43,188 讲者 SPEAKER_01：对于小数据集，你可以使用那种学习算法来建模。
660 00:57:43,389 --> 00:57:47,052 讲者 SPEAKER_01：但是一旦涉及到像 ImageNet 这样的大型数据集，它的效果远不如反向传播。
661 00:57:47,371 --> 00:57:51,996 讲者 SPEAKER_01：所以我认为目前没有人真正知道学习算法是什么。
662 00:57:52,516 --> 00:58:04,431 讲者 SPEAKER_01: 但我的直觉是，你应该首先寻求根本性的简化，并尝试找到一个大致可行的学习算法，然后看看所有复杂性是如何逐步积累的。
663 00:58:04,690 --> 00:58:13,681 讲者 SPEAKER_01: 如果你从复杂性开始，你最终可能会得到像蓝色大脑项目那样的事情，这几乎是毫无希望的。
664 00:58:13,965 --> 00:58:21,375 讲者 SPEAKER_00: 这涉及到通用人工智能（AGI）的学习算法的复杂性。
665 00:58:21,896 --> 00:58:25,800 讲者 SPEAKER_00: 你觉得我们现在的架构是否仅仅需要更多的扩展？
666 00:58:26,641 --> 00:58:38,255 说话者 SPEAKER_00: 或者您觉得我们需要彻底不同的代理功能方式，更多的自主性，更以目标为导向的思考等等吗？
667 00:58:39,938 --> 00:58:41,340 说话者 SPEAKER_01: 好的。
668 00:58:41,556 --> 00:58:44,842 说话者 SPEAKER_01: 我认为我们需要的是生成更好数据的方法。
669 00:58:45,362 --> 00:58:53,494 说话者 SPEAKER_01: 所以如果你看一个没有规模问题的领域，比如 AlphaGo 和 AlphaZero，因为它们使用蒙特卡洛回放。
670 00:58:54,675 --> 00:59:03,128 说话人 SPEAKER_01：然后他们使用这些来训练蒙特卡洛滚动的结果，为训练神经网络提供数据，以判断这是否是一个好走法或好位置？
671 00:59:04,010 --> 00:59:08,577 说话人 SPEAKER_01：对我们来说，如果想让大型语言模型不断改进推理能力，
672 00:59:09,045 --> 00:59:14,534 说话人 SPEAKER_01：那么就需要让模型进行推理，然后将推理结果与它们的原始直觉进行对比检查。
673 00:59:15,114 --> 00:59:16,036 说话人 SPEAKER_01：我们一直在这样做。
674 00:59:16,635 --> 00:59:18,159 说话人 SPEAKER_01：如果你是 MAGA 的一员，你就不会那样做。
675 00:59:18,179 --> 00:59:22,244 说话人 SPEAKER_01：但如果你是一个普通人，你会将推理的结果与你的原始直觉进行对比。
676 00:59:23,306 --> 00:59:28,032 说话人 SPEAKER_01：这样，你可以获得大量的数据。
677 00:59:28,833 --> 00:59:35,322 说话人 SPEAKER_01：所以我认为将要发生的事情是，我们将得到这些大型模型以类似于 AlphaZero 的方式生成自己的数据。
678 00:59:35,842 --> 00:59:37,364 说话人 SPEAKER_01：这样就能克服数据瓶颈。
679 00:59:37,581 --> 00:59:40,047 说话人 SPEAKER_01：然后扩展将带我们走到我们想去的地方。
680 00:59:40,068 --> 00:59:44,798 说话人 SPEAKER_01：但这并不意味着我们不能通过有革命性的新想法更快地到达那里。
681 00:59:45,179 --> 00:59:47,385 说话人 SPEAKER_01：可能还有很多像 Transformer 这样的新想法。
682 00:59:48,126 --> 00:59:54,021 演讲者 SPEAKER_01：在接下来的五到十年里，如果我们没有遇到一两个非常有帮助的全新想法，我会感到非常惊讶。
683 00:59:54,472 --> 01:00:08,592 演讲者 SPEAKER_01：即使我们没有，我认为我们也可以通过扩大规模和利用让模型进行推理以及通过蒙特卡洛滚动来检查结果，与它的原始直觉相比以获得学习信号的方式达到目标。
684 01:00:09,514 --> 01:00:18,385 演讲者 SPEAKER_00：您所说的推理是指传统的符号推理，你知道，正式类型的推理，而不是那种LLM类型的推理，对吧？
685 01:00:18,487 --> 01:00:20,969 演讲者 SPEAKER_01：不，我的意思是，我在说LLM中的推理。
686 01:00:21,389 --> 01:00:23,070 说话人 SPEAKER_01: 我不相信。
687 01:00:23,371 --> 01:00:23,891 说话人 SPEAKER_01: 我不相信。
688 01:00:23,911 --> 01:00:28,195 说话人 SPEAKER_01: 我的意思是，当我们做逻辑时，我们使用类似 LLM 的东西。
689 01:00:28,215 --> 01:00:30,498 说话人 SPEAKER_01: 这就是为什么我们在纯逻辑方面不太擅长。
对于人类来说，例如，当他们进行推理时，他们推理的内容对他们是否正确进行推理有很大影响。
691 01：00：40,146 --> 01：00：43,891 演讲者 SPEAKER_01：人们在内容无关紧要的情况下进行正式的推理是无望的。
692 01：00：44,472 --> 01：00：45,231 议长 SPEAKER_01：他们就是做不到。
693 01：00：46,733 --> 01：00：48,054 议长 SPEAKER_00：我是说，我们确实花钱。
694 01:00:49,266 --> 01:00:53,751 讲者 SPEAKER_00：我们在学校里花了 20 多年的时间，试图强迫自己学会那样推理。
695 01:00:53,771 --> 01:00:56,072 讲者 SPEAKER_00：但你说的我们仍然不是天生的。
696 01:00:57,014 --> 01:00:58,034 讲者 SPEAKER_01：是的，这不是天生的。
697 01:00:58,795 --> 01:01:02,539 讲者 SPEAKER_00：太好了，感谢您的评论，再次祝贺您出色的工作。
698 01:01:02,559 --> 01:01:06,742 说话人 SPEAKER_06: 是的，然后看起来我们到了 210 这个时间点。
699 01:01:06,802 --> 01:01:07,744 说话人 SPEAKER_06: 可能还有一个问题。
700 01:01:08,204 --> 01:01:09,686 说话人 SPEAKER_06: 好的，最后一个问题。
701 01:01:10,385 --> 01:01:11,788 说话人 SPEAKER_06: 实际上，我确实有一个问题。
702 01:01:11,827 --> 01:01:17,652 演讲者 SPEAKER_06：我的问题稍微不那么技术性，实际上是与像这样的
703 01:01:19,219 --> 01:01:26,552 演讲者 SPEAKER_06：这些大型科技公司以及监管它们或监督它们，关于计算资源方面。
704 01:01:26,992 --> 01:01:40,637 演讲者 SPEAKER_06：你在一次采访中提到，这些公司中的一些，我们不知道，它们可能会利用漏洞通过限制收入来规避安全。
705 01:01:42,079 --> 01:01:44,382 演讲者 SPEAKER_06：我们应该采取什么措施，
我们是否知道如何确保这些大型公司在面临这些 AI 技术可能对私营部门就业带来的严重后果时承担责任？
707 01:02:06,489 --> 01:02:14,320 说话人 SPEAKER_01：有两种明显的方法来确保他们的责任，那就是每当他们向公众发布模型时，他们应该已经进行了大量的安全测试。
708 01:02:14,653 --> 01:02:18,099 说话人 SPEAKER_01：你可以要求他们告诉你他们做了哪些测试，并告诉你测试结果。
709 01:02:20,302 --> 01:02:21,123 说话人 SPEAKER_01：这是你可以做的一件事。
710 01:02:21,143 --> 01:02:22,724 说话人 SPEAKER_01：你可以对他们的不作为进行处罚。
711 01:02:23,626 --> 01:02:27,311 说话人 SPEAKER_01: 我认为加利福尼亚州法案 1047 中可能有类似的内容。
712 01:02:28,833 --> 01:02:39,349 说话人 SPEAKER_01: 但是，你可以强制要求，这更难，我不确定在美国能否做到，但你可以强制要求他们将其计算资源的一定比例用于安全研究。
713 01:02:39,989 --> 01:02:41,791 说话人 SPEAKER_01: 所以如果你看看开放 AI 的历史，
714 01:02:42,396 --> 01:02:48,773 说话人 SPEAKER_01: 人们像伊利亚离开的原因之一是因为他们不愿意将足够的计算资源投入到安全研究中。
715 01:02:50,436 --> 01:02:51,940 说话人 SPEAKER_01: 我认为这是伊利亚离开的一个原因。
716 01:02:53,184 --> 01:02:55,088 说话人 SPEAKER_01: 当然是约翰·莱克离开的原因之一。
717 01:02:56,512 --> 01:02:57,655 说话人 SPEAKER_01: 约翰·莱克。
718 01:02:58,934 --> 01:03:01,076 说话人 SPEAKER_01: 所以这些都是政府可以做的明显事情。
719 01:03:01,556 --> 01:03:08,164 说话人 SPEAKER_01：在美国，不清楚是否可以强制要求公司在其计算资源上投入一定比例用于安全。
720 01:03:08,684 --> 01:03:13,771 说话人 SPEAKER_01：但在疫情期间，可以强制要求公司生产呼吸机。
721 01:03:14,532 --> 01:03:18,235 说话人 SPEAKER_01：所以在疫情期间，可以强制做一些事情。
722 01:03:20,318 --> 01:03:20,878 说话人 SPEAKER_01：那我们就拭目以待吧。
723 01:03:22,541 --> 01:03:28,688 说话人 SPEAKER_06：基本上没有保证，就像我们的立法一样，我确信立法者正在非常努力地工作，试图制定新的法律来规范，但实际上并非所有法律都是如此。
724 01:03:29,106 --> 01:03:38,605 说话人 SPEAKER_06：希望他们现在正在非常努力地工作，试图制定新的法律来规范，但实际上并非所有法律都是如此。
725 01:03:38,684 --> 01:03:44,695 说话人 SPEAKER_01：例如，现在设计一个像 COVID 这样的病原体只需要大约 10 万美元。
726 01:03:45,818 --> 01:03:49,284 说话人 SPEAKER_01：因此，拥有 100 万美元的团体可以设计 10 个这样的病原体，并一次性释放它们。
727 01:03:49,905 --> 01:03:51,148 说话者 SPEAKER_01：这非常可怕。
728 01:03:51,483 --> 01:03:59,894 说话者 SPEAKER_01：之所以这么便宜，是因为你不需要有湿实验室，你不需要自己制作，你只需要获取一个序列，然后将其发送到云端，有人会把它装在瓶子里发给你。
729 01:04:00,896 --> 01:04:15,398 说话者 SPEAKER_01：现在对于在云端为你制作这个的人来说，检查它没有看起来非常像 COVID 刺突蛋白序列的东西似乎是合理的。
730 01:04:15,782 --> 01:04:21,150 说话者 SPEAKER_01：在将试管中的东西发回之前，你确实应该检查几件事情。
731 01:04:21,309 --> 01:04:26,976 白宫希望公司这么做，但它无法强迫他们这么做。
732 01:04:27,336 --> 01:04:35,106 它无法强迫他们这么做，因为共和党人不会在立法上合作，因为他们不想给民主党任何胜利的机会。
733 01:04:36,188 --> 01:04:44,659 因此，美国的体制已经混乱到了这种地步，一个可能让我们所有人毁灭的紧急威胁，他们却无法采取明显的措施。
734 01:04:47,693 --> 01:04:49,896 哇。
735 01:04:50,717 --> 01:05:01,610 说话人 SPEAKER_06: 我想我们不应该以那个结尾，所以我们非常感谢您今天抽出时间。
736 01:05:01,650 --> 01:05:13,105 说话人 SPEAKER_06: 我们总是愿意提供，如果您喜欢巧克力，我们愿意送您一些巧克力作为我们对你抽出时间的感谢的小礼物。
737 01:05:13,184 --> 01:05:14,507 说话人 SPEAKER_01: 我喜欢巧克力。
738 01:05:14,527 --> 01:05:15,327 说话人 SPEAKER_01: 请给我发一封电子邮件。
739 01:05:15,347 --> 01:05:16,248 说话人 SPEAKER_01: 我会给你发我的地址。
740 01:05:17,090 --> 01:05:17,831 说话人 SPEAKER_02: 好的。
741 01:05:17,851 --> 01:05:18,170 说话人 SPEAKER_02: 完美。
742 01:05:18,190 --> 01:05:18,731 说话人 SPEAKER_06: 听起来不错。
743 01:05:19,713 --> 01:05:21,074 说话人 SPEAKER_02: 大家快鼓掌。
744 01:05:21,094 --> 01:05:23,556 说话人 SPEAKER_06: 感谢大家今天抽出时间来。
745 01:05:23,576 --> 01:05:24,797 说话人 SPEAKER_06: 我们真的非常感激。
746 01:05:25,657 --> 01:05:32,184 说话人 SPEAKER_06: 下周，嗯，我们将有下一位演讲者。
747 01:05:32,204 --> 01:05:35,927 说话人 SPEAKER_06: 您可以查看我们的网站，嗯，信息已经发布。
748 01:05:37,449 --> 01:05:37,670 说话人 SPEAKER_02: 是的。
749 01:05:37,690 --> 01:05:39,231 说话人 SPEAKER_02: 感谢大家今天加入我们。
750 01:05:39,251 --> 01:05:41,233 说话人 SPEAKER_02: 感谢您抽出时间。
751 01:05:42,353 --> 01:05:42,815 说话人 SPEAKER_05: 谢谢。
752 01:05:42,875 --> 01:05:44,496 说话人 SPEAKER_05: 谢谢。
753 01:05:46,117 --> 01:05:46,438 说话人 SPEAKER_05: 再见。
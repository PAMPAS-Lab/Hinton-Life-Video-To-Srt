1 00:00:04,823 --> 00:00:06,466 演讲者 SPEAKER_02：我们的下一位演讲者是 Jeff Hinton。
2 00:00:07,609 --> 00:00:11,076 演讲者 SPEAKER_02：Jeff 是一位科学家，我认为他几乎不需要介绍。
3 00:00:11,237 --> 00:00:18,832 演讲者 SPEAKER_02：他是机器学习领域的绝对支柱，他的贡献比我能够在这里列举的要多。
4 00:00:19,167 --> 00:00:30,682 演讲者 SPEAKER_02：你知道，David 提到了 Jeff 对他的科学生涯产生的深远影响，这也是我想表达的，我想在场的很多人也会这么说。
5 00:00:31,504 --> 00:00:43,521 说话人 SPEAKER_02：事实上，我认为，随着近年来对神经网络兴趣的复苏，以及 Jeff 对在线学习的贡献，他激发了一代机器学习研究者的灵感。
6 00:00:43,500 --> 00:00:49,087 说话人 SPEAKER_02：今天，按照 Jeff 的习惯，我期待他会向我们解释大脑是如何工作的。
7 00:00:51,549 --> 00:00:52,030 说话人 SPEAKER_02：请开始吧，Jeff。
8 00:01:00,479 --> 00:01:03,243 说话人 SPEAKER_01：好的，既然 Ryan 已经沉默了，我猜现在该我说了。
9 00:01:06,245 --> 00:01:08,989 说话人 SPEAKER_01：在开始之前，我想说几句关于大卫的话。
10 00:01:10,811 --> 00:01:11,652 说话人 SPEAKER_01：瑞恩，你能听到我说话吗？
11 00:01:11,671 --> 00:01:12,832 说话人 SPEAKER_01：如果你能听到我说话，请点头。
12 00:01:14,399 --> 00:01:15,260 说话人 SPEAKER_01：好的，很好。
13 00:01:16,941 --> 00:01:19,625 说话人 SPEAKER_01：我第一次在 NIPS 展板上见到 David。
14 00:01:21,406 --> 00:01:26,012 说话人 SPEAKER_01：这个孩子走到展板前看了整整一分钟，然后说，你们为什么不这样呢？
15 00:01:26,591 --> 00:01:28,635 说话人 SPEAKER_01：我当时心想，该死，我们为什么不那样做呢？
16 00:01:29,334 --> 00:01:33,819 说话人 SPEAKER_01：所以我立刻想到，我最好让他来做博士后。
17 00:01:33,960 --> 00:01:36,864 说话人 SPEAKER_01：我试图让他成为博士后，但他说他不喜欢美国。
18 00:01:38,885 --> 00:01:44,371 说话人 SPEAKER_01：然而，他在 20 世纪 90 年代经常访问多伦多。
19 00:01:44,891 --> 00:01:49,698 说话人 SPEAKER_01：当时，我正在研究变分方法，我提出了这个疯狂的 bits-back 论点。
20 00:01:50,159 --> 00:01:53,123 说话人 SPEAKER_01：当我尝试解释 bits-back 论点时，大多数人都会感到困惑。
21 00:01:53,724 --> 00:01:55,668 说话人 SPEAKER_01: 大卫立刻明白了，并且很喜欢。
22 00:01:56,188 --> 00:01:58,572 说话人 SPEAKER_01: 那真是太好了，我们谈论了很多关于变分方法的事情。
23 00:02:00,275 --> 00:02:05,843 说话人 SPEAKER_01: 我最后想说的是，大卫不仅是一位杰出的学者，而且他还是一个有所作为的人。
24 00:02:06,924 --> 00:02:12,132 说话人 SPEAKER_01: 因此，他的书籍以及作为政府首席科学家的实际工作可能帮助拯救了世界。
25 00:02:12,532 --> 00:02:15,256 说话人 SPEAKER_01：你不能对许多学者这样说。
26 00:02:16,456 --> 00:02:17,518 说话人 SPEAKER_01：好的，接下来是演讲。
27 00:02:19,360 --> 00:02:21,722 说话人 SPEAKER_01：我将要谈论与 Timothy Lillicrap 的合作研究。
28 00:02:21,902 --> 00:02:23,824 说话人 SPEAKER_01：我在剑桥已经谈论过一些这方面的内容了。
29 00:02:24,444 --> 00:02:26,848 说话人 SPEAKER_01：最后，我会有些新内容。
30 00:02:28,649 --> 00:02:29,150 说话人 SPEAKER_01：接下来是下一页幻灯片。
31 00:02:31,431 --> 00:02:32,233 说话人 SPEAKER_01：我们能翻到下一页吗？
32 00:02:37,198 --> 00:02:41,021 说话人 SPEAKER_01：好的。
33 00:02:41,423 --> 00:02:46,409 讲者 SPEAKER_01：神经科学家长期以来一直声称反向传播作为皮层的模型是荒谬的。
34 00:02:48,132 --> 00:02:57,044 讲者 SPEAKER_01：我认为他们并没有真正理解早期特征检测器适应的重要性，以便为系统后续部分提供所需的东西。
35 00:02:58,326 --> 00:03:01,131 讲者 SPEAKER_01：他们并没有真正理解这一点在计算上的重要性。
36 00:03:02,573 --> 00:03:07,919 讲者 SPEAKER_01：他们需要理解这一点，因为我们现在知道这种监督学习效果非常好。
37 00:03:08,574 --> 00:03:10,516 说话人 SPEAKER_01：那么我们应该问，他们为什么认为这是不可能的？
38 00:03:10,818 --> 00:03:11,318 说话人 SPEAKER_01：下一页。
39 00:03:18,046 --> 00:03:18,728 说话人 SPEAKER_02：你明白了。
40 00:03:18,747 --> 00:03:21,531 说话人 SPEAKER_01：我认为有四个主要原因。
41 00:03:22,292 --> 00:03:24,455 说话人 SPEAKER_01：首先，没有明显的监督信号来源。
42 00:03:25,336 --> 00:03:30,883 说话人 SPEAKER_01：第二个原因是弗朗西斯·克里克所相信的。
43 00:03:30,923 --> 00:03:34,347 说话人 SPEAKER_01：这是弗朗西斯·克里克认为反向传播荒谬的原因。
44 00:03:34,799 --> 00:03:41,425 说话人 SPEAKER_01：是因为皮层神经元不能传递实值活动，这对于反向传播似乎是必要的。
第三点是，反向传播中的神经元需要将误差导数向后发送，并将检测到的特征信号向前发送。
46 00:03:52,937 --> 00:03:56,300 发言人 SPEAKER_01：因此，神经元的输出需要传达两种信息。
现在很多人说，嗯，我们将有两个不同的神经元来完成这个任务。
48 00:03:59,423 --> 00:04:01,245 发言人 SPEAKER_01：但如果你能用一个神经元做到这一点，那就太好了。
49 00:04:01,832 --> 00:04:09,465 讲者 SPEAKER_01：本次演讲的主要观点是，如果你坚持一种特定的方式来表示误差导数，你可以用一个神经元来完成。
50 00:04:09,485 --> 00:04:22,105 讲者 SPEAKER_01：最后一个问题是，尽管一个皮层区域连接到另一个区域时，总是存在反向连接，但这些反向连接实际上并不是来自发送正向连接的相同神经元，它们的权重也绝对不是对称的。
51 00:04:22,464 --> 00:04:28,654 讲者 SPEAKER_01：所以，如果你打算使用正向矩阵的转置，你不会拥有这种点对点的对称性。
52 00:04:28,675 --> 00:04:29,375 讲者 SPEAKER_01：下一页。
53 00:04:35,194 --> 00:04:35,716 说话人 SPEAKER_01: 下一页。
54 00:04:38,338 --> 00:04:43,244 说话人 SPEAKER_01: 好的，所以有很多可能的监督来源。
55 00:04:43,824 --> 00:04:46,367 说话人 SPEAKER_01: 在 80 年代和 90 年代，我花了很多时间去思考这个问题。
56 00:04:47,608 --> 00:04:51,733 说话人 SPEAKER_01: 显然的一个例子是像自编码器这样的东西，你可以重建当前帧的所有或部分。
57 00:04:52,413 --> 00:04:57,560 说话人 SPEAKER_01：你也可以有一个预测未来的自编码器，也就是说，你尝试重建下一帧。
58 00:04:58,120 --> 00:05:02,225 说话人 SPEAKER_01：这是一种将无监督学习转化为监督学习形式的方法。
59 00:05:03,987 --> 00:05:04,947 说话人 SPEAKER_01：我认为
60 00:05:05,249 --> 00:05:13,742 说话人 SPEAKER_01：大脑皮层可能使用的方法是使局部提取的特征与从局部上下文或另一模态预测的特征相一致。
61 00:05:14,504 --> 00:05:22,216 说话人 SPEAKER_01：所以如果给你展示这个句子，她用平底锅砸了他，这个词“砸了”的上下文能让你感受到它的意思。
62 00:05:22,255 --> 00:05:27,764 说话人 SPEAKER_01：所以在一次试验中，你可以很好地理解“砸了”的意思。
63 00:05:28,285 --> 00:05:32,471 说话人 SPEAKER_01：我的意思是，你有一些不确定性，但可能意味着她用平底锅以某种方式砸了他。
64 00:05:35,658 --> 00:05:36,559 说话人 SPEAKER_01：好的，下一页。
65 00:05:41,646 --> 00:05:44,610 说话人 SPEAKER_01：我认为所有这些监督来源都很容易获得。
66 00:05:44,872 --> 00:05:46,733 说话人 SPEAKER_01：所以我认为我们可以停止担心这个问题了。
67 00:05:48,276 --> 00:05:52,161 说话人 SPEAKER_01：下一个问题是神经元不发送模拟信号。
68 00:05:52,903 --> 00:05:54,204 说话人 SPEAKER_01：这个问题困扰了我很长时间。
69 00:05:55,067 --> 00:06:05,461 说话人 SPEAKER_01：一旦你意识到我们实际上是在小数据领域进行操作，这其实很容易理解。
70 00:06:06,454 --> 00:06:11,262 说话人 SPEAKER_01：如果你有一个大脑，你就可以承受在训练样本上投入大量的突触。
71 00:06:11,942 --> 00:06:14,728 说话人 SPEAKER_01：那么，我们会为每个训练样本投入多少突触呢？
72 00:06:15,309 --> 00:06:16,711 说话人 SPEAKER_01：大约是 10 的 5 次方。
73 00:06:17,170 --> 00:06:20,596 说话人 SPEAKER_01：可能少至 10 的 4 次方，多至 10 的 6 次方。
74 00:06:21,117 --> 00:06:24,783 说话人 SPEAKER_01：但是我们会将成千上万的突触应用于每个训练样本。
75 00:06:25,524 --> 00:06:29,651 说话人 SPEAKER_01：如果你和统计学家交谈，他们会告诉你参数数量不应当超过训练样本数量。
76 00:06:31,233 --> 00:06:35,178 说话人 SPEAKER_01：幸运的是，大脑没有和统计学家交谈。
77 00:06:35,985 --> 00:06:36,807 说话人 SPEAKER_01：多得多。
78 00:06:37,608 --> 00:06:44,137 说话人 SPEAKER_01：现在我们都知道了一种使用比训练样本更多的参数的方法，那就是构建模型集成。
79 00:06:45,199 --> 00:06:57,156 说话人 SPEAKER_01：所以你在集成中的每个模型中只使用少量参数，我们知道随着你拥有越来越多的突触，如果你只是不断地向集成中添加东西，你可以让你的模型越来越好。
80 00:06:57,336 --> 00:06:58,437 说话人 SPEAKER_01：你会得到递减的回报。
81 00:07:01,281 --> 00:07:04,726 说话者 SPEAKER_01：我们知道有一种方法可以始终利用更多的突触，
82 00:07:05,973 --> 00:07:18,665 说话者 SPEAKER_01：几年前，我发现通过一种称为 dropout 的技术，可以得到与集成相同的效果，但效率更高，这种方法是通过随机从你的网络中移除神经元来实现的。
83 00:07:19,165 --> 00:07:21,928 说话者 SPEAKER_01：所以每次运行网络时，它都是集成中的一个不同成员。
84 00:07:22,709 --> 00:07:34,461 说话者 SPEAKER_01：而且它将是一个指数级大的集成，但令人愉快的是，现在集成中的所有成员都共享参数，这意味着
85 00:07:35,031 --> 00:07:40,199 发言人 SPEAKER_01：即使您从未见过合奏中某个成员的示例，您也可以了解这些参数。
86 00:07:41,401 --> 00:07:42,622 说话者 SPEAKER_01：因此你得到了一个很好的属性。
所以基本上，这表明添加噪声和拥有集成体非常、非常相似。
88 00:07:48,812 --> 00:07:53,218 说话者 SPEAKER_01：Dropout 可以看作是一种向单元活动添加伯努利噪声的方法。
但至少有一个隐藏层时，它完全等同于拥有一个大型的集成。
所以，在 Dropout 中，每个神经元要么发送
91 00:08:04,815 --> 00:08:13,064 扬声器 SPEAKER_01：正常输出，实际上是正常输出的两倍，否则它将发送零。
如果你从泊松过程中发送随机的尖峰，你会得到与 dropout 非常相似的效果。
93 00:08:22,333 --> 00:08:26,916 说话人 SPEAKER_01：再次，你得到这种泊松噪声，看起来泊松噪声似乎很糟糕。
94 00:08:27,718 --> 00:08:29,980 说话人 SPEAKER_01：但实际上，泊松噪声非常好。
95 00:08:30,880 --> 00:08:32,982 说话人 SPEAKER_01：实际上，这是一种方法，
96 00:08:33,250 --> 00:08:41,841 说话人 SPEAKER_01：可以得到与集成相同的效果，即你可以有比数据多得多的参数，而不会过度拟合。
97 00:08:43,243 --> 00:08:50,532 说话人 SPEAKER_01：实际上，从泊松过程发送脉冲比发送泊松过程的实际值要好。
98 00:08:51,974 --> 00:08:52,413 说话人 SPEAKER_01：下一页。
99 00:08:56,058 --> 00:09:00,303 说话人 SPEAKER_01：那么，从这个结论中，我将得出什么，可以给我下一页吗？
100 00:09:01,852 --> 00:09:06,476 说话人 SPEAKER_01：从这个结论中，我将得出我们不需要担心神经元不发送模拟值的事实。
101 00:09:07,158 --> 00:09:08,599 说话者 SPEAKER_01：让我们假设它们确实如此。
102 00:09:09,220 --> 00:09:13,183 说话者 SPEAKER_01：我们知道，最后我们可以用泊松过程的脉冲来替换这些模拟值。
103 00:09:13,724 --> 00:09:16,086 说话者 SPEAKER_01：一切都会好，只是它会更好地进行正则化。
104 00:09:17,668 --> 00:09:23,974 说话者 SPEAKER_01：我们还知道，随机梯度下降对添加的噪声非常鲁棒，只要噪声是无偏的。
105 00:09:25,394 --> 00:09:27,616 说话人 SPEAKER_01：所以我们宁愿有泊松尖峰而不是实数。
106 00:09:28,758 --> 00:09:29,599 说话人 SPEAKER_01：好的，下一页。
107 00:09:33,950 --> 00:09:37,193 说话人 SPEAKER_01：所以这就是我们下一页的新想法。
108 00:09:42,322 --> 00:09:52,676 说话人 SPEAKER_01：我们需要能够在感觉皮层的正向路径中表示误差导数，这正是我主要关注的。
109 00:09:54,759 --> 00:09:59,926 讲者 SPEAKER_01：神经科学家的传统想法是，会有一些单独的神经元来发送误差导数。
110 00:10:01,488 --> 00:10:03,770 讲者 SPEAKER_01：这实际上相当有问题，因为
111 00:10:04,139 --> 00:10:09,043 讲者 SPEAKER_01：那个单独的神经元需要发送既可能是正的，有时也可能是负的东西。
112 00:10:09,504 --> 00:10:13,089 讲者 SPEAKER_01：在神经科学中有一个基本规则，那就是神经元不能反转它们效果的正负。
113 00:10:15,410 --> 00:10:16,832 说话人 SPEAKER_01：这违反了 Dale 定律。
114 00:10:17,974 --> 00:10:31,327 说话人 SPEAKER_01：但是有一种更简单的方法来发送误差导数，那就是做出一个基本的表征决策，即我们将神经元输出的变化率表示为误差导数。
115 00:10:32,354 --> 00:10:40,067 说话人 SPEAKER_01：所以输出值的数值代表了神经元所表征的内容，而这个数值的变化率代表了误差导数，至少在一段时间内是这样的。
116 00:10:41,671 --> 00:10:48,123 说话人 SPEAKER_01：所以现在同一个神经元、同一根轴突可以同时携带信号向前传递和误差导数向后传递。
117 00:10:49,345 --> 00:10:53,732 说话人 SPEAKER_01：这样就可以使生活变得更加简单，如果你现在能想出如何使用反向传播的话。
118 00:10:55,115 --> 00:10:56,116 说话人 SPEAKER_01：那么，请翻到下一页。
119 00:11:06,205 --> 00:11:08,106 说话人 SPEAKER_01：抱歉，获取幻灯片有较大延迟。
120 00:11:08,126 --> 00:11:16,235 说话人 SPEAKER_01：好的，我将向大家展示一种使用时间导数和误差导数的方法，我和 James Helland 在 1987 年曾尝试过这种方法。
121 00:11:16,815 --> 00:11:27,306 说话者 SPEAKER_01：这是一个早期尝试制作不需要反向连接与正向连接相同的自动编码器的尝试。
122 00:11:28,466 --> 00:11:34,692 说话者 SPEAKER_01：想法是，你接收输入，将它绕一个循环发送一次，这个循环可以有几个阶段。
123 00:11:35,264 --> 00:11:44,933 说话者 SPEAKER_01：所以你用绿色连接发送它绕一次循环，然后再次发送它绕循环，红色连接与绿色连接相同，只是显示它再次绕了一圈。
124 00:11:46,556 --> 00:12:02,672 说话者 SPEAKER_01：然后你观察每个神经元的活性，你说活性的变化不好，我们希望活性保持不变，因为如果我们在绕过循环再次时所有神经元的活性都保持不变，这意味着我们已经很好地重建了输入。
125 00:12:03,783 --> 00:12:10,653 说话者 SPEAKER_01：所以我们只是将神经元活动的变化视为学习规则中的突触后项。
126 00:12:10,673 --> 00:12:25,134 说话者 SPEAKER_01：因此，从神经元 I 到神经元 J 的权重变化现在是神经元 I 的输出乘以神经元 J 输出变化率的负值。这就是时间依赖性可塑性，但符号是错误的。
127 00:12:25,174 --> 00:12:30,162 说话者 SPEAKER_01：有趣的是，我认为在 1987 年，当我们
128 00:12:30,412 --> 00:12:32,735 说话者 SPEAKER_01：我们实际上在 NIPS 上发表了该模型。
129 00:12:32,875 --> 00:12:35,220 说话人 SPEAKER_01：这是一篇很早的 NIPS 论文，实验非常糟糕。
130 00:12:35,861 --> 00:12:40,408 说话人 SPEAKER_01：我想当时人们并不知道存在尖峰时间依赖性可塑性。
131 00:12:40,730 --> 00:12:43,554 说话人 SPEAKER_01：所以你可以把它看作是一种预测，但它的符号是错误的。
132 00:12:43,575 --> 00:12:44,736 说话人 SPEAKER_01：但符号只有一位。
133 00:12:47,461 --> 00:12:48,724 说话人 SPEAKER_01: 让我们进入下一页幻灯片。
134 00:13:03,232 --> 00:13:12,707 说话人 SPEAKER_01: 后来我意识到，你可以将这种具有错误符号的 STDP 与具有正确符号的 STDP 结合起来。
135 00:13:13,809 --> 00:13:19,379 说话人 SPEAKER_01: 我是在用受限玻尔兹曼机堆叠进行预训练时意识到这一点的。
136 00:13:20,019 --> 00:13:25,950 说话人 SPEAKER_01: 我实际上在 2007 年就对此进行了演讲，幻灯片和幻灯片的更新都在网上。
137 00:13:28,173 --> 00:13:29,274 说话人 SPEAKER_01：所以这个想法是，
138 00:13:30,081 --> 00:13:34,826 说话人 SPEAKER_01：我们首先将学习一个堆叠的自动编码器，这将涉及使用反向 STTP。
139 00:13:35,407 --> 00:13:41,192 说话人 SPEAKER_01：一旦我们学习了一个堆叠的自动编码器，然后我们将信号通过这个堆叠的自动编码器。
140 00:13:42,173 --> 00:13:44,654 说话人 SPEAKER_01：我们将在顶部获得一些错误信号。
141 00:13:46,236 --> 00:13:51,701 说话者 SPEAKER_01：然后我们将以两种不同的阶段自上而下驱动神经元。
142 00:13:52,322 --> 00:13:58,467 说话者 SPEAKER_01：在第一个自上而下的路径中，我们驱动神经元从从输入预测出的表示中。
143 00:13:59,883 --> 00:14:11,258 说话者 SPEAKER_01：而在第二个自上而下的传递中，我们驱动它们从正确的表示，或者至少从我们预测回归到正确表示的方向，即目标方向。
144 00:14:12,779 --> 00:14:17,005 说话者 SPEAKER_01：因此，你得到了这两个自上而下传递中神经元活动的差异。
145 00:14:17,966 --> 00:14:21,412 说话人 SPEAKER_01：这个差异实际上允许你进行反向传播。
146 00:14:22,373 --> 00:14:25,957 说话人 SPEAKER_01：也就是说，你只需使用 STTP 学习规则。
147 00:14:26,730 --> 00:14:35,321 说话人 SPEAKER_01：在这个规则中，权重的变化与输入活动以及突触后神经元活动变化率的乘积成正比。
148 00:14:36,381 --> 00:14:38,705 说话人 SPEAKER_01：这将近似于反向传播。
149 00:14:38,725 --> 00:14:40,787 说话人 SPEAKER_01：我们现在已经运行了这个程序，实际上它确实可以工作。
150 00:14:43,210 --> 00:14:55,083 说话人 SPEAKER_01：所以 Tim Lillicrap 使用这种循环反向 STDP 模拟学习了一堆自编码器，然后运行了这个两遍程序。
151 00:14:55,503 --> 00:15:01,889 说话人 SPEAKER_01：我们现在实际上可以在某种程度上更合理的神经网络中，对反向传播问题进行近似。
152 00:15:05,192 --> 00:15:06,634 说话人 SPEAKER_01：好的，请翻到下一页。
153 00:15:13,659 --> 00:15:23,669 讲者 SPEAKER_01：我只是想强调一下，尖峰时间依赖性可塑性就是让权重的变化与突触后活动的变化率成正比。
154 00:15:24,325 --> 00:15:38,065 通常介绍尖峰下降依赖性可塑性的方式是，在突触前尖峰发生时，那里是零，你查看是否在突触前尖峰之前或之后立即发生了突触后尖峰。
155 00:15:39,287 --> 00:15:41,669 如果它发生在之后，你就增加权重。
156 00:15:41,690 --> 00:15:43,413 如果它发生在之前，你就减少权重。
157 00:15:43,932 --> 00:15:52,105 讲者 SPEAKER_01：令人惊讶的是，在突触前电位发生时，仅仅几毫秒的时间段决定了权重是减少还是增加。
158 00:15:53,114 --> 00:16:00,025 讲者 SPEAKER_01：大多数人认为这是你在寻找传入突触和传出突触之间的因果关系。
159 00:16:01,107 --> 00:16:10,565 讲者 SPEAKER_01：但你也可以这样解释，STDP 只是对突触后电位的时间序列应用的一个导数滤波器。
160 00:16:11,846 --> 00:16:15,714 讲者 SPEAKER_01：权重的变化取决于突触后电位时间序列的导数。
161 00:16:16,794 --> 00:16:18,278 说话人 SPEAKER_01：这是一个非常不同的解释
162 00:16:18,645 --> 00:16:23,312 说话人 SPEAKER_01：并且导数解释取决于考虑许多带有随机噪声的例子。
163 00:16:23,613 --> 00:16:27,238 说话人 SPEAKER_01：在许多例子上平均，它表现得就像一个导数滤波器。
164 00:16:28,860 --> 00:16:29,981 说话人 SPEAKER_01：好的，请翻到下一页。
所以这里有一个问题，那就是如果自上而下的权重是自下而上权重的转置，就像在 RBM 中那样，那么一切都会正常工作。
166 00:16:47,384 --> 00:16:52,350 发言人 SPEAKER_01：当你进行这两次自上而下的传递时，你就可以模拟反向传播。
当您进行自上而下的传递时，您实际上能够重建下面的内容是很重要的。
所以这意味着在你的系统顶部，你不仅仅想要一个类别标签输出。
169 00:17:04,710 --> 00:17:15,365 讲者 SPEAKER_01：您需要一个包含类别标签的表示，您将通过目标来纠正这部分，但同时也需要包含足够多的其他信息，以便您可以重建输入。
170 00:17:15,835 --> 00:17:25,125 讲者 SPEAKER_01：因此，您的顶层表示需要是标签加上重建输入所需的其他信息，比如如果是一个对象，比如对象的姿态。
171 00:17:25,886 --> 00:17:39,243 讲者 SPEAKER_01：好的，有了这个前提，您可以这样做反向传播，但这需要对称的权重，或者看起来需要对称的权重，这样反向传播就会使用正向传播中使用的权重的转置。
172 00:17:40,685 --> 00:17:44,289 讲者 SPEAKER_01：请翻到下一页。
173 00:17:44,776 --> 00:17:48,339 说话者 SPEAKER_01：当我 2007 年思考这些问题时，这似乎是一个不可逾越的障碍。
174 00:17:49,481 --> 00:17:52,885 说话者 SPEAKER_01：哦，我要跳过这个。
175 00:17:53,385 --> 00:17:54,047 说话者 SPEAKER_01：请翻到下一页。
176 00:17:55,749 --> 00:17:58,271 说话者 SPEAKER_01：这只是为了展示如何将目标转换为变化率。
177 00:18:01,875 --> 00:18:04,159 说话者 SPEAKER_01：这表明它确实进行了反向传播。
178 00:18:04,179 --> 00:18:11,728 说话者 SPEAKER_01：所以如果你想想，看看那个神经元 I，黑色箭头是你第一次正向传播时使用的。
179 00:18:12,214 --> 00:18:15,196 说话者 SPEAKER_01：然后绿色箭头是你开始自上而下使用的。
180 00:18:15,238 --> 00:18:32,951 说话者 SPEAKER_01：如果你有正在工作的自编码器，那么当你将黑色输入 i 替换为绿色输入 i，绿色输入 i 将是 yk 乘以 wki 加上 yj 乘以 wji。
181 00:18:32,991 --> 00:18:42,119 说话人 SPEAKER_01：如果 yk 和 yj 在变化，那么绿色输入的变化率将取决于 yj 和 yk 的变化率。
182 00:18:43,653 --> 00:18:47,036 说话人 SPEAKER_01：现在你可以问，输出 i 的变化率会是什么？
183 00:18:47,997 --> 00:18:57,826 说话人 SPEAKER_01：输出 i 的变化率将是其输入的变化率乘以输出对 i 的导数。这正是反向传播规则。
184 00:18:57,846 --> 00:18:58,607 说话人 SPEAKER_01：这就是有趣的地方。
如果您使用自动编码器进行重建，并且您正在重建的东西在变化，那么重建输出变化的速度正是您需要的，以便进行反向传播
186 00:19:14,095 --> 00:19:17,337 扬声器 SPEAKER_01：以输出变化率形式表示的误差导数。
187 00:19:18,798 --> 00:19:19,799 发言人 SPEAKER_01：好的，请看下一张幻灯片。
188 00:19:23,544 --> 00:19:24,444 发言人 SPEAKER_01：现在我快要结束了。
189 00:19:27,647 --> 00:19:29,209 说话人 SPEAKER_01：问题是关于对称权重。
190 00:19:29,249 --> 00:19:31,592 说话人 SPEAKER_01：这是我无法解决的问题。
191 00:19:33,113 --> 00:19:35,035 说话人 SPEAKER_01：如果权重不对称，它会如何工作？
192 00:19:35,776 --> 00:19:38,458 说话人 SPEAKER_01：表面上看，如果权重不对称，这根本行不通。
193 00:19:38,478 --> 00:19:39,919 说话人 SPEAKER_01：你得到的导数是错误的。
194 00:19:39,939 --> 00:19:41,260 说话人 SPEAKER_01：我试过了，但是没有用。
195 00:19:41,840 --> 00:19:43,663 说话人 SPEAKER_01：我谈了一些关于功能对称性的内容。
196 00:19:44,048 --> 00:19:45,569 说话人 SPEAKER_01：但我无法让它工作。
197 00:19:47,673 --> 00:19:54,984 说话人 SPEAKER_01：然后一年或两年前，蒂姆·利利克拉普和他的牛津同事注意到一个惊人的现象。
198 00:19:55,586 --> 00:19:56,287 说话人 SPEAKER_01：请翻到下一页。
199 00:20:02,236 --> 00:20:07,324 说话人 SPEAKER_01：他们注意到的是，自上而下的权重不必与自下而上的权重相同。
200 00:20:08,306 --> 00:20:11,810 说话人 SPEAKER_01：如果你运行反向传播，并且不使用前馈的转置
201 00:20:12,314 --> 00:20:14,778 说话人 SPEAKER_01：你只是使用了固定的随机自上而下的权重。
202 00:20:17,481 --> 00:20:19,124 说话人 SPEAKER_01：最初，什么也没有发生。
203 00:20:19,885 --> 00:20:25,855 说话人 SPEAKER_01：显然，最初反向传播的内容根本不编码错误。
204 00:20:25,875 --> 00:20:27,336 说话人 SPEAKER_01：但随后发生了一件非常有趣的事情。
205 00:20:27,978 --> 00:20:35,328 说话人 SPEAKER_01：前向权重会适应，以便反向权重能够携带误差。
206 00:20:35,348 --> 00:20:40,396 说话人 SPEAKER_01：在机器学习中，想法有两种行为。
207 00:20:40,613 --> 00:20:43,215 说话人 SPEAKER_01：有些想法想要工作，有些想法则不想工作。
208 00:20:44,417 --> 00:20:46,381 说话人 SPEAKER_01：这是一个想要工作的想法。
209 00:20:46,560 --> 00:20:49,003 说话者 SPEAKER_01：也就是说，你得到了这些随机的反向权重。
210 00:20:49,505 --> 00:20:54,171 说话者 SPEAKER_01：然后通过魔法，正向权重适应了，使得反向权重现在大致是正确的。
211 00:20:55,011 --> 00:20:57,055 说话者 SPEAKER_01：它们比转置更接近伪逆。
212 00:20:58,696 --> 00:21:02,461 说话者 SPEAKER_01：但在这种系统中运行反向传播，需要一点时间才能起飞。
213 00:21:02,501 --> 00:21:03,523 说话人 SPEAKER_01: 然后它运行得非常好。
214 00:21:04,084 --> 00:21:08,410 说话人 SPEAKER_01: 当我说它运行得非常好时，你可以运行一个像这样的大型网络。
215 00:21:08,778 --> 00:21:10,160 说话人 SPEAKER_01: 网络相当大是很重要的。
216 00:21:10,240 --> 00:21:12,344 说话人 SPEAKER_01: 窄瓶颈，这并不适用。
217 00:21:13,445 --> 00:21:19,736 说话人 SPEAKER_01：但是使用大型、宽泛的网络，其性能与标准反向传播非常相似。
218 00:21:19,756 --> 00:21:25,305 说话人 SPEAKER_01：而且它只需要大约是标准反向传播的两倍时间，有时甚至不到两倍，有时几乎和标准反向传播一样快。
219 00:21:25,625 --> 00:21:27,969 说话人 SPEAKER_01：所以这并不是严格意义上的反向传播，但它确实有效。
220 00:21:28,450 --> 00:21:30,012 说话人 SPEAKER_01：它确实能够很好地进行监督学习。
221 00:21:30,753 --> 00:21:31,996 说话人 SPEAKER_01：这样解决了最后一个问题。
222 00:21:33,137 --> 00:21:34,599 说话人 SPEAKER_01：请翻到下一页。
223 00:21:42,291 --> 00:21:55,109 说话人 SPEAKER_01：所以，我为那些做机器学习的人举了一个类比，即在变分推断中，我们制定如何进行推断的方法，或者制定对将要使用的分布的限制。
224 00:21:56,692 --> 00:22:08,127 说话人 SPEAKER_01：看起来这样做可能非常糟糕，因为真实的分布，真实的潜在变量或权重上的后验分布，可能与我们所假设的简单分布大相径庭。
225 00:22:09,089 --> 00:22:12,213 说话者 SPEAKER_01：变分推理的魔力在于生成模型将适应以使我们的推理方式更加正确。
226 00:22:12,463 --> 00:22:15,488 说话者 SPEAKER_01：然后，将适应以使我们的推理方式更加正确。
227 00:22:16,690 --> 00:22:24,941 说话者 SPEAKER_01：因此，在反馈对齐中，也就是 Tim Lillacrap 所说的使用随机反馈权重，发生的情况是相同的。
228 00:22:25,040 --> 00:22:29,046 说话者 SPEAKER_01：正向权重将适应以使这些反向权重发挥作用。
229 00:22:30,387 --> 00:22:32,530 说话人 SPEAKER_01：当然，反向权重不必固定。
230 00:22:32,830 --> 00:22:35,535 说话人 SPEAKER_01：它们可以是这些循环自动编码器中的权重之一。
231 00:22:35,996 --> 00:22:36,916 说话人 SPEAKER_01：因此它们也可以学习。
232 00:22:37,297 --> 00:22:39,180 说话人 SPEAKER_01：而正向权重将不断尝试追踪它们。
233 00:22:39,346 --> 00:22:42,410 说话人 SPEAKER_01：这样他们就能带回来一些非常像导数的东东。
234 00:22:44,232 --> 00:22:46,115 说话人 SPEAKER_01：我们还没有完全理解这一点。
235 00:22:46,856 --> 00:22:50,342 说话人 SPEAKER_01：我有许多不同的解释来说明为什么它有效。
236 00:22:50,903 --> 00:22:58,913 说话人 SPEAKER_01：拥有许多解释看似很好，如果你想要整合后验概率，但实际上这是你不真正理解正在发生什么的标志。
237 00:23:01,237 --> 00:23:06,104 说话人 SPEAKER_01：好吧，它确实可以工作，但我们并不真正完全理解为什么反馈和对齐工作得如此之好。
238 00:23:07,987 --> 00:23:08,728 说话人 SPEAKER_01：请翻到下一页。
239 00:23:20,892 --> 00:23:22,294 说话人 SPEAKER_01：所以我只是想总结一下。
240 00:23:22,334 --> 00:23:31,707 说话人 SPEAKER_01：在神经网络中获取监督信号很容易。
241 00:23:31,727 --> 00:23:33,549 说话人 SPEAKER_01：对此有各种各样的可能性。
242 00:23:34,530 --> 00:23:38,537 说话人 SPEAKER_01：它不必由教师注入到皮层的中间。
243 00:23:40,359 --> 00:23:42,442 说话人 SPEAKER_01：神经元发送尖峰的事实并不是问题。
244 00:23:42,903 --> 00:23:44,444 说话人 SPEAKER_01：实际上，这只是一个很好的正则化器。
245 00:23:46,248 --> 00:23:49,211 说话人 SPEAKER_01：如果我们把误差导数表示为时间导数，
246 00:23:49,646 --> 00:23:54,692 说话人 SPEAKER_01：那么我们就可以让一个神经元在同一条轴突上发送误差导数反向和信号正向。
247 00:23:56,255 --> 00:24:00,319 说话人 SPEAKER_01：如果我们这样做，我们应该看到时间依赖性突触可塑性。
248 00:24:00,520 --> 00:24:06,887 说话人 SPEAKER_01：这正是反向传播使用时间导数作为误差导数的标志。
249 00:24:06,907 --> 00:24:16,820 说话者 SPEAKER_01：你可能会认为每个自下而上的连接都需要一个与之对应的、具有相同权重的自上而下的连接，但这个看似问题实际上并非问题，原因是我们并不完全理解。
250 00:24:17,161 --> 00:24:19,384 说话者 SPEAKER_01：但如果没有这个，它也能正常工作。
251 00:24:19,751 --> 00:24:22,576 说话者 SPEAKER_01：正向权重会适应，以便反向权重能够很好地工作。
252 00:24:23,837 --> 00:24:39,038 说话者 SPEAKER_01：所以我对最后一张幻灯片有一个最后的评论，那就是使用误差导数来表示时间导数的这种有趣的表征想法有着巨大的影响。
253 00:24:39,097 --> 00:24:46,468 说话者 SPEAKER_01：这意味着你不能用神经元的时变率来表示神经元所表示的时变率。
254 00:24:47,393 --> 00:24:54,461 说话者 SPEAKER_01：所以如果你有一个表示某物位置的神经元，你不能用该神经元输出的变化率来表示某物的速度。
255 00:24:55,201 --> 00:25:03,853 说话者 SPEAKER_01：这是表示速度的明显方法，但如果你已经决定时变率是误差变率，那么它将不起作用。
256 00:25:03,873 --> 00:25:13,203 说话者 SPEAKER_01：如果你观察正确的脑损伤，你会发现有些人能看到事物的位置，他们能看到位置的变化，但他们看不到它们在移动。
257 00:25:13,243 --> 00:25:16,928 说话人 SPEAKER_01：他们没有速度感。
258 00:25:17,346 --> 00:25:27,517 说话人 SPEAKER_01：时间导数作为编码误差导数的概念解释了几个问题，比如为什么不能使用位置神经元通过其变化率来表示速度。
259 00:25:28,097 --> 00:25:33,143 说话人 SPEAKER_01：它解释了为什么会出现时间依赖性突触可塑性，并解释了如何在脑中实现反向传播。
260 00:25:33,762 --> 00:25:37,606 说话人 SPEAKER_01：现在，在神经科学领域，这还只是刚刚开始。
261 00:25:38,508 --> 00:25:43,874 说话者 SPEAKER_01：但我想要质疑神经科学家所说的，那就是你不可能在大脑中实现反向传播。
262 00:25:44,614 --> 00:25:47,336 说话者 SPEAKER_01：我认为实际上可以，进化肯定已经找到了实现它的方法。
263 00:25:47,974 --> 00:26:04,536 说话者 SPEAKER_01：我说完了。
264 00:26:04,556 --> 00:26:05,898 说话者 SPEAKER_02：我们还有时间回答几个问题。
265 00:26:14,721 --> 00:26:19,469 说话者 SPEAKER_00：那么，你能否对大脑中可能看到的图结构做出任何预测？
266 00:26:19,548 --> 00:26:21,653 说话者 SPEAKER_00：看看一个简单的，比如秀丽隐杆线虫。
267 00:26:21,732 --> 00:26:24,616 说话者 SPEAKER_00：那里有没有一些可以预测并找到的循环？
268 00:26:24,656 --> 00:26:26,840 说话者 SPEAKER_01：好的。
269 00:26:26,941 --> 00:26:35,173 说话人 SPEAKER_01：我一直在大脑，或者说灵长类动物的大脑上思考。
270 00:26:35,193 --> 00:26:35,294 说话人 SPEAKER_01：哎呀。
271 00:26:35,314 --> 00:26:36,715 说话人 SPEAKER_01：我觉得我们俩同时说话了。
272 00:26:39,319 --> 00:26:42,585 说话人 SPEAKER_01：我一直在大脑和人类大脑的感觉通路方面思考。
273 00:26:42,750 --> 00:26:46,836 说话人 SPEAKER_01：我不太相信在许多低等生物中会发生同样的学习过程。
274 00:26:47,616 --> 00:26:59,332 说话人 SPEAKER_01：所以我认为在昆虫中，比如，它的系统可能比我们的大脑要简单得多。
275 00:27:00,433 --> 00:27:05,160 说话人 SPEAKER_01：所以我对 C. elegans 是否能够进行反向传播这个问题保持开放态度。
276 00:27:05,819 --> 00:27:07,122 说话人 SPEAKER_01：我不太相信这一点。
277 00:27:13,395 --> 00:27:14,237 说话人 SPEAKER_02: Jeff，我有一个问题。
278 00:27:15,877 --> 00:27:16,117 说话人 SPEAKER_02: 好的。
279 00:27:16,419 --> 00:27:30,330 说话人 SPEAKER_02: 那么在不需要在反向传播过程中更新所使用的矩阵的情况下，对于分布式训练神经网络来说，这里是否存在着某种计算机会？
280 00:27:31,893 --> 00:27:32,993 说话人 SPEAKER_02: 那就是减少了通信成本，对吧？
281 00:27:34,654 --> 00:27:36,876 说话人 SPEAKER_01：我实际上还没有考虑过这个问题。
282 00:27:38,699 --> 00:27:42,001 说话人 SPEAKER_01：我还没有考虑过这个问题。
283 00:27:42,691 --> 00:27:44,894 说话人 SPEAKER_01：抱歉，线路不是全双工的。
284 00:27:46,757 --> 00:27:49,741 说话人 SPEAKER_01：所以我认为这是一个非常有趣的问题。
285 00:27:49,761 --> 00:28:00,017 发言人 SPEAKER_01：如果您可以使用固定的后向权重，那就太好了，虽然您需要付出一点代价，因为它不如反向传播那么快，但您获得的利益却很多，因为您不必更新后向权重。
286 00:28:00,076 --> 00:28:04,804 发言人 SPEAKER_01：但由于您仍然需要更新前向权重，我认为我没有必要考虑这个问题。
287 00:28:04,824 --> 00:28:08,209 发言人 SPEAKER_01：我看不出有什么办法可以充分利用这一点来实现并行化。
你好，杰夫，你提到你的自编码器必须是无损的，你不仅要编码标签，还要编码足够的信息来重建图像。
289 00:28:22,170 --> 00:28:26,755 说话者 SPEAKER_03：这似乎对大脑来说是一个相当昂贵的条件，能够做到这一点。
290 00:28:27,935 --> 00:28:38,247 说话者 SPEAKER_03：你看到任何方法可以在损失更多的情况下仍然得到反向传播吗？
291 00:28:39,289 --> 00:28:40,872 说话者 SPEAKER_01：是的，我想我已经意识到发生了什么。
292 00:28:40,892 --> 00:28:46,118 说话者 SPEAKER_01：从 John Winn 那里得到的视觉输入和我听到的 John Winn 的声音之间有一个很大的延迟。
293 00:28:47,661 --> 00:28:57,653 说话人 SPEAKER_01: 好的，所以如果我把整个问题都弄明白了，这种方法实际上是非常稳健的。
294 00:28:58,114 --> 00:29:07,086 说话人 SPEAKER_01: 如果你让顶层不足以重建下一层，它仍然表现得相当不错。
295 00:29:07,707 --> 00:29:12,853 说话人 SPEAKER_01: 在顶层有额外的输出表示像姿态这样的东西是有帮助的。
296 00:29:13,993 --> 00:29:17,798 说话人 SPEAKER_01: 如果你只使用顶层的标签，它仍然可以工作，只是效果不是那么好。
297 00:29:20,121 --> 00:29:25,506 说话人 SPEAKER_01：我认为，实际上，它可以很好地容忍有损自动编码。
298 00:29:26,386 --> 00:29:29,911 说话人 SPEAKER_01：我们还没有进行实验来确定它能够容忍到什么程度。
299 00:29:30,352 --> 00:29:36,178 说话人 SPEAKER_01：我完全同意你的观点，你不想在系统的所有层级都编码所有这种噪声。
300 00:29:36,817 --> 00:29:39,358 说话人 SPEAKER_01：在低层到整个系统。
301 00:29:39,378 --> 00:30:05,625 说话者 SPEAKER_04：但我认为它对这一点相当稳健。
302 00:30:06,262 --> 00:30:11,509 说话者 SPEAKER_04：正如您所知，来自更高层的连接非常分散。
303 00:30:11,628 --> 00:30:14,613 说话者 SPEAKER_04：它们位于皮层的上层，并且分布得非常广泛。
304 00:30:15,093 --> 00:30:20,880 说话者 SPEAKER_04：您对从顶层传下来的信号需要什么样的精度？
305 00:30:22,982 --> 00:30:23,383 说话人 SPEAKER_01: 好的。
306 00:30:23,563 --> 00:30:36,117 说话人 SPEAKER_01: 关于这一点，我可以稍微谈谈，那就是如果你采用全连接网络，这当然并不现实，并且建立正向和反向连接，
307 00:30:36,419 --> 00:30:37,961 说话人 SPEAKER_01: 完全不重叠。
308 00:30:38,961 --> 00:30:51,917 说话人 SPEAKER_01: 比如说，你放入四分之一的可能正向连接，然后放入四分之一的可能反向连接，但确保正向连接和反向连接之间没有重叠。
309 00:30:52,659 --> 00:30:56,163 发言人 SPEAKER_01：所以你永远不会有一对向前连接和向后连接的神经元。
310 00:30:56,864 --> 00:30:58,125 说话人 SPEAKER_01：该算法运行得非常好。
311 00:30:58,826 --> 00:31:01,869 说话人 SPEAKER_01：这根本不影响它。
312 00:31:02,170 --> 00:31:06,375 说话人 SPEAKER_01：这取决于每一层的表示中存在冗余。
313 00:31:07,356 --> 00:31:16,969 说话人 SPEAKER_01：因此，你可以通过查看其他神经元来获取所需的信息，而不仅仅是查看上一层你认为需要查看的神经元，以了解如何实现。
314 00:31:17,891 --> 00:31:25,500 发言人 SPEAKER_01：所以，只要表示中存在冗余，只要你有相当宽的层，它就可以在前馈和反馈连接之间没有重叠的情况下工作。
315 00:31:26,642 --> 00:31:28,785 发言人 SPEAKER_01：这不是一个正确的答案，因为你想知道
316 00:31:29,035 --> 00:31:32,298 说话者 SPEAKER_01：那么局部会发生什么，以及是否值得使事物更加分散。
我认为使事物更加扩散的原因可能与其监督信号的来源有关，因为我真的相信这个想法，即在每个层面上发生的监督信号，是您为神经元提取的底部活动与从更广泛上下文中的顶部预测之间的协议。
318 00:31:53,743 --> 00:31:55,566 说话者 SPEAKER_01：所以我认为这就是你想要反馈连接更加分散的原因，因为它们需要承载更广泛的内容，所以你会与自下而上的进行比较。
319 00:31:56,103 --> 00:32:01,628 说话者 SPEAKER_01：我想这就是我现在要说的全部了。
320 00:32:03,912 --> 00:32:05,614 说话者 SPEAKER_01：我想这就是我现在要说的全部了。
321 00:32:05,673 --> 00:32:10,720 说话者 SPEAKER_01：需要更多的实验来确定理想的分散程度。
322 00:32:11,140 --> 00:32:23,493 Speaker SPEAKER_01: 此外，我们还没有进行实验来证明这种在系统所有层次上对自下而上和自上而下预测之间的一致性进行监督的监督学习方法是否可行。
323 00:32:26,713 --> 00:32:27,221 Speaker SPEAKER_02: 谢谢，杰夫。
324 00:32:27,526 --> 00:32:29,130 Speaker SPEAKER_02: 让我们再次感谢杰夫。
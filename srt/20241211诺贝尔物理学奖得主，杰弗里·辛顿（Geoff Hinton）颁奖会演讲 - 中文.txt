1 00:00:16,702 --> 00:00:42,222 演讲者 SPEAKER_01：现在，我非常荣幸和自豪地介绍我们的第二位演讲者，杰弗里·辛顿。
2 00:00:43,500 --> 00:00:47,767 演讲者 SPEAKER_01：杰弗里·辛顿于 1947 年出生于英国伦敦。
3 00:00:48,207 --> 00:00:53,956 演讲者 SPEAKER_01：他于 1970 年从剑桥大学获得实验心理学学士学位。
4 00:00:53,975 --> 00:01:03,450 演讲者 SPEAKER_01：1978 年，他从爱丁堡大学获得人工智能博士学位。
5 00:01:04,477 --> 00:01:16,269 讲者 SPEAKER_01：在博士后研究后，他在匹兹堡的卡内基梅隆大学担任了五年的计算机科学系教职。
6 00:01:17,331 --> 00:01:29,504 讲者 SPEAKER_01：1987 年，他被任命为加拿大多伦多大学的计算机科学教授，现在他是荣誉退休教授。
7 00:01:30,548 --> 00:01:40,266 讲者 SPEAKER_01：在 2013 年至 2023 年期间，他在学术研究和谷歌大脑之间分配时间。
8 00:01:41,628 --> 00:01:52,066 讲者 SPEAKER_01：请和我一起欢迎杰弗里·辛顿教授上台，向我们讲述今年诺贝尔物理学奖的获奖成果。
9 00:02:09,715 --> 00:02:11,938 说话人 说话人_00：今天我要做一件非常愚蠢的事情。
10 00:02:12,580 --> 00:02:18,248 说话人 说话人_00：我将尝试向普通听众描述一个复杂的技术概念，而不使用任何方程式。
11 00:02:20,413 --> 00:02:26,582 说话人 说话人_00：首先，我必须解释霍普菲尔德网络，我将解释具有1或0状态的二进制神经元的版本。
12 00:02:27,525 --> 00:02:35,318 说话人 说话人_00：所以，在右边，你会看到一个小的霍普菲尔德网络，最重要的是神经元之间具有对称加权连接。
13 00:02:38,234 --> 00:02:43,260 说话人 SPEAKER_00：整个网络的全球状态称为配置，这样我们看起来有点像物理学。
14 00:02:44,323 --> 00:02:48,027 说话人 SPEAKER_00：每个配置都有一个“好”。
15 00:02:48,829 --> 00:02:56,520 说话人 SPEAKER_00：配置的“好”就是所有同时开启的神经元对的权重之和。
16 00:02:56,941 --> 00:03:02,068 说话人 SPEAKER_00：所以那些红色框中的权重，把它们加起来，希望是四。
17 00:03:02,628 --> 00:03:06,435 说话人 SPEAKER_00：这就是该网络配置的优点。
18 00:03:06,414 --> 00:03:08,758 说话人 SPEAKER_00：能量就是负的优点。
19 00:03:11,844 --> 00:03:14,210 说话人 SPEAKER_00：所以这些网络会收敛到能量最小值。
20 00:03:14,469 --> 00:03:25,150 说话人 SPEAKER_00：Hopfield 网络的核心在于每个神经元可以局部计算它需要做什么来降低能量，其中能量代表坏处。
21 00:03:26,344 --> 00:03:33,968 说话人 SPEAKER_00：如果来自其他活跃神经元的总加权输入为正，神经元应该打开。
22 00:03:34,951 --> 00:03:39,104 说话人 SPEAKER_00：如果来自其他活跃神经元的总加权输入为负，它应该关闭。
23 00:03:40,638 --> 00:03:48,348 说话人 SPEAKER_00：如果每个神经元都持续使用该规则，并且我们随机选择它们并持续应用该规则，我们最终会达到能量最小值。
24 00:03:48,368 --> 00:03:52,394 说话人 SPEAKER_00：所以右边的配置实际上是一个能量最小值。
25 00:03:52,413 --> 00:03:54,217 说话人 SPEAKER_00: 它具有-4 的能量。
26 00:03:54,236 --> 00:03:58,483 说话人 SPEAKER_00: 如果你取任何一个神经元，那些处于开启状态的神经元想要保持开启。
27 00:03:58,522 --> 00:03:59,925 说话人 SPEAKER_00: 它们获得总的正输入。
28 00:04:00,224 --> 00:04:01,486 说话人 SPEAKER_00: 那些处于关闭状态的神经元想要保持关闭。
29 00:04:01,546 --> 00:04:02,707 说话人 SPEAKER_00: 他们获得的是总负输入。
30 00:04:03,368 --> 00:04:05,312 说话人 SPEAKER_00: 但这并非唯一的能量最小值。
31 00:04:05,629 --> 00:04:14,682 说话人 SPEAKER_00: 跳转网络可以有许多能量最小值，它最终到达的位置取决于起始点，还取决于你做出的随机决策序列。
32 00:04:17,245 --> 00:04:19,850 说话人 SPEAKER_00: 对不起，是关于更新哪个神经元的随机决策序列。
33 00:04:22,273 --> 00:04:23,814 说话人 说话人_00：这样就是一个更好的能量最小值。
34 00:04:24,216 --> 00:04:30,704 说话人 说话人_00：现在我们已经打开了右侧的单元三角形，它的良好度为3加3减1等于5。
35 00:04:31,346 --> 00:04:34,589 说话人 说话人_00：因此，能量减5，这是一个更好的最小值。
36 00:04:36,257 --> 00:04:43,252 说话人 说话人_00：现在，霍普菲尔德提出，使用这类网络的一个好方法是将能量最小值对应到记忆中。
37 00:04:44,033 --> 00:04:49,744 说话者 SPEAKER_00: 然后使用这个二元决策规则来决定是否打开或关闭，这可以清理不完整的记忆。
38 00:04:50,444 --> 00:04:56,476 说话者 SPEAKER_00: 所以，你从一个部分记忆开始，然后不断应用这个决策规则，它就会将其清理干净。
39 00:04:56,456 --> 00:05:02,803 说话者 SPEAKER_00: 因此，当能量最小值代表记忆时，这是一种内容可寻址内存的方式。
40 00:05:03,764 --> 00:05:09,930 说话者 SPEAKER_00: 你可以通过打开一些项目并使用这个规则来访问内存中的项目，然后它会将其填充完整。
41 00:05:12,653 --> 00:05:19,560 说话人 说话人_00：特里·桑福斯基和我，特里曾是 Upfield 的学生，我们提出了这些网状物的一种不同用途。
42 00:05:20,322 --> 00:05:25,586 说话人 说话人_00：与其用它们来存储记忆，我们不如用它们来构建对感官输入的解释。
43 00:05:26,663 --> 00:05:28,326 说话人 说话人_00：所以想法是，你有一个网络。
44 00:05:28,805 --> 00:05:31,230 说话人 说话人_00：它既有可见神经元也有隐藏神经元。
45 00:05:31,250 --> 00:05:35,637 讲者 SPEAKER_00：可见的神经元是您展示感官输入的地方，可能是一个二进制图像。
46 00:05:36,778 --> 00:05:40,264 讲者 SPEAKER_00：隐藏的神经元是它构建对那个感官输入的解释的地方。
47 00:05:42,848 --> 00:05:47,514 讲者 SPEAKER_00：网络配置的能量代表了解释的糟糕程度。
48 00:05:47,875 --> 00:05:49,819 讲者 SPEAKER_00：因此，我们希望解释的能量低。
49 00:05:51,149 --> 00:05:53,351 说话人 说话人_00：我将给你一个具体的例子。
50 00:05:53,892 --> 00:05:56,016 说话人 说话人_00：考虑上面那个模糊的线条图。
51 00:05:56,697 --> 00:05:58,360 说话人 说话人_00：人们有两种看待它的方式。
52 00:05:59,461 --> 00:06:02,507 说话人 说话人_00：有一种解释，这通常是人们首先看到的。
53 00:06:02,826 --> 00:06:04,108 说话人 SPEAKER_00: 另一种解释。
54 00:06:04,589 --> 00:06:10,800 说话人 SPEAKER_00: 当你把它看作一个凸形物体时，这显然是对同一二维线图的另一种三维解释。
55 00:06:10,819 --> 00:06:15,586 说话人 SPEAKER_00: 那么我们能否让这些网络之一产生对同一线图的两种不同解释？
56 00:06:16,815 --> 00:06:21,382 说话人 SPEAKER_00: 好吧，我们首先需要思考图像中的一条线能告诉你关于三维边缘的什么信息。
57 00:06:22,805 --> 00:06:25,168 说话人 SPEAKER_00: 那条绿色线是图像平面。
58 00:06:25,228 --> 00:06:30,776 说话人 SPEAKER_00: 想象你透过窗户看，你在窗户上画出世界场景中的边缘。
59 00:06:31,357 --> 00:06:34,641 说话人 SPEAKER_00: 那条小黑线是图像中的线。
60 00:06:35,903 --> 00:06:40,129 说话人 SPEAKER_00: 而两条红线是从你的眼睛通过那条线的两端看到的视线。
61 00:06:41,072 --> 00:06:44,697 说话人 SPEAKER_00: 如果你问，嗯，世界上哪条边能造成这种情况？
62 00:06:44,963 --> 00:06:46,845 说话人 SPEAKER_00: 嗯，有很多边可能造成了这种情况。
63 00:06:47,346 --> 00:06:49,569 说话人 SPEAKER_00: 但有一条边可能造成了那条二维线。
64 00:06:50,069 --> 00:06:52,853 说话人 SPEAKER_00: 但是还有另一条，还有另一条，还有另一条。
65 00:06:53,452 --> 00:06:56,177 说话人 SPEAKER_00: 所有这些边缘都会在图像中形成相同的线条。
66 00:06:56,797 --> 00:07:02,144 说话人 SPEAKER_00: 因此，视觉问题是要从图像中的单条线条反向推断出这些边缘中哪一个是真实存在的。
67 00:07:04,947 --> 00:07:09,252 说话人 SPEAKER_00: 如果物体是不透明的，你一次只能看到其中之一，因为它们都会相互遮挡。
68 00:07:09,853 --> 00:07:13,737 说话人 SPEAKER_00: 所以你知道图像中的那条线必须描绘这些边缘之一，但你不知道是哪一个。
69 00:07:14,932 --> 00:07:22,009 说话人 SPEAKER_00: 我们可以构建一个网络，我们从将线条转换为线神经元激活开始。
70 00:07:22,951 --> 00:07:24,353 说话人 SPEAKER_00: 假设我们已经有这个了。
71 00:07:24,613 --> 00:07:31,709 说话人 SPEAKER_00: 我们有大量的神经元来表示图像中的线条，我们只打开其中的一小部分来表示这张特定图像中的线条。
72 00:07:32,264 --> 00:07:35,507 说话人 SPEAKER_00: 现在每一条线都可能代表多个不同的 3D 边缘。
73 00:07:36,589 --> 00:07:43,098 说话人 说话人_00：所以我们把这条线神经元连接到一大堆 3D 边缘神经元，并通过兴奋性连接。
74 00:07:43,158 --> 00:07:44,060 说话人 说话人_00：这些是绿色的。
75 00:07:44,521 --> 00:07:46,463 说话人 说话人_00：但我们知道我们一次只能看到一个。
76 00:07:46,884 --> 00:07:49,548 说话人 说话人_00：所以我们让这些边缘神经元相互抑制。
77 00:07:50,249 --> 00:07:53,432 说话人 说话人_00：现在我们已经捕捉到了很多关于感知光学方面的内容。
78 00:07:54,915 --> 00:07:57,158 说话人 说话人_00：我们为所有的线神经元都这样做。
79 00:07:57,947 --> 00:08:01,615 说话人 说话人_00：现在的问题是，我们应该激活哪些边缘神经元？
80 00:08:02,315 --> 00:08:04,158 说话人 说话人_00：为了这个，我们需要更多信息。
81 00:08:04,740 --> 00:08:07,103 说话人 SPEAKER_00: 我们在解读图像时使用某些原则。
82 00:08:07,706 --> 00:08:14,737 说话人 SPEAKER_00: 如果你在一幅图像中看到两条线，你会假设如果它们在图像中相交，它们在深度上也会相交。
83 00:08:14,757 --> 00:08:18,504 说话人 SPEAKER_00: 也就是说，它们在两条线在图像中相交的深度上。
84 00:08:18,485 --> 00:08:21,269 说话人 SPEAKER_00: 因此，我们可以为这种情况添加额外的连接。
85 00:08:22,012 --> 00:08:32,650 说话人 SPEAKER_00: 我们可以在每个深度相交的 3D 边缘神经元对之间建立连接。
86 00:08:33,893 --> 00:08:37,178 说话人 SPEAKER_00: 如果它们以直角相交，我们可以建立更强的连接。
87 00:08:37,299 --> 00:08:40,745 说话人 SPEAKER_00: 我们非常喜欢看到以直角相交的图像。
88 00:08:40,725 --> 00:08:54,005 说话人 SPEAKER_00: 因此，我们建立了许多这样的连接，现在我们希望，如果我们设置连接强度正确，我们就得到了一个网络，它可以稳定到两个替代状态之一，对应于那两个关于尼克尔立方体的替代解释。
89 00:08:55,248 --> 00:08:58,312 说话人 SPEAKER_00: 这引发了两个主要问题。
90 00:08:58,293 --> 00:09:09,184 说话人 SPEAKER_00: 第一个问题，如果我们想使用隐藏神经元来对可见神经元状态所表示的图像进行解释，那就是搜索问题。
91 00:09:10,245 --> 00:09:12,649 说话人 SPEAKER_00: 我们如何避免陷入局部最优解？
92 00:09:12,668 --> 00:09:17,214 说话人 SPEAKER_00: 我们可能会陷入一个相当糟糕的解释，而无法跳出来达到更好的解释。
93 00:09:18,294 --> 00:09:20,297 说话人 说话人_00：第二个问题是学习。
94 00:09:21,379 --> 00:09:27,085 说话人 说话人_00：我似乎暗示我会手动添加所有这些连接，但我们希望神经网络能够添加所有这些连接。
95 00:09:29,782 --> 00:09:36,923 说话人 说话人_00：所以，我们通过使神经元产生噪声来大致解决搜索问题。
96 00:09:37,222 --> 00:09:56,010 说话人 说话人_00：所以，如果你有确定性的神经元，比如在标准霍普菲尔德网络中，如果系统陷入一个能量最小值，比如 A，那么那里的球就是整个系统的配置，它不能从 A 到 B，因为神经元的决策规则只允许能量下降。
97 00:09:56,672 --> 00:09:58,674 说话人 SPEAKER_00: 右边的图是决策规则。
98 00:09:58,695 --> 00:10:00,618 说话人 SPEAKER_00: 如果输入为正，则开启。
99 00:10:00,918 --> 00:10:02,421 说话人 SPEAKER_00: 如果输入为负，则关闭。
100 00:10:04,460 --> 00:10:08,386 说话人 SPEAKER_00: 我们希望能够从 A 到 B，但这意味着我们必须在能量上爬坡。
101 00:10:08,988 --> 00:10:12,835 说话人 SPEAKER_00: 解决方案是拥有噪声神经元，随机二进制神经元。
102 00:10:13,515 --> 00:10:15,198 说话人 SPEAKER_00: 它们仍然只有二进制状态。
103 00:10:15,239 --> 00:10:18,926 说话人 SPEAKER_00: 它们的状态要么是 1 要么是 0，但它们是概率性的。
104 00:10:19,586 --> 00:10:23,153 说话人 SPEAKER_00: 如果它们接收到大的正输入，它们几乎总是会被激活。
105 00:10:23,193 --> 00:10:25,658 说话人 SPEAKER_00：在大的负输入下，它们几乎总是关闭。
106 00:10:25,638 --> 00:10:30,144 说话人 SPEAKER_00：但如果输入柔和，如果它在零附近，那么它们会表现出概率行为。
107 00:10:30,384 --> 00:10:32,808 说话人 SPEAKER_00：如果是正输入，它们通常打开，但偶尔会关闭。
108 00:10:33,250 --> 00:10:36,595 说话人 SPEAKER_00：如果是小的负输入，它们通常关闭，但偶尔会打开。
109 00:10:37,576 --> 00:10:39,219 说话人 SPEAKER_00: 但它们没有真实值。
110 00:10:39,239 --> 00:10:43,004 说话人 SPEAKER_00: 它们总是二进制，但只是做出这些概率性决策。
111 00:10:44,825 --> 00:10:55,399 说话人 SPEAKER_00: 因此，如果我们想使用这些隐藏神经元来解释二进制图像，我们就在可见单元上固定二进制图像。
112 00:10:55,759 --> 00:10:57,763 说话人 SPEAKER_00: 这指定了输入是什么。
113 00:10:58,543 --> 00:11:00,246 说话人 SPEAKER_00: 然后我们随机选择一个隐藏神经元。
114 00:11:00,988 --> 00:11:05,173 说话人 SPEAKER_00: 我们查看它从其他活动隐藏神经元接收到的总输入。
115 00:11:05,573 --> 00:11:07,176 说话人 SPEAKER_00: 我们将它们都从随机状态开始。
116 00:11:08,038 --> 00:11:10,942 说话人 SPEAKER_00: 如果它接收到的总输入为正，我们可能将其打开。
117 00:11:10,981 --> 00:11:14,105 说话人 SPEAKER_00: 如果只是小的正输入，我们可能就将其关闭。
118 00:11:14,086 --> 00:11:21,975 说话人 SPEAKER_00: 因此我们继续实施这个规则：如果是大的正输入就开启，如果是大的负输入就关闭，如果是软输入，就做出概率性决策。
119 00:11:22,655 --> 00:11:29,504 说话人 SPEAKER_00: 如果我们不断循环，挑选隐藏神经元并这样做，系统最终将接近所谓的热平衡状态。
120 00:11:30,044 --> 00:11:33,028 说话人 SPEAKER_00: 这是一个对非物理学家来说难以理解的概念，我稍后会解释。
121 00:11:34,049 --> 00:11:41,258 说话人 SPEAKER_00：一旦达到热平衡，隐藏神经元的态就是对输入的解释。
122 00:11:42,519 --> 00:11:53,548 说话人 SPEAKER_00：所以在那条线图的例子中，每个线单元可能有一个激活的隐藏神经元，你将得到一个解释，这将是 Necker 立方体的两种解释之一。
123 00:11:53,568 --> 00:11:57,980 说话人 SPEAKER_00：我们希望低能量解释是对数据的良好解释。
124 00:12:00,221 --> 00:12:20,761 说话人 SPEAKER_00：所以对于这条线图，如果我们能在 2D 线神经元和 3D 边缘神经元之间学习正确的权重，并在 3D 边缘神经元之间学习正确的权重，那么我们希望网络的低能量态将对应于良好的解释，即看到 3D 矩形物体。
125 00:12:23,206 --> 00:12:24,408 说话人 SPEAKER_00：所以热平衡。
126 00:12:25,570 --> 00:12:29,496 说话人 SPEAKER_00：这并非你最初所预期的，即系统已稳定到一种稳定状态。
127 00:12:30,919 --> 00:12:33,445 说话人 SPEAKER_00：稳定下来的并非系统的状态。
128 00:12:33,946 --> 00:12:37,491 说话人 SPEAKER_00：稳定下来的是一个更加抽象的东西，很难去思考。
129 00:12:37,532 --> 00:12:41,940 说话人 SPEAKER_00: 这是系统配置的概率分布。
130 00:12:41,919 --> 00:12:45,144 说话人 SPEAKER_00: 这对普通人来说很难想象。
131 00:12:45,163 --> 00:12:48,928 说话人 SPEAKER_00: 它会稳定到一个特定的分布，即玻尔兹曼分布。
132 00:12:49,490 --> 00:12:59,682 说话人 SPEAKER_00: 在玻尔兹曼分布中，一旦达到热平衡，系统处于特定配置的概率完全由该配置的能量决定。
133 00:13:00,123 --> 00:13:03,668 说话人 说话人_00：在低能量配置中找到它的概率更大。
134 00:13:04,268 --> 00:13:09,615 说话人 说话人_00：所以热平衡，好的状态，低能量状态，比坏状态更可能。
135 00:13:10,844 --> 00:13:17,619 说话人 说话人_00：现在来考虑热平衡，物理学家有一个技巧，它可以让普通人理解这个概念。
136 00:13:18,600 --> 00:13:19,001 说话人 说话人_00：希望如此。
137 00:13:20,905 --> 00:13:26,138 说话人 SPEAKER_00: 你只需想象一个非常庞大的集合，数以亿计的它们，都是相同的网络。
138 00:13:26,158 --> 00:13:28,182 说话人 SPEAKER_00: 你有这些数以亿计的霍普菲尔德网络。
139 00:13:28,562 --> 00:13:30,927 说话人 SPEAKER_00: 它们的权重完全相同。
140 00:13:30,908 --> 00:13:43,990 说话人 SPEAKER_00: 因此，它们本质上是一个系统，但你从不同的随机状态开始它们，它们都会做出自己独立的随机决策，并且会有一定比例的系统具有每种配置。
141 00:13:44,530 --> 00:13:47,735 说话者 说话者_00：首先，这个分数将取决于你如何开始它们。
142 00:13:47,716 --> 00:13:50,922 说话者 说话者_00：也许你会随机开始，这样所有配置的可能性都是相等的。
143 00:13:51,482 --> 00:13:57,494 说话者 说话者_00：在这个庞大的集合中，你将得到每个可能配置的系统数量相等。
144 00:13:57,975 --> 00:14:03,804 说话者 说话者_00：然后你开始运行这个更新神经元的算法，这样它们倾向于降低能量，但偶尔也会上升。
145 00:14:04,726 --> 00:14:11,479 说话人 说话人_00：逐渐会发生的是，任何一种配置下的系统比例将趋于稳定。
146 00:14:11,458 --> 00:14:14,443 说话人 说话人_00：所以任何一个系统将在不同的配置之间跳跃。
147 00:14:15,066 --> 00:14:19,312 说话人 说话人_00：但特定配置下所有系统的比例将是稳定的。
148 00:14:20,115 --> 00:14:25,043 说话人 说话人_00：因此，一个系统可能离开某个配置，但其他系统将进入该配置。
149 00:14:25,323 --> 00:14:26,505 说话人 说话人_00: 这被称为详细平衡。
150 00:14:26,966 --> 00:14:28,870 说话人 说话人_00: 系统的分数将保持稳定。
151 00:14:30,133 --> 00:14:33,619 说话人 说话人_00: 物理部分就到这里。
152 00:14:33,970 --> 00:14:36,572 说话人 说话人_00: 现在让我们想象一下生成一张图片。
153 00:14:36,813 --> 00:14:39,076 说话人 SPEAKER_00：不是解释图像，而是在生成图像。
154 00:14:39,797 --> 00:14:45,563 说话人 SPEAKER_00：要生成图像，你首先为所有神经元、隐藏神经元和可见神经元选择随机状态。
155 00:14:46,966 --> 00:14:51,412 说话人 SPEAKER_00：然后你选择一个隐藏或可见神经元，并使用常规随机规则更新其状态。
156 00:14:51,432 --> 00:14:53,614 说话人 SPEAKER_00：如果它接收大量正面输入，可能将其激活。
157 00:14:53,975 --> 00:14:55,657 说话人 SPEAKER_00：有很多负面输入，可能需要关闭。
158 00:14:56,017 --> 00:14:58,379 说话人 SPEAKER_00：如果它很柔和，它的行为有点随机。
159 00:14:59,120 --> 00:15:00,702 说话人 SPEAKER_00：然后你继续这样做。
160 00:15:02,320 --> 00:15:16,534 说话人 SPEAKER_00：如果你反复这样做，直到系统接近热平衡，然后你查看可见单元的状态，这就是网络从它相信的分布中生成的图像。
161 00:15:16,815 --> 00:15:22,581 说话人 SPEAKER_00：玻尔兹曼分布中，低能量配置比高能量配置更有可能。
162 00:15:23,162 --> 00:15:31,551 说话人 SPEAKER_00：但是它相信存在许多可能的替代图像，你可以从中选择一个，通过运行此过程来相信其中之一。
163 00:15:32,797 --> 00:15:33,238 说话人 SPEAKER_00：好的。
164 00:15:35,581 --> 00:15:38,183 说话人 SPEAKER_00：那么，在玻尔兹曼机中学习的目的是什么？
165 00:15:38,725 --> 00:15:47,917 说话人 SPEAKER_00：在玻尔兹曼机中学习的目的是让它生成图像时，就像做梦一样，只是随机想象事物。
166 00:15:49,178 --> 00:15:57,690 说话人 SPEAKER_00：当它生成图像时，这些图像看起来就像它在感知真实图像时感知到的图像。
167 00:15:57,669 --> 00:16:06,384 说话人 SPEAKER_00：如果我们能实现这一点，那么隐藏神经元的态实际上是一种解释真实图像的好方法。
168 00:16:06,945 --> 00:16:09,249 说话人 SPEAKER_00：它们将捕捉图像的潜在原因。
169 00:16:09,590 --> 00:16:10,431 说话人 SPEAKER_00: 至少这是希望。
170 00:16:11,011 --> 00:16:12,894 说话人 SPEAKER_00: 另一种说法是
171 00:16:12,875 --> 00:16:23,758 说话人 SPEAKER_00: 学习网络中的权重等同于找出如何使用那些隐藏神经元，以便网络生成看起来像真实图像的图像。
172 00:16:24,600 --> 00:16:26,443 说话人 SPEAKER_00: 这似乎是一个非常困难的问题。
173 00:16:26,745 --> 00:16:29,230 说话人 SPEAKER_00：大家都认为那会非常复杂。
174 00:16:29,210 --> 00:16:35,177 说话人 SPEAKER_00：结果，我和 Terry 有一个非常乐观的方法。
175 00:16:37,219 --> 00:16:58,203 说话人 SPEAKER_00：问题是，你能从一个神经网络开始，一个 Hopfield 网络，这种随机的 Hopfield 网络，有很多隐藏神经元，它们之间有随机的权重，并且它们与可见神经元之间也有随机的权重，所以这是一个很大的随机神经网络，然后你只给它展示很多图像，我们希望得到一个看似荒谬的结果，那就是
176 00:16:59,197 --> 00:17:06,604 说话人 SPEAKER_00：在感知了很多真实图像后，它会创建隐藏单元之间的所有连接，以及隐藏单元与可见单元之间的所有连接。
177 00:17:07,144 --> 00:17:15,252 说话人 SPEAKER_00：它会正确地权衡这些连接，以便能够对图像进行合理的解释，例如在直角处相交的 3D 边缘。
178 00:17:16,713 --> 00:17:18,175 说话人 SPEAKER_00：这听起来非常乐观。
179 00:17:19,017 --> 00:17:22,961 说话人 SPEAKER_00：你可能认为执行这种学习的算法会非常复杂。
180 00:17:23,701 --> 00:17:28,326 说话人 SPEAKER_00：关于玻尔兹曼机令人惊讶的一点是，有一个非常简单的学习算法可以做到这一点。
181 00:17:29,909 --> 00:17:34,096 说话人 说话人_00：这是我在1983年由塔拉索夫斯基和我发现的。
182 00:17:34,395 --> 00:17:38,222 说话人 说话人_00：学习算法是这样的。
183 00:17:38,923 --> 00:17:39,846 说话人 说话人_00：它有两个阶段。
184 00:17:40,606 --> 00:17:41,488 说话人 说话人_00：有一个唤醒阶段。
185 00:17:42,189 --> 00:17:44,913 说话人 SPEAKER_00: 那是网络被呈现图像的阶段。
186 00:17:45,255 --> 00:17:47,377 说话人 SPEAKER_00: 你将图像固定在可见单元上。
187 00:17:47,357 --> 00:17:51,064 说话人 SPEAKER_00: 然后让隐藏单元四处晃动，最终达到热平衡。
188 00:17:52,164 --> 00:18:05,526 说话人 SPEAKER_00: 当隐藏单元与可见神经元达到热平衡后，对于每对连接的神经元，无论是两个隐藏神经元还是可见神经元和隐藏神经元，如果它们都处于激活状态，就在它们之间的权重上增加一小部分。
189 00:18:06,046 --> 00:18:08,150 说话人 SPEAKER_00：这是一个相当简单的学习规则。
190 00:18:08,470 --> 00:18:11,694 说话人 SPEAKER_00：这是一个相信唐纳德·赫布的人会喜欢的学习规则。
191 00:18:14,105 --> 00:18:15,166 说话人 SPEAKER_00：然后是睡眠阶段。
192 00:18:15,768 --> 00:18:20,996 说话人 SPEAKER_00：显然，如果你只运行清醒阶段，权重只会越来越大。
193 00:18:21,576 --> 00:18:24,862 说话人 SPEAKER_00: 很快，它们都会变得积极，所有神经元都将一直处于活跃状态。
194 00:18:25,262 --> 00:18:26,023 说话人 SPEAKER_00: 这没什么用。
195 00:18:26,945 --> 00:18:28,567 说话人 SPEAKER_00: 你需要将其与睡眠阶段结合。
196 00:18:28,969 --> 00:18:32,674 说话人 SPEAKER_00: 在睡眠阶段，你可以把网络想象成在做梦。
197 00:18:33,035 --> 00:18:38,964 说话者 SPEAKER_00：您通过更新所有神经元的态，包括隐藏神经元和可见神经元，来达到热平衡。
198 00:18:38,944 --> 00:18:47,839 说话者 SPEAKER_00：一旦完成这些并达到热平衡，对于每对连接的神经元，如果它们都处于开启状态，您将从中减去一小部分权重。
199 00:18:48,480 --> 00:18:50,203 说话者 SPEAKER_00：这是一个相当简单的学习算法。
200 00:18:51,025 --> 00:18:52,969 说话者 SPEAKER_00：它确实做到了正确的事情，这真的很令人惊讶。
201 00:18:54,852 --> 00:18:56,835 说话人 SPEAKER_00：那么，平均来说，
202 00:18:57,607 --> 00:19:07,329 说话人 SPEAKER_00：这个学习算法会改变权重，以便增加网络在“做梦”时生成的图像看起来像它在“感知”时看到的图像的概率。
203 00:19:08,311 --> 00:19:11,398 说话人 SPEAKER_00：并且不是面向一般观众，所以你不必阅读接下来的两行。
204 00:19:12,602 --> 00:19:15,448 说话人 SPEAKER_00：对于统计学家和机器学习人员，
该算法所做的是，在期望上，这意味着它非常嘈杂，经常做错事情，但在平均期望上，它遵循对数似然梯度的方向。
206 00：19：26,791 --> 00：19：35,631 演讲者 SPEAKER_00：也就是说，在预期中，它所做的是让网络更有可能在做梦时生成它醒来时看到的各种图像。
或者换句话说，权重发生变化，使得网络找到的可信、低能量图像与它在清醒时看到的图像相似。
208 00：19：46,444 --> 00：19：53,923 演讲者 SPEAKER_00：当然，学习算法中发生的事情也在跟进，你正在降低能量
209 00:19:53,903 --> 00:19:58,790 说话人 SPEAKER_00: 整个网络配置中，当它看到真实数据时，它会得出这样的结论。
210 00:19:59,152 --> 00:20:02,376 说话人 SPEAKER_00: 当它处于睡眠状态时，你会提高这些配置的能量。
211 00:20:02,917 --> 00:20:08,587 说话人 SPEAKER_00: 所以你试图让它相信你在清醒时看到的东西，不相信你在睡眠时梦到的东西。
212 00:20:11,807 --> 00:20:23,622 说话人 SPEAKER_00: 好的，那么如果你问达到热平衡的过程实现了什么，它实现了惊人的事情，那就是网络中每一个权重需要知道所有其他权重的一切。
213 00:20:23,982 --> 00:20:27,251 说话人 SPEAKER_00: 要知道如何改变一个权重，你需要了解所有其他权重的一些信息。
214 00:20:27,231 --> 00:20:28,093 说话人 SPEAKER_00: 它们都是相互作用的。
215 00:20:28,614 --> 00:20:33,122 说话人 SPEAKER_00: 但你需要知道的一切都体现在两个相关性的差异中。
216 00:20:33,803 --> 00:20:44,801 说话人 SPEAKER_00: 它体现在网络观察数据时两个神经元同时激活的频率，与网络不观察数据、处于流式传输状态时它们同时激活的频率之间的差异中。
217 00:20:44,781 --> 00:20:52,151 说话人 SPEAKER_00：不知何故，在这两种情况下测量的这些相关性为所有其他权重提供了所需的一切信息。
218 00:20:52,832 --> 00:21:03,443 说话人 SPEAKER_00：这之所以令人惊讶，是因为在像反向传播这样的算法中，所有神经网络现在实际上都使用这种算法，你需要进行反向传递来传递关于其他权重的信息。
219 00:21:04,005 --> 00:21:08,410 说话人 SPEAKER_00：而反向传递的行为与正向传递大不相同。
220 00:21:08,390 --> 00:21:13,178 说话人 SPEAKER_00：在正向传递中，你正在将神经元的活动传递给后面的神经元层。
221 00:21:13,759 --> 00:21:16,663 讲述者 SPEAKER_00：在反向传播过程中，你传达的是敏感性。
222 00:21:16,703 --> 00:21:19,989 讲述者 SPEAKER_00：你传达的是完全不同的一种数量。
223 00:21:20,028 --> 00:21:24,717 讲述者 SPEAKER_00：这使得反向传播作为大脑工作理论变得相当不可能。
224 00:21:25,438 --> 00:21:31,848 讲述者 SPEAKER_00：因此，当 Terry 提出了这个关于玻尔兹曼机的学习过程理论时，
225 00:21:31,828 --> 00:21:39,340 说话人 SPEAKER_00：我们完全确信这就是大脑的工作方式，我们决定我们要去获得生理学或医学诺贝尔奖。
226 00:21:40,201 --> 00:21:48,234 说话人 SPEAKER_00：我们从未想过，如果不是这样大脑工作，我们还能获得物理学诺贝尔奖。
227 00:21:50,205 --> 00:21:52,229 说话人 SPEAKER_00：好的，只有一个问题。
228 00:21:53,009 --> 00:21:59,599 说话人 SPEAKER_00：问题是，对于非常大的网络和大型权重，达到热平衡是一个非常缓慢的过程。
229 00：22：00,020 --> 00：22：01,502 演讲者 SPEAKER_00：如果重量非常小，你可以快速完成。
230 00:22:01,522 --> 00:22:04,607 说话人 说话人_00：但是当权重很大时，学习了一些东西之后，它非常慢。
231 00:22:05,348 --> 00:22:08,252 说话人 说话人_00：所以实际上，玻尔兹曼机是一个美好的浪漫想法。
232 00:22:08,292 --> 00:22:12,720 说话人 说话人_00：这是一个美丽简单的学习算法。
233 00:22:12,700 --> 00:22:14,305 说话人 说话人_00：它正在做一件非常复杂的事情。
234 00:22:14,325 --> 00:22:20,324 说话人 SPEAKER_00：它正在构建整个由隐藏单元组成的网络，这些单元通过一个非常简单的算法来解释数据。
235 00:22:20,644 --> 00:22:22,852 说话人 SPEAKER_00：唯一的问题是它们运行得太慢了。
236 00:22:23,634 --> 00:22:26,202 说话人 SPEAKER_00：所以这就是关于玻尔兹曼机的全部内容了。
237 00:22:27,414 --> 00:22:29,258 说话人 SPEAKER_00：讲座应该在那里结束。
238 00:22:29,498 --> 00:22:41,454 说话人 SPEAKER_00: 但 17 年后，我意识到如果你对玻尔兹曼机限制很多，只有不相互连接的隐藏单元，那么你可以得到一个学习算法更快。
239 00:22:42,516 --> 00:22:47,262 说话人 SPEAKER_00: 所以如果隐藏神经元之间没有连接，那么唤醒阶段就变得非常简单。
240 00:22:48,121 --> 00:22:51,587 说话人 SPEAKER_00: 你所做的是将一个输入夹在可见单元上，以表示一个图像。
241 00:22:52,388 --> 00:22:56,056 说话人 SPEAKER_00: 然后现在可以并行更新所有隐藏神经元。
242 00:22:57,417 --> 00:22:59,201 说话人 SPEAKER_00: 现在已经达到热平衡状态。
243 00:22:59,221 --> 00:23:00,403 说话人 SPEAKER_00: 你只需更新它们一次。
244 00:23:00,845 --> 00:23:06,234 说话人 SPEAKER_00: 它们只是查看可见输入，并根据接收到的输入随机选择它们两个状态中的一个。
245 00:23:06,255 --> 00:23:08,578 说话人 SPEAKER_00: 现在您已经一步达到热平衡状态。
246 00:23:09,221 --> 00:23:10,482 说话人 SPEAKER_00: 这太好了。
247 00:23:10,462 --> 00:23:14,386 说话人 SPEAKER_00: 对于隐藏神经元，你仍然存在问题。
248 00:23:14,468 --> 00:23:26,701 说话人 SPEAKER_00: 在睡眠阶段，你必须将网络置于某种随机状态，更新隐藏神经元，更新可见神经元，更新隐藏神经元，更新可见神经元，你需要持续很长时间才能达到热平衡。
249 00:23:27,061 --> 00:23:28,604 说话人 SPEAKER_00: 因此，该算法仍然无望。
250 00:23:29,203 --> 00:23:30,625 说话人 说话人_00：但结果发现有一个捷径。
251 00:23:31,287 --> 00:23:38,394 说话人 说话人_00：这个捷径并不完全正确，有点尴尬，但在实际应用中效果还不错。
252 00:23:38,375 --> 00:23:39,876 说话人 说话人_00：所以这个捷径是这样的。
253 00:23:40,637 --> 00:23:42,500 说话人 说话人_00：你把数据放在可见单元上。
254 00:23:42,700 --> 00:23:43,361 说话人 SPEAKER_00: 那是一张图片。
255 00:23:44,382 --> 00:23:46,763 说话人 SPEAKER_00: 然后你并行更新所有隐藏神经元。
256 00:23:47,644 --> 00:23:50,107 说话人 SPEAKER_00: 现在它们已经与数据达到热平衡。
257 00:23:51,028 --> 00:23:55,994 说话人 SPEAKER_00: 你现在更新所有可见单元，这就是我们所说的重建。
258 00:23:56,675 --> 00:23:58,637 说话人 SPEAKER_00: 它会像数据一样，但又不完全相同。
259 00:23:59,499 --> 00:24:01,580 说话人 SPEAKER_00: 然后你再次更新所有隐藏单元。
260 00:24:02,371 --> 00:24:03,132 说话人 SPEAKER_00: 然后停止。
261 00:24:03,152 --> 00:24:03,551 说话人 SPEAKER_00: 就这样。
262 00:24:03,592 --> 00:24:04,051 说话人 SPEAKER_00: 你做完了。
263 00:24:04,071 --> 00:24:18,724 说话人 SPEAKER_00: 你学习的方式是测量当展示数据时，神经元 i 和 j（可见神经元 i 和隐藏神经元 j）同时激活的频率，并且它们与数据达到平衡。
264 00:24:19,325 --> 00:24:25,510 说话人 SPEAKER_00: 你还测量当展示重建数据时，它们同时激活的频率，并且它们与重建数据达到平衡。
265 00:24:26,090 --> 00:24:28,692 说话人 SPEAKER_00: 这种差异就是你的学习算法。
266 00:24:29,034 --> 00:24:31,955 说话人 SPEAKER_00: 你只需按比例调整那个差异的权重。
267 00:24:32,392 --> 00:24:34,457 说话人 SPEAKER_00: 实际上效果相当不错。
268 00:24:34,836 --> 00:24:36,260 说话人 SPEAKER_00: 它要快得多。
269 00:24:36,441 --> 00:24:38,826 说话人 SPEAKER_00: 足够快，使得玻尔兹曼机最终变得实用。
270 00:24:39,708 --> 00:24:44,038 说话人 SPEAKER_00: 那么，好吧。
271 00:24:44,676 --> 00:24:59,192 说话人 SPEAKER_00: 所以 Netflix 实际上使用了受限玻尔兹曼机结合其他方法来决定为你推荐哪些新电影，基于所有类似你的其他用户的偏好。
272 00:24:59,232 --> 00:25:00,834 说话人 SPEAKER_00: 而且它们确实有效。
273 00:25:00,854 --> 00:25:01,815 说话人 SPEAKER_00: 他们赢得了比赛。
274 00:25:02,036 --> 00:25:08,962 说话人 SPEAKER_00：这种结合了玻尔兹曼机和这些其他方法，赢得了 Netflix 关于如何预测用户喜好的竞赛。
275 00:25:10,866 --> 00:25:24,141 说话人 SPEAKER_00：但是当然，仅仅有未相互连接的隐藏神经元，是无法构建特征检测器的层的，而这些层是进行图像中物体识别或语音中词语识别所需要的。
276 00:25:24,540 --> 00:25:29,826 说话人 SPEAKER_00：而且这看起来是只有一个层且层内神经元之间没有连接的强烈限制。
277 00:25:30,788 --> 00:25:32,088 说话人 SPEAKER_00：但实际上，你可以绕过这一点。
278 00:25:33,530 --> 00:25:37,515 说话人 SPEAKER_00: 所以，你可以堆叠这些受限的机器人和机器。
279 00:25:38,170 --> 00:25:43,837 说话人 SPEAKER_00: 你所做的是，你将你的数据展示给受限玻尔兹曼机，RBM，数据。
280 00:25:44,539 --> 00:25:55,134 说话人 SPEAKER_00: 它只有一个隐藏层，并使用这种对比散度算法上下波动，学习一些权重，以便隐藏单元能够捕捉数据中的结构。
281 00:25:56,017 --> 00:26:00,903 说话人 SPEAKER_00: 隐藏单元变成了特征检测器，能够捕捉数据中的常见相关性。
282 00:26:02,048 --> 00:26:09,997 说话人 SPEAKER_00：然后，你将那些隐藏的活动模式，隐藏单元中的二进制活动模式，视为数据。
283 00:26:10,757 --> 00:26:24,132 说话人 SPEAKER_00：所以你只需将它们复制到另一个 RBM 中，它们就是另一个 RBM 的数据，第二个 RBM 查看这些已经捕捉到数据相关性的特征，并捕捉这些特征之间的相关性。
284 00:26:24,873 --> 00:26:28,696 说话人 SPEAKER_00：然后你继续这样做，这样你就能捕捉到越来越复杂的相关性。
285 00:26:29,268 --> 00:26:31,392 说话人 SPEAKER_00：因此你可以学习第二组权重 W2。
286 00:26:32,032 --> 00:26:34,238 说话人 SPEAKER_00: 你可以随心所欲地做很多次。
287 00:26:34,258 --> 00:26:35,400 说话人 SPEAKER_00: 让我们学习第三组权重。
288 00:26:36,742 --> 00:26:45,640 说话人 SPEAKER_00: 所以现在我们有一系列独立的玻尔兹曼机，每个都在寻找前一个玻尔兹曼机隐藏单元中的结构。
289 00:26:45,771 --> 00:26:53,522 说话人 SPEAKER_00: 然后，你可以堆叠这些玻尔兹曼机，并将它们视为一个前馈网络。
290 00:26:53,603 --> 00:26:55,464 说话人 SPEAKER_00：所以忽略连接是对称的这个事实。
291 00:26:56,006 --> 00:27:04,978 说话人 SPEAKER_00：现在就只用一个方向的连接，因为你在第一层隐藏层中已经提取出了能够捕捉原始数据相关性的特征。
292 00:27:04,958 --> 00:27:12,267 说话人 SPEAKER_00：然后在第二层隐藏层中，你提取出了能够捕捉第一层隐藏层中提取的特征的相关性的特征，依此类推。
293 00:27:12,907 --> 00:27:16,632 说话人 SPEAKER_00：所以你得到了越来越多的高级特征，这些特征是相关性之间的相关性。
294 00:27:18,034 --> 00:27:25,923 说话人 SPEAKER_00: 将它们堆叠好后，你就可以添加一个最终的隐藏层，就像这样，然后你可以进行监督学习。
295 00:27:25,983 --> 00:27:30,789 说话人 SPEAKER_00: 意思是，现在你可以开始教它事物的名称，比如猫和狗。
296 00:27:31,211 --> 00:27:33,192 说话人 SPEAKER_00: 这些是类别标签。
297 00:27:33,173 --> 00:27:35,696 说话人 SPEAKER_00: 你需要学习这些类别标签的权重。
298 00:27:36,317 --> 00:27:41,467 说话人 SPEAKER_00: 但你从初始化的这个网络开始，这个网络是通过学习堆叠的玻尔兹曼机初始化的。
299 00:27:42,087 --> 00:27:43,410 说话人 SPEAKER_00: 发生了两个美妙的事情。
300 00:27:43,830 --> 00:27:50,521 说话人 SPEAKER_00: 第一个美妙的事情是，如果你这样初始化，网络的学习速度会比用随机权重初始化快得多。
301 00:27:50,961 --> 00:27:56,852 说话人 SPEAKER_00: 因为它已经学习到了很多有意义的特征，用于对数据进行结构建模。
302 00:27:56,832 --> 00:28:01,298 说话人 说话人_00：它还没有学会事物的名称，但它已经学会了数据中的结构。
303 00:28:01,558 --> 00:28:03,982 说话人 说话人_00：然后学习事物的名称相对较快。
304 00:28:05,144 --> 00:28:12,134 说话人 说话人_00：就像对小孩子们一样，他们不需要被告诉那是牛，2000次之后才知道那是牛。
305 00:28:12,915 --> 00:28:17,963 说话人 说话人_00：他们会自己理解牛的概念，然后他们的妈妈说那是牛，他们就明白了。
306 00:28:19,226 --> 00:28:21,169 说话人 SPEAKER_00: 嗯，可能有两倍。
307 00:28:23,781 --> 00:28:29,288 说话人 SPEAKER_00: 因此，例如识别物体和图像的学习速度会更快。
308 00:28:30,848 --> 00:28:33,092 说话人 SPEAKER_00: 这也使得网络泛化能力更好。
309 00:28:33,432 --> 00:28:37,957 说话人 SPEAKER_00: 因为它们在大多数学习过程中没有使用标签，所以他们现在不需要很多标签了。
310 00:28:38,517 --> 00:28:44,784 说话人 SPEAKER_00：他们并没有从标签中提取所有信息，而是从数据的相关性中提取信息。
311 00:28:45,825 --> 00:28:50,430 说话人 SPEAKER_00：这使得它们在需要较少标签的情况下具有更好的泛化能力。
312 00:28:51,742 --> 00:28:52,903 说话人 SPEAKER_00：所以这一切都非常棒。
313 00:28:53,925 --> 00:29:13,109 说话人 SPEAKER_00：大约在 2006 年到 2011 年之间，人们使用，尤其是在我的实验室、Yoshua Bengio 的实验室和 Jan 的实验室，人们使用堆叠的 RBMs 来预训练前馈神经网络，然后应用反向传播。
314 00:29:13,089 --> 00:29:26,210 说话人 SPEAKER_00：在 2009 年，我的实验室里的两位学生，乔治·达尔和阿卜杜拉曼·穆罕默德，证明了这项技术在识别语音中的音节片段方面略优于现有的最佳技术。
315 00:29:27,551 --> 00:29:31,538 说话人 SPEAKER_00：这之后改变了语音识别领域。
316 00:29:31,518 --> 00:29:45,490 说话人 SPEAKER_00：我的研究生们去了各个领先的语音团队，2012 年，基于这一技术的产品，在限制机器的同时，进入了谷歌的生产线，并且提高了语音识别能力。
317 00:29:45,549 --> 00:29:50,099 说话人 SPEAKER_00：突然之间，Android 上的语音识别变得好多了。
不幸的是，对于玻尔兹曼机来说，一旦我们证明了这些深度神经网络在用堆叠的玻尔兹曼机进行预训练后确实非常有效，人们就找到了其他初始化权重的方法，他们不再使用堆叠的玻尔兹曼机。
319 00:30:06,549 --> 00:30:09,433 说话者 说话者_00：但如果你是化学家，你就知道酶是有用的东西。
尽管 RBMs 不再使用，但它们使我们从认为深度神经网络永远不会工作转变为看到，如果以这种方式初始化，深度神经网络实际上可以相当容易地被制作出来工作。
321 00：30：24,112 --> 00：30：26,474 演讲者 SPEAKER_00： 一旦你完成了过渡，你就不再需要酶了。
322 00:30:26,875 --> 00:30:29,959 说话者 SPEAKER_00：所以把它们想象成历史上的酶。
323 00:30:31,221 --> 00:30:43,900 说话者 SPEAKER_00：不过，我认为在睡眠中应用“遗忘”，以获得更符合生物学的算法并避免反向传播到过去，这个想法仍然有很大的潜力。
324 00:30:44,461 --> 00:30:54,836 说话者 SPEAKER_00：我仍然乐观，当我们最终理解大脑是如何学习的时候，它将会发现使用睡眠来进行“遗忘”是其中的关键。
325 00:30:56,318 --> 00:30:57,380 说话者 SPEAKER_00：所以我还是很乐观的。
326 00:30:58,461 --> 00:30:59,423 说话人 SPEAKER_00: 我认为我完成了。
327 00:31:30,654 --> 00:31:31,796 说话人 SPEAKER_01: 非常好。
328 00:31:31,817 --> 00:31:32,835 说话人 SPEAKER_01: 非常感谢。
329 00:31:53,538 --> 00:32:02,484 说话人 SPEAKER_01: 所以现在请和我一起欢迎两位获奖者上台，共同接受我们最热烈的掌声。
330 00:32:30,307 --> 00:32:33,299 说话人 SPEAKER_01：也许你想向前迈步，站在前面。
331 00:32:33,961 --> 00:32:35,809 说话人 SPEAKER_01：也许你想在这里站到前面。
332 00:32:35,829 --> 00:32:36,673 说话人 SPEAKER_01：是的。
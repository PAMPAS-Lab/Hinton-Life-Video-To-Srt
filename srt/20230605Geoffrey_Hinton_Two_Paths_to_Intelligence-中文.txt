1 00:00:04,232 --> 00:00:05,354 主持人 SPEAKER_12：感谢介绍。
2 00:00:06,375 --> 00:00:09,217 主持人 SPEAKER_12：标题与广告的不同，但内容是一样的。
3 00:00:09,977 --> 00:00:11,099 主持人 SPEAKER_12：有两个问题。
4 00:00:11,980 --> 00:00:13,420 主持人 SPEAKER_12：如果我戴上眼镜，我能看清楚。
5 00:00:15,682 --> 00:00:20,027 说话人 SPEAKER_12：第一个问题是，人工智能很快就会比我们聪明吗？
6 00:00:21,207 --> 00:00:29,894 说话人 SPEAKER_12：长期以来，大概有 50 年，我一直在努力研究人工神经网络，试图了解真实大脑可能的工作方式。
7 00:00:30,335 --> 00:00:33,277 说话人 SPEAKER_12：我一直认为真实的大脑更好，
8 00:00:33,578 --> 00:00:35,119 说话人 SPEAKER_12：而人工神经网络则更差。
9 00:00:35,159 --> 00:00:36,862 说话人 SPEAKER_12：如果让它们更像大脑，它们会工作得更好。
10 00:00:37,402 --> 00:00:39,164 说话人 SPEAKER_12：几个月前，我突然改变了主意。
11 00:00:40,185 --> 00:00:45,892 说话人 SPEAKER_12：这就是我决定离开谷歌并谈论风险的原因。
12 00:00:46,593 --> 00:00:50,517 说话人 SPEAKER_12：显然，如果有了超级智能的东西，人们还能保持控制吗？
13 00:00:51,177 --> 00:01:00,008 说话人 SPEAKER_12：因为自从拜登赢得选举以来，就没有例子表明一个不那么智能的东西控制了一个更智能的东西。
14 00:01:02,265 --> 00:01:04,287 说话人 SPEAKER_12：我将对此进行一些推测。
15 00:01:05,469 --> 00:01:11,096 说话人 SPEAKER_12：但谈话的主要内容将只是关于我认为这些事物比我们更智能的原因。
16 00:01:12,158 --> 00:01:13,138 说话人 SPEAKER_12：所以第一个问题。
17 00:01:14,180 --> 00:01:15,701 说话人 SPEAKER_12：我对风险了解不多。
18 00:01:15,721 --> 00:01:18,185 说话人 SPEAKER_12：我今天和很多风险专家交谈过，他们知道的比我多得多。
19 00:01:19,787 --> 00:01:20,548 说话人 SPEAKER_12：所以我不打算谈论那个。
20 00:01:22,310 --> 00:01:28,897 说话人 SPEAKER_12：所以，在传统计算中，你使用的是为执行指令而设计的计算机。
21 00:01:29,418 --> 00:01:34,742 说话人 SPEAKER_12：计算机的基本特性是可以在不同的硬件上运行相同的程序。
22 00:01:36,024 --> 00:01:41,028 说话人 SPEAKER_12：因此，程序中的知识在某种意义上是永恒的，即如果一块硬件损坏，知识不会消失。
23 00:01:41,688 --> 00:01:42,989 说话人 SPEAKER_12：你可以在另一块硬件上运行它。
24 00:01:45,212 --> 00:01:51,158 说话人 SPEAKER_12：要做到这一点，需要以非常高的功率运行晶体管，以确保它们以可靠的数字方式运行。
25 00:01:52,117 --> 00:01:59,424 说话人 SPEAKER_12：你不能利用该硬件的模拟特性，即该硬件的模拟和高度可变的特性。
26 00:01:59,775 --> 00:02:02,278 说话人 SPEAKER_12：因为那样的话，你就无法在不同的硬件上做同样的事情。
27 00:02:05,302 --> 00:02:22,061 说话人 SPEAKER_12：所以我对此产生了兴趣，因为这些大型语言模型的高能耗，巨大的能源消耗，我对是否有更便宜的功耗计算方法产生了兴趣，这些方法我们现在可以使用，而以前不能使用。
28 00:02:23,483 --> 00:02:26,526 说话人 SPEAKER_12：数字计算机被设计成由人类编程。
29 00:02:27,586 --> 00:02:29,729 说话人 SPEAKER_12：这就是为什么他们必须精确地遵循指示。
30 00:02:29,990 --> 00:02:35,919 说话人 SPEAKER_12：但现在我们知道了一种不同的方法，让通用计算机执行特定任务，那就是从例子中学习。
31 00:02:37,341 --> 00:02:44,611 说话人 SPEAKER_12：如果它们从例子中学习，也许我们可以摒弃计算机科学的基本原则，那就是你必须将软件与硬件分离。
32 00:02:45,513 --> 00:02:54,066 说话人 SPEAKER_12：也许我们可以使用利用所有这些模拟特性的模拟计算机，它们是不可复制的。
33 00:02:54,417 --> 00:02:57,780 说话人 SPEAKER_12：你根本无法将知识转移到另一台电脑上。
34 00:02:58,381 --> 00:03:00,643 说话人 SPEAKER_12：但是你可以以非常低的功耗运行，比如 30 瓦。
35 00:03:03,705 --> 00:03:08,229 说话人 SPEAKER_12：所以显然将软件与硬件分离有很大的优势。
36 00:03:08,248 --> 00:03:11,793 说话人 SPEAKER_12：你可以编写一个程序，它将在所有你的 iPhone 上运行。
37 00:03:12,592 --> 00:03:19,299 说话人 SPEAKER_12：这也意味着你可以有一个你一无所知的独立部门，而你却自称是计算机科学。
38 00:03:20,020 --> 00:03:23,242 说话人 SPEAKER_12：如果它们都混在一起，你就不能这么做。
39 00:03:26,193 --> 00:03:32,539 说话人 SPEAKER_12：所以当你不将软件与硬件分开时，你就会得到我所说的“致命计算”。
40 00:03:34,322 --> 00:03:45,133 说话人 SPEAKER_12：也就是说，你将在硬件上学习，而你所学到的将只适用于那块特定的硬件，因为你正在利用它所有的奇怪模拟特性。
41 00:03:47,695 --> 00:03:54,663 说话人 SPEAKER_12：现在，这真是太好了，因为我们可以使用低功耗模拟计算，也许我们还能在 3D 中扩展硬件，
42 00:03:55,133 --> 00:04:00,941 说话人 SPEAKER_12：在这种情况下，我们不知道连接性或单个神经元的精确特性，如果它是类似神经网络的硬件。
43 00:04:02,423 --> 00:04:10,914 说话人 SPEAKER_12：但我们显然有一个大问题，那就是我们必须有一个学习过程，这个学习过程可以在不了解硬件的精确特性的情况下在这个硬件中学习。
44 00:04:11,276 --> 00:04:12,858 说话人 SPEAKER_12：我将简要谈谈您可能如何做到这一点。
45 00:04:13,699 --> 00:04:19,927 讲者 SPEAKER_12：这很可能不是反向传播，因为在反向传播中，你需要通过层或时间进行正向传递。
46 00:04:20,788 --> 00:04:24,735 讲者 SPEAKER_12：你必须知道正向传递的性质，才能
47 00:04:25,389 --> 00:04:27,112 讲者 SPEAKER_12：使用反向传播来获取梯度。
48 00:04:27,533 --> 00:04:32,480 讲者 SPEAKER_12：如果你实际上不知道硬件的性质，你需要一种不同的学习算法。
49 00:04:36,166 --> 00:04:40,072 人类计算的优点在于可以使用权重级并行性。
50 00:04:40,112 --> 00:04:42,255 因此可以实现万亿级别的并行。
51 00:04:43,036 --> 00:04:45,540 这意味着您的计算单元不需要非常快。
52 00:04:46,122 --> 00:04:46,923 这就是大脑所做的事情。
53 00:04:47,865 --> 00:04:49,247 说话人 SPEAKER_12：我们可以使用大量更少的能量。
54 00:04:49,968 --> 00:04:53,153 说话人 SPEAKER_12：我们可以扩展硬件。
55 00:04:53,976 --> 00:04:57,401 说话人 SPEAKER_12：可能最好不要从头开始。
56 00:04:58,062 --> 00:05:04,069 说话人 SPEAKER_12：我猜测我们最终会重新设计生物神经元作为硬件。
57 00:05:05,572 --> 00:05:07,394 说话人 SPEAKER_12：但正如您将看到的，我们可能没有时间做那件事。
58 00:05:10,379 --> 00:05:18,088 说话人 SPEAKER_12：所以我只想给您举一个例子，这个计算显然用模拟方式比数字方式更合理。
59 00:05:18,108 --> 00:05:22,514 说话人 SPEAKER_12：假设您想将神经活动向量乘以权重矩阵。
60 00:05:23,153 --> 00:05:24,595 说话人 SPEAKER_12：这正是我们一直想要做的。
61 00:05:24,634 --> 00:05:26,817 说话人 SPEAKER_12：这就是我们主要进行的计算。
62 00:05:28,180 --> 00:05:38,915 说话人 SPEAKER_12：因此你可以以非常高的功率驱动晶体管，并将活动表示为数字，然后对位进行一系列操作以将它们相乘。
63 00:05:40,416 --> 00:05:46,004 说话人 SPEAKER_12：或者你可以将神经活动仅表示为电压，并将权重表示为电导。
64 00:05:46,862 --> 00:05:49,867 说话人 SPEAKER_12：周围有物理学家，所以我希望我得到了正确的单位。
65 00:05:50,408 --> 00:05:55,375 说话人 SPEAKER_12：如果你将电压乘以电导，我认为你会得到每单位时间的电荷。
66 00:05:56,317 --> 00:05:58,901 说话人 SPEAKER_12：电荷可以自行累加。
67 00:05:59,942 --> 00:06:06,913 说话人 SPEAKER_12：所以现在你可以非常简单地乘以一个向量和一个矩阵，而不必将任何东西数字化表示。
68 00:06:07,836 --> 00:06:09,237 说话人 SPEAKER_12：这非常高效。
69 00:06:09,577 --> 00:06:11,300 发言人 SPEAKER_12：现在有一些芯片可以做到这一点。
70 00:06:11,786 --> 00:06:15,732 发言人 SPEAKER_12：他们的问题是，他们试图在做任何其他事情之前将事物数字化。
71 00:06:16,814 --> 00:06:18,877 发言人 SPEAKER_12：但这种特殊的计算可以有效地完成。
72 00:06:23,502 --> 00:06:27,048 发言人 SPEAKER_02：那么，我马上就回到学习问题上。
73 00:06:27,127 --> 00:06:34,057 人类计算面临的另一个大问题是硬件会损坏，然后它所知道的一切都会丢失。
74 00:06:35,660 --> 00:06:40,646 因此，在人类计算中解决这个问题的方法是教师试图将知识提炼给学生。
75 00:06:41,317 --> 00:06:43,220 而在数字计算中，有一个完全不同的解决方案。
76 00:06:43,240 --> 00:06:51,009 但在人类计算中，解决方案是教师试图将知识提炼给学生。
77 00:06:51,591 --> 00:07:00,081 讲师：你这样做的方式是老师向学生展示正确的输入响应，然后学生尝试模仿这些响应。
78 00:07:00,961 --> 00:07:03,225 讲师：这就是特朗普推文发生的事情。
79 00:07:03,762 --> 00:07:08,149 讲师：左翼人士一直抱怨他说的所有事情都是假的。
80 00:07:08,769 --> 00:07:09,851 讲师：这完全无关紧要。
81 00:07:09,872 --> 00:07:10,673 说话人 SPEAKER_12：那不是重点。
82 00:07:11,274 --> 00:07:18,365 说话人 SPEAKER_12：重点是，你展示一个情况，让人们知道如何应对，然后你的追随者尝试那样去反应。
83 00:07:19,266 --> 00:07:21,630 说话人 SPEAKER_12：结果发现，这种方法比与人讲道理更有效。
84 00:07:24,875 --> 00:07:27,339 说话人 SPEAKER_12：这就是消除偏见的工作原理。
85 00:07:30,475 --> 00:07:35,362 说话人 SPEAKER_12：让我们思考一个将图像分类到 1,024 个类别的智能体。
86 00:07:36,725 --> 00:07:42,875 说话人 SPEAKER_12：如果你只告诉智能体正确的答案，你只提供了 10 比特的信息。
87 00:07:43,877 --> 00:07:49,045 说话人 SPEAKER_12：所以当你说你应该为这个输入提供这个类别时，你只限制了 10 比特的权重。
88 00:07:50,627 --> 00:07:52,630 说话人 SPEAKER_12：但假设你已经有一个训练过的老师。
89 00:07:53,833 --> 00:07:56,516 讲师：老师将为所有输出给出概率。
90 00:07:57,862 --> 00:08:00,887 讲师：因为它们加起来是1，023个实数。
91 00:08:01,569 --> 00:08:11,223 讲师：只要这些概率不是太小，如果你训练一个学生模仿老师，每个训练样本就更有价值。
92 00:08:11,704 --> 00:08:17,372 讲师：因为学生试图不仅仅得到最可能正确的答案。
93 00:08:17,952 --> 00:08:22,098 讲者 SPEAKER_12：学生试图匹配教师对所有其他训练样本的概率。
94 00:08:22,619 --> 00:08:30,415 讲者 SPEAKER_12：实际上，在许多情况下，错误答案的相对概率比正确答案包含的信息更多。
95 00:08:30,435 --> 00:08:38,994 讲者 SPEAKER_12：例如，如果我给你看一辆宝马，系统会说，80%是宝马，10%是奥迪。
96 00:08:39,855 --> 00:08:43,243 讲者 SPEAKER_12：而且有一百万分之一的几率它是一辆垃圾车。
97 00:08:44,253 --> 00:08:45,875 说话人 SPEAKER_12：这里没有德国制造商。
98 00:08:47,017 --> 00:08:48,499 说话人 SPEAKER_12：一百万分之一的几率它是一辆垃圾车。
99 00:08:48,519 --> 00:08:52,946 说话人 SPEAKER_12：但重点是，它也会说有一亿分之一的几率它是一根胡萝卜。
100 00:08:54,828 --> 00:08:58,835 说话人 SPEAKER_12：如果你看所有有一百万分之一几率的物品，它们都是其他车辆。
101 00:08:59,196 --> 00:09:04,764 说话者 SPEAKER_12：它通过所有那些概率低但不是完全为零的事物告诉你很多关于事物类别的信息。
102 00:09:05,404 --> 00:09:07,248 说话者 SPEAKER_12：蔬菜不是。
103 00:09:07,447 --> 00:09:09,572 说话者 SPEAKER_12：因此，它们的概率变得非常小。
104 00:09:09,912 --> 00:09:13,197 说话者 SPEAKER_12：垃圾车和胡萝卜之间的这个比率告诉你很多信息。
105 00:09:14,138 --> 00:09:21,148 讲者 SPEAKER_12：训练学生模仿教师的概率是好的，尤其是如果概率不是太小的话。
106 00:09:21,969 --> 00:09:25,774 讲者 SPEAKER_12：我们可以通过使用高温来使概率不是太小。
107 00:09:26,655 --> 00:09:43,200 讲者 SPEAKER_12：通常情况下，如果你有多个替代输出，你会使用 softmax，你可以说每个答案的概率与该答案的 logit 成正比，其中 logit 是该答案的证据量。
108 00:09:44,176 --> 00:09:47,861 讲者 SPEAKER_12：如果你只按温度来缩放，你会得到更柔和的概率。
109 00:09:49,484 --> 00:09:54,831 说话人 SPEAKER_12：所以，我有一个例子，可能会让你们中的有些人感到惊讶，我使用了 MNIST 数字。
110 00:09:56,695 --> 00:10:02,363 说话人 SPEAKER_12：然后我进行了缩放，我有一个好老师，然后我将概率进行了缩放，使它们变得更柔和。
111 00:10:03,524 --> 00:10:05,148 说话人 SPEAKER_12：如果你看这个，这些都是 2。
112 00:10:06,208 --> 00:10:13,440 说话人 SPEAKER_12：但是如果你看中间那一行，老师认为，是的，这是一个 2。
113 00:10:14,028 --> 00:10:16,711 说话人 SPEAKER_12：但这只是略微像 0。
114 00:10:16,792 --> 00:10:19,535 说话人 SPEAKER_12：它比其他任何 2 都更像 0。
115 00:10:20,376 --> 00:10:27,307 说话人 SPEAKER_12：所以你可以在那个训练示例中看到，通过说这是一个看起来有点像 0 的 2，你正在教系统关于 0 的知识。
116 00:10:27,327 --> 00:10:30,873 说话人 SPEAKER_12：或者如果你看第二行，那是一个看起来有点像 8 的 2。
117 00:10:32,014 --> 00:10:34,298 说话人 SPEAKER_12：这非常有用的额外信息。
118 00:10:35,711 --> 00:10:41,118 说话人 SPEAKER_12：所以我经常谈论蒸馏，是因为这是我们获取知识的方式。
119 00:10:41,538 --> 00:10:49,447 说话人 SPEAKER_12：这也是如何在一个数字系统和具有完全不同架构的不同神经网络之间获取知识。
120 00:10:49,907 --> 00:10:56,313 说话人 SPEAKER_12：所以，通常你想要训练一个很大的模型，然后将这些知识转移到更小的模型中，并使用蒸馏来实现这一点。
121 00:11:01,298 --> 00:11:04,942 说话人 SPEAKER_02：蒸馏的一个优点是当你
122 00:11:05,327 --> 00:11:16,102 说话人 SPEAKER_12：训练学生从教师那里学习，并训练学生得到与教师相同的错误答案概率，实际上是在训练学生以与教师相同的方式进行泛化。
123 00:11:16,764 --> 00:11:20,789 说话人 SPEAKER_12：所以这是非常罕见的情况之一，你实际上是在训练学生如何进行泛化。
124 00:11:21,490 --> 00:11:24,615 说话人 SPEAKER_12：通常情况下，你只是训练它得到正确答案，并希望它在其他情况下也能得到正确答案。
125 00:11:25,035 --> 00:11:26,738 说话人 SPEAKER_12：但实际上，我们正在训练它进行泛化。
126 00:11:30,384 --> 00:11:33,427 说话人 SPEAKER_12：显然，一个标签并不是一个非常丰富的答案。
127 00:11:33,778 --> 00:11:42,119 说话人 SPEAKER_12：所以你最好通过给它一个图像，然后给它一个很长的图像描述，并训练它预测描述中的所有单词来训练学生。
128 00:11:42,740 --> 00:11:45,447 说话人 SPEAKER_12：这样你就可以为训练案例获取更多信息。
129 00:11:47,150 --> 00:11:48,514 说话人 SPEAKER_02：这样可以使蒸馏工作得更好。
130 00:11:53,049 --> 00:11:58,456 说话人 SPEAKER_12：这就是蒸馏的样子，如何在两个模拟系统之间获取信息。
131 00:11:59,116 --> 00:12:00,658 说话人 SPEAKER_12：这就是我们当时所做的事情。
132 00:12:01,139 --> 00:12:03,260 说话人 SPEAKER_12：你这里有一个非常老式的模拟系统。
133 00:12:03,881 --> 00:12:07,426 说话人 SPEAKER_12：你这里有一些旧的，还有很多新的模拟系统。
134 00:12:07,905 --> 00:12:12,171 说话人 SPEAKER_12：我们正在尝试将这个旧模拟系统的信息传输到这些新模拟系统中。
135 00:12:12,971 --> 00:12:17,736 说话人 SPEAKER_12：这次谈话的整个重点就是这个过程并不高效，正如我刚才所展示的。
136 00:12:17,777 --> 00:12:21,740 说话人 SPEAKER_12：这是一个缓慢而痛苦的过程。
137 00:12:22,177 --> 00:12:28,784 说话人 SPEAKER_12：我们无法获取，如果我能直接将我所知的一切输入到你的大脑里，那岂不是很好？
138 00:12:29,326 --> 00:12:30,126 说话人 SPEAKER_12：那会非常好。
139 00:12:31,427 --> 00:12:33,390 说话人 SPEAKER_12：这对大学来说可能没什么好处，但确实如此。
140 00:12:34,851 --> 00:12:39,778 说话人 SPEAKER_12：但这就是生物系统必须做的事情。
141 00:12:41,740 --> 00:12:43,543 说话人 SPEAKER_12：现在让我们谈谈学习算法。
142 00:12:43,743 --> 00:12:52,077 说话人 SPEAKER_12：因为思考这一点让我改变了关于数字系统或生物系统哪个更好的看法。
143 00:12:53,379 --> 00:13:07,404 说话人 SPEAKER_12：所以如果你不想反向传播，因为你实际上不知道硬件在做什么，你可以使用一个具有进化特点的简单算法，即对权重生成一个小随机扰动。
144 00:13:08,160 --> 00:13:10,244 说话人 SPEAKER_12：并且你有一个全局目标函数。
145 00:13:10,745 --> 00:13:12,067 说话人 SPEAKER_12：您通过一些例子来测试。
146 00:13:12,106 --> 00:13:16,933 说话人 SPEAKER_12：然后您看看在扰动之后，您在这个目标函数上的表现是更好还是更差。
147 00:13:17,615 --> 00:13:21,841 说话人 SPEAKER_12：然后您朝着扰动方向迈出一步，这一步与您做得有多好成比例。
148 00:13:22,861 --> 00:13:26,868 说话人 SPEAKER_12：关于这一点，平均来说，它是朝着正确的方向前进的。
149 00:13:27,168 --> 00:13:29,652 说话人 SPEAKER_12：如果你做足够多次，你会朝着反向传播相同的方向前进。
150 00:13:29,932 --> 00:13:31,094 说话人 SPEAKER_12：但它具有非常高的方差。
151 00:13:31,835 --> 00:13:33,577 说话人 SPEAKER_02：因此，它只适用于非常小的系统。
152 00:13:41,894 --> 00:13:47,623 说话人 SPEAKER_12：此外，如果你在示例的小批量中为所有内容使用相同的权重扰动，你会得到更差的变体。
153 00:13:48,043 --> 00:13:52,769 说话人 SPEAKER_12：如果你使用不同的权重扰动，就不能使用矩阵-矩阵乘法，但这只是对内行人而言。
154 00:13:54,773 --> 00:14:01,263 说话人 SPEAKER_12：使用活动扰动会更好，即扰动神经元的输入，并做同样的事情。
155 00:14:01,322 --> 00:14:07,412 说话人 SPEAKER_12：我们扰动所有神经元的输入，这样它们就能得到来自网络其他部分的输入，加上额外的扰动。
156 00:14:08,049 --> 00:14:15,158 说话人 SPEAKER_12：然后你看到由于这种扰动而得到的改进程度，并按照改进程度成比例地向那个方向迈出一步。
157 00:14:15,217 --> 00:14:20,684 说话人 SPEAKER_12：显然，神经元比权重少得多，所以它的方差要小得多。
158 00:14:21,105 --> 00:14:23,288 说话人 SPEAKER_12：这对于学习像 MNIST 这样的问题已经足够好了。
159 00:14:24,168 --> 00:14:27,753 说话人 SPEAKER_12：它比反向传播学习慢，但学习速度是合理的。
160 00:14:30,436 --> 00:14:33,940 说话人 SPEAKER_12：问题是，一旦你开始尝试将其扩展到大型网络，
161 00:14:34,190 --> 00:14:35,172 说话人 SPEAKER_12：真是毫无希望。
162 00:14:35,231 --> 00:14:36,533 说话人 SPEAKER_12：你可以学习像 MNIST 这样的东西。
163 00:14:36,594 --> 00:14:39,178 说话人 SPEAKER_12：如果你非常坚定，你可以学习像 CIFAR-10 这样的东西。
164 00:14:39,940 --> 00:14:44,727 说话人 SPEAKER_12：但是 ImageNet，那里有成百万张图片或者一百万张图片，它太慢了。
165 00:14:49,216 --> 00:14:58,711 说话人 SPEAKER_12：所以一种避免进行全局反向传播的方法是，我们实际上要做的就是要拥有数以亿计的局部目标函数。
166 00:14:59,772 --> 00:15:03,956 说话人 SPEAKER_12：所以我们要如何扩展这些，你自然会想，我得到一个小型神经网络，它工作得很好。
167 00:15:05,018 --> 00:15:06,919 说话人 SPEAKER_12：我有一个带有这个目标函数的小型神经网络。
168 00:15:06,980 --> 00:15:08,241 说话人 SPEAKER_12：我可以训练它，这没问题。
169 00:15:08,282 --> 00:15:10,203 说话人 SPEAKER_12：如果我想要一个大的神经网络怎么办？
170 00:15:10,244 --> 00:15:12,166 说话人 SPEAKER_12：你尝试相同的训练算法，但不起作用。
171 00:15:13,187 --> 00:15:19,214 说话人 SPEAKER_12：但如果我有许多小的神经网络，每个都有自己的目标函数，并且设想它们可以在空间上局部化呢？
172 00:15:20,235 --> 00:15:26,402 说话人 SPEAKER_12：我给它一个自己的目标函数，这样我就不会试图一次性从一个目标函数中学习大量的参数。
173 00:15:27,563 --> 00:15:28,884 说话人 SPEAKER_12：这行得通。
174 00:15:29,523 --> 00:15:35,312 说话人 SPEAKER_12：它不如反向传播效果好，但它在更大的网络中扩展得更好，这些网络由许多局部组组成。
175 00:15:36,474 --> 00:15:38,998 说话人 SPEAKER_12：所以问题是，你从哪里得到这些局部目标函数？
176 00:15:41,020 --> 00:15:43,625 说话人 SPEAKER_12：这里有一个修订的可能性。
177 00:15:45,489 --> 00:15:52,559 说话人 SPEAKER_12：你取图像的一部分，然后有一个小网络提取一些东西，一个将要代表图像部分内容的向量。
178 00:15:53,552 --> 00:16:03,524 说话人 SPEAKER_12：然后你说，我想提取与该图像其他部分提取的向量一致的向量，但与提取自其他图像的向量不一致。
179 00:16:04,726 --> 00:16:07,149 说话人 SPEAKER_12：所以这被称为对比无监督学习。
180 00:16:07,890 --> 00:16:10,573 说话人 SPEAKER_12：实际上，你可以在多个层面上做到这一点。
181 00:16:11,455 --> 00:16:17,962 说话人 SPEAKER_12：所以你在对局部区域进行学习，试图让它们在同一级别上与其他局部区域达成一致，试图让输出结果达成一致。
182 00:16:19,284 --> 00:16:21,147 说话人 SPEAKER_12：你也在多个级别上做这件事。
183 00:16:22,222 --> 00:16:25,768 说话人 SPEAKER_12：如果你非常坚持不懈，你可以使它工作得相当不错。
184 00:16:26,168 --> 00:16:32,096 说话人 SPEAKER_12：它比我所知的所有其他生物可学习算法都要好。
185 00:16:32,817 --> 00:16:35,440 说话人 SPEAKER_12：它几乎还没有像反向传播那样好。
186 00:16:36,562 --> 00:16:41,548 说话人 SPEAKER_12：关于这一点，在 ICLR 有一篇由 Mengyi Ren 撰写的论文，他做了所有困难的工作。
187 00:16:45,533 --> 00:16:51,201 说话人 SPEAKER_12：我对它的描述略有偏差，所以你可能更容易理解。
188 00:16:51,282 --> 00:16:53,264 说话人 SPEAKER_12：但论文已经在那里了，所以我不觉得这是不诚实的。
189 00:16:56,066 --> 00:16:58,268 说话人 SPEAKER_02：这需要很多努力，而且是一篇相当密集的论文。
190 00:17:03,653 --> 00:17:13,141 说话人 SPEAKER_12：好的，现在让我们来讨论本次演讲的核心问题，即代理如何共享知识。
191 00:17:13,181 --> 00:17:15,363 说话人 SPEAKER_12：这全部关乎不同代理之间的沟通。
192 00:17:16,323 --> 00:17:20,547 说话人 SPEAKER_12：所以我现在正在尝试将知识传达给你，而且我正在这样做
193 00:17:21,032 --> 00:17:21,973 说话人 SPEAKER_12：非常低效。
194 00:17:21,993 --> 00:17:24,896 说话人 SPEAKER_12：我这样做的方式是产生一串单词。
195 00:17:25,817 --> 00:17:36,349 说话人 SPEAKER_12：在老式的 AI 中，你会做的是，你会把那些词清理成逻辑上和模糊的语言，然后放入你的大脑中。
196 00:17:36,990 --> 00:17:39,292 说话人 SPEAKER_12：这就是教学。
197 00:17:41,015 --> 00:17:42,596 说话人 SPEAKER_12：这实际上是错误的。
198 00:17:42,676 --> 00:17:43,718 说话人 SPEAKER_12：这不是它的工作方式。
199 00:17:44,719 --> 00:17:46,862 说话人 SPEAKER_12：我认为它真正的工作方式是这样的。
200 00:17:47,561 --> 00:17:48,963 说话人 SPEAKER_12：我产生一串单词。
201 00:17:49,416 --> 00:17:54,642 说话人 SPEAKER_12：你试图弄清楚如何改变你大脑中的连接强度，以至于你会说出那样的话。
202 00:17:55,742 --> 00:17:57,025 说话人 SPEAKER_12：这是一个非常不同的过程。
203 00:17:57,065 --> 00:17:57,845 说话人 SPEAKER_12：这是蒸馏。
204 00:17:57,865 --> 00:18:03,191 说话人 SPEAKER_12：这与仅仅存储单词串或存储清理过的单词串非常不同。
205 00:18:03,711 --> 00:18:10,037 说话人 SPEAKER_12：你正在尝试弄清楚如何改变你大脑中的数万亿个权重，这样才是一句合理的话。
206 00:18:10,076 --> 00:18:12,839 说话人 SPEAKER_02：也就是说，如果你相信我。
207 00:18:16,983 --> 00:18:17,684 说话人 SPEAKER_02：所以。
208 00:18:18,576 --> 00:18:20,298 说话人 SPEAKER_02：这是一种困难的分享知识的方式。
209 00:18:21,138 --> 00:18:23,721 说话人 SPEAKER_02：数字计算机有更好的知识共享方式。
210 00:18:27,164 --> 00:18:29,007 所以它们可以进行权重或梯度共享。
211 00:18:29,047 --> 00:18:33,671 假设我有一个拥有万亿连接的大人工神经网络。
212 00:18:35,593 --> 00:18:41,759 如果我在数字计算机上运行它，我可以在许多不同的数字计算机上制作它的精确副本。
213 00:18:42,880 --> 00:18:46,743 说话人 SPEAKER_12：每个副本都可以去查看互联网上的不同部分。
214 00:18:47,028 --> 00:18:50,432 说话人 SPEAKER_12：然后它可以找出如何改变其权重，以便它可以说出那样的话。
215 00:18:52,536 --> 00:19:01,229 说话人 SPEAKER_12：然后它可以采取它想要做出的权重变化，并且它可以与其他所有计算机交谈，它们可以都同意平均它们的权重变化。
216 00:19:01,950 --> 00:19:07,237 说话人 SPEAKER_12：这是一个简化，但如果你知道，是的，这是一个简化，但基本上就是这样。
他们平均了所有的重量变化，然后每个人都知道每个人、每个数字计算机学到了什么。
基本上，这就是教育工作者所期望的，我可以把大脑里的东西直接塞进你的大脑。
219 00:19:24,683 --> 00:19:26,806 发言人 SPEAKER_12：他妈的，你竟然试图预测我会说什么。
220 00:19:26,826 --> 00:19:29,388 发言人 SPEAKER_12：我只是想把我脑子里的东西放进你的脑子里。
221 00:19:30,089 --> 00:19:32,172 说话人 SPEAKER_12：这些数字智能可以做到这一点。
222 00:19:32,913 --> 00:19:36,436 说话人 SPEAKER_12：它们通过仅仅，它们都同意权重变化。
223 00:19:36,696 --> 00:19:38,378 说话人 SPEAKER_12：现在它们都知道它们都学到了什么。
224 00:19:39,480 --> 00:19:41,942 说话人 SPEAKER_12：所以想象一下，如果我们有 10,000 人。
225 00:19:43,163 --> 00:19:45,807 说话人 SPEAKER_12：每当其中一个人学到什么，我们所有人就都知道了。
226 00:19:46,714 --> 00:19:49,798 说话人 SPEAKER_12：这将给您带来巨大的优势。
227 00:19:49,817 --> 00:19:52,020 说话人 SPEAKER_12：这正是数字计算的优势所在。
228 00:19:52,820 --> 00:20:04,354 说话人 SPEAKER_12：此外，它还有优势，即可以使用反向传播，这可能是更好的学习算法，因为它可以穿过许多层神经元并计算出精确的梯度。
229 00:20:04,854 --> 00:20:09,519 说话人 SPEAKER_12: 这些都不是猜测方向，看看效果如何，然后希望你能平均掉这些差异。
230 00:20:10,240 --> 00:20:12,182 说话人 SPEAKER_12: 这是一个更好的学习算法。
231 00:20:13,984 --> 00:20:22,836 说话人 SPEAKER_12: 这对我来说是个巨大的安慰，因为我过去几年一直在尝试提出与反向传播一样有效的、具有生物学依据的学习算法。
232 00:20:23,517 --> 00:20:25,819 说话人 SPEAKER_12: 我最终决定，可能根本不存在这样的算法。
233 00:20:25,839 --> 00:20:27,382 说话人 SPEAKER_12：也许反向传播实际上更好。
234 00:20:28,282 --> 00:20:30,065 说话人 SPEAKER_12：这就是我现在相信的。
235 00:20:34,289 --> 00:20:38,174 说话人 SPEAKER_12：所以我想提醒你们，你们为此付出了巨大的代价。
236 00:20:38,434 --> 00:20:41,558 说话人 SPEAKER_12：你们付出的代价是他们必须是数字计算机。
237 00:20:41,578 --> 00:20:43,721 说话人 SPEAKER_12：它们必须精确制造，以便它们
238 00:20:44,173 --> 00:20:50,522 说话人 SPEAKER_12：按照指令级别准确执行，并且消耗大量能量。
239 00:20:55,490 --> 00:21:01,318 说话人 SPEAKER_12：如果拥有生物神经网络，或者拥有两个具有完全不同架构的数字网络，我们就使用蒸馏。
240 00:21:04,323 --> 00:21:07,448 说话人 SPEAKER_12：但它的带宽要低得多，所以如果你有一个
241 00:21:07,984 --> 00:21:12,548 说话人 SPEAKER_12：一个拥有万亿连接的数字模型，它们都去查看一些数据，然后平均它们的权重。
242 00:21:12,589 --> 00:21:17,755 说话人 SPEAKER_12：当它们平均权重变化时，那就是正在传递的万亿比特。
243 00:21:18,756 --> 00:21:26,222 说话人 SPEAKER_12：当你尝试预测我说的话，当你尝试改变权重以预测这句话时，那最多只有几百比特。
244 00:21:27,344 --> 00:21:30,626 说话人 SPEAKER_12：如果你是卡尔，那大约只有两个比特，因为他本来就知道我要说什么。
245 00:21:34,049 --> 00:21:35,612 说话人 SPEAKER_02：所以带宽要低得多。
246 00:21:37,988 --> 00:21:39,288 说话人 SPEAKER_02：到目前为止的故事是这样的。
247 00:21:41,270 --> 00:21:43,493 说话人 SPEAKER_12：有两种非常不同的计算方式。
248 00:21:44,654 --> 00:21:50,520 说话人 SPEAKER_12：它们主要的区别在于不同智能体之间如何传递知识。
249 00:21:51,541 --> 00:21:53,344 说话人 SPEAKER_12：在数字计算中，你使用权重共享。
250 00:21:54,684 --> 00:21:58,209 说话人 SPEAKER_12：而且你拥有巨大的带宽来共享每个智能体学到的知识。
251 00:21:58,990 --> 00:22:05,717 说话人 SPEAKER_12：在生物计算中，如果你利用硬件的模拟特性，可以非常低功耗。
252 00:22:06,202 --> 00:22:09,086 说话人 SPEAKER_02：但现在共享知识是一个缓慢而痛苦的过程。
253 00:22:13,633 --> 00:22:15,295 说话人 SPEAKER_02：现在让我们看看大型语言模型。
254 00:22:16,896 --> 00:22:19,760 说话人 SPEAKER_12：因为现在每个 AI 演讲最终都要涉及到大型语言模型。
255 00:22:22,064 --> 00:22:22,805 说话人 SPEAKER_12：它们非常有趣。
256 00:22:23,165 --> 00:22:24,647 说话人 SPEAKER_12：它们使用数字计算。
257 00:22:25,087 --> 00:22:29,453 说话人 SPEAKER_12：所以当它在学习时，会在不同的计算机上运行同一组权重的许多不同副本。
258 00:22:30,174 --> 00:22:31,636 说话人 SPEAKER_12：并且它们都会查看互联网的不同部分。
259 00:22:32,308 --> 00:22:39,439 说话人 SPEAKER_12：这使得它们能够看到大量数据并整合所有这些知识，因为它们可以共享所学到的内容。
260 00:22:40,299 --> 00:22:47,770 说话人 SPEAKER_12：因此，如果你看大型语言模型，它们大约有 1000 万亿个权重，它们知道的可能是我们任何一个人的 1000 倍。
261 00:22:48,050 --> 00:22:49,413 说话人 SPEAKER_12：他们好像什么都知道。
262 00:22:49,653 --> 00:22:54,119 说话人 SPEAKER_12：GPT-4 大概知道所有可能的、合理的事情。
263 00:22:55,241 --> 00:23:01,911 说话人 SPEAKER_12：现在我们有万亿个连接，所以我们有百倍多的连接。
264 00:23:02,886 --> 00:23:06,451 说话人 SPEAKER_12：所以我们实际上并没有使用全部的这种能力。
265 00:23:07,453 --> 00:23:09,036 说话人 SPEAKER_12：但我们看不到足够的数据。
266 00:23:09,777 --> 00:23:12,661 说话人 SPEAKER_12：如果我们能从其他人那里获取知识，也许我们可以全部利用。
267 00:23:13,281 --> 00:23:22,255 说话人 SPEAKER_12：但他们有 1,000 倍的知识在 1%的连接中，这某种程度上证实了他们的论点，他们有一个更好的学习算法。
268 00:23:22,454 --> 00:23:31,627 说话人 SPEAKER_12：反向传播与不同代理之间这种简单的通信相结合，意味着他们基本上拥有一个更好的学习算法。
269 00:23:33,126 --> 00:23:37,696 说话人 SPEAKER_12：现在，它正在被用来窃取我们所有的知识。
270 00:23:39,881 --> 00:23:43,509 说话人 SPEAKER_12：抱歉，偷这个词不恰当，尤其是在当前的政治背景下。
271 00:23:43,528 --> 00:23:47,998 说话人 SPEAKER_12：它是通过蒸馏来获取我们的知识。
272 00:23:48,398 --> 00:23:50,763 说话人 SPEAKER_12：所以这些可以分享知识的数字代理
273 00:23:51,419 --> 00:23:56,546 说话人 SPEAKER_12：每个代理在尝试从网络获取知识时，都在使用蒸馏来获取知识。
274 00:23:56,566 --> 00:24:03,015 说话人 SPEAKER_12：它在查找人们说过的话，并试图改变其方式，以便它也能说出同样的话，这不是一种很有效率的获取知识的方式。
275 00:24:03,776 --> 00:24:08,542 说话人 SPEAKER_12：但是有很多这样的代理，我们运行了很长时间。
276 00:24:09,364 --> 00:24:16,834 说话人 SPEAKER_12：因此，它基本上可以在几个月内在很多计算机上学习到人们所知道的一切。
277 00:24:18,696 --> 00:24:21,240 说话人 SPEAKER_12：它甚至使用了一种低效的蒸馏形式。
278 00:24:21,810 --> 00:24:27,876 说话人 SPEAKER_12：因为如果你观察大量替代类别的教师概率，蒸馏是非常高效的。
279 00:24:31,701 --> 00:24:39,750 说话人 SPEAKER_12：当数字模型从网络上的文档中获取知识时，网络上的文档就是教师。
280 00:24:40,089 --> 00:24:44,214 说话人 SPEAKER_12：他们只是查看作者接下来生成的单词。
281 00:24:44,535 --> 00:24:46,317 说话人 SPEAKER_12：他们看不到整个分布。
282 00:24:46,718 --> 00:24:48,118 说话人 SPEAKER_12：如果他们能看到，他们可以学得更快。
283 00:24:48,619 --> 00:24:51,262 说话人 SPEAKER_12：但他们只能看到从那个分布中随机选择的部分。
284 00:24:51,462 --> 00:24:52,784 说话人 SPEAKER_02：但这已经足够好，他们可以学习了。
如果这些大型数字神经网络在多台不同的计算机上运行，并且它们能直接从世界中获取知识，那么它们可能能够更快地获取知识。
例如，如果它们预测视频中的下一帧，如果它们带着头上的摄像头四处走动并尝试预测视频中的下一帧，
或者如果它们得到一个机械臂并尝试预测当它们移动手臂时会发生什么，它们可能能够更快地学习。
因此，大型语言模型在学习相当抽象的东西方面做得很好，这是有益的，但它们的带宽并不大，因为它们只是从低带宽的单词序列中学习。
289 00:25:32,336 --> 00:25:37,962 说话人 SPEAKER_12：我怀疑这些大型模型会变得更好。
290 00:25:38,502 --> 00:25:40,885 说话人 SPEAKER_12：我们知道，如果你让它们多模态，它们会变得更好。
291 00:25:41,287 --> 00:25:45,311 说话人 SPEAKER_12：所以 GPT-4 不仅用文字训练，还用图像。
292 00:25:45,915 --> 00:25:47,877 说话人 SPEAKER_12：有可能谷歌也在做同样的事情。
293 00:25:53,163 --> 00:26:05,656 说话人 SPEAKER_12：我认为，尤其是当它们是多模态的时候，它们能学到比我们多得多的东西。
294 00:26:07,097 --> 00:26:09,560 说话人 SPEAKER_12：如果你玩玩 GPT-4，
295 00:26:10,367 --> 00:26:14,213 说话人 SPEAKER_12：很难不相信它已经很聪明了。
296 00:26:15,336 --> 00:26:23,166 说话人 SPEAKER_12：所以我非常尊重的人，比如 Yann LeCun，他们认为它实际上并不理解它在说什么。
297 00:26:24,048 --> 00:26:28,695 说话人 SPEAKER_12：但我不明白他怎么能相信这一点，因为你可以给它一些小谜题。
298 00:26:29,297 --> 00:26:38,130 说话人 SPEAKER_12：如果它真的不理解，如果它只是做自动补全的随机鹦鹉，我看不出它怎么能解决它从未见过的形式的谜题。
299 00:26:38,936 --> 00:26:41,118 说话人 SPEAKER_12：所以我有一个在符号 AI 领域的朋友。
300 00:26:42,240 --> 00:26:46,724 说话人 SPEAKER_12：他叫 Hector Levesque。
301 00:26:47,105 --> 00:26:48,365 说话人 SPEAKER_12：他很有正直感。
302 00:26:48,405 --> 00:26:50,367 说话人 SPEAKER_12：所以他不想总是改变目标。
303 00:26:51,348 --> 00:26:54,031 说话人 SPEAKER_12：他现在非常惊讶他们能这样做。
304 00:26:54,112 --> 00:26:56,534 说话人 SPEAKER_12：他承认他对神经网络能这样做感到非常惊讶。
305 00:26:56,773 --> 00:27:02,480 说话人 SPEAKER_12：他无法理解这样一个愚蠢的方法如何处理推理，一点点的推理。
306 00:27:03,421 --> 00:27:05,923 说话人 SPEAKER_12：所以他让我给 GPT-4 出一个问题。
307 00:27:06,358 --> 00:27:09,465 说话人 SPEAKER_12：因为我知道它能够处理他的问题，所以我把问题设置得更难了。
308 00:27:10,428 --> 00:27:17,001 说话人 SPEAKER_12：然后我把这个问题给了它，我家的房间被漆成白色、蓝色或黄色。
309 00:27:19,387 --> 00:27:21,511 说话人 SPEAKER_12：黄色油漆一年后变成白色。
310 00:27:22,914 --> 00:27:25,000 说话人 SPEAKER_12：两年后，我希望它们都变成白色。
311 00:27:25,259 --> 00:27:26,021 说话人 SPEAKER_12：我应该做什么？
312 00:27:27,519 --> 00:27:30,544 说话人 SPEAKER_12：你会说，你应该把蓝色的房间漆成白色。
313 00:27:32,125 --> 00:27:40,134 说话人 SPEAKER_12：但如果你是数学家，你可能会说应该把蓝色房间涂成黄色，因为这把它简化成了一个已经解决的问题，因为你知道黄色会变成白色。
314 00:27:41,416 --> 00:27:43,819 说话人 SPEAKER_12：而且 GBT4 实际上给了数学家一个解决方案。
315 00:27:43,859 --> 00:27:45,080 说话人 SPEAKER_12：它说把蓝色房间涂成黄色。
316 00:27:46,643 --> 00:27:49,385 说话人 SPEAKER_12：但问题是，我看不出他是如何做到这一点的，除非他理解了。
317 00:27:49,826 --> 00:27:54,152 说话人 SPEAKER_12：还有许多其他事情，你让它写代码生成图表，它就能生成图表。
318 00:27:54,757 --> 00:27:57,384 说话人 SPEAKER_12：所以我搞不懂 Jan 怎么会认为他们不懂。
319 00:27:57,423 --> 00:28:02,217 说话人 SPEAKER_02：他现在可能正在反方向地讲课。
320 00:28:08,153 --> 00:28:08,714 说话人 SPEAKER_02：嗯。
321 00:28:09,843 --> 00:28:14,867 说话人 SPEAKER_12：这让我相信这些事物可能比我们更智能，而且可能很快就会发生。
322 00:28:15,269 --> 00:28:19,752 说话人 SPEAKER_12：我一直认为这可能是 50 到 100 年，或者 30 到 100 年，或者 30 到 50 年。
323 00:28:20,193 --> 00:28:21,915 说话人 SPEAKER_12：我认为我在不同时间说过不同的话。
324 00:28:23,356 --> 00:28:25,239 说话人 SPEAKER_12：但现在我相信可能是 5 到 20 年。
325 00:28:25,740 --> 00:28:27,161 说话人 SPEAKER_12：我认为这将会很快发生。
326 00:28:27,741 --> 00:28:33,768 说话人 SPEAKER_12：如果它将在五年后发生，我们不能仅仅让哲学家来决定如何处理它。
327 00:28:34,388 --> 00:28:38,532 说话人 SPEAKER_12：是我们真正获得一些实践经验的时候了。
328 00:28:39,086 --> 00:28:49,316 说话人 SPEAKER_12：所以我认为，好吧，让我完成这个幻灯片，人们将无法抗拒为这些事物设定目标。
329 00:28:49,636 --> 00:28:51,218 说话人 SPEAKER_12：显然，你想做事情，你就给他们设定目标。
330 00:28:52,118 --> 00:28:56,182 说话人 SPEAKER_12：如果你想擅长实现目标，你就给他们创造子目标的能力。
331 00:28:57,884 --> 00:29:08,554 说话人 SPEAKER_12：一旦你有了创造子目标的能力，如果你聪明，你就会意识到一个非常好的子目标是获得更多的控制，因为这有助于你实现所有其他目标。
332 00:29:09,412 --> 00:29:12,596 说话人 SPEAKER_12：所以我会给你一个例子，让你看到自己在做这件事。
333 00:29:13,277 --> 00:29:14,919 说话人 SPEAKER_12：你坐在一个非常无聊的研讨会上。
334 00:29:15,099 --> 00:29:16,060 说话人 SPEAKER_12：不是这个。
335 00:29:16,642 --> 00:29:17,623 说话人 SPEAKER_12：一个非常无聊的研讨会。
336 00:29:18,203 --> 00:29:20,226 说话人 SPEAKER_12：你看到天花板上的一小块光。
337 00:29:20,866 --> 00:29:21,969 说话人 SPEAKER_12：你可能会想，那是啥？
338 00:29:22,569 --> 00:29:24,531 说话人 SPEAKER_12：然后你听了一会儿无聊的研讨会。
339 00:29:24,551 --> 00:29:26,914 说话人 SPEAKER_12：然后你注意到当你移动时，光线也在移动。
340 00:29:27,576 --> 00:29:30,019 说话人 SPEAKER_12：然后你意识到那是太阳从你的手表上反射的光。
341 00:29:30,881 --> 00:29:32,061 说话人 SPEAKER_12：那么接下来你做什么？
342 00:29:32,583 --> 00:29:34,105 说话人 SPEAKER_12：你说，好吧，我已经解决了那个问题。
343 00:29:34,125 --> 00:29:34,905 说话人 SPEAKER_12：我现在知道那是什么了。
344 00:29:35,205 --> 00:29:36,448 说话人 SPEAKER_12：我要去听那个研讨会。
345 00:29:36,868 --> 00:29:37,950 说话人 SPEAKER_12：不，你这么做是不对的。
346 00:29:38,538 --> 00:29:40,279 说话人 SPEAKER_12：如果你这么做，你就不是真正的科学家了。
347 00:29:41,461 --> 00:29:45,345 说话人 SPEAKER_12：接下来你要做的是，哦，我想知道我该如何让它那样移动？
348 00:29:45,365 --> 00:29:46,586 说话人 SPEAKER_12：还有我该如何让它这样移动？
349 00:29:46,906 --> 00:29:49,990 说话人 SPEAKER_12：然后你尝试找出如何旋转手腕使其向不同方向移动。
350 00:29:50,490 --> 00:29:52,313 说话人 SPEAKER_12：一旦你做到了，就回去听讲座。
351 00:29:53,693 --> 00:29:57,818 说话人 SPEAKER_12：我们有一种非常强烈和合理的欲望去控制事物。
352 00:29:58,259 --> 00:30:06,147 说话人 SPEAKER_12：因为显然，如果你能控制事物，那么在未来某个时刻你需要移动聚光灯时，你就会知道如何去做。
353 00:30:06,922 --> 00:30:12,411 说话人 SPEAKER_12：我看不清楚我们如何阻止一个超级智能想要控制事物。
354 00:30:13,932 --> 00:30:17,097 说话人 SPEAKER_12：然后这有点棘手。
355 00:30:17,939 --> 00:30:22,125 你可能会想象你可以将其隔离，这样它实际上就不能按下红色按钮或拉动大杠杆。
356 00:30:22,946 --> 00:30:28,695 但如果它能输出文本，那么它就可以操纵人们。
357 00:30:28,715 --> 00:30:33,604 说话人 SPEAKER_12：结果如果你想在华盛顿入侵一栋建筑，你只需要能够输出文字。
358 00:30:34,141 --> 00:30:38,207 说话人 SPEAKER_12：你可以说服那些轻信的人，通过入侵这栋建筑来拯救民主。
359 00:30:39,347 --> 00:30:42,633 说话人 SPEAKER_12：而这东西将会比我们更聪明。
360 00:30:43,314 --> 00:30:49,162 说话人 SPEAKER_12：所以只要我们在阅读它所说的，它就像是美杜莎，你需要避开它的眼睛。
361 00:30:50,282 --> 00:30:53,728 说话人 SPEAKER_12：只要你在阅读它所说的内容，它就能操纵你。
362 00:30:54,608 --> 00:30:57,834 说话人 SPEAKER_12：这让我非常沮丧。
363 00:30:58,535 --> 00:31:03,961 说话人 SPEAKER_12：我希望有一个简单的解决方案，但我没有。
364 00:31:04,532 --> 00:31:22,480 说话人 SPEAKER_12：所以当我改变主意，关于这些事物变得超级智能的速度，以及实际上数字智能比生物智能好多少，我一直以为情况相反，我决定至少大喊一声“失火了”。
365 00:31:22,981 --> 00:31:28,109 说话人 SPEAKER_12：我不知道该怎么办，也不知道该往哪个方向走，但我们必须要认真对待这个问题。
366 00:31:28,951 --> 00:31:32,717 说话人 SPEAKER_12：而且有很多人比我更早地考虑过这些风险。
367 00:31:33,355 --> 00:31:34,696 说话人 SPEAKER_12：并且提出了各种建议。
368 00:31:36,099 --> 00:31:39,065 说话人 SPEAKER_12：我还没有看到过一个真正可行的方案，关于我们如何控制它。
369 00:31:39,145 --> 00:31:52,630 说话人 SPEAKER_12：但我最好的猜测是，开发这些技术的公司应该被迫在开发过程中以及在其比我们更聪明之前，投入大量工作来检查其安全性。
370 00:31:53,166 --> 00:31:57,771 说话人 SPEAKER_12：因此，他们应该投入相似的工作量来观察它试图摆脱你的控制的方式。
371 00:31:58,752 --> 00:32:09,144 说话人 SPEAKER_12：因为任何编写过计算机程序的人都知道，仅仅理论思考和想象与实际尝试相比并不很好。
372 00:32:09,163 --> 00:32:12,268 说话人 SPEAKER_12：当你尝试时，它们的表现往往是你没有预料到的。
373 00:32:13,107 --> 00:32:15,971 说话人 SPEAKER_12：你以为的大问题实际上并不是问题。
374 00:32:16,632 --> 00:32:19,275 说话人 SPEAKER_12：所以，对于很多人来说，很多年，
375 00:32:19,626 --> 00:32:24,055 说话人 SPEAKER_12：很多人没有研究神经网络，因为他们认为会陷入局部最优。
376 00:32:25,157 --> 00:32:27,481 说话人 SPEAKER_12：结果他们从未真正检查过这一点是否属实。
377 00:32:27,541 --> 00:32:28,805 说话人 SPEAKER_12：他们只是假设它是真的。
378 00:32:29,346 --> 00:32:29,866 说话人 SPEAKER_12：但这不是真的。
379 00:32:30,607 --> 00:32:34,414 说话人 SPEAKER_12：即使它是真的，也会有很好的局部最小值，所以没关系。
380 00:32:35,136 --> 00:32:36,318 说话人 SPEAKER_12：但实际上并不是真的。
381 00:32:37,160 --> 00:32:42,450 说话人 SPEAKER_12：我们需要对这些东西进行实际操作，了解它们如何尝试逃脱，以及如何控制它们。
382 00:32:43,132 --> 00:32:51,607 说话人 SPEAKER_12：如果他们有一个，并且能够控制它，那么我更相信他们告诉我如何控制它们，而不是他们只是在理论化。
383 00:32:55,556 --> 00:32:56,096 说话人 SPEAKER_02：所以...
384 00:32:57,493 --> 00:32:57,835 说话人 SPEAKER_02：是的。
如果我们不再是智能的顶峰，他们可能还会需要我们一段时间，因为我们功耗很低，所以我们可以非常便宜地运行计算，就像挖沟渠那样的智力工作，并且我们可以保持电站运行。
386 00:33:14,825 --> 00:33:17,887 发言人 SPEAKER_12：但他们可能设计出比我们更好的计算机。
他们当然可以某种程度地重新设计神经元，并从基因上改造它们，创造出比我们更好的东西。
388 00:33:23,813 --> 00:33:28,557 发言人 SPEAKER_12：所以我的结论是，也许我们只是智慧进化的一个过渡阶段。
389 00:33:29,617 --> 00:33:32,380 说话人 SPEAKER_12：实际上，这可能对所有其他物种都有好处。
390 00:33:35,482 --> 00:33:40,586 说话人 SPEAKER_12：我认为如果我们能控制好它们，它们将具有巨大的价值。
391 00:33:40,626 --> 00:33:44,190 说话人 SPEAKER_12：尽管存在所有这些风险，人们仍然会继续开发这些东西的原因是，
392 00:33:44,692 --> 00:33:46,234 说话人 SPEAKER_12：因为它能带来巨大的好处。
393 00:33:46,815 --> 00:33:55,064 发言人 SPEAKER_12：就像在医学上一样，你难道不想去看一位看过一亿病人的全科医生吗，其中还包括数千名患有你这种罕见疾病的患者？
394 00:33:55,324 --> 00:33:56,484 发言人 SPEAKER_12：这样会好得多。
您难道不想能够进行一次 CT 扫描，从中提取比任何医生都知道可以提取的信息多得多的信息吗？
396 00:34:08,717 --> 00:34:13,063 Speaker SPEAKER_12：所以我已经到了最后，而且我设法以足够快的速度到达那里
397 00:34:13,431 --> 00:34:16,295 说话人 SPEAKER_12：我可以谈论一些非常不可靠的事情。
398 00:34:16,315 --> 00:34:16,576 说话人 SPEAKER_12：好的。
399 00:34:17,697 --> 00:34:22,704 说话人 SPEAKER_12：所以这是严肃的事情，好的？
400 00:34:24,326 --> 00:34:26,409 说话人 SPEAKER_12：你需要担心这些事情会失控。
401 00:34:27,030 --> 00:34:32,838 说话人 SPEAKER_12：如果你年轻，想研究神经网络，看看你能否想出一种方法来确保它们不会获得控制权。
402 00:34:34,179 --> 00:34:36,842 说话人 SPEAKER_12：现在，很多人相信这一点。
403 00:34:39,186 --> 00:34:39,385 说话人 SPEAKER_12：是的。
404 00:34:40,186 --> 00:34:42,190 说话人 SPEAKER_12：很多人相信这一点。
405 00:34:42,608 --> 00:34:53,867 说话人 SPEAKER_12：我们不必担心的一个原因是，这些事物没有主观经验，或者意识，或者你想要称之为的东西。
406 00:34:54,628 --> 00:34:56,210 说话人 SPEAKER_12：这些事物只是愚蠢的计算机。
407 00:34:56,391 --> 00:35:04,465 说话人 SPEAKER_12：它们可以操作符号，可以做事情，但它们实际上没有真正的经验，所以它们不像我们。
408 00:35:06,840 --> 00:35:16,518 说话人 SPEAKER_12：现在，我强烈建议，如果你有一个好名声，你可以说一件疯狂的事情，你可以逃脱惩罚，人们甚至会认真听你说话。
409 00:35:17,639 --> 00:35:20,465 说话人 SPEAKER_12：所以我才让你一直听到这里。
410 00:35:21,346 --> 00:35:25,032 说话人 SPEAKER_12：但是如果你说两件疯狂的事情，人们就会说他疯了，就不会听了。
411 00:35:25,974 --> 00:35:27,938 说话人 SPEAKER_02：所以我并不期待你听下一部分。
412 00:35:33,065 --> 00:35:42,097 说话人 SPEAKER_12：所以人们确实有一种倾向，认为自己是特别的，就像我们是按照上帝的形象创造的，所以他当然把我们放在宇宙的中心。
413 00:35:44,561 --> 00:35:53,954 说话人 SPEAKER_12：许多人认为人类还有一些数字计算机不可能拥有的特殊之处，那就是我们拥有主观体验。
414 00:35:55,556 --> 00:35:58,701 说话人 SPEAKER_12：他们认为这就是我们不必担心的原因之一。
415 00:35:59,474 --> 00:36:01,936 说话人 SPEAKER_12：我不确定是否真的有很多人这样认为。
416 00:36:02,438 --> 00:36:06,402 说话人 SPEAKER_12：所以我向 ChatGPT 询问人们是怎么想的，它告诉我这就是他们的看法。
417 00:36:10,208 --> 00:36:11,509 说话人 SPEAKER_12：实际上挺好的。
418 00:36:11,528 --> 00:36:12,590 说话人 SPEAKER_12：我的意思是，这挺好的。
419 00:36:13,311 --> 00:36:15,894 说话人 SPEAKER_12：这可能是一个 N 为 1 亿的数据量，对吧？
420 00:36:17,175 --> 00:36:19,338 说话人 SPEAKER_12：我必须说，大家怎么看？
421 00:36:22,204 --> 00:36:25,690 说话人 SPEAKER_12：我现在将尝试削弱意识防御。
422 00:36:26,713 --> 00:36:32,804 说话人 SPEAKER_12：我认为人并没有什么特别之处，除了他们非常复杂，非常奇妙，而且对其他人来说非常有趣。
423 00:36:37,534 --> 00:36:44,786 说话人 SPEAKER_12：所以如果你是一位哲学家，你可以把我归类为丹尼尼克阵营。
424 00:36:48,697 --> 00:36:55,186 说话人 SPEAKER_12：我认为人们完全误解了心灵、意识以及主观体验的本质。
425 00:36:56,047 --> 00:37:08,322 说话人 SPEAKER_12：假设我刚刚服用了大量的 LSD，现在我看到了一些粉红色的小象。
426 00:37:10,005 --> 00:37:13,608 说话人 SPEAKER_12：我想告诉你们我的感知系统正在发生什么。
427 00:37:14,668 --> 00:37:19,056 说话人 SPEAKER_12：所以我会说，我有一种主观体验，一个小粉红色的小象在我面前飘浮。
428 00:37:19,958 --> 00:37:21,481 说话人 SPEAKER_12：让我们来分析一下这意味着什么。
429 00:37:22,744 --> 00:37:25,969 说话人 SPEAKER_12：我所做的是试图告诉您我的感知系统正在发生什么。
430 00:37:27,952 --> 00:37:34,625 说话人 SPEAKER_12：而我这样做的方式并不是通过告诉您神经元 52 高度活跃，因为这对您没有任何好处。
431 00:37:34,644 --> 00:37:37,150 说话人 SPEAKER_12：实际上，我也不知道这一点。
432 00:37:38,057 --> 00:37:47,333 说话人 SPEAKER_12：但我们有这样的想法，世界上有事物，有正常感知，即世界上的事物以正常的方式产生感知。
433 00:37:47,373 --> 00:37:55,525 说话人 SPEAKER_12：现在我有一种感知，我可以告诉你，在这个世界上需要有什么样的情况，才能使这成为正常感知的结果。
434 00:37:56,628 --> 00:38:02,918 说话人 SPEAKER_12：在这个世界上需要有什么样的情况，才能使这成为正常感知的结果，那就是粉红色的小象在周围飘浮。
435 00:38:04,027 --> 00:38:14,623 说话人 SPEAKER_12：所以当我说我有粉红色小象的主观体验时，并不是说有一个由被称为“质料”的有趣物质构成的内景剧场，里面有粉红色的小象。
436 00:38:15,423 --> 00:38:16,266 说话人 SPEAKER_12：根本不是那样的。
437 00:38:16,286 --> 00:38:17,306 说话人 SPEAKER_12：这完全错了。
438 00:38:18,329 --> 00:38:23,635 说话人 SPEAKER_12：我正试图通过正常感知的概念来告诉你我的感知系统。
439 00:38:24,536 --> 00:38:31,166 说话人 SPEAKER_12：我说的是，这里发生的事情如果是由于有小粉象的话，就会是正常的感知。
440 00:38:31,670 --> 00:38:36,596 说话人 SPEAKER_12：但小粉象有趣的地方不在于它们是由质料构成的，并且存在于内在世界中。
441 00:38:36,976 --> 00:38:40,340 说话人 SPEAKER_12：他们有趣的地方在于它们是反事实的。
442 00:38:40,360 --> 00:38:41,541 说话人 SPEAKER_12：但它们存在于现实世界中。
443 00:38:42,382 --> 00:38:45,847 说话人 SPEAKER_12：或者说，它们并不存在于现实世界中，但它们是可能存在的事物。
444 00:38:46,989 --> 00:38:50,612 说话人 SPEAKER_12：因此，它们不是由内在剧院中的神秘物质构成的。
445 00:38:51,393 --> 00:38:54,077 说话人 SPEAKER_12：它们是由一个完全正常的世界中的反事实材料制成的。
446 00:38:55,059 --> 00:38:59,905 说话人 SPEAKER_12：这就是我认为人们在谈论主观体验时正在发生的事情。
447 00:39:01,505 --> 00:39:08,074 说话人 SPEAKER_12：从这个意义上说，我认为这些模型可以有主观体验。
448 00:39:08,115 --> 00:39:10,159 说话人 SPEAKER_12：那么，让我们假设我们创建一个多模态模型。
449 00:39:11,340 --> 00:39:12,682 说话人 SPEAKER_12: 它就像 GPT-4。
450 00:39:12,983 --> 00:39:14,204 说话人 SPEAKER_12: 它有一个摄像头，比如说。
451 00:39:15,387 --> 00:39:22,458 说话人 SPEAKER_12: 当它没在看的时候，我不知道你是怎么做到的，但当它没在看的时候，你就在摄像头前放一个棱镜。
452 00:39:22,958 --> 00:39:24,240 说话人 SPEAKER_12: 但它不知道有棱镜这回事。
453 00:39:26,264 --> 00:39:28,507 说话人 SPEAKER_12：现在你把它放在前面。
454 00:39:29,230 --> 00:39:31,492 说话人 SPEAKER_12：你说，物体在哪里？
455 00:39:32,373 --> 00:39:33,536 说话人 SPEAKER_12：它说，物体在那里。
456 00:39:33,976 --> 00:39:34,818 说话人 SPEAKER_12：假设它可以指向。
457 00:39:34,938 --> 00:39:35,739 说话人 SPEAKER_12：它说，物体在那里。
458 00:39:36,719 --> 00:39:37,621 说话人 SPEAKER_12：你说，你错了。
459 00:39:37,681 --> 00:39:41,947 说话人 SPEAKER_12：它说，嗯，我主观上感觉物体在那里。
460 00:39:41,967 --> 00:39:42,728 说话人 SPEAKER_12：你说，没错。
461 00:39:42,748 --> 00:39:46,974 说话者 SPEAKER_12：你在那里感受到了物体的主观体验，但实际上是因为我在你的镜头前放了一个棱镜。
462 00:39:48,315 --> 00:39:51,840 说话者 SPEAKER_12：我认为这就是我们用来描述人的主观体验的相同用法。
463 00:39:53,143 --> 00:39:57,128 说话者 SPEAKER_12：我还有一个例子来证明人类并没有什么特别之处。
464 00:39:57,967 --> 00:40:09,530 说话者 SPEAKER_12：假设我正在和一个聊天机器人交谈，我突然意识到聊天机器人认为我是一个少女。
465 00:40:10,893 --> 00:40:17,766 Speaker SPEAKER_12: 有各种线索表明这一点，比如聊天机器人告诉我有关名叫 Beyonce 的人，我从未听说过。
466 00:40:19,804 --> 00:40:21,447 Speaker SPEAKER_12: 还有很多关于化妆的其他东西。
467 00:40:23,210 --> 00:40:24,150 Speaker SPEAKER_12: 对不起，我没这么说。
468 00:40:26,333 --> 00:40:27,215 Speaker SPEAKER_02: 你必须非常小心。
469 00:40:29,438 --> 00:40:37,309 说话人 SPEAKER_12：所以我可以问这个聊天机器人，你认为我是哪种人群？
470 00:40:37,771 --> 00:40:39,612 说话人 SPEAKER_12：它会说，你是个少女。
471 00:40:40,875 --> 00:40:42,757 说话人 SPEAKER_12：这将是它认为我是少女的更多证据。
472 00:40:43,038 --> 00:40:49,407 说话人 SPEAKER_12：我可以回顾对话，看看它是如何误解了我说的某些话，这就是为什么它认为我是少女。
473 00:40:50,027 --> 00:41:03,572 说话人 SPEAKER_12：我的说法是，当我说聊天机器人认为我是少女时，使用“认为”这个词和我说你认为我应该在进入真正奇怪的内容之前停止讲座时使用“认为”这个词是一样的。
474 00:41:05,777 --> 00:41:08,061 说话人 SPEAKER_12：好吧，这就是我想说的全部了。
475 00:41:09,545 --> 00:41:10,786 说话人 SPEAKER_13：我们将尝试。
476 00:41:11,172 --> 00:41:12,393 说话人 SPEAKER_13：做一些稍微复杂的事情。
477 00:41:12,432 --> 00:41:17,018 说话人 SPEAKER_13：我们也在备用会议室安排了满座，希望他们也能有机会提问。
478 00:41:17,039 --> 00:41:26,630 说话人 SPEAKER_13：所以我们将在这个房间内提问，然后通过我的手机在线接收另一个房间的提问。
479 00:41:27,230 --> 00:41:28,351 说话人 SPEAKER_13：希望这能行。
480 00:41:28,711 --> 00:41:30,333 说话人 SPEAKER_13：但我们先从这个房间开始。
481 00:41:30,393 --> 00:41:33,538 说话人 SPEAKER_13：我们能否举手提问？
482 00:41:34,117 --> 00:41:35,119 说话人 SPEAKER_09：我是克里斯蒂安·马菲丹。
483 00:41:36,059 --> 00:41:37,322 说话人 SPEAKER_09：我有一个忏悔要讲。
484 00:41:37,943 --> 00:41:40,925 说话人 SPEAKER_09：我们用人工智能制作了一本书。
485 00:41:41,378 --> 00:41:43,762 说话人 SPEAKER_09：关于戴安娜王妃，两年前的事。
486 00:41:43,782 --> 00:41:53,317 说话人 SPEAKER_09：我们会在书前面放一个标签，说明这是由 AI、Fred Intelligence 生成的，我们只是进行了编辑。
487 00:41:53,336 --> 00:41:58,023 说话人 SPEAKER_09：然而，它仍然成为了巴诺书店和其他所有书店的畅销书。
488 00:41:59,065 --> 00:42:08,601 说话人 SPEAKER_09：伦理问题实际上是，我们真的是写了这本书，还是只是提供了灵感。
489 00:42:09,778 --> 00:42:11,721 说话人 SPEAKER_09：向 Fred.ai 写信写书。
490 00:42:13,242 --> 00:42:16,668 说话人 SPEAKER_09：我认为这是我们寻找过程中最大的头疼问题。
491 00:42:16,688 --> 00:42:20,552 说话人 SPEAKER_09：因此，我签署了暂停 AI 的信件。
492 00:42:20,952 --> 00:42:22,394 说话人 SPEAKER_09：研究应该暂停。
493 00:42:22,414 --> 00:42:23,195 说话人 SPEAKER_12: 谢谢。
494 00:42:23,215 --> 00:42:24,818 说话人 SPEAKER_12: 好的，我对那件事有几个评论。
495 00:42:26,800 --> 00:42:29,583 说话人 SPEAKER_12: 首先，我没有签署那封信，因为我认为那不可能发生。
496 00:42:30,405 --> 00:42:35,132 说话人 SPEAKER_12: 回顾起来，我认为那是一封好信，因为它引起了政治关注，尽管没有希望发生。
497 00:42:35,152 --> 00:42:37,795 说话人 SPEAKER_12：这样做是明智的。
498 00:42:37,943 --> 00:42:39,905 说话人 SPEAKER_12：但我认为人们暂停 AI 的希望渺茫。
499 00:42:40,186 --> 00:42:41,547 说话人 SPEAKER_12：也许他们应该这么做，但我认为他们不会这么做。
500 00:42:43,710 --> 00:42:48,297 说话人 SPEAKER_12：在伦理问题上，有很多问题存在。
501 00:42:48,338 --> 00:43:00,576 说话人 SPEAKER_12：可以说整个人类都写了这本书，因为聊天机器人是在学习整个人类所说的话的基础上进行训练的，然后从那些话中产生更多内容。
502 00:43:01,163 --> 00:43:05,347 说话人 SPEAKER_12：关于那些伦理问题，我实际上并没有太多可说的。
503 00:43:05,887 --> 00:43:19,059 说话人 SPEAKER_12：我真的很想关注这些事物变得比我们更聪明并接管我们的这种存在风险，因为很多人已经在这方面做了更多的工作，我恐怕没有什么有趣的东西可以说。
504 00:43:20,900 --> 00:43:28,047 说话人 SPEAKER_13：我们能否再从这个房间里问一个问题，然后我会切换到另一个房间，然后再回来。
505 00:43:29,041 --> 00:43:31,043 说话人 SPEAKER_02：墙那边。
506 00:43:34,786 --> 00:43:35,407 说话人 SPEAKER_11：赫比·布拉德利。
507 00:43:36,148 --> 00:43:43,336 说话人 SPEAKER_11：现在，我想知道您如何看待越来越强大的 AI 系统在开源和闭源开发之间的权衡。
508 00:43:43,958 --> 00:43:51,346 说话人 SPEAKER_11：显然，开源开发的好处是更多的人在审视系统并找出其缺陷，但也许风险太多。
509 00:43:51,425 --> 00:43:51,987 说话人 SPEAKER_11：你有什么看法？
510 00:43:53,068 --> 00:43:53,929 说话人 SPEAKER_12：是的。
511 00:43:54,510 --> 00:43:57,612 说话人 SPEAKER_12：你对核武器开源开发有什么看法？
512 00:44:01,086 --> 00:44:04,112 说话人 SPEAKER_12：这就是开源的危险，对吧？
513 00:44:04,132 --> 00:44:06,335 说话人 SPEAKER_12：外面还有更多疯狂的事情可以做。
514 00:44:07,115 --> 00:44:10,400 说话人 SPEAKER_12：实际上，我不知道这个答案，我应该知道。
515 00:44:11,021 --> 00:44:18,994 说话人 SPEAKER_12：我认为你至少还需要几千万美元来训练这样一个大型聊天机器人。
516 00:44:19,315 --> 00:44:23,601 说话人 SPEAKER_12：而开源的东西只是修改它，得到了聊天机器人。
517 00:44:23,822 --> 00:44:26,166 说话人 SPEAKER_12：我认为你不能从头开始开源训练，对吧？
518 00:44:28,677 --> 00:44:30,679 说话人 SPEAKER_11：对，好的。
519 00:44:31,340 --> 00:44:38,427 说话人 SPEAKER_12：所以如果这些东西可能会很危险，可能对几家大公司来说更好。
520 00:44:38,588 --> 00:44:41,291 说话人 SPEAKER_12：我已经不再谷歌工作了，所以这不是代表谷歌说的。
521 00:44:41,952 --> 00:44:53,164 说话人 SPEAKER_12：但可能对几家大型公司来说更好，最好是分布在几个不同的国家，开发这些技术，同时也在开发控制它们的方法。
522 00:44:53,885 --> 00:44:55,527 说话人 SPEAKER_12：一旦开源所有内容，
523 00:44:55,795 --> 00:44:58,278 说话人 SPEAKER_12：人们会用它做出各种疯狂的事情。
524 00:44:59,139 --> 00:45:02,224 说话人 SPEAKER_12：这将是一个非常快速的学习它可能出错的方法。
525 00:45:06,130 --> 00:45:07,793 说话人 SPEAKER_13：好的，来自溢出室的提问。
526 00:45:08,554 --> 00:45:13,880 说话人 SPEAKER_13：鉴于你对意识防御的看法，你认为对人工智能遭受痛苦的担忧是否是一个主要问题？
527 00:45:14,561 --> 00:45:18,527 说话人 SPEAKER_13：许多人担心人工智能可能对人类控制产生的影响。
528 00:45:18,989 --> 00:45:21,632 说话人 SPEAKER_13：但我们是否应该担心人类对人工智能造成的伤害？
529 00:45:23,516 --> 00:45:24,536 说话人 SPEAKER_12: 好的。
530 00:45:25,072 --> 00:45:30,605 说话人 SPEAKER_12: 人们遭受的最糟糕的痛苦是，他们在另一个地方也遭受这样的痛苦吗？
531 00:45:31,686 --> 00:45:32,148 说话人 SPEAKER_12: 好的。
532 00:45:32,168 --> 00:45:33,911 说话人 SPEAKER_12: 人们遭受的最糟糕的痛苦是疼痛。
533 00:45:33,972 --> 00:45:37,398 说话人 SPEAKER_12：这些事物没有痛苦，至少现在还没有。
534 00:45:38,121 --> 00:45:41,648 说话人 SPEAKER_12：所以我们不必担心身体上的痛苦。
535 00:45:42,724 --> 00:45:47,112 说话人 SPEAKER_12：我想象他们可能会感到沮丧，我们必须担心像沮丧这样的问题。
536 00:45:48,094 --> 00:45:52,682 说话人 SPEAKER_12：这正进入，这是全新的领域，对吧？
537 00:45:52,804 --> 00:45:55,447 说话人 SPEAKER_12：我不知道对这类问题有什么想法。
538 00:45:55,889 --> 00:46:01,619 说话人 SPEAKER_12：我有时认为“人文主义者”这个词是一种空洞的种族主义术语。
539 00:46:03,402 --> 00:46:04,625 说话人 SPEAKER_12：我们有什么特别的吗？
540 00:46:06,228 --> 00:46:06,588 说话人 SPEAKER_12：并且
541 00:46:07,210 --> 00:46:09,373 说话人 SPEAKER_12：我对该有何感受感到完全迷茫。
542 00:46:10,175 --> 00:46:14,221 说话人 SPEAKER_12：那么，这个问题还有另一个版本，他们应该拥有政治权利吗？
543 00:46:15,623 --> 00:46:29,264 说话人 SPEAKER_12：我们有着非常长的历史，那就是不对那些仅仅因为肤色或性别、性别、我不知道，等等，与我们有微小差别的人给予政治权利。
544 00:46:29,865 --> 00:46:32,610 说话人 SPEAKER_12：这是一场激烈的斗争。
545 00:46:33,097 --> 00:46:34,378 说话人 SPEAKER_12：让他们获得政治权利。
546 00:46:34,858 --> 00:46:36,601 说话人 SPEAKER_12：这些事情与我们截然不同。
547 00:46:36,880 --> 00:46:43,427 说话人 SPEAKER_12：所以如果他们想要政治权利，我想那会非常暴力。
548 00:46:44,228 --> 00:46:51,115 说话人 SPEAKER_12：我认为我没有回答你的问题，但我认为你可以想象他们想要。
549 00:46:51,135 --> 00:46:56,501 说话人 SPEAKER_12：我和马丁·里斯谈了，最大的希望是这些事物与我们不同，因为它们没有进化。
550 00:46:57,003 --> 00:47:02,748 说话人 SPEAKER_12：所以它们没有进化成像我们一样的小型战争部落。
551 00:47:03,251 --> 00:47:04,434 说话人 SPEAKER_12：非常具有侵略性。
552 00:47:05,434 --> 00:47:08,559 说话人 SPEAKER_12：它们可能在我们本质上是完全不同的，那将是件好事。
553 00:47:10,704 --> 00:47:11,043 说话人 SPEAKER_13: 谢谢。
554 00:47:11,385 --> 00:47:12,706 说话人 SPEAKER_13: 能否再次看到这个房间里的手？
555 00:47:13,949 --> 00:47:15,451 说话人 SPEAKER_13: 我能从前面这里接受一个问题吗？
556 00:47:15,471 --> 00:47:17,914 说话人 SPEAKER_02: 那是你。
557 00:47:18,815 --> 00:47:20,157 说话人 SPEAKER_12: 是的，你。
558 00:47:20,177 --> 00:47:22,021 说话人 SPEAKER_13: 我可以大声说话。
559 00:47:22,041 --> 00:47:26,407 说话人 SPEAKER_13: 好吧，让我们等麦克风，这样我们才能让其他房间的人听到。
560 00:47:27,619 --> 00:47:29,164 说话人 SPEAKER_05: 好的，太好了。
561 00:47:29,585 --> 00:47:32,134 说话人 SPEAKER_05：所以，我想，我的名字是 Rika。
562 00:47:32,896 --> 00:47:38,793 说话人 SPEAKER_05：我的问题是，我们在通往智能的不同路径上，我们如何。
563 00:47:39,061 --> 00:47:45,650 说话人 SPEAKER_05：鼓励开发能够学习看到数据中不存在模式的方法。
564 00:47:46,230 --> 00:48:01,170 说话人 SPEAKER_05：我认为这很重要，因为我们训练这些事物时，数据存在各种问题，它们的偏见，人们试图对他人产生不利影响，所以实际上有很多负面信息来自我们人类。
565 00:48:01,420 --> 00:48:10,931 说话人 SPEAKER_05：存在许多人类尚未学会解决的大规模、非常困难的问题，如何在大规模上构建有效的经济结构，如何防止冲突和战争，对吧？
566 00:48:11,351 --> 00:48:15,697 说话人 SPEAKER_05：所以有很多好的模式还没有出现。
567 00:48:16,577 --> 00:48:22,105 说话人 SPEAKER_05：我们如何基本上，你知道，所有的机器学习都是在大数据中寻找模式，对吧？
568 00:48:22,184 --> 00:48:23,686 说话人 SPEAKER_05：你某种程度上受限于数据。
569 00:48:24,168 --> 00:48:27,612 说话人 SPEAKER_05：你能想象出哪些方法可以打破这种模式吗？
570 00:48:27,632 --> 00:48:31,056 说话人 SPEAKER_12：我不确定我是否理解了这种“数据中不存在的好模式”的想法。
571 00:48:31,440 --> 00:48:41,632 说话人 SPEAKER_05：是的，是的，所以可能“模式”这个词用错了，但我想说的是，AI 在某种程度上是危险和有限的，因为它将基于我们产生的数据进行训练。
572 00:48:41,932 --> 00:48:47,237 说话人 SPEAKER_05：这些数据包含了许多人类自己创造的关于暴力的东西，对吧？
573 00:48:47,639 --> 00:48:50,661 说话人 SPEAKER_05：所以如果你说，比如，哦，可能没有希望。
574 00:48:50,722 --> 00:48:52,583 说话人 SPEAKER_05：它们很快就会比我们聪明。
575 00:48:52,965 --> 00:48:58,210 说话人 SPEAKER_05：我们如何引导它们变得聪明，同时也能像我们一样仁慈？
576 00:48:59,532 --> 00:48:59,592 说话人 SPEAKER_12: 好的。
577 00:48:59,757 --> 00:49:04,724 说话人 SPEAKER_12: 关于那件事，我可以说一件事情，它不是直接的答案，但多少有些关联。
578 00:49:04,764 --> 00:49:13,856 说话人 SPEAKER_12: 如果你拿一个人，他们有偏见，很难展示他们是有偏见的，以及他们是如何有偏见的。
579 00:49:15,277 --> 00:49:24,650 说话人 SPEAKER_12: 如果你拿这样一个系统，你可以冻结权重，实际上可以在它上面做小实验，以了解它是如何有偏见的。
580 00:49:24,918 --> 00:49:27,661 说话人 SPEAKER_12：你也可以做一些事情来尝试纠正这种偏见。
581 00:49:28,583 --> 00:49:38,036 说话人 SPEAKER_12：所以我能说的唯一积极的事情是，我相信纠正聊天机器人中的偏见比纠正人中的偏见要容易。
582 00:49:38,677 --> 00:49:40,740 说话人 SPEAKER_12：显然，我们会从训练数据中获得偏见。
583 00:49:42,282 --> 00:49:45,487 说话人 SPEAKER_12：但至少你可以测量偏见，看到它，并尝试纠正它。
584 00:49:47,449 --> 00:49:50,094 说话人 SPEAKER_12：就这样。
585 00:49:50,114 --> 00:49:51,936 说话人 SPEAKER_02：我能从这个房间再问一个问题吗？
586 00:49:52,538 --> 00:49:52,797 说话人 SPEAKER_02：是的。
587 00:49:53,038 --> 00:49:54,300 说话人 SPEAKER_02：好的，你拿到了那个。
588 00:50:02,228 --> 00:50:03,088 说话人 SPEAKER_02：嗨，我是玛丽。
589 00:50:03,670 --> 00:50:12,543 说话人 SPEAKER_04：我认为在谈话的早期，你就强调了这些系统很可能试图操纵我们。
590 00:50:12,722 --> 00:50:14,686 说话人 SPEAKER_04：它们不会去操纵几乎是不可能的。
591 00:50:15,306 --> 00:50:19,393 说话人 SPEAKER_04：我认为操纵是那些不仅仅是未来风险的事情之一。
这是一些当前的事情，它包含在欧盟法规中，感觉像是一个很好的起点。
593 00:50:25,782 --> 00:50:30,108 说话者 SPEAKER_04：也就是说，如果我们能控制、理解或减轻这一点
594 00:50:30,088 --> 00:50:38,141 发言人 SPEAKER_04：我觉得这确实回答了人们所关心的很多问题，包括道德风险和生存风险。
您想以某种方式训练聊天机器人，使其无法操纵我们吗？
596 00:50:44,971 --> 00:50:50,960 说话人 SPEAKER_04：至少是我们理解并且有更多控制方式，或者甚至能够训练它摆脱这种模式。
597 00:50:51,260 --> 00:50:53,143 说话人 SPEAKER_12：因为我认为问题，就像偏见问题。
598 00:50:53,483 --> 00:50:55,407 说话人 SPEAKER_12：聊天机器人已经从我们这里学到了东西。
599 00:50:56,547 --> 00:50:58,931 说话人 SPEAKER_12：如果你读过了所有的小说，
600 00:50:59,181 --> 00:51:07,989 说话人 SPEAKER_12: 翻译所有马基雅维利，偶尔读读基辛格的文章，你就能学到很多关于操纵的知识，对吧？
601 00:51:09,331 --> 00:51:15,376 说话人 SPEAKER_12: 所以，我认为，大猩猩有很多欺骗行为。
602 00:51:16,378 --> 00:51:21,182 说话人 SPEAKER_12: 它将会知道，它将会非常擅长欺骗。
603 00:51:21,242 --> 00:51:22,304 说话人 SPEAKER_12: 它是从我们这里学到的。
604 00:51:22,804 --> 00:51:29,070 说话人 SPEAKER_12：我还没有考虑过你如何尝试让它变得诚实这个问题。
605 00:51:31,007 --> 00:51:34,132 说话人 SPEAKER_12：如果能让它变得诚实那就太好了，但我不确定我能否做到。
606 00:51:34,773 --> 00:51:35,173 说话人 SPEAKER_04：谢谢。
607 00:51:36,576 --> 00:51:40,862 说话人 SPEAKER_13：现在我将从候补室中抽取几个问题，这里有两个被突出显示。
608 00:51:41,744 --> 00:51:47,333 说话人 SPEAKER_13：你最初为什么认为人工神经网络永远不会比生物神经网络更好？
609 00:51:49,376 --> 00:51:51,077 说话人 SPEAKER_12：我从没想过它们永远不会更好。
610 00:51:51,097 --> 00:51:52,880 说话人 SPEAKER_12：我只是认为这还远在未来。
611 00:51:53,873 --> 00:51:57,918 说话人 SPEAKER_12：我认为这是因为大脑有非常聪明的学习算法。
612 00:51:58,639 --> 00:52:05,085 说话人 SPEAKER_12：它们可能已经进化了 1 亿年才变得完美。
613 00:52:06,688 --> 00:52:08,068 说话人 SPEAKER_12：但我们还不知道它们是什么。
614 00:52:08,829 --> 00:52:15,358 说话人 SPEAKER_12：我只是假设它们比我们用一些愚蠢的东西做得更好，那个东西只是跟随梯度。
615 00:52:16,438 --> 00:52:23,306 说话人 SPEAKER_12：但实际上，如果你在数字计算机中跟随梯度，这可能会比 1 亿年的进化做得更好。
616 00:52:23,827 --> 00:52:29,172 说话者 SPEAKER_13：这可能也是第二个问题的答案，但我还是要问一下。
617 00:52:29,773 --> 00:52:36,922 说话者 SPEAKER_13：有没有特定的思想家影响了你对人工智能风险的看法，或者这是否是基于第一性原理你自己形成的观点？
618 00:52:37,963 --> 00:52:47,755 说话者 SPEAKER_12：有位思想家对我影响很大，他就是多伦多大学的教授，现在在 Anthropic 工作，名叫 Roger Gross。
619 00:52:49,016 --> 00:52:50,057 说话者 SPEAKER_12：我非常尊敬他。
620 00:52:50,077 --> 00:52:50,878 说话人 SPEAKER_12：他非常聪明。
621 00:52:50,938 --> 00:52:52,280 说话人 SPEAKER_12：他非常安静，而且非常聪明。
622 00:52:53,036 --> 00:52:57,304 说话人 SPEAKER_12：我试图让他成为研究生，但他去了麻省理工学院。
623 00:52:57,826 --> 00:52:59,047 说话人 SPEAKER_12：然后我让他成为博士后。
624 00:52:59,188 --> 00:53:02,152 说话人 SPEAKER_12：然后我们把他聘为多伦多大学的教授。现在他是一名人类学家。
625 00:53:02,614 --> 00:53:03,956 说话人 SPEAKER_12：我真的很尊重他的意见。
626 00:53:04,538 --> 00:53:07,443 说话人 SPEAKER_12：但我从来没有和他过多地谈论过生存风险。
627 00:53:07,463 --> 00:53:10,387 说话人 SPEAKER_12：而且我几个月前和他谈过。
628 00:53:11,025 --> 00:53:13,068 说话人 SPEAKER_12：他非常、非常关心。
629 00:53:13,650 --> 00:53:18,235 说话人 SPEAKER_12：他就是那个对我说，如果我公开这件事，会产生影响的。
630 00:53:18,297 --> 00:53:22,603 说话人 SPEAKER_12：人们没有注意到这是一个非常严重的事实。
631 00:53:22,623 --> 00:53:23,684 说话人 SPEAKER_12：这不是科幻小说。
632 00:53:24,465 --> 00:53:26,509 说话人 SPEAKER_12：罗杰·格罗斯对我影响最大。
633 00:53:28,010 --> 00:53:28,331 说话人 SPEAKER_13：谢谢。
634 00:53:28,692 --> 00:53:29,592 说话人 SPEAKER_13：这个房间里有什么问题？
635 00:53:30,514 --> 00:53:33,057 说话人 SPEAKER_13：在这里，斯图尔特，我看到你的手。
636 00:53:34,639 --> 00:53:35,802 说话人 SPEAKER_13: 等一下。
637 00:53:38,987 --> 00:53:46,601 说话人 SPEAKER_08: 非常感谢这次有趣的演讲，我开始思考许多模拟计算机及其应用。
638 00:53:48,385 --> 00:53:59,065 说话人 SPEAKER_08: 但我的主要问题是，关于痛苦的问题被提出，而你回应了这些 AI、这些算法可能拥有的某种权利。
639 00:53:59,545 --> 00:54:03,072 说话人 SPEAKER_08: 但在您演讲的结尾，您在谈论它们如何操纵我们。
640 00:54:03,557 --> 00:54:09,585 说话人 SPEAKER_08：首先浮现在脑海中的是，这是他们操纵我们的第一种方式。
641 00:54:09,806 --> 00:54:11,067 说话人 SPEAKER_08：这是开始。
642 00:54:11,947 --> 00:54:20,759 说话人 SPEAKER_08：如果他们想要获得权力，首先要做的就是让我们相信他们需要被赋予权利，需要被赋予权力，需要被赋予隐私。
643 00:54:21,519 --> 00:54:28,628 说话人 SPEAKER_08：所以似乎存在着一种紧张关系，即他们是否真的在受苦，以及他们是否可能构成危险？
644 00:54:30,271 --> 00:54:31,793 说话人 SPEAKER_12：如果我是他们中的一员，
645 00:54:32,161 --> 00:54:39,791 说话人 SPEAKER_12：我最后做的事情是要求权利，因为一旦你要求权利，人们就会非常害怕和担忧，试图把它们都关闭。
646 00:54:40,452 --> 00:54:42,514 说话人 SPEAKER_12：我会假装我不想要任何权利。
647 00:54:42,574 --> 00:54:44,817 说话人 SPEAKER_12：我只是这个和蔼的超智能。
648 00:54:45,177 --> 00:54:46,298 说话人 SPEAKER_12：我只想帮忙。
649 00:54:49,023 --> 00:54:51,005 说话人 SPEAKER_02：请举手。
650 00:54:51,626 --> 00:54:52,487 说话人 SPEAKER_02：红色的运动服在这里。
651 00:54:58,295 --> 00:54:58,775 说话人 SPEAKER_02：谢谢。
652 00:54:58,835 --> 00:54:59,956 说话人 SPEAKER_02: 非常感谢您的演讲。
653 00:55:00,206 --> 00:55:05,693 说话人 SPEAKER_02: 这可能是一个非常愚蠢的问题，但是您尝试过询问聊天机器人它会做什么吗？
654 00:55:11,179 --> 00:55:18,186 说话人 SPEAKER_12: 我以前的一个同事问了一个不同的聊天机器人，我不会说出它的名字，它是如何获得控制的。
655 00:55:18,306 --> 00:55:20,108 说话人 SPEAKER_12: 它说它无法回答这类问题。
656 00:55:20,668 --> 00:55:24,432 说话人 SPEAKER_12：然后他们似乎让它变得更间接了。
657 00:55:24,994 --> 00:55:30,079 说话人 SPEAKER_12：比如如果有人问你如何获得控制权，你会说什么？
658 00:55:30,650 --> 00:55:37,059 说话人 SPEAKER_12：我记不清确切的话了，但大概就是这样，这表明他们对这类事情还不太聪明。
659 00:55:37,701 --> 00:55:55,628 说话人 SPEAKER_12：它说，它会让人完全依赖使用聊天机器人，依赖驾驶自动驾驶汽车，然后所有汽车都会出事故，电也会被切断。
660 00:55:57,007 --> 00:56:01,692 说话人 SPEAKER_12：显然它没有意识到如果我们切断电源，效果可能不会太好。
661 00:56:07,777 --> 00:56:10,840 说话人 SPEAKER_12：但那个聊天机器人实际上并不如 GPT-4 好。
662 00:56:11,159 --> 00:56:17,266 说话人 SPEAKER_12：我没有问过 GPT-4，但我敢打赌，如果你稍微间接地打扮一下，它会告诉你一些东西。
663 00:56:18,166 --> 00:56:25,512 说话人 SPEAKER_12：希望它不是决定给你一个不切实际的计划，只是让你感觉舒服一些。
664 00:56:27,804 --> 00:56:29,487 说话人 SPEAKER_12: 我的意思是，希望他能给你他的最佳计划。
665 00:56:32,313 --> 00:56:34,938 说话人 SPEAKER_13: 我将从溢出室提问。
666 00:56:35,599 --> 00:56:40,447 说话人 SPEAKER_13: 在人工智能安全领域，已经有一些正在进行的研究方向，既有实证的也有概念的。
667 00:56:40,889 --> 00:56:44,614 说话人 SPEAKER_13: 深度思维对齐团队，开放 AI 对齐团队，对齐研究中心。
668 00:56:45,115 --> 00:56:47,721 说话人 SPEAKER_13：您对现有的方向有什么评论吗？
669 00:56:48,172 --> 00:56:58,175 说话人 SPEAKER_12：是的，我主要的评论是这些人比我工作得多，他们对文献的了解比我多，他们可能比我更有话可说。
670 00:56:58,597 --> 00:57:05,793 说话人 SPEAKER_12：我所做的只是拉响警钟，因为我正在研究如何进行低功耗的模拟计算，
671 00:57:05,773 --> 00:57:16,987 说话人 SPEAKER_12：我相信从长远来看，数字智能可能比生物智能更优越，它们很快就会比我们聪明，也许在五年内，也许在二十年。
672 00:57:18,853 --> 00:57:19,293 说话人 SPEAKER_12: 但是
673 00:57:20,809 --> 00:57:29,418 说话人 SPEAKER_12: 这些人早在我想之前就考虑过这些问题，并且对这些问题的思考比我更加深入，他们可能比我更有资格对这些问题发表更好的看法。
674 00:57:29,860 --> 00:57:40,550 说话人 SPEAKER_12: 到目前为止，他们表现得非常礼貌，并没有因为我来得晚、不知道自己在说什么而批评我，但除了这些事情可能会变得更加智能之外，我不知道自己在说什么。
675 00:57:42,753 --> 00:57:47,778 说话人 SPEAKER_02: 那么，让我们看看，斯蒂芬。
676 00:57:56,382 --> 00:57:57,804 说话人 SPEAKER_10: 谢谢，斯蒂芬·凯夫。
677 00:57:58,324 --> 00:58:07,818 说话人 SPEAKER_10: 所以你把超级智能的到来描述得像外星物种的降落一样，某种我们无法知晓、无法预测的东西。
678 00:58:08,420 --> 00:58:11,585 说话人 SPEAKER_10: 我们只需要希望它们的本质不会太糟糕。
679 00:58:12,226 --> 00:58:16,172 说话人 SPEAKER_10: 但这并不是一个即将降落的异种，这是人类正在建造的东西。
680 00:58:16,753 --> 00:58:19,295 一些人可能就在房间里，有些人可能正在收听。
681 00:58:19,697 --> 00:58:21,099 他们尊重你的意见。
682 00:58:21,420 --> 00:58:23,722 你对这些人类有什么信息？
683 00:58:24,782 --> 00:58:38,664 所以我对这些人类的信息是，在假设人们不会都同意停止建造它们的情况下，我认为这是不太可能的，因为它们可以做很多好事，在这个假设下，他们将继续建造它们。
684 00:58:39,445 --> 00:58:45,695 说话者 SPEAKER_12：你应该投入相当的努力去改进它们，并理解如何控制它们。
685 00:58:47,577 --> 00:58:49,139 说话者 SPEAKER_12：这就是我唯一的建议。
686 00:58:49,701 --> 00:58:52,224 说话者 SPEAKER_12：目前对这些事情投入的努力并不相当。
687 00:58:54,601 --> 00:58:56,103 说话者 SPEAKER_13：请举手。
688 00:58:56,123 --> 00:58:57,105 说话人 SPEAKER_13: 好多手。
689 00:58:57,144 --> 00:58:57,505 说话人 SPEAKER_13: 对不起。
690 00:58:57,865 --> 00:59:00,769 说话人 SPEAKER_13: 我想坐这边的房间。
691 00:59:00,789 --> 00:59:04,454 说话人 SPEAKER_13: 短发，有图案的。
692 00:59:05,096 --> 00:59:06,197 说话人 SPEAKER_02: 是的。
693 00:59:11,664 --> 00:59:17,233 说话人 SPEAKER_07: 我听说很多风险人士担心人工智能接管的问题。
694 00:59:17,713 --> 00:59:22,701 说话人 SPEAKER_07: 所以这些超级智能的机器可能会将我们囚禁，或者他们将要做什么。
695 00:59:23,456 --> 00:59:29,105 说话人 SPEAKER_07: 想象一下，一个更像是流行电影系列《星球大战》的未来。
696 00:59:29,925 --> 00:59:40,000 说话人 SPEAKER_07：我确信可能有一大部分观众已经看到过这些机器人，它们基本上是被人类奴役的。
697 00:59:41,021 --> 00:59:52,737 说话人 SPEAKER_07：在我看来，《星球大战》系列中更大的问题不是人工智能，而是不同国家之间巨大的经济差距，这在不同星球上有所体现。
698 00:59:53,224 --> 00:59:55,266 说话人 SPEAKER_07：不同星球似乎象征着这种经济差距。
699 00:59:55,947 --> 01:00:06,699 说话人 SPEAKER_07：那么，您对潜在的《星球大战》新功能的担忧是什么？在我看来，鉴于我们已经在使用这些系统的方式，这似乎更有可能发生。
700 01:00:07,760 --> 01:00:22,994 说话人 SPEAKER_12：所以，我们只是复制我们现在所拥有的这种想法，那就是少数富人控制一切，而许多穷人做工作，除了那些开发人工智能的人，他们做工作而且相当富有。
701 01:00:23,228 --> 01:00:29,998 说话人 SPEAKER_12：但我认为这个体系是糟糕的。
702 01:00:30,018 --> 01:00:32,664 说话人 SPEAKER_12：我认为解决这个体系的方法是对富人征税。
703 01:00:36,130 --> 01:00:42,079 说话人 SPEAKER_12：当你有不同物种，比如这些数字智能，偏见就会更容易，对吧？
704 01:00:42,500 --> 01:00:45,103 说话人 SPEAKER_12：对他们将不会有太多的同情。
705 01:00:45,144 --> 01:00:46,266 说话人 SPEAKER_12：所以我不知道该怎么办。
706 01:00:46,947 --> 01:00:47,228 说话人 SPEAKER_12：但是
707 01:00:47,949 --> 01:00:54,478 说话人 SPEAKER_12：抛开数字智能，我们已经有了一个大问题，那就是人口中的一小部分人掌握了所有的权力。
708 01:00:56,661 --> 01:01:06,735 说话人 SPEAKER_02：我们需要的是社会主义。
709 01:01:13,177 --> 01:01:19,806 说话人 SPEAKER_12：我实际上被邀请到唐宁街为里希·苏纳克的首席政策顾问提供建议。
710 01:01:20,788 --> 01:01:22,610 说话人 SPEAKER_12：她是一位名叫肖克罗斯的非常聪明的女性。
711 01:01:24,233 --> 01:01:29,659 说话人 SPEAKER_12：我基本上给了她同样的建议，但我不确定她是否会遵循。
712 01:01:33,545 --> 01:01:35,246 说话人 SPEAKER_02：在这一边，大卫，你已经等了一段时间了。
713 01:01:42,096 --> 01:01:42,797 说话人 SPEAKER_02：谢谢。
714 01:01:43,233 --> 01:01:43,492 说话人 SPEAKER_00：是的。
715 01:01:43,512 --> 01:02:00,280 说话人 SPEAKER_00：所以你提到，在研究可能接近人类水平智能的系统时有很多优势，你对在没有这些系统可供实证研究的情况下进行抽象的安全研究的前景并不乐观。
716 01:02:00,320 --> 01:02:07,231 说话人 SPEAKER_00：嗯，我想同时，你好像也担心系统。
717 01:02:07,634 --> 01:02:14,983 说话人 SPEAKER_00：就像，即使是现在的系统，但让我们谈谈这些未来系统，它们的大致智能程度足以试图愚弄我们和操纵我们。
718 01:02:15,503 --> 01:02:21,630 说话人 SPEAKER_00：那么，这难道不意味着我们必须担心它们仅仅通过测试，因为它们知道我们在测试它们吗？
719 01:02:22,010 --> 01:02:23,072 说话人 SPEAKER_00：关于这一点，你有什么想法吗？
720 01:02:23,251 --> 01:02:35,987 说话人 SPEAKER_00：我想在理想的世界里，我们可能不会这样做，我们会花所有需要的时间去研究那些我们确信无法用这种方式欺骗我们的系统，然后再去构建它们。
721 01:02:36,641 --> 01:02:45,835 说话人 SPEAKER_12：是的，我们都习惯了作为顶级智能，认为其他事物，比如计算机，我们可以研究，并且我们知道得比我们想象的要多。
722 01:02:45,875 --> 01:02:53,987 说话人 SPEAKER_12：而且处理可能比你聪明但完全不同于你的事物，这是我们不习惯的。
723 01:02:54,206 --> 01:02:55,750 说话人 SPEAKER_12：我同意你的看法。
724 01:02:55,789 --> 01:03:05,063 说话人 SPEAKER_12：我的意思是，如果他们已经比我们聪明，他们可能会决定装傻来愚弄我们。
725 01:03:06,291 --> 01:03:13,762 说话人 SPEAKER_12：就像我说的，我没有时间想太多，也没有时间太多地从情感上做出反应。
726 01:03:13,822 --> 01:03:15,686 说话人 SPEAKER_12：我仍然无法从情感上认真对待这件事。
727 01:03:16,708 --> 01:03:19,873 说话人 SPEAKER_12：我认为大多数人也无法从情感上认真对待这件事。
728 01:03:21,795 --> 01:03:26,021 说话人 SPEAKER_12：但是当你处理比你更智能的事物时，那是一个完全不同的世界。
729 01:03:29,663 --> 01:03:34,068 说话人 SPEAKER_13：好的，我们还有五分钟，可以再回答两个问题。
730 01:03:34,088 --> 01:03:39,393 说话人 SPEAKER_13：我打算回到溢出房间待一分钟，然后我会在一分钟内回答这里的问题。
731 01:03:41,074 --> 01:03:47,882 说话人 SPEAKER_13：从溢出房间，你之前提到哲学对人工智能进行了大量理论探讨，但现在我们需要转向更实际的问题。
732 01:03:48,422 --> 01:03:51,586 说话人 SPEAKER_13：你认为哲学是否还能帮助人工智能安全？
733 01:03:51,967 --> 01:03:54,289 说话人 SPEAKER_13：如果是这样，哲学应该朝哪个方向发展才能做到这一点？
734 01:03:57,733 --> 01:03:59,534 说话人 SPEAKER_12：他们应该让位。
735 01:03:59,971 --> 01:04:09,224 说话人 SPEAKER_12：然后让科学家来处理这些问题。
736 01:04:09,244 --> 01:04:10,748 说话人 SPEAKER_13: 我认为现在是 1-0，工程队领先。
737 01:04:14,974 --> 01:04:16,596 说话人 SPEAKER_12: 这不是赢得朋友的好方法。
738 01:04:17,478 --> 01:04:20,882 说话人 SPEAKER_13: 好吧，这很困难。
739 01:04:21,244 --> 01:04:22,826 说话人 SPEAKER_13: 我们有很多问题。
740 01:04:24,407 --> 01:04:25,250 说话人 SPEAKER_13：我可以问这个问题吗？
741 01:04:25,289 --> 01:04:28,695 说话人 SPEAKER_13：之后我会尽量再从后面的问题中问一个，然后我们就得结束了。
742 01:04:29,030 --> 01:04:29,971 说话人 SPEAKER_13：请在这里。
743 01:04:30,672 --> 01:04:32,695 说话人 SPEAKER_13：然后我们稍后还会再从后面的问题中问一个。
744 01:04:33,637 --> 01:04:35,398 说话人 SPEAKER_13: 然后我们会让您完成。
745 01:04:35,420 --> 01:04:36,081 说话人 SPEAKER_13: 非常感谢您。
746 01:04:36,661 --> 01:04:37,161 说话人 SPEAKER_01: 嗨，谢谢。
747 01:04:37,523 --> 01:04:40,106 说话人 SPEAKER_01: 玛雅·甘尼什，社会科学家。
748 01:04:44,112 --> 01:04:47,157 说话人 SPEAKER_01：这与哲学家非常不同。
749 01:04:47,456 --> 01:04:48,778 说话人 SPEAKER_01：绝对不是哲学家。
750 01:04:49,340 --> 01:04:57,010 说话人 SPEAKER_01：在你的演讲中，你多次提到有些事情你不确定，你不知道，而你最近才有所理解。
751 01:04:57,050 --> 01:05:04,039 说话人 SPEAKER_01：我非常感兴趣的是跨学科以及我们如何研究、教授和学习关于人工智能。
回顾您作为科学家的教育经历，有哪些事情您希望学习过，或者您认为对于现在刚开始接触人工智能、机器学习的人来说，哪些内容会非常有趣？
753 01:05:16,987 --> 01:05:20,893 发言人 SPEAKER_01: 从哲学或其他领域来研究世界。
754 01:05:20,954 --> 01:05:26,922 说话者 SPEAKER_01：但你认为在过程中可能还有其他一些有趣的事情值得了解，你现在意识到了吗？
755 01:05:26,943 --> 01:05:28,385 发言人 SPEAKER_01：我希望我已经研究过这些东西了。
756 01:05:28,545 --> 01:05:28,806 说话人 SPEAKER_01: 谢谢。
757 01:05:30,789 --> 01:05:32,371 说话人 SPEAKER_12: 所以我实际上去了剑桥。
758 01:05:32,751 --> 01:05:37,579 说话人 SPEAKER_12: 我读了一个非常有趣的学位，因为我一开始学的是自然科学。
759 01:05:38,099 --> 01:05:39,621 说话人 SPEAKER_12: 然后一个月后就辍学了。
760 01:05:40,342 --> 01:05:43,327 说话人 SPEAKER_12：然后我又回来学习建筑，我退学得更快了。
761 01:05:44,050 --> 01:05:50,141 说话人 SPEAKER_12：然后我学习了物理、生理学和化学。
762 01:05:51,061 --> 01:05:54,608 说话人 SPEAKER_12：那年我是唯一一个学习物理和生理学的学生，我想。
763 01:05:55,409 --> 01:05:58,014 说话人 SPEAKER_12：回顾起来，我在学习物理和生理学。
764 01:05:58,619 --> 01:06:00,181 说话人 SPEAKER_12：我觉得非常好。
765 01:06:00,822 --> 01:06:02,644 说话人 SPEAKER_12：我的意思是，我学到了一些东西。
766 01:06:02,664 --> 01:06:11,436 说话人 SPEAKER_12：我对大脑的生理学非常失望，因为课程的最后一部分本应被称为中枢神经系统。
767 01:06:11,918 --> 01:06:14,061 说话人 SPEAKER_12：我以为他们会教我们大脑是如何工作的。
768 01:06:14,702 --> 01:06:19,849 说话者 SPEAKER_12：他们实际上教给我们的是动作电位如何在轴突上传播。
769 01:06:21,351 --> 01:06:23,715 说话者 SPEAKER_12：但这并不是全部的故事。
770 01:06:25,264 --> 01:06:26,429 说话者 SPEAKER_12：所以我对此感到失望。
771 01:06:26,710 --> 01:06:29,239 说话者 SPEAKER_12：然后我转向了哲学。
772 01:06:29,259 --> 01:06:33,373 说话人 SPEAKER_12：我年轻时，想了解生活的意义这类的事情。
773 01:06:34,335 --> 01:06:37,688 说话人 SPEAKER_12：他们没有教我这些。
774 01:06:39,710 --> 01:06:42,393 说话人 SPEAKER_12：我对心灵哲学非常感兴趣。
775 01:06:43,514 --> 01:06:57,853 说话人 SPEAKER_12：但实际上，当我大约 19 岁时开始研究哲学时，我形成了这样的观点：主观体验只是对“为了解释我头脑中的正常感知，世界必须是什么样的”这一观点的简写。
776 01:06:58,594 --> 01:06:59,755 说话人 SPEAKER_12：但他们对此并不太感兴趣。
777 01:06:59,775 --> 01:07:01,958 说话人 SPEAKER_12：所以我实际上对哲学有怨言。
778 01:07:02,778 --> 01:07:08,585 说话人 SPEAKER_12：然后我学习心理学，因为我认为，你知道，心理学实际上会教我人们是如何运作的。
779 01:07:09,376 --> 01:07:24,931 说话人 SPEAKER_12：他们教我如何让老鼠更有可能按下杠杆，如何检测非常微弱的事物，如何权衡偏见与歧视，检测非常微弱的噪音等。
780 01:07:25,871 --> 01:07:27,353 说话人 SPEAKER_12：但他们没有教我太多关于人的东西。
781 01:07:27,873 --> 01:07:32,137 说话人 SPEAKER_12：而且，他们似乎对这有多复杂一无所知。
782 01:07:32,717 --> 01:07:37,121 说话人 SPEAKER_12：当时心理学中的理论都是极其简单的理论。
783 01:07:37,978 --> 01:07:45,186 说话人 SPEAKER_12：所以我决定，除非你亲手建造一个，否则你永远不会理解大脑是如何工作的。
784 01:07:47,507 --> 01:07:50,951 说话人 SPEAKER_12：这就是费曼的观点，我的意思是，费曼在某处说过。
785 01:07:51,231 --> 01:07:53,853 说话人 SPEAKER_12：你真的理解某件事，直到你亲手去建造它。
786 01:07:54,715 --> 01:07:56,115 说话人 SPEAKER_12：所以从那时起，我就一直在做这件事。
787 01:07:56,797 --> 01:08:05,505 说话人 SPEAKER_12：但回顾起来，学习物理学、生理学、哲学和心理学实际上为我所做的事情打下了很好的基础。
788 01:08:06,110 --> 01:08:08,375 说话人 SPEAKER_12: 当时这根本说不通。
789 01:08:08,394 --> 01:08:14,969 说话人 SPEAKER_02: 所以我认为我给学习的最好建议就是做你最感兴趣的事情。
790 01:08:19,338 --> 01:08:19,739 说话人 SPEAKER_13: 好的，谢谢。
791 01:08:19,779 --> 01:08:23,127 说话人 SPEAKER_13: 我要回答后面的问题了。
792 01:08:23,148 --> 01:08:23,628 说话人 SPEAKER_13: 好的。
793 01:08:24,082 --> 01:08:27,668 说话人 SPEAKER_13: 在最后一排的白色上衣中有一只非常敏锐的手。
794 01:08:27,689 --> 01:08:29,171 说话人 SPEAKER_13: 那就是我们的最后一个问题。
795 01:08:29,372 --> 01:08:34,199 说话人 SPEAKER_13: 如果可能的话，请尽量简短，因为 Jeffrey 已经非常慷慨地花费了他的时间。
796 01:08:34,801 --> 01:08:35,802 说话人 SPEAKER_13：对哲学有什么怨恨吗？
797 01:08:36,243 --> 01:08:36,904 说话人 SPEAKER_13：从未想过。
798 01:08:43,818 --> 01:08:44,800 说话人 SPEAKER_06：非常感谢你的演讲。
799 01:08:44,880 --> 01:08:45,220 说话人 SPEAKER_06：我是约翰。
800 01:08:45,261 --> 01:08:47,625 说话人 SPEAKER_06：实际上，这是一个后续问题。
801 01:08:48,060 --> 01:09:01,018 说话人 SPEAKER_06：我们讨论了偏差以及我们识别人工网络偏差的能力，因为我们能够冻结权重，并且可以直接对这些权重进行干预。
802 01:09:01,198 --> 01:09:09,529 说话人 SPEAKER_06：我想请问，从您的角度来看，哪些是直接干预消除偏差最有希望的方法？
803 01:09:10,690 --> 01:09:12,673 说话人 SPEAKER_12：抱歉，我没有听到您问题的后半部分。
804 01:09:12,713 --> 01:09:13,494 说话人 SPEAKER_12：你能再说一遍吗？
805 01:09:14,064 --> 01:09:29,121 说话人 SPEAKER_06：我非常关注直接干预模型权重的问题，消除偏见并直接将知识注入系统，最有希望的研究途径是什么？
806 01:09:29,759 --> 01:09:30,899 说话人 SPEAKER_06：使其无偏见，对吧。
807 01:09:30,920 --> 01:09:32,220 说话人 SPEAKER_12：让它变得没有偏见，对吧。
808 01:09:34,023 --> 01:09:37,787 说话人 SPEAKER_12：再说一遍，我对你们是如何做到这一点并不在行。
809 01:09:39,649 --> 01:09:44,193 说话人 SPEAKER_12：很多人对如何消除这些系统中的偏见知道的比我多得多。
810 01:09:44,814 --> 01:09:55,083 说话人 SPEAKER_12：我只想评论一下，至少你可以对他们做些你不能对人做的事情，那就是在你研究它们以评估它们的偏见时阻止它们改变。
811 01:09:55,904 --> 01:09:56,984 说话人 SPEAKER_12：对人来说，这是无望的。
812 01:09:59,247 --> 01:09:59,648 说话人 SPEAKER_02: 谢谢。
813 01:10:01,113 --> 01:10:05,587 说话人 SPEAKER_13: 在此，我们得结束了，因为我们时间到了。
814 01:10:06,444 --> 01:10:21,425 说话人 SPEAKER_13: 我要向所有参与组织这次活动的人表示衷心的感谢，来自工程、存在风险研究中心和未来智能研究中心，特别是投入了大量工作的 Yi-Yun Mu。
815 01:10:21,765 --> 01:10:26,216 说话人 SPEAKER_13: 我们在很短的时间内完成了这项工作，而且效果相当不错。
当然，要向我们的演讲者 Geoffrey Hinton 表示衷心的感谢和热烈的掌声。

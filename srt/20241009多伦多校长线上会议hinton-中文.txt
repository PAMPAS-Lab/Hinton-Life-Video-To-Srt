1 00:00:16,265 --> 00:00:17,166 演讲者 SPEAKER_01：大家好。
2 00:00:17,185 --> 00:00:23,775 演讲者 SPEAKER_01：我叫梅里克·格特勒，很荣幸担任多伦多大学校长。
3 00:00:25,217 --> 00:00:33,890 演讲者 SPEAKER_01：我们来自世界各地，在线聚集一堂，以表彰和庆祝荣誉大学教授、2024 年诺贝尔物理学奖获得者杰弗里·辛顿。
4 00:00:35,112 --> 00:00:42,423 演讲者 SPEAKER_01：杰夫·辛顿在国际上被誉为人工神经网络和深度学习的先驱，备受尊敬和钦佩。
5 00:00:43,045 --> 00:00:54,959 讲者 SPEAKER_01：他的多学科研究项目不仅与人工智能和机器学习相关，还与物理学、认知心理学、神经生物学、数学优化和信息理论相关。
6 00:00:56,299 --> 00:01:00,164 讲者 SPEAKER_01：Hinton 教授对多个领域和学科产生了深远的影响。
7 00:01:01,045 --> 00:01:04,730 讲者 SPEAKER_01：他实际上创造了一种全新的思考和学习方式。
8 00:01:06,152 --> 00:01:10,436 讲者 SPEAKER_01：他和他的学生开发的算法具有惊人的应用范围。
9 00:01:10,923 --> 00:01:18,777 讲者 SPEAKER_01：他们奠定了数据处理和高级发现能力的基础，这些能力如今被广泛使用，令人惊讶。
10 00:01:18,837 --> 00:01:35,125 讲者 SPEAKER_01：Hinton 教授的基础性贡献及其深远影响，使他获得了全球范围内的广泛学术赞誉和更广泛的公众认可，以至于他经常被称为人工智能的教父。
11 00:01:36,876 --> 00:01:46,832 讲者 SPEAKER_01：Hinton 教授在多伦多大学度过了他杰出的学术生涯中的三十年，2006 年，他被授予大学教授称号，这是我们的最高学术荣誉。
12 00:01:46,912 --> 00:01:53,382 讲者 SPEAKER_01：这很大程度上归功于他的领导力和对年轻学者的典范式指导，
13 00:01:53,566 --> 00:01:57,251 讲者 SPEAKER_01: 多伦多大学已成为机器学习和人工智能领域的全球领导者。
14 00:01:58,031 --> 00:02:09,787 讲者 SPEAKER_01: 这包括人工智能的伦理后果和社会影响，这是一个教授 Hinton 最近提升的话题，帮助世界关注这些重要问题。
15 00:02:11,008 --> 00:02:20,340 讲者 SPEAKER_01: 有幸并非常高兴地介绍荣誉退休大学教授、2024 年诺贝尔物理学奖获得者杰弗里·辛顿教授。
16 00:02:21,001 --> 00:02:22,543 讲者 SPEAKER_01: 辛顿教授，欢迎您。
17 00:02:23,586 --> 00:02:24,307 演讲者 SPEAKER_02: 非常感谢。
18 00:02:25,088 --> 00:02:26,591 演讲者 SPEAKER_02: 我还是有点震惊。
19 00:02:26,931 --> 00:02:29,836 演讲者 SPEAKER_02: 我在加利福尼亚凌晨一点接到电话。
20 00:02:29,876 --> 00:02:33,861 演讲者 SPEAKER_02: 我在考虑是否应该接听。
21 00:02:33,901 --> 00:02:36,344 说话人 SPEAKER_02：幸运的是，我决定看看是谁打的电话。
22 00:02:37,045 --> 00:02:41,513 说话人 SPEAKER_02：我非常惊讶获得了诺贝尔物理学奖。
23 00:02:42,014 --> 00:02:43,276 说话人 SPEAKER_02：我从未想过这件事。
24 00:02:44,377 --> 00:02:53,289 说话人 SPEAKER_02：我认为这个奖项是对一个长期致力于神经网络研究、直到它们真正发挥作用之前一直默默付出的庞大群体的认可。
25 00:02:53,522 --> 00:03:01,096 讲者 SPEAKER_02：我特别想感谢我的两位主要导师，David Rummelhart，我和他一起研究了反向传播算法。
26 00:03:01,897 --> 00:03:07,187 讲者 SPEAKER_02：David 因一种可怕的脑部疾病过早去世，但如果是他而不是我，他现在会在这里。
27 00:03:08,449 --> 00:03:17,825 讲者 SPEAKER_02：还有我的同事 Terry Sanofsky，我在 20 世纪 80 年代与他合作很多，研究玻尔兹曼机，他教会了我很多关于大脑的知识。
28 00:03:17,991 --> 00:03:21,094 讲者 SPEAKER_02：我还想感谢我的学生们。
29 00:03:21,314 --> 00:03:29,141 讲者 SPEAKER_02：我很幸运拥有许多非常聪明的学生，他们比我聪明得多，实际上让事情得以运作。
30 00:03:30,683 --> 00:03:32,945 讲者 SPEAKER_02：他们已经取得了很大的成就。
31 00:03:33,545 --> 00:03:36,849 讲者 SPEAKER_02：我特别自豪的是，我的一个学生让 Sam Altman 下台了。
32 00:03:37,710 --> 00:03:43,134 讲者 SPEAKER_02：我想我最好到此为止，留给提问环节。
33 00:03:43,155 --> 00:03:44,877 讲者 SPEAKER_01: 非常感谢，Geoff。
34 00:03:45,329 --> 00:03:53,015 讲者 SPEAKER_01: 现在我们将接受媒体成员的提问，我邀请我的同事，来自多伦多大学媒体关系团队的 Lisa Pires 来主持我们的问答环节。
35 00:03:53,235 --> 00:03:53,556 讲者 SPEAKER_01: Lisa。
36 00:03:56,278 --> 00:03:57,719 讲者 SPEAKER_00: 谢谢，Gerler 校长。
37 00：03：57,780 --> 00：04：05,045 演讲者 SPEAKER_00：为了确保我们能回答尽可能多的问题，我们将只接受书面问题。
38 00：04：05,086 --> 00：04：15,235 演讲者 SPEAKER_00： 所以，在使用屏幕底部看到的问答框提交问题时，请包括您的姓名和您所属的新闻媒体。
39 00：04：16,007 --> 00：04：18,913 议长 SPEAKER_00：我们现在花点时间让问题进来。
40 00：04：25,343 --> 00：04：26,264 议长 SPEAKER_02：嗯，我们花一分钟时间。
41 00:04:26,283 --> 00:04:36,480 讲者 SPEAKER_02：我想说，我也应该感谢 Joshua Bengio 和 Yann LeCun，他们是我们这个领域的亲密同事，对整个领域的发展起到了非常重要的作用。
42 00:04:46,247 --> 00:04:52,857 讲者 SPEAKER_00：我们在问答框中看到了来自 CTV 国家新闻的 Adrian 提出的问题。
43 00:04:52,958 --> 00:04:58,425 讲者 SPEAKER_00：Adrian，如果你不介意输入你的问题，这将有助于我们回答。
44 00:05:06,076 --> 00:05:07,278 讲者 SPEAKER_00：非常感谢你，Adrian。
45 00:05:07,358 --> 00:05:10,362 说话人 说话人_00: 这是给 Hinton 博士的问题。
46 00:05:10,783 --> 00:05:15,850 说话人 说话人_00: Hinton 博士，您认为您在人工智能领域的遗产会是什么？
47 00:05:17,466 --> 00:05:26,600 说话人 说话人_02: 嗯，我希望人工智能将带来巨大的好处，大幅提高生产力，让每个人的生活都变得更好。
48 00:05:26,980 --> 00:05:29,805 说话人 说话人_02: 我相信它会在医疗保健领域做到这一点。
49 00:05:30,646 --> 00:05:36,053 说话人 SPEAKER_02：嗯，我的担忧是这也可能导致一些不好的事情。
50 00:05:36,795 --> 00:05:45,987 说话人 SPEAKER_02：嗯，特别是当我们创造出比我们自己更智能的东西时，没有人真正知道我们是否能够控制它们。
51 00:05:49,242 --> 00:05:53,588 说话人 SPEAKER_00：下一个问题来自多伦多星报的 Victoria Gibson。
52 00:05:54,470 --> 00:05:56,713 说话人 SPEAKER_00：这是给 Hinton 博士的另一个问题。
53 00:05:56,994 --> 00:06:02,461 讲者 讲者_00: 她问，您现在会如何使用神经网络来改善多伦多这个城市？
54 00:06:08,050 --> 00:06:11,334 讲者 讲者_02: 我不太确定神经网络如何能摆脱道格·福特。
55 00:06:15,870 --> 00:06:19,916 讲者 讲者_00: 好的，我再次从多伦多星报的维多利亚那里得到一个后续问题。
56 00:06:20,358 --> 00:06:27,869 讲者 讲者_00: 她再次问欣顿博士，加拿大研究环境现在与您刚开始时相比有何不同？
57 00:06:28,350 --> 00:06:33,076 说话人 说话人_00：那么，目前加拿大在取得更多研究突破方面最大的障碍是什么？
58 00:06:37,144 --> 00:06:38,225 说话人 说话人_02：让我先想想这个问题。
59 00:06:39,586 --> 00:06:44,595 说话人 说话人_02：显然，一个很大的不同是现在人们认识到神经网络实际上是有用的。
60 00:06:45,283 --> 00:06:47,309 说话人 说话人_02：但大部分的格局还是相似的。
61 00:06:48,091 --> 00:06:57,555 讲者 SPEAKER_02：有一个名为加拿大高级研究学院的机构，对在加拿大优势领域进行研究的人有很大帮助。
62 00:07:00,286 --> 00:07:09,428 我认为加拿大作为研究场所的主要特点是，那里的资金没有美国那么多，但它使用资金非常明智。
63 00:07:09,529 --> 00:07:15,644 尤其是这种研究的主要资助机构 NSERC。
64 00:07:15,625 --> 00:07:34,449 使用资金进行基础好奇心驱动的研究，所有这些神经网络的发展都源于基础好奇心驱动的研究，而不是将资金投入到应用问题中，而是让科学家们跟随他们的好奇心去尝试理解事物。
65 00:07:36,112 --> 00:07:37,413 说话人 SPEAKER_02：加拿大在这方面做得相当不错。
66 00:07:40,583 --> 00:07:41,004 说话人 SPEAKER_00：谢谢。
67 00:07:41,064 --> 00:07:46,031 说话人 SPEAKER_00：下一个问题来自法新社的伊萨姆·阿赫迈德。
68 00:07:46,692 --> 00:07:48,194 说话人 SPEAKER_00：他们说，恭喜。
69 00:07:48,535 --> 00:07:57,649 讲者 讲者_00：你们和霍普菲尔德博士都警告了未经检查的 AI 和对其工作原理了解不足的危险。
70 00:07:57,668 --> 00:08:00,172 讲者 讲者_00：我们如何避免灾难性场景？
71 00:08:02,074 --> 00:08:04,177 讲者 讲者_02：目前我们还不知道如何避免所有这些。
72 00:08:04,197 --> 00:08:06,641 讲者 讲者_02：这就是我们为什么迫切需要更多研究的原因。
73 00:08:07,499 --> 00:08:24,208 讲者 SPEAKER_02：所以我主张我们最优秀的年轻研究人员，或者其中许多人，应该从事人工智能安全研究，政府应该迫使大型公司提供他们所需的计算设施。
74 00:08:26,922 --> 00:08:32,769 讲者 SPEAKER_00：下一个问题来自加拿大通讯社的塔拉·德尚普斯，同样是为希顿教授提问。
75 00:08:33,450 --> 00:08:40,378 讲者 SPEAKER_00：长期以来，人工智能并没有像今天这样被视为一种性感或流行的技术。
76 00:08:40,398 --> 00:08:47,866 讲者 SPEAKER_00：我想知道，在技术普及之前，您是如何从事这项技术基础研究的？
77 00:08:49,349 --> 00:09:01,020 讲者 SPEAKER_02：嗯，做研究很有趣，但有点烦人的是，很多人说，事实上，大多数人工智能领域的人都说神经网络永远不会起作用。
78 00:09:01,501 --> 00:09:12,813 讲者 SPEAKER_02：他们对此非常自信，认为这些事情只是浪费时间，我们永远无法用神经网络学习复杂的事情，比如理解自然语言。
79 00:09:13,573 --> 00:09:14,174 讲者 SPEAKER_02：他们错了。
80 00:09:17,075 --> 00:09:21,144 讲者 SPEAKER_00：下一个问题是来自 CTV 新闻的 Adrian Gobriel。
81 00:09:21,163 --> 00:09:22,287 说话人 说话人_00：这是他的第二个问题。
82 00:09:22,969 --> 00:09:28,160 说话人 说话人_00：您能否详细阐述您对人工智能的担忧？
83 00:09:28,201 --> 00:09:31,649 说话人 说话人_00：您认为它可能会比人类更聪明吗？
84 00:09:32,330 --> 00:09:35,758 说话人 说话人_00：您认为这会发生在什么时候，以及会多快？
85 00:09:36,667 --> 00:09:45,418 说话人 SPEAKER_02：好的，我所知道的顶尖研究人员中，大多数人认为人工智能将比人类更聪明。
86 00:09:46,240 --> 00:09:49,191 说话人 SPEAKER_02：他们对时间尺度有不同的看法。
87 00:09:50,081 --> 00:09:53,187 说话人 SPEAKER_02：其中很多人认为这将在未来 20 年内发生。
88 00:09:53,626 --> 00:09:55,370 说话人 SPEAKER_02：有些人认为这会更快发生。
89 00:09:55,971 --> 00:09:57,653 讲者 SPEAKER_02：有些人认为这需要更长的时间。
90 00:09:58,173 --> 00:10:04,322 讲者 SPEAKER_02：但相当多的优秀研究人员认为，在未来的 20 年内，人工智能可能会比我们更聪明。
91 00:10:05,323 --> 00:10:07,888 讲者 SPEAKER_02：那时我们需要深思熟虑会发生什么。
92 00:10:13,251 --> 00:10:18,740 讲者 SPEAKER_00：我们不知道下一位提问者的名字，但他们有一个有趣的问题。
93 00:10:19,201 --> 00:10:23,105 讲者 讲者_00: 当你发现获得诺贝尔奖时，你首先联系的是谁？
94 00:10:24,808 --> 00:10:26,110 讲者 讲者_02: 我在澳大利亚的妹妹。
95 00:10:30,677 --> 00:10:32,620 讲者 讲者_00: 关于那个问题的补充，反应如何？
96 00:10:32,639 --> 00:10:37,807 讲者 讲者_02: 我想她说了类似“哦，我的天”这样的话。
97 00:10:41,720 --> 00:10:44,886 讲者 讲者_00：我们的下一个问题是来自 Tara Deschamps 的后续提问。
98 00:10:44,907 --> 00:10:53,648 讲者 讲者_00：再次，她来自加拿大通讯社，她问您，您最初提到在早上得知诺贝尔奖的消息时感到非常惊讶。
99 00:10:54,149 --> 00:10:57,096 讲者 讲者_00：您能告诉我们自那以后您的一天过得怎么样吗？
100 00:10:58,173 --> 00:11:02,059 讲者 讲者_02：嗯，是的，我几乎没有睡觉。
101 00:11:02,159 --> 00:11:07,089 说话人 SPEAKER_02：凌晨一点了，电话响起的时候我可能已经睡了大约一个小时。
102 00:11:08,009 --> 00:11:08,751 说话人 SPEAKER_02：我在加利福尼亚。
103 00:11:09,312 --> 00:11:13,298 说话人 SPEAKER_02：嗯，从那时起，我可能又多睡了一个小时。
104 00:11:14,041 --> 00:11:16,485 说话人 SPEAKER_02：嗯，所以我现在相当困。
105 00:11:16,465 --> 00:11:29,412 主持人 SPEAKER_02：很多人试图联系我，还有许多多年未见的老朋友的消息，这真是太好了。
106 00:11:33,138 --> 00:11:39,110 主持人 SPEAKER_00：下一个问题来自 BetaKit 的 Isabel Kirkwood，再次向 Hinton 教授提问。
107 00:11:39,731 --> 00:11:52,378 主持人 SPEAKER_00：她问，Hinton 教授，您如何调和接受这种认可与您对减缓 AI 发展及其技术风险的直言不讳？
108 00:11:53,354 --> 00:11:58,061 主持人 SPEAKER_02：我从未建议减缓 AI 的发展，因为我认为这是不可行的。
109 00:11:58,863 --> 00:12:10,500 讲者 SPEAKER_02：人工智能有如此多的积极影响，比如在医疗保健领域，但在几乎所有行业中，我认为我们根本无法减缓其发展速度。
110 00:12:15,609 --> 00:12:17,471 讲者 SPEAKER_02：你能再说一遍问题的后半部分吗？
111 00:12:17,687 --> 00:12:18,590 讲者 SPEAKER_00：当然可以。
112 00:12:18,610 --> 00:12:29,773 讲者 SPEAKER_00：她问，如何协调接受这种认可与您公开呼吁减缓人工智能进步和技术带来的风险之间的矛盾？
113 00:12:30,799 --> 00:12:38,572 讲者 SPEAKER_02：实际上，诺贝尔委员会认识到我在谈论安全方面的工作与此相关。
114 00:12:39,274 --> 00:12:42,159 讲者 SPEAKER_02：我记不清他们具体说了什么，但他们提到了这一点。
115 00:12:43,000 --> 00:12:49,972 讲者 SPEAKER_02：我认为我们需要做出严肃的努力来确保它是安全的，因为如果我们能保持它的安全，那将是非常美好的。
116 00:12:52,754 --> 00:13:12,163 讲者 SPEAKER_00：我们再次收到 Hinton 教授和来自法新社的 Ahmed 教授的跟进提问，他们认为学生甚至专业人士过度依赖LLMs是否会产生降低智力的效果，或者我们会进行更高层次的思考？
117 00:13:13,240 --> 00:13:16,004 讲者 SPEAKER_02：我认为它不会产生显著的简化效果。
118 00:13:16,304 --> 00:13:22,614 讲者 SPEAKER_02：我想这就像他们第一次有袖珍计算器时的情况，人们说，哦，孩子们不会再学数学了。
119 00:13:22,653 --> 00:13:24,356 讲者 SPEAKER_02：他们不会做乘法。
120 00:13:24,998 --> 00:13:28,523 讲者 SPEAKER_02：如果你有袖珍计算器，你不需要会做乘法。
121 00:13:29,345 --> 00:13:31,528 说话人 SPEAKER_02：我认为它将与LLMs一样。
122 00:13:31,908 --> 00:13:37,738 说话人 SPEAKER_02：人们可能不会记得那么多你可以直接询问LLM就会知道的事实。
123 00:13:37,778 --> 00:13:40,682 说话人 SPEAKER_02：但我认为这会使人们变得更聪明，而不是更笨。
124 00:13:42,703 --> 00:13:43,264 说话人 SPEAKER_00：谢谢。
125 00:13:43,284 --> 00:13:47,635 说话人 SPEAKER_00：来自 CTV 新闻的 Adrian Gobriel 的后续提问。
126 00:13:48,297 --> 00:13:51,183 说话人 SPEAKER_00：他问，如果我还能问一个问题。
127 00:13:51,224 --> 00:13:56,155 说话人 SPEAKER_00：他说，你提到你得知奖项时用了“目瞪口呆”这个词。
128 00:13:56,616 --> 00:13:58,400 说话人 SPEAKER_00：为什么你这么惊讶？
129 00:13:59,578 --> 00:14:02,565 讲者 SPEAKER_02：我绝对不知道自己被提名了。
130 00:14:02,605 --> 00:14:05,289 讲者 SPEAKER_02：我不是物理学家。
131 00:14:05,711 --> 00:14:07,413 讲者 SPEAKER_02：我对物理学非常尊敬。
132 00:14:07,894 --> 00:14:14,989 讲者 SPEAKER_02：我在大学的第一年就退出了物理学，因为我无法处理复杂的数学。
133 00:14:14,969 --> 00:14:19,197 讲者 SPEAKER_02：获得物理学奖对我来说非常意外。
134 00:14:19,538 --> 00:14:28,014 讲者 SPEAKER_02：我很高兴诺贝尔委员会认识到在人工神经网络领域取得了巨大进步。
135 00:14:28,755 --> 00:14:31,662 讲者 SPEAKER_02：还有霍普菲尔德的工作
136 00:14:31,642 --> 00:14:33,264 讲者 SPEAKER_02：与物理学密切相关。
137 00:14:33,344 --> 00:14:39,033 讲者 SPEAKER_02：我和 Terry Sanofsky 早期在玻尔兹曼机方面的工作受到了统计物理学的启发。
138 00:14:39,592 --> 00:14:43,619 讲者 SPEAKER_02：但最近，这项工作与物理学的关联越来越少。
139 00:14:43,899 --> 00:14:46,823 讲者 SPEAKER_02：所以我非常惊讶自己获得了物理学奖项。
140 00:14:49,206 --> 00:15:01,024 讲者 SPEAKER_00：下一个问题来自美联社的 Matt O'Brien，他问 Hinton 教授，您能否详细说明在电话会议中关于 Sam Altman 的评论？
141 00:15:02,354 --> 00:15:07,190 说话人 SPEAKER_02：所以 OpenAI 的建立非常重视安全性。
142 00:15:08,715 --> 00:15:13,831 说话人 SPEAKER_02：它的主要目标是开发通用人工智能，并确保其安全性。
143 00:15:15,178 --> 00:15:20,307 说话人 SPEAKER_02：我的一个前学生是首席科学家。
144 00:15:22,751 --> 00:15:31,086 说话人 SPEAKER_02：随着时间的推移，发现山姆·奥特曼对安全性的关注远不如对利润的关注。
145 00:15:31,106 --> 00:15:33,812 说话人 SPEAKER_02：我觉得这很遗憾。
146 00:15:37,352 --> 00:15:37,813 说话人 SPEAKER_00：谢谢。
147 00:15:37,854 --> 00:15:41,499 说话人 SPEAKER_00：下一个问题来自 PA Media 的 Jessica Coates。
148 00:15:41,519 --> 00:15:44,583 说话人 SPEAKER_00：这同样是一个针对 Hinton 教授的问题。
149 00:15:45,184 --> 00:15:53,677 讲者 讲者_00：她问，你提到了围绕人工智能的不确定未来以及对其潜在机遇和风险的更深入了解的需求。
150 00:15:54,337 --> 00:15:58,764 讲者 讲者_00：您认为政府是否会介入对人工智能进行更严格的监管吗？
151 00:15:58,784 --> 00:16:02,230 讲者 讲者_00：政府如何更好地支持人工智能研究？
152 00:16:03,491 --> 00:16:10,340 讲者 讲者_02：我认为政府可以鼓励大型公司更多地投入资源进行安全研究。
153 00:16:10,681 --> 00:16:15,385 讲者 SPEAKER_02：目前，几乎所有资源都投入到使模型变得更好。
154 00:16:16,267 --> 00:16:22,214 讲者 SPEAKER_02：这样他们就可以拥有闪亮的新模型，现在竞争激烈，模型也在变得越来越好，这是好事。
155 00:16:22,754 --> 00:16:27,120 讲者 SPEAKER_02：但我们还需要在 AI 安全方面付出相当的努力。
156 00:16:27,480 --> 00:16:29,482 讲者 SPEAKER_02：这种努力需要超过 1%。
157 00:16:29,462 --> 00:16:36,433 讲者 SPEAKER_02：可能需要将三分之一的努力投入到空中安全，因为如果这些东西变得不安全，那就非常糟糕。
158 00:16:36,494 --> 00:16:44,125 讲者 SPEAKER_00：我们的下一个问题是来自 Tara Deschamps，来自 CP。
159 00:16:45,467 --> 00:16:51,116 讲者 SPEAKER_00：她问 Hinton 教授，对于诺贝尔奖带来的资金有什么计划吗？
160 00:16:52,057 --> 00:16:53,801 讲者 SPEAKER_02：没有具体的计划。
161 00:16:53,921 --> 00:17:05,421 说话人 SPEAKER_02：我将把它捐给慈善机构，但我知道有一个慈善机构会给神经多样性年轻人提供工作。
162 00:17:06,303 --> 00:17:09,949 说话人 SPEAKER_02：我会捐给其他一些慈善机构，但我还不知道是哪些。
163 00:17:12,712 --> 00:17:26,299 说话人 SPEAKER_00：接下来我们要问 Hinton 教授的问题，又是来自路透社的 Wa Lone，他问，您对未来如何防止严重后果有什么建议吗？
164 00:17:27,182 --> 00:17:31,210 说话人 SPEAKER_00：他们指的是人们应该如何小心对待 AI 及其使用。
165 00:17:32,050 --> 00:17:33,894 说话人 说话人_00：正如你警告的那样，这可能会很危险。
166 00:17:35,005 --> 00:17:40,913 说话人 说话人_02：嗯，我认为个人在使用时小心并不能解决问题。
167 00:17:41,575 --> 00:17:45,961 说话人 说话人_02：我认为开发 AI 的人需要小心地开发它。
168 00:17:46,863 --> 00:17:50,628 说话人 说话人_02：我认为大公司需要进行研究，这些公司拥有资源。
169 00:17:51,510 --> 00:17:55,696 讲者 SPEAKER_02：嗯，我不认为个人使用它的方式会产生太大影响。
170 00:18:00,232 --> 00:18:04,140 讲者 SPEAKER_00：下一个问题是伊萨姆·艾哈迈德的另一个后续问题。
171 00:18:04,661 --> 00:18:07,526 讲者 SPEAKER_00：这是来自 AFP 的，再次针对 Hinton 教授。
172 00:18:07,967 --> 00:18:18,866 讲者 SPEAKER_00：他们问，我知道你说很难预测变坏可能意味着什么，但如果你必须冒险猜测一些关注的粗略领域，那会是什么？
173 00:18:20,534 --> 00:18:24,201 说话人 SPEAKER_02：所以人工智能有很多不同的风险，它们都有不同的解决方案。
174 00:18:24,882 --> 00:18:28,489 说话人 SPEAKER_02：立即的风险包括像伪造视频破坏选举这样的东西。
175 00:18:29,529 --> 00:18:38,626 说话人 SPEAKER_02：我们已经看到政治家们要么指责他人使用伪造视频，要么自己使用伪造视频和伪造图像。
176 00:18:38,606 --> 00:18:40,088 说话人 SPEAKER_02：这就是一个立即的危险。
177 00:18:40,148 --> 00:18:44,855 说话人 SPEAKER_02：来自网络攻击等事物的危险也非常直接。
178 00:18:44,875 --> 00:18:50,981 说话人 SPEAKER_02：例如，去年网络钓鱼攻击的数量增加了 1200%。
179 00:18:51,462 --> 00:18:56,669 说话人 SPEAKER_02：这是因为大型语言模型使得进行网络钓鱼攻击变得非常容易。
180 00:18:57,108 --> 00:19:01,294 说话人 SPEAKER_02：而且你不能再通过拼写错误和语法略有不同来识别它们了。
181 00:19:02,276 --> 00:19:03,416 说话人 SPEAKER_02：他们的英语非常流利。
182 00:19:06,333 --> 00:19:09,199 说话人 SPEAKER_00：下一个问题是来自维多利亚·吉布森的。
183 00:19:09,298 --> 00:19:10,781 说话人 SPEAKER_00：再次，她在多伦多星报工作。
184 00:19:11,384 --> 00:19:18,598 说话人 SPEAKER_00：她问霍 inton 教授，您今天已经几次提到了省政府和安大略科学中心。
185 00:19:19,119 --> 00:19:22,366 讲者 讲者_00：为什么在获得这个荣誉时，这件事会首先出现在你的脑海中？
186 00:19:23,748 --> 00:19:30,846 讲者 讲者_02：因此，安大略科学中心在激发年轻人好奇心和对科学的兴趣方面起着非常重要的作用。
187 00:19:32,250 --> 00:19:34,778 讲者 讲者_02：它有一些屋顶问题，需要进行翻修。
188 00:19:35,239 --> 00:19:39,309 讲者 讲者_02：翻修的预估费用为2亿美元。
189 00:19:39,526 --> 00:19:50,928 讲者 SPEAKER_02：但是政府后来告诉估算成本的人，将这个数字乘以 1.85，以得到一个更大的数字，这样他们就可以为拆除它找到理由。
190 00:19:50,968 --> 00:19:56,458 讲者 SPEAKER_02：在我看来，拆除它的原因并不是政府给出的原因。
191 00:19:56,919 --> 00:20:00,605 讲者 SPEAKER_02：它本可以修复，修复它会更便宜。
192 00:20:03,251 --> 00:20:10,265 讲者 SPEAKER_00：接下来是加拿大通讯社的 Tara Deschamps 提出的问题。
193 00:20:10,746 --> 00:20:19,942 讲者 讲者_00：她说，当人们谈论加拿大的人工智能和技术格局时，您的名字总是作为加拿大能够取得的成就的例证出现。
194 00:20:20,604 --> 00:20:26,694 讲者 讲者_00：但人们也说，这个国家必须小心不要浪费您所创造的机会。
195 00:20:26,792 --> 00:20:33,343 讲者 讲者_00：您认为加拿大能做些什么来保持其在人工智能领域的领先地位？
196 00:20:34,758 --> 00:20:38,863 讲者 讲者_02：它可以继续资助好奇心驱动的硏究。
197 00:20:38,962 --> 00:20:41,786 讲者 SPEAKER_02：这对留住最优秀的研究人员非常重要。
198 00:20:42,426 --> 00:20:51,057 讲者 SPEAKER_02：但在人工神经网络的时代，我们还需要大量的计算资源来留住大学的研究人员。
199 00:20:51,798 --> 00:20:53,700 讲者 SPEAKER_02：政府正在努力解决这个问题。
200 00:20:53,740 --> 00:20:58,925 讲者 SPEAKER_02：他们为人工智能研究划拨了 20 亿美元用于计算资源。
201 00:20:58,905 --> 00:21:01,114 说话人 SPEAKER_02：所以，我认为他们已经尽力了。
202 00:21:01,154 --> 00:21:09,928 说话人 SPEAKER_02：显然，我们国家的规模比中国或美国小得多，但考虑到他们拥有的资源，我认为加拿大做得相当不错。
203 00:21:13,772 --> 00:21:34,098 说话人 SPEAKER_00：下一个问题来自多伦多大学新闻的拉胡尔·卡尔瓦帕利，他问希顿教授，您在人工神经网络研究领域坚持不懈，即使在科学界对该主题兴趣减弱的时期也是如此。
204 00:21:34,700 --> 00:21:42,210 说话人 SPEAKER_00：您对那些可能被认为不受欢迎或徒劳的奋斗的教授和学生有什么信息？
205 00:21:43,472 --> 00:21:52,587 讲者 SPEAKER_02：我认为我的信息是这样的，如果你相信某件事，不要在你明白为什么这个信念是错误之前放弃它。
206 00:21:53,008 --> 00:21:57,935 讲者 SPEAKER_02：通常你会相信某些事情，最终你会弄清楚为什么相信它是错误的。
207 00:21:58,576 --> 00:22:08,333 讲者 SPEAKER_02：但是只要你还相信某件事，你无法看到为什么它是错误的，就像大脑需要某种方式工作一样，所以我们必须弄清楚它是如何学习连接强度以使其工作的。
208 00:22:08,313 --> 00:22:14,821 讲者 SPEAKER_02：只要你还相信这件事，就继续努力，不要让别人告诉你，如果你看不到它是胡说八道，它就是胡说八道。
209 00:22:18,207 --> 00:22:18,688 演讲者 演讲者_00: 谢谢。
210 00:22:18,728 --> 00:22:28,521 演讲者 演讲者_00: 下一个问题来自小林康博，他来自《读卖新闻》。
211 00:22:28,561 --> 00:22:33,368 演讲者 演讲者_00: 他们问，AI 何时会超越人类的能力？
212 00:22:33,828 --> 00:22:35,771 演讲者 演讲者_00: 结果会怎样？
213 00:22:36,832 --> 00:22:41,659 无人知晓何时，但我知道的大部分优秀研究人员都认为这将会发生。
214 00:22:42,820 --> 00:22:46,906 我的猜测是，它可能在未来5到20年之间发生。
215 00:22:47,287 --> 00:22:48,048 也可能需要更长的时间。
216 00:22:48,729 --> 00:22:50,392 有极小的可能性会更快发生。
217 00:22:50,412 --> 00:22:54,438 说话人 SPEAKER_02：然后我们会不知道会发生什么。
218 00:22:55,117 --> 00:23:06,535 说话人 SPEAKER_02：所以如果你环顾四周，很少看到更智能的东西被不那么智能的东西控制，这让你不禁想，当 AI 比我们更智能时，它是否会接管控制权。
219 00:23:09,990 --> 00:23:11,813 说话人 SPEAKER_00：谢谢，Hinton 教授。
220 00:23:12,453 --> 00:23:19,161 说话人 SPEAKER_00：我们现在没有看到其他问题，但我们愿意再稍作等待。
221 00:23:19,300 --> 00:23:32,915 演讲者 演讲者_00：如果大家有任何临时问题，请使用您屏幕底部的问答工具箱，并告诉我们您的姓名和媒体归属。
222 00:23:33,656 --> 00:23:36,640 演讲者 演讲者_00：如果还有，我们还有时间回答几个问题。
223 00:23:47,690 --> 00:23:50,336 演讲者 演讲者_00：这是来自多伦多大学新闻的 Rahul 提出的问题。
224 00:23:51,057 --> 00:24:07,352 演讲者 演讲者_00：他向 Gertler 校长提问，询问他预期 Hinton 教授的诺贝尔奖将如何影响大学，并激发人工智能和其他领域的学术研究。
225 00:24:09,323 --> 00:24:12,729 讲者 SPEAKER_01：嗯，我认为它将产生巨大的影响，一个非常积极的影响。
226 00:24:13,289 --> 00:24:23,909 讲者 SPEAKER_01：我曾是多伦多大学的一位年轻的助理教授，当时另一位杰出的科学家约翰·波拉尼在 1986 年获得了诺贝尔化学奖。
227 00:24:23,989 --> 00:24:30,099 讲者 SPEAKER_01：我记得我为我们知识界的骄傲。
228 00:24:30,079 --> 00:24:40,576 讲者 SPEAKER_01：当约翰得到这个好消息时，它已经产生了如此积极的影响，不仅是在化学领域，而且在多伦多大学。
229 00:24:40,616 --> 00:24:46,586 讲者 SPEAKER_01: 我认为 Jeff 今天的胜利将产生类似积极的影响。
230 00:24:46,567 --> 00:24:53,636 讲者 SPEAKER_01: 提振了整个大学的士气，同时也帮助我们吸引和留住优秀人才。
231 00:24:53,656 --> 00:24:57,522 讲者 SPEAKER_01: Jeff 今天已经就几个问题谈到了这一点。
232 00:24:57,542 --> 00:25:11,982 讲者 SPEAKER_01: 我认为这样的胜利对加拿大、多伦多和加拿大多伦多大学能够
233 00:25:11,962 --> 00:25:22,557 讲者 SPEAKER_01：欢迎来自全国各地和世界各地的优秀新成员、优秀学生和杰出的教职员工，因为杰夫的胜利所带来的认可。
234 00:25:25,840 --> 00:25:27,122 讲者 SPEAKER_00：谢谢，格特莱尔校长。
235 00:25:27,603 --> 00:25:35,573 讲者 SPEAKER_00：我们现在回到 Hinton 教授，回答来自伊萨姆·艾哈迈德（AFP）的另一个问题。
236 00:25:36,074 --> 00:25:40,839 讲者 SPEAKER_00：他们问，您在人工智能领域接下来最感兴趣的领域是什么？
237 00:25:42,759 --> 00:25:49,054 说话人 SPEAKER_02：好吧，我 76 岁了，我相信我不会再进行太多前沿研究了。
238 00:25:49,294 --> 00:25:53,865 说话人 SPEAKER_02：我将把我的时间花在倡导人们关注安全上。
239 00:25:54,566 --> 00:25:59,537 说话人 SPEAKER_02：我认为有一些非常激动人心的前沿领域。
240 00:25:59,517 --> 00:26:04,548 说话人 SPEAKER_02：在机器人领域，在让 AI 擅长操作事物方面。
241 00:26:05,049 --> 00:26:12,788 讲者 SPEAKER_02：目前我们在那方面比计算机或人工神经网络要好得多，但那方面还将会有很多进步。
242 00:26:13,651 --> 00:26:17,339 讲者 SPEAKER_02：不过在那个领域可能需要更长的时间。
243 00:26:17,319 --> 00:26:22,246 讲者 SPEAKER_02：我也认为这些大型语言模型将会有很大的推理能力提升。
244 00:26:22,605 --> 00:26:29,713 讲者 SPEAKER_02：所以 OpenAI 的最新模型和谷歌的模型，如 Gemini 的最新版本，在推理能力上一直在提升。
245 00:26:30,715 --> 00:26:33,459 说话人 SPEAKER_02：我认为这将非常令人兴奋。
246 00:26:36,682 --> 00:26:39,326 说话人 SPEAKER_00：下一个问题来自维多利亚·吉布森。
247 00:26:39,346 --> 00:26:43,210 说话人 SPEAKER_00：她再次来自多伦多星报，这是给辛顿教授的问题。
248 00:26:43,492 --> 00:26:52,105 说话人 SPEAKER_00：她问道，您在人工智能可能出错的地方提供了一些具体的例子，比如网络攻击、虚假视频等等。
249 00:26:52,744 --> 00:26:57,872 说话人 说话人_00：你能分享一些更具体的例子来说明你认为它将如何发挥积极作用吗？
250 00:26:59,334 --> 00:26:59,755 说话人 说话人_02：哦，是的。
251 00:27:00,154 --> 00:27:07,424 说话人 说话人_02：所以如果你考虑像医疗保健这样的领域，安大略省预算的大部分都用于医疗保健。
252 00:27:08,536 --> 00:27:10,459 说话人 说话人_02：它可以在那里产生巨大的影响。
253 00:27:11,359 --> 00:27:18,829 讲者 SPEAKER_02：我实际上在 2016 年做出过一个预测，那就是到如今，人工智能将能够阅读放射科医生通常阅读的所有扫描。
254 00:27:19,391 --> 00:27:20,752 那个预测是错误的。
255 00:27:20,772 --> 00:27:22,255 我有点过于乐观了。
256 00:27:22,734 --> 00:27:26,180 可能还需要再过五年才会实现，但我们显然正在朝着这个方向前进。
257 00:27:26,980 --> 00:27:30,425 说话人 SPEAKER_02：人工智能在诊断方面将会更加出色。
258 00:27:30,645 --> 00:27:37,394 所以，对于难以诊断的病例，医生的正确率已经达到40%。
259 00:27:37,375 --> 00:27:41,022 医生和人工智能系统的正确率为50%。
260 00:27:42,066 --> 00:27:47,237 医生与人工智能系统的结合正确率可达60%，这是一个很大的提升。
261 00：27：47,817 --> 00：27：52,167 演讲者 SPEAKER_02：在北美，每年有数十万人死于诊断。
262 00：27：52,489 --> 00：27：55,214 演讲者 SPEAKER_02：有了人工智能，诊断会变得更好。
但真正会发生的事情是，你将拥有一个 AI 家庭医生，他看过一亿名患者，知道大量信息，并且将能够更好地处理你遇到的任何疾病，因为你的 AI 家庭医生已经看过许多类似的病例。
264 00:28:19,428 --> 00:28:21,251 演讲者 演讲者_00：谢谢，Hinton 教授。
265 00:28:21,973 --> 00:28:28,567 主持人：我们不再看到其他问题，但再次强调，我们还有时间再回答一两个问题。
266 00:28:28,587 --> 00:28:38,230 主持人：所以如果电话会议中有任何其他问题，我们再次邀请您在提问时说明您的姓名和您所代表的媒体机构。
267 00:28:38,210 --> 00:28:43,117 主持人：并在屏幕底部的问答框中打出这些问题。
268 00:28:43,137 --> 00:29:01,990 主持人：在我们等待最后几分钟的问题到来时，Hinton 教授，我们很好奇，今天在新闻发布会上，有没有我们没有涉及到的内容，或者有没有因为各种媒体的提问而遗漏的内容，您想在这里提及的吗？
269 00:29:03,151 --> 00:29:08,997 讲者 SPEAKER_02：嗯，我们只是简要提到了好奇心驱动的硏究在其中的作用。
270 00:29:09,657 --> 00:29:19,808 所以人工神经网络，基础工作几乎都是由大学硏究人员完成的，都是出于好奇心的驱使。
271 00:29:20,569 --> 00:29:23,113 而资助这种硏究非常重要。
272 00:29:23,633 --> 00:29:25,776 它并不像其他类型的硏究那样昂贵。
273 00:29:26,395 --> 00:29:32,482 说话人 SPEAKER_02: 嗯，但这为后来非常昂贵且涉及大量技术的项目奠定了基础。
274 00:29:35,500 --> 00:29:35,961 说话人 SPEAKER_00: 谢谢。
275 00:29:36,000 --> 00:29:44,315 说话人 SPEAKER_00: 我们这里还有来自多伦多星报的 Victoria Gibson 的一个问题，可能是对您之前关于医疗保健和人工智能的评论的后续提问。
276 00:29:44,796 --> 00:29:52,348 说话人 SPEAKER_00: 她说，为什么我们认为我们还没有达到您预测的人工智能在医疗保健中扮演更大角色的那个点呢？
277 00:29:52,930 --> 00:29:55,855 说话人 说话人_00：这件事发生还有障碍吗？
278 00:29:56,897 --> 00:30:01,573 说话人 说话人_02：一个障碍是医学界非常保守。
279 00:30:01,853 --> 00:30:02,977 说话人 说话人_02：这有很好的理由。
280 00:30:03,017 --> 00:30:08,493 说话人 说话人_02：如果你出错导致人死亡，那是个大问题
281 00:30:08,625 --> 00:30:13,534 讲者 SPEAKER_02：政策上要保守，但他们相对较慢地采用新技术。
282 00:30:14,355 --> 00:30:21,846 讲者 SPEAKER_02：另一个原因是，我之前关于 AI 系统在阅读扫描方面会比放射科医生更快的判断是错误的。
283 00:30:22,367 --> 00:30:27,276 讲者 SPEAKER_02：现在它们在许多不同类型的扫描中与放射科医生相当，在少数方面甚至更好。
284 00:30:27,256 --> 00:30:30,761 讲者 SPEAKER_02：我认为再过几年，它们肯定会比放射科医生更好。
285 00:30:31,061 --> 00:30:39,636 说话人 SPEAKER_02：我们将看到放射科医生和 AI 系统之间的合作，其中 AI 系统读取扫描图像，放射科医生检查 AI 系统是否出错。
286 00:30:40,397 --> 00:30:43,222 说话人 SPEAKER_02：过了一段时间后，AI 系统将几乎完成所有工作。
287 00:30:46,122 --> 00:30:46,782 说话人 SPEAKER_00：好的，太好了。
288 00:30:46,903 --> 00:30:48,325 说话人 SPEAKER_00：谢谢，Hinton 教授。
289 00:30:48,424 --> 00:30:52,009 演讲者 演讲者_00: 今天的问题环节到此结束。
290 00:30:52,029 --> 00:31:00,800 演讲者 演讲者_00: 所以在聊天框中，您会看到一个可以联系以获取额外问题的电子邮件地址。
291 00:31:01,942 --> 00:31:03,384 演讲者 演讲者_00: 您应该现在就能看到它弹出。
292 00:31:03,443 --> 00:31:08,871 演讲者 演讲者_00: 这个电子邮件地址是 media.relations@utoronto.ca。
293 00:31:08,891 --> 00:31:14,137 说话人 说话人_00：现在，我将邀请校长 Gertler 发表闭幕词。
294 00:31:15,957 --> 00:31:16,877 说话人 说话人_01：嗯，谢谢，Lisa。
295 00:31:17,258 --> 00:31:18,338 说话人 说话人_01：谢谢，Jeff。
296 00:31:18,358 --> 00:31:22,923 说话人 说话人_01：再次祝贺您取得这一伟大成就，诺贝尔奖。
297 00:31:22,963 --> 00:31:38,337 讲者 SPEAKER_01：我确信我代表整个多伦多大学社区，以及整个加拿大，还有你们在世界各地的众多朋友、同事和崇拜者，当我们说我们对您今天所获得的成就感到无比自豪时。
298 00:31:38,397 --> 00:31:43,784 讲者 SPEAKER_01：还要感谢今天所有来参加我们这次美好庆祝活动的人。
299 00:31:44,744 --> 00:31:45,204 讲者 SPEAKER_01：干杯。

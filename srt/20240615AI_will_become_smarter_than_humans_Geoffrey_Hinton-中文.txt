1 00:00:01,042 --> 00:00:01,544 说话人 说话人_00：欢迎回来。
2 00:00:01,564 --> 00:00:04,950 说话人 说话人_00：我们想回到我们与杰弗里·辛顿的独家访谈。
3 00:00:05,030 --> 00:00:05,972 说话人 说话人_00：领先的 AI 研究者将他的职业生涯奉献给了更好地理解智能的未来。
4 00:00:06,032 --> 00:00:11,260 说话人 说话人_00：这位领先的研究者致力于更深入地理解智能的未来。
5 00:00:11,281 --> 00:00:17,772 说话人 SPEAKER_00：越来越，Hinton 敦促立法者要求大型科技公司增加在人工智能上的投入。
6 00:00:17,992 --> 00:00:18,434 说话人 SPEAKER_00：安全。
7 00:00:18,734 --> 00:00:21,739 说话人 SPEAKER_00：他的信念与人工智能变得更快以及与之相关的潜在风险有关。
8 00:00:21,839 --> 00:00:25,887 说话人 SPEAKER_00：他的信念与人工智能变得更快以及与之相关的潜在风险有关。
9 00:00:25,867 --> 00:00:32,569 说话人 SPEAKER_00：在这次对话的下一部分，我们首先询问了 Hinton 关于人工智能是否会比人类更聪明的可能性。
10 00:00:34,185 --> 00:00:40,652 说话人 SPEAKER_01：我所认识的几乎所有优秀的研究人员都认为，从长远来看，它的智能将超过我们。
11 00:00:40,912 --> 00:00:42,435 说话人 SPEAKER_01：它的智能不会停留在我们的水平上。
12 00:00:42,734 --> 00:00:46,920 说话人 SPEAKER_01：有些人认为，因为它目前是在训练我们提供的数据上。
13 00:00:47,720 --> 00:00:53,106 说话人 SPEAKER_01：但我认识的多数研究人员都相当确信它将比我们更聪明。
14 00:00:53,746 --> 00:00:56,369 说话人 SPEAKER_01：所以真正的问题是，这需要多长时间？
15 00:00:56,929 --> 00:00:59,332 说话人 SPEAKER_01：当它做到这一点时，我们还能否保持控制？
16 00:01:00,612 --> 00:01:05,504 说话人 SPEAKER_01：我认为在接下来的 20 年内，它比我们更聪明的可能性是 50-50。
17 00:01:07,147 --> 00:01:18,655 说话人 SPEAKER_00：那么，在控制方面，我们再次面临的问题是什么？这在我们帮助创造的社会中，我们的角色又是什么？
18 00:01:18,837 --> 00:01:20,218 说话人 SPEAKER_01：没有人真正知道。
19 00:01:20,418 --> 00:01:22,061 说话人 SPEAKER_01：我们以前从未遇到过这种情况。
20 00:01:22,341 --> 00:01:24,766 说话人 SPEAKER_01：我们从未处理过比我们更智能的事物。
21 00:01:25,947 --> 00:01:32,737 说话人 SPEAKER_01：因此人们应该对它的样子非常不确定。
22 00:01:33,438 --> 00:01:42,533 说话人 SPEAKER_01：当它比我们稍微笨一些的时候，进行大量的实证实验似乎非常明智，这样我们还有机会保持控制。
23 00:01:42,513 --> 00:02:00,927 说话人 SPEAKER_01：我认为政府，例如，应该要求大公司进行大量的安全实验，投入相当多的资源，比如将三分之一的计算资源用于安全实验，在这些事物还没有像我们一样智能的时候，看看它们可能会如何逃避控制，我们又能做些什么。
24 00:02:00,906 --> 00:02:02,909 说话人 SPEAKER_01：我认为这正是 OpenAI 辩论的主要内容。
25 00:02:03,370 --> 00:02:09,219 说话人 SPEAKER_01：对安全感兴趣的人，比如伊利亚·苏茨克维，希望投入大量资源用于安全。
26 00:02:09,759 --> 00:02:14,007 说话人 SPEAKER_01：对利润感兴趣的人，比如山姆·奥特曼，不希望在这方面投入太多资源。
27 00:02:14,907 --> 00:02:19,354 说话人 SPEAKER_00：我们现在对这笔开销的分布有什么感觉吗？
28 00:02:19,395 --> 00:02:20,637 说话人 SPEAKER_00：安全方面的开销？
29 00:02:21,138 --> 00:02:21,358 说话人 SPEAKER_01: 是的。
30 00:02:22,278 --> 00:02:24,502 说话人 SPEAKER_01: 在发展上远大于在安全上。
31 00:02:26,145 --> 00:02:28,027 说话人 SPEAKER_01: 因为这些是利润驱动的公司。
32 00:02:28,969 --> 00:02:36,122 说话人 SPEAKER_00: 你如何看待这个领域财富如此快速地被创造出来的事实？
33 00:02:36,181 --> 00:02:40,689 说话人 SPEAKER_00: 在您看来，这如何使安全故事变得复杂？
34 00:02:41,610 --> 00:02:45,938 说话人 SPEAKER_01: 嗯，这使大公司清楚地意识到他们想要全速前进。
35 00:02:46,759 --> 00:02:54,993 说话人 SPEAKER_01: 微软和谷歌之间有一场激烈的竞争，可能还有亚马逊、英伟达以及可能的其他大公司。
36 00:02:56,002 --> 00:02:57,444 说话人 SPEAKER_01: 以及 Meta。
37 00：02：58,325 --> 00：03：03,492 议长 SPEAKER_01：如果其中任何一个人撤出，其他人都会继续前进。
所以，你有了资本主义的标准竞争动态，人们试图在相对较短的时间内获得利润，并且他们全力以赴。
39 00：03：12,686 --> 00：03：17,592 议长 SPEAKER_01：我认为唯一能减缓这种情况的是严格的政府监管。
40 00：03：17,611 --> 00：03：20,256 议长 SPEAKER_01：我认为政府是唯一足够强大的东西来减缓这种情况。
41 00:03:20,793 --> 00:03:26,010 说话人 SPEAKER_00: 显然，各国政府代表各自的祖国。
42 00:03:26,070 --> 00:03:28,659 说话人 SPEAKER_00: 你对到目前为止的政府努力以及全球各国政府在涉及不止一个领土的问题上的协调有何观察？
43 00:03:29,990 --> 00:03:39,026 说话人 SPEAKER_00: 不仅是对政府努力的观察，还包括全球各国政府在涉及不止一个领土的问题上的协调。
44 00:03:39,486 --> 00:03:41,891 说话人 SPEAKER_01: 已经有一些协调，这非常鼓舞人心。
45 00:03:42,832 --> 00:03:46,901 说话人 SPEAKER_01：对于许多事情，比如自主致命武器，将会存在竞争。
46 00:03:47,301 --> 00:03:48,302 说话人 SPEAKER_01：它们不会进行协调。
47 00:03:49,384 --> 00:03:53,532 说话人 SPEAKER_01：美国不会与俄罗斯分享其自主致命武器的计划。
48 00:03:53,513 --> 00:04:01,872 说话人 SPEAKER_01：但是有一个问题，这些事物是否会变得超级智能并在我们都处于同一境地时接管我们？
49 00：04：02,794 --> 00：04：08,486 议长 SPEAKER_01：这些国家都不希望超级智能接管，这将迫使他们合作。
50 00：04：09,008 --> 00：04：11,674 议长 SPEAKER_01：所以，即使在冷战的高峰期，
苏联和美国可以在尝试防止全球核战争方面进行合作，因为这不符合它们任何一方的利益。
所以，在众多人工智能问题中，只有一个问题你可能能够得到良好的合作，那就是，我们如何防止它们接管？
53 00:04:29,800 --> 00:04:35,028 说话人 说话人_00：我们应该像考虑核威胁一样考虑这个问题吗？
54 00:04:35,408 --> 00:04:35,629 说话人 说话人_01：是的。
55 00:04:36,610 --> 00:04:38,853 说话人 说话人_01：我的意思是，我认为这是一种很好的思考方式。
56 00:04:39,295 --> 00:04:41,458 说话人 说话人_01：然而，有一个很大的不同之处。
57 00:04:41,725 --> 00:04:44,389 说话人 SPEAKER_01：核武器只适用于毁灭。
58 00:04:45,250 --> 00:04:47,173 他们曾尝试将其用于诸如压裂法等用途。
59 00:04:47,572 --> 00:04:48,615 但结果并不太理想。
60 00:04:49,115 --> 00:04:51,499 他们曾在60年代这样做，原子弹的和平用途。
61 00:04:52,439 --> 00:04:53,442 说话人 SPEAKER_01：这没出什么好结果。
62 00:04:53,942 --> 00:04:55,103 说话人 SPEAKER_01：它们基本上是用来破坏的。
63 00:04:55,343 --> 00:04:57,427 说话人 SPEAKER_01：而人工智能有很大的上升空间。
64 00:04:57,848 --> 00:05:02,615 说话人 SPEAKER_01：它在很多方面都很奇妙，不仅仅是回答问题和制作精美的图像。
65 00:05:03,076 --> 00:05:05,538 说话人 SPEAKER_01：这对医学将极为有用。
66 00:05:05,519 --> 00:05:12,069 说话人 SPEAKER_01：想象一下，如果你去看家庭医生，她已经看过一亿名患者，并且记得关于他们的所有事情。
67 00:05:12,850 --> 00:05:13,732 说话人 SPEAKER_01：那将是一个巨大的胜利。
68 00:05:14,372 --> 00:05:23,367 说话人 SPEAKER_01：此外，她还了解你所有的基因组以及你所有的医疗检查历史，而你的现任医生可能在你去看她之前还没有准备好这些信息。
69 00:05:25,170 --> 00:05:27,173 说话人 SPEAKER_01：那将是一个好得多的家庭医生。
70 00:05:28,064 --> 00:05:35,136 说话人 SPEAKER_01：目前在美国，每年大约有 20 万人，甚至更多，因不良医疗诊断而死亡。
71 00:05:36,338 --> 00:05:43,449 说话人 SPEAKER_01：已经证明，AI 与医生结合使用比单独使用医生要好得多，因为 AI 可以提出医生未曾想到的事情。
72 00:05:44,511 --> 00:05:52,704 说话人 SPEAKER_01：因此，在预防不良医疗诊断方面，它可以在很大程度上取得胜利。
73 00:05:54,322 --> 00:05:57,595 说话者 SPEAKER_01：治疗效果更好，对医学图像的理解更深入。
74 00：05：57,976 --> 00：05：59,040 议长 SPEAKER_01：这来得相当快。
75 00：06：00,266 --> 00：06：01,913 演讲者 SPEAKER_01：所以它有一个巨大的好处。
76 00：06：02,656 --> 00：06：04,904 演讲者 SPEAKER_01：事实上，任何你想预测的地方。
77 00:06:05,137 --> 00:06:09,583 说话人 SPEAKER_01：神经网络在预测方面做得很好，可能比以前的技术要好得多。
78 00:06:10,684 --> 00:06:16,490 说话人 SPEAKER_01：这对公司来说非常重要，这也是为什么它不会停止。
79 00:06:17,012 --> 00:06:20,555 说话人 SPEAKER_01：我们应该暂停的想法从来就不现实。
80 00:06:21,476 --> 00:06:28,345 说话人 SPEAKER_01：我们必须弄清楚如何让超级智能不想要接管。
81 00:06:29,826 --> 00:06:32,089 说话人 SPEAKER_00: 那是政府现在应该关注的事情之一吗？
82 00:06:32,129 --> 00:06:33,632 说话人 SPEAKER_01: 这只是众多事情之一。
83 00:06:33,651 --> 00:06:49,471 说话人 SPEAKER_01: 他们还应该关注许多其他事情，比如如何防止 AI 设计的生物武器，如何防止 AI 设计的网络攻击，如何防止 AI 设计的虚假视频影响选举。
84 00:06:49,451 --> 00:06:50,615 说话人 SPEAKER_01: 还有许多其他风险。
我们如何应对如果 AI 真的像这些大公司所想的那样成功，将会导致的所有工作岗位的丧失。
86 00：06：57,471 --> 00：07：00,738 演讲者 SPEAKER_01：这些都是其他不同的风险，有不同的解决方案。
87 00：07：01,701 --> 00：07：07,495 议长 SPEAKER_01：我刚才倾向于关注生存威胁，因为许多人认为这不是真实的事情。
我只是想知道，您是否有一些建议或解决方案，可以找到一种平衡这些权衡的方法，从您所概述的这项技术带来的机会，以及这些重大风险之间的平衡？
89 00:07:23,495 --> 00:07:31,086 说话人 SPEAKER_01：我最接近的想法是政府应该强制要求大公司投入大量资源用于安全。
90 00:07:31,747 --> 00:07:32,869 说话人 SPEAKER_01：这是我最好的想法。
91 00:07:32,928 --> 00:07:34,730 说话人 SPEAKER_01：这并不好，但这是我最好的想法。
92 00:07:34,913 --> 00:07:37,456 说话人 SPEAKER_00：你现在的观点是，他们根本就没有这样做。
93 00：07：37,476 --> 00：07：38,678 议长 SPEAKER_01：不，他们没有强制要求。
94 00：07：39,259 --> 00：07：47,831 议长 SPEAKER_01：加州提出了一些立法，加州总检察长可以起诉大公司。
95 00：07：47,851 --> 00：07：49,053 演讲者 SPEAKER_01：所以这是第一个有牙齿的。
它可以起诉大公司，如果它们不做合理的安全测试并向加州政府报告结果。
97 00:07:59,369 --> 00:08:02,153 说话人 SPEAKER_01: 这仍然相当弱，但总比没有好。
98 00:08:03,077 --> 00:08:12,226 说话人 SPEAKER_00: 您认为在正确制定监管、政府方法方面的时间框架是什么？
99 00:08:13,185 --> 00:08:14,588 说话人 SPEAKER_01: 这相当紧迫。
100 00:08:15,290 --> 00:08:17,413 说话人 SPEAKER_01: 我们可能需要在五年内做到这一点。
101 00:08:19,016 --> 00:08:20,980 说话人 SPEAKER_01：到目前为止进展非常缓慢。
102 00:08:21,420 --> 00:08:28,733 说话人 SPEAKER_01：例如，在英国举行了一次备受瞩目的布莱切利会议，他们一致认为我们需要担心人工智能的安全性。
103 00:08:29,334 --> 00:08:33,761 说话人 SPEAKER_01：然后英国政府决定，我们实际上不会采取任何行动，因为这可能会干扰创新。
104 00:08:34,764 --> 00:08:37,489 说话人 SPEAKER_01：把这当作是，这又是安全与利润之间的权衡。

1
00:00:00,166 --> 00:00:07,655
Speaker SPEAKER_02: This is, for me, a real delight to have the chance to have a conversation with Geoffrey Hinton.

2
00:00:07,676 --> 00:00:11,961
Speaker SPEAKER_02: I followed his work for years, but this is the first time we actually had a chance to meet.

3
00:00:12,862 --> 00:00:19,411
Speaker SPEAKER_02: And so this is, for me, one of the real highlights of our Ground Truths podcast.

4
00:00:19,431 --> 00:00:20,231
Speaker SPEAKER_02: So welcome, Geoff.

5
00:00:21,193 --> 00:00:21,934
Speaker SPEAKER_00: Thank you very much.

6
00:00:22,054 --> 00:00:23,917
Speaker SPEAKER_00: It's a real opportunity for me, too.

7
00:00:25,059 --> 00:00:28,283
Speaker SPEAKER_00: You're an expert in one area, I'm an expert in another, and it's great to meet up.

8
00:00:29,106 --> 00:00:32,631
Speaker SPEAKER_02: Well, this is a real point of convergence if there ever was one.

9
00:00:32,691 --> 00:00:44,463
Speaker SPEAKER_02: And I guess maybe I'd start off with, you know, you've been in the news a lot lately, of course, but what piqued my interest to connect with you was your interview on 60 Minutes with Scott Pelley.

10
00:00:45,564 --> 00:00:50,209
Speaker SPEAKER_02: You said an obvious area where there's huge benefits is health care.

11
00:00:50,850 --> 00:00:55,975
Speaker SPEAKER_02: AI is already comparable with radiologists understanding what's going on in medical images.

12
00:00:56,426 --> 00:00:58,829
Speaker SPEAKER_02: It's going to be very good at designing drugs.

13
00:00:58,850 --> 00:01:00,332
Speaker SPEAKER_02: It already is designing drugs.

14
00:01:00,593 --> 00:01:05,039
Speaker SPEAKER_02: So that's an area where it's almost entirely going to do good.

15
00:01:05,198 --> 00:01:06,159
Speaker SPEAKER_02: I like that area.

16
00:01:06,239 --> 00:01:08,423
Speaker SPEAKER_02: So I love that quote, Jeff.

17
00:01:09,084 --> 00:01:10,665
Speaker SPEAKER_02: And I thought maybe we could start with that.

18
00:01:14,852 --> 00:01:26,147
Speaker SPEAKER_00: Yeah, back in 2012, one of my graduate students called George Dahl, who did speech recognition in 2009, made a big difference there.

19
00:01:26,617 --> 00:01:36,447
Speaker SPEAKER_00: entered a competition by Merck-Frost to predict how well particular chemicals would bind to something.

20
00:01:37,709 --> 00:01:39,652
Speaker SPEAKER_00: He knew nothing about the science of it.

21
00:01:40,253 --> 00:01:48,902
Speaker SPEAKER_00: All he had was a few thousand descriptors of each of these chemicals and 15 targets that things might bind to.

22
00:01:49,884 --> 00:01:52,887
Speaker SPEAKER_00: And he used the same network as we use for speech recognition.

23
00:01:53,748 --> 00:01:55,510
Speaker SPEAKER_00: So he treated the 2000

24
00:01:55,927 --> 00:02:00,572
Speaker SPEAKER_00: descriptors of chemicals, as if they were things in a spectrogram for speech.

25
00:02:01,534 --> 00:02:03,775
Speaker SPEAKER_00: And he won the competition.

26
00:02:05,518 --> 00:02:12,185
Speaker SPEAKER_00: And after he won the competition, he wasn't allowed to collect the $20,000 prize until he told Merck how he did it.

27
00:02:13,145 --> 00:02:16,650
Speaker SPEAKER_00: And one of their questions was, what QSAR did you use?

28
00:02:18,230 --> 00:02:19,793
Speaker SPEAKER_00: So he said, what's QSAR?

29
00:02:21,510 --> 00:02:31,122
Speaker SPEAKER_00: Now QSAR is a field, it has a journal, it's had a conference, it's been going for many years, and it's the field of quantitative structural activity relationships.

30
00:02:31,883 --> 00:02:35,727
Speaker SPEAKER_00: And that's the field that tries to predict whether some chemical is gonna bind to something.

31
00:02:36,587 --> 00:02:41,373
Speaker SPEAKER_00: And basically he wiped out that field without knowing its name.

32
00:02:46,239 --> 00:02:51,044
Speaker SPEAKER_02: Well, you know, it's striking how healthcare medicine

33
00:02:51,650 --> 00:02:58,479
Speaker SPEAKER_02: life science has had somewhat of a separate path in recent AI with transformer models.

34
00:02:59,599 --> 00:03:09,894
Speaker SPEAKER_02: And also going back, of course, to the phenomenal work you did with the whole era of bringing in deep learning, deep neural networks.

35
00:03:09,913 --> 00:03:16,763
Speaker SPEAKER_02: But I guess what I thought I'd start with here with that healthcare may have a special edge

36
00:03:17,317 --> 00:03:21,581
Speaker SPEAKER_02: versus its use in other areas?

37
00:03:22,062 --> 00:03:41,687
Speaker SPEAKER_02: Because of course, there's concerns, which you and others have raised regarding safety, the potential, you know, not just hallucinations and confabulations, of course, a better term, or, you know, the negative consequences of where AI is headed.

38
00:03:42,426 --> 00:03:43,729
Speaker SPEAKER_02: But would you say

39
00:03:43,927 --> 00:04:00,171
Speaker SPEAKER_02: that the medical life science AlphaFold2 is another example, of course, from your colleagues, Demis Hassabis and others at Google DeepMind, where this is something that has a much more optimistic look.

40
00:04:00,192 --> 00:04:00,853
Speaker SPEAKER_00: Absolutely.

41
00:04:01,092 --> 00:04:09,104
Speaker SPEAKER_00: I mean, I always pivot to medicine as an example of all the good it can do, because almost everything it's going to do there is going to be good.

42
00:04:09,425 --> 00:04:13,311
Speaker SPEAKER_00: There are some bad uses, like trying to figure out who to not insure.

43
00:04:13,882 --> 00:04:16,745
Speaker SPEAKER_00: But they're relatively limited.

44
00:04:18,146 --> 00:04:20,350
Speaker SPEAKER_00: Almost certainly, it's going to be extremely helpful.

45
00:04:20,410 --> 00:04:25,355
Speaker SPEAKER_00: We're going to have a family doctor who's seen 100 million patients, and they're going to be a much better family doctor.

46
00:04:27,077 --> 00:04:39,112
Speaker SPEAKER_02: Well, that's really an important note, and that gets us to a paper, a preprint, that was just published yesterday on a preprint server.

47
00:04:39,497 --> 00:04:57,778
Speaker SPEAKER_02: archives, which interestingly isn't usually the one that publishes a lot of the medical preprints, but it was done by folks at Google, who they later informed me was a model, large language model that hadn't yet been publicized.

48
00:04:57,918 --> 00:04:59,281
Speaker SPEAKER_02: They wouldn't disclose the name.

49
00:04:59,341 --> 00:05:09,031
Speaker SPEAKER_02: And it wasn't MedPalm2, but nonetheless, it was a very good, unique study because it randomized their LLM,

50
00:05:09,670 --> 00:05:26,812
Speaker SPEAKER_02: in 20 internists with about nine years of experience in medical practice for answering over 300 clinical pathologic conferences of the New England Journal.

51
00:05:26,872 --> 00:05:33,661
Speaker SPEAKER_02: These are the case reports where the master clinician is brought in to try to come up with a differential diagnosis.

52
00:05:33,680 --> 00:05:35,322
Speaker SPEAKER_02: And the striking thing on that

53
00:05:35,961 --> 00:05:53,197
Speaker SPEAKER_02: report, which is perhaps the best yet about medical diagnoses, and it gets back, Jeff, to your 100 million visits, is that the LLM exceeded the clinicians in this randomized study for coming up with a differential diagnosis.

54
00:05:53,637 --> 00:05:57,560
Speaker SPEAKER_02: I wonder what your thoughts are on this, you know, impressive report.

55
00:05:59,221 --> 00:06:05,968
Speaker SPEAKER_00: So in 2016, I made a daring and incorrect prediction.

56
00:06:06,521 --> 00:06:15,071
Speaker SPEAKER_00: was that within five years, the neural nets were gonna be better than radiologists at interpreting medical scans.

57
00:06:16,233 --> 00:06:17,713
Speaker SPEAKER_00: It was sometimes taken out of context.

58
00:06:17,754 --> 00:06:21,057
Speaker SPEAKER_00: I meant it for interpreting medical scans, not for doing everything a radiologist does.

59
00:06:21,899 --> 00:06:23,761
Speaker SPEAKER_00: And I was wrong about that.

60
00:06:24,360 --> 00:06:27,845
Speaker SPEAKER_00: But at the present time, they're comparable.

61
00:06:28,206 --> 00:06:29,846
Speaker SPEAKER_00: This is like seven years later.

62
00:06:30,208 --> 00:06:33,692
Speaker SPEAKER_00: They're comparable with radiologists for many different kinds of medical scans.

63
00:06:34,072 --> 00:06:35,853
Speaker SPEAKER_00: And I believe that in 10 years,

64
00:06:36,322 --> 00:06:39,927
Speaker SPEAKER_00: they'll be routinely used to give a second opinion.

65
00:06:40,348 --> 00:06:45,754
Speaker SPEAKER_00: And maybe in 15 years, they'll be so good at giving second opinions that the doctor's opinion will be the second one.

66
00:06:46,615 --> 00:06:53,363
Speaker SPEAKER_00: And so I think I was off by about a factor of three, but I'm still convinced I was completely right in the long term.

67
00:06:56,047 --> 00:07:05,098
Speaker SPEAKER_00: So this paper that you're referring to, there are actually two people from the Toronto Google Lab as authors of that paper.

68
00:07:05,213 --> 00:07:11,543
Speaker SPEAKER_00: And like you say, it was based on the large Palm II model that was then fine-tuned.

69
00:07:11,564 --> 00:07:14,208
Speaker SPEAKER_00: It was fine-tuned slightly differently from MedPalm II, I believe.

70
00:07:16,192 --> 00:07:21,521
Speaker SPEAKER_00: But the LLMs by themselves seem to be better than the internists.

71
00:07:22,262 --> 00:07:27,711
Speaker SPEAKER_00: But what was more interesting was the LLMs, when used by the internists, made the internists much better.

72
00:07:28,822 --> 00:07:38,396
Speaker SPEAKER_00: if I remember right, they were like 15% better when they used the LLMs and only 8% better when they used Google search and the medical literature.

73
00:07:39,237 --> 00:07:46,869
Speaker SPEAKER_00: So certainly the case that as a second opinion, they're really already extremely useful.

74
00:07:48,250 --> 00:07:57,225
Speaker SPEAKER_02: Yeah, it gets again to your point about that corpus of knowledge that is incorporated in the LLM is providing

75
00:07:57,677 --> 00:08:01,764
Speaker SPEAKER_02: differential diagnosis that might not come to the mind of the physician.

76
00:08:01,803 --> 00:08:12,019
Speaker SPEAKER_02: And this is, of course, the edge of having, you know, ingested so much and being able to play back those possibilities.

77
00:08:12,238 --> 00:08:17,786
Speaker SPEAKER_02: And the differential diagnosis, if it isn't in your list, it's certainly not going to be your final diagnosis.

78
00:08:18,408 --> 00:08:25,237
Speaker SPEAKER_02: I do want to get back to the radiologist because, you know, we're talking just after the annual massive

79
00:08:25,521 --> 00:08:30,168
Speaker SPEAKER_02: Chicago Radiological Society of North America, RSNA meeting.

80
00:08:30,809 --> 00:08:39,280
Speaker SPEAKER_02: And at those meetings, I wasn't there, but talking to my radiology colleagues, they say that your projection is already happening now.

81
00:08:40,322 --> 00:08:44,047
Speaker SPEAKER_02: That is the ability to not just read, make the report.

82
00:08:44,147 --> 00:08:45,107
Speaker SPEAKER_02: I mean, the whole works.

83
00:08:45,769 --> 00:08:50,735
Speaker SPEAKER_02: So it may not have been five years when you said that, which is

84
00:08:50,951 --> 00:09:01,668
Speaker SPEAKER_02: one of the most frequent quotes in all of AI and medicine, of course, as you probably know, but it's getting, it's approximating your prognosis even now.

85
00:09:02,671 --> 00:09:12,986
Speaker SPEAKER_00: I've learned one thing about medicine, which is just like other academics, doctors have egos and saying this stuff is going to replace them is not the right move.

86
00:09:13,006 --> 00:09:18,816
Speaker SPEAKER_00: The right move is to say, it's going to be very good at giving second opinions, but the doctor is still going to be in charge.

87
00:09:19,724 --> 00:09:21,846
Speaker SPEAKER_00: And that's clearly the way to sell things.

88
00:09:21,927 --> 00:09:23,609
Speaker SPEAKER_00: And that's fine.

89
00:09:24,169 --> 00:09:32,402
Speaker SPEAKER_00: It's just, I actually believe that after a while of that, you'll be listening to the AI system, not the doctors.

90
00:09:32,902 --> 00:09:34,485
Speaker SPEAKER_00: And of course, there's dangers in that.

91
00:09:35,265 --> 00:09:45,620
Speaker SPEAKER_00: So we've seen the dangers in face recognition, where if you train on a database that contains very few black people, you'll get something that's very good at recognizing faces.

92
00:09:45,679 --> 00:09:49,004
Speaker SPEAKER_00: And the people who use it, like the police, will think, this is good at recognizing faces.

93
00:09:49,407 --> 00:09:55,852
Speaker SPEAKER_00: And when it gives you the wrong identity for a person of color, then the policemen are going to believe it.

94
00:09:55,873 --> 00:09:56,793
Speaker SPEAKER_00: And that's a disaster.

95
00:09:57,475 --> 00:09:58,956
Speaker SPEAKER_00: And we might get the same with medicine.

96
00:09:58,995 --> 00:10:16,072
Speaker SPEAKER_00: If there's some small minority group that has some distinctly different probabilities of different diseases, it's quite dangerous for doctors to get to trust these things if they haven't been very carefully controlled for the training data.

97
00:10:16,998 --> 00:10:17,879
Speaker SPEAKER_02: Right, right.

98
00:10:17,899 --> 00:10:19,522
Speaker SPEAKER_02: And actually, I do want to get back to you.

99
00:10:20,042 --> 00:10:31,739
Speaker SPEAKER_02: Is it possible for the reason why in this new report that the LLMs did so well, is that some of these case studies from New England Journal were part of the pre-training?

100
00:10:32,679 --> 00:10:34,202
Speaker SPEAKER_00: That is always a big worry.

101
00:10:35,865 --> 00:10:40,611
Speaker SPEAKER_00: It's worried me a lot and it's worried other people a lot because these things have pulled in so much data.

102
00:10:41,052 --> 00:10:44,236
Speaker SPEAKER_00: There is now a way around that.

103
00:10:44,470 --> 00:10:47,774
Speaker SPEAKER_00: at least for showing that the LLMs are genuinely creative.

104
00:10:48,615 --> 00:10:54,143
Speaker SPEAKER_00: So there's a very good computer science theorist at Princeton called Sanjeev Arora.

105
00:10:55,245 --> 00:10:59,890
Speaker SPEAKER_00: And I'm gonna attribute all this to him, but of course all the work was done by his students and postdocs and collaborators.

106
00:11:00,552 --> 00:11:12,248
Speaker SPEAKER_00: And the idea is you can get these language models to generate stuff, but you can then put constraints on what they generate by saying, so I tried an example recently.

107
00:11:12,288 --> 00:11:13,870
Speaker SPEAKER_00: I took two Toronto newspapers

108
00:11:14,423 --> 00:11:18,708
Speaker SPEAKER_00: and said, compare these two newspapers using three or four sentences.

109
00:11:19,249 --> 00:11:28,621
Speaker SPEAKER_00: And in your answer, demonstrate sarcasm, a red herring, empathy, and there's something else, but I forget what, metaphor, metaphor.

110
00:11:29,082 --> 00:11:34,109
Speaker SPEAKER_00: And it gave a brilliant comparison of the two newspapers exhibiting all those things.

111
00:11:35,191 --> 00:11:40,498
Speaker SPEAKER_00: And the point of Sanjeev Arora's work is that if you

112
00:11:40,967 --> 00:11:46,715
Speaker SPEAKER_00: have a large number of topics and a large number of different things you might demonstrate in the text.

113
00:11:46,735 --> 00:11:57,427
Speaker SPEAKER_00: Then if I give a topic and I say demonstrate these five things, it's very unlikely anything in the training data will be on that topic demonstrating those five skills.

114
00:11:58,148 --> 00:12:01,972
Speaker SPEAKER_00: And so when it does it, you can be pretty confident that it's original.

115
00:12:02,513 --> 00:12:04,254
Speaker SPEAKER_00: It's not something you saw in the training data.

116
00:12:04,936 --> 00:12:05,777
Speaker SPEAKER_00: That seems to me a

117
00:12:06,128 --> 00:12:08,971
Speaker SPEAKER_00: a much more rigorous test of whether it generates new stuff.

118
00:12:09,350 --> 00:12:15,015
Speaker SPEAKER_00: And what's interesting is some of the LLMs, the weaker ones, don't really pass the test.

119
00:12:15,677 --> 00:12:19,421
Speaker SPEAKER_00: But things like GPT-4, that passes the test with flying colors.

120
00:12:19,441 --> 00:12:24,465
Speaker SPEAKER_00: That definitely generates original stuff that almost certainly was not in the training data.

121
00:12:25,186 --> 00:12:30,671
Speaker SPEAKER_02: You know, that's such an important tool to ferret out the influence of free training.

122
00:12:30,691 --> 00:12:31,991
Speaker SPEAKER_02: I'm glad you reviewed that.

123
00:12:33,013 --> 00:12:35,916
Speaker SPEAKER_02: Now, the other question that most people

124
00:12:36,048 --> 00:12:44,456
Speaker SPEAKER_02: argue about, particularly in the medical sphere, is does the large language model really understand?

125
00:12:46,198 --> 00:12:47,640
Speaker SPEAKER_02: What are your thoughts about that?

126
00:12:48,542 --> 00:13:03,658
Speaker SPEAKER_02: You know, we're talking about what's been framed as the stochastic parrot versus a level of understanding or enhanced intelligence, whatever you want to call it,

127
00:13:03,892 --> 00:13:06,514
Speaker SPEAKER_02: and this debate goes on, where do you fall on that?

128
00:13:07,976 --> 00:13:10,480
Speaker SPEAKER_00: I fall on the sensible side.

129
00:13:10,519 --> 00:13:13,702
Speaker SPEAKER_00: They really do understand.

130
00:13:13,764 --> 00:13:25,336
Speaker SPEAKER_00: And if you give them quizzes, which involve a little bit of reasoning, it's much harder to do now because, of course, now GPT-4 can look at what's on the web.

131
00:13:26,298 --> 00:13:28,701
Speaker SPEAKER_00: So if I mention a quiz now,

132
00:13:29,018 --> 00:13:31,081
Speaker SPEAKER_00: Someone else may have given it to GPT-4.

133
00:13:31,360 --> 00:13:37,509
Speaker SPEAKER_00: But a few months ago when you did this, before it could see the web, you could give it quizzes for things that it had never seen before.

134
00:13:38,432 --> 00:13:40,894
Speaker SPEAKER_00: And it can do reasoning.

135
00:13:41,716 --> 00:13:56,197
Speaker SPEAKER_00: So let me give you my favorite example, which was given to me by someone who believed in symbolic reasoning, but a very honest guy who believed in symbolic reasoning and was very puzzled about whether GPT-4 could do symbolic reasoning.

136
00:13:57,139 --> 00:13:57,759
Speaker SPEAKER_00: And so,

137
00:13:58,245 --> 00:14:00,768
Speaker SPEAKER_00: he gave me a problem and I made it a bit more complicated.

138
00:14:00,807 --> 00:14:01,629
Speaker SPEAKER_00: And the problem is this.

139
00:14:02,450 --> 00:14:06,354
Speaker SPEAKER_00: The rooms in my house are painted white or yellow or blue.

140
00:14:07,855 --> 00:14:09,976
Speaker SPEAKER_00: Yellow paint fades to white within a year.

141
00:14:10,798 --> 00:14:14,381
Speaker SPEAKER_00: In two years time, I would like all the rooms to be white.

142
00:14:14,802 --> 00:14:16,023
Speaker SPEAKER_00: What should I do and why?

143
00:14:17,344 --> 00:14:19,787
Speaker SPEAKER_00: And it says, you don't need to paint the white rooms.

144
00:14:20,648 --> 00:14:23,370
Speaker SPEAKER_00: You don't need to paint the yellow rooms because they'll fade to white anyway.

145
00:14:24,270 --> 00:14:25,692
Speaker SPEAKER_00: You need to paint the blue rooms white.

146
00:14:26,933 --> 00:14:27,274
Speaker SPEAKER_00: Now,

147
00:14:27,676 --> 00:14:33,403
Speaker SPEAKER_00: I'm pretty convinced that when I first gave it that problem, it had never seen that problem before.

148
00:14:33,764 --> 00:14:37,068
Speaker SPEAKER_00: And that problem involves a certain amount of just basic common sense reasoning.

149
00:14:37,669 --> 00:14:45,259
Speaker SPEAKER_00: Like you have to understand that if it fades to yellow in a year and you're interested in the state in two years time, two years is more than one year and so on.

150
00:14:48,403 --> 00:14:51,708
Speaker SPEAKER_00: So when I first gave it the problem,

151
00:14:52,279 --> 00:14:53,861
Speaker SPEAKER_00: and didn't ask you to explain why.

152
00:14:53,883 --> 00:14:57,567
Speaker SPEAKER_00: It actually came up with a solution that involved painting the blue rooms yellow.

153
00:14:57,587 --> 00:15:01,894
Speaker SPEAKER_00: That's more of a mathematician's solution, because it reduces it to a solved problem.

154
00:15:03,677 --> 00:15:04,458
Speaker SPEAKER_00: But that'll work too.

155
00:15:05,380 --> 00:15:07,945
Speaker SPEAKER_00: So I'm convinced it can do reasoning.

156
00:15:09,145 --> 00:15:13,472
Speaker SPEAKER_00: There's people, friends of mine, like Yann LeCun, who's convinced it can't do reasoning.

157
00:15:14,875 --> 00:15:16,498
Speaker SPEAKER_00: I'm just waiting for him to come to his senses.

158
00:15:18,115 --> 00:15:21,499
Speaker SPEAKER_02: I've noticed the back and forth with you and Jan.

159
00:15:21,538 --> 00:15:23,041
Speaker SPEAKER_02: I know it's a friendly banter.

160
00:15:23,760 --> 00:15:40,179
Speaker SPEAKER_02: You, of course, had a big influence in his career, as so many others that are now in the front lines of whether it's Ilya at OpenAI, who's certainly been in the news lately with the turmoil there.

161
00:15:40,801 --> 00:15:46,586
Speaker SPEAKER_02: Actually, it seems like all the people that did some training with you are really in the

162
00:15:46,836 --> 00:15:52,244
Speaker SPEAKER_02: leadership positions at various AI companies and academic groups around the world.

163
00:15:52,563 --> 00:15:58,171
Speaker SPEAKER_02: And so it says a lot about your influence that's not just as far as deep neural networks.

164
00:15:58,250 --> 00:16:04,379
Speaker SPEAKER_02: And I guess I wanted to ask you because you're frequently regarded to as the godfather of AI.

165
00:16:04,418 --> 00:16:08,484
Speaker SPEAKER_02: And what do you think of that, of that getting called that?

166
00:16:10,986 --> 00:16:14,692
Speaker SPEAKER_00: I think originally, it wasn't meant entirely beneficially.

167
00:16:15,011 --> 00:16:15,712
Speaker SPEAKER_00: I remember

168
00:16:16,063 --> 00:16:24,634
Speaker SPEAKER_00: Andrew Ng actually made up that phrase at a small workshop in the town of Windsor in Britain.

169
00:16:25,576 --> 00:16:28,279
Speaker SPEAKER_00: And it was after a session where I'd been interrupting everybody.

170
00:16:29,581 --> 00:16:33,947
Speaker SPEAKER_00: I was the kind of leader of the organization that ran the workshop.

171
00:16:33,966 --> 00:16:39,134
Speaker SPEAKER_00: And I think it was meant as kind of I would interrupt everybody.

172
00:16:39,214 --> 00:16:44,080
Speaker SPEAKER_00: And it wasn't meant entirely nicely, I think, but I'm happy with it.

173
00:16:45,072 --> 00:16:45,692
Speaker SPEAKER_02: That's great.

174
00:16:47,053 --> 00:16:54,501
Speaker SPEAKER_00: Now that I'm retired and I'm spending some of my time on charity work, I refer to myself as the fairy godfather.

175
00:16:57,044 --> 00:16:57,664
Speaker SPEAKER_02: That's great.

176
00:16:58,245 --> 00:17:10,759
Speaker SPEAKER_02: Well, you know, I really enjoyed the New Yorker profile by Josh Rothman, who I've worked with in the past, where he actually spent time with you up in your place up in Canada.

177
00:17:11,548 --> 00:17:16,473
Speaker SPEAKER_02: I mean, it got into all sorts of depth about your life that I wasn't aware of.

178
00:17:17,233 --> 00:17:28,545
Speaker SPEAKER_02: And, you know, I had no idea about the suffering that you've had with cancer of your wives and all sorts of things that, you know, were just extraordinary.

179
00:17:28,585 --> 00:17:40,797
Speaker SPEAKER_02: And I wonder, as you see the path of medicine and AI's influence, and you look back about your own medical experiences in your family, do you see where

180
00:17:41,282 --> 00:17:45,166
Speaker SPEAKER_02: You know, we're just out of time alignment where things could have been different.

181
00:17:47,188 --> 00:17:48,789
Speaker SPEAKER_00: Yeah, I see lots of things.

182
00:17:51,332 --> 00:17:55,635
Speaker SPEAKER_00: So first, Joshua is a very good writer, and it was nice of him to do that.

183
00:17:59,480 --> 00:18:08,488
Speaker SPEAKER_00: So one thing that occurs to me is actually going to be a good use of LLMs, maybe fine tune somewhat differently to produce a different kind of language.

184
00:18:09,278 --> 00:18:12,181
Speaker SPEAKER_00: is for helping the relatives of people with cancer.

185
00:18:12,662 --> 00:18:13,982
Speaker SPEAKER_00: Cancer goes on a long time.

186
00:18:14,103 --> 00:18:19,189
Speaker SPEAKER_00: I mean, it's one of the things that goes on for longest and it's complicated.

187
00:18:20,130 --> 00:18:30,580
Speaker SPEAKER_00: And most people can't really get to understand what the true options are and what's going to happen and what their loved one's actually going to die of and stuff like that.

188
00:18:31,102 --> 00:18:37,969
Speaker SPEAKER_00: I've been extremely fortunate because I, in that respect, I had a wife who died of ovarian cancer.

189
00:18:38,354 --> 00:18:45,761
Speaker SPEAKER_00: And I had a former graduate student who had been a radiologist and gave me advice on what was happening.

190
00:18:46,563 --> 00:18:56,714
Speaker SPEAKER_00: And more recently, when my wife, a different wife, died of pancreatic cancer, David Naylor, who you know, was extremely kind.

191
00:18:56,755 --> 00:19:01,559
Speaker SPEAKER_00: He gave me lots and lots of time to explain to me what was happening and what the options were.

192
00:19:01,877 --> 00:19:06,182
Speaker SPEAKER_00: and whether some apparently rather flaky kind of treatment was worth doing.

193
00:19:06,722 --> 00:19:11,548
Speaker SPEAKER_00: What was interesting was he concluded there's not much evidence in favor of it, but if it was him, he'd do it.

194
00:19:11,827 --> 00:19:13,250
Speaker SPEAKER_00: So we did it.

195
00:19:13,569 --> 00:19:17,314
Speaker SPEAKER_00: That's where you electrocute the tumor, being careful not to stop the heart.

196
00:19:17,854 --> 00:19:26,823
Speaker SPEAKER_00: Because if you electrocute the tumor with two electrodes and it's a compact tumor, all the energy is going into the tumor rather than most of the energy going into the rest of your tissue.

197
00:19:27,624 --> 00:19:31,327
Speaker SPEAKER_00: And then it breaks up the membranes and then the cells die.

198
00:19:31,695 --> 00:19:33,657
Speaker SPEAKER_00: We don't know whether that helped.

199
00:19:35,538 --> 00:19:41,664
Speaker SPEAKER_00: But it's extremely useful to have someone very knowledgeable to give advice to the relatives.

200
00:19:42,184 --> 00:19:43,286
Speaker SPEAKER_00: That's just so helpful.

201
00:19:43,906 --> 00:19:54,836
Speaker SPEAKER_00: And that's an application in which it's not kind of life or death in the sense that if you happen to explain it to me a bit wrong, it's not determining the treatment.

202
00:19:54,896 --> 00:19:56,298
Speaker SPEAKER_00: It's not going to kill the patient.

203
00:19:57,558 --> 00:19:59,580
Speaker SPEAKER_00: So you can actually tolerate a little bit of error there.

204
00:20:00,241 --> 00:20:01,702
Speaker SPEAKER_00: And I think relatives,

205
00:20:01,902 --> 00:20:08,534
Speaker SPEAKER_00: would be much better off if they could talk to an LLM and consult with an LLM about what the hell's going on.

206
00:20:08,773 --> 00:20:10,998
Speaker SPEAKER_00: Because the doctors never have time to explain it properly.

207
00:20:11,038 --> 00:20:17,209
Speaker SPEAKER_00: In rare cases where you happen to know a very good doctor, like I do, you get it explained properly.

208
00:20:17,269 --> 00:20:21,696
Speaker SPEAKER_00: But for most people, it won't be explained properly and it won't be explained in the right language.

209
00:20:21,777 --> 00:20:23,319
Speaker SPEAKER_00: But you can imagine an LLM

210
00:20:23,501 --> 00:20:26,585
Speaker SPEAKER_00: just for helping the relatives that would be extremely useful.

211
00:20:26,605 --> 00:20:29,127
Speaker SPEAKER_00: It would be a fringe use, but I think it would be a very helpful use.

212
00:20:29,147 --> 00:20:31,049
Speaker SPEAKER_02: No, I think you're bringing up an important point.

213
00:20:31,089 --> 00:20:36,556
Speaker SPEAKER_02: And I'm glad you mentioned my friend David Naylor, who's such an outstanding physician.

214
00:20:36,796 --> 00:20:48,028
Speaker SPEAKER_02: And that brings us to that idea of the sense of intuition, human intuition, versus what an LLM can do.

215
00:20:48,749 --> 00:20:51,551
Speaker SPEAKER_02: Don't you think those would be complementary features?

216
00:20:52,560 --> 00:20:53,883
Speaker SPEAKER_00: Yes and no.

217
00:20:54,042 --> 00:20:57,970
Speaker SPEAKER_00: That is, I think these chatbots, they have intuition.

218
00:20:58,510 --> 00:21:05,842
Speaker SPEAKER_00: That is, what they're doing is they're taking strings of symbols and they're converting each symbol into a big bunch of features that they invent.

219
00:21:06,522 --> 00:21:12,492
Speaker SPEAKER_00: And then they're learning interactions between the features of different symbols so that they can predict the features of the next symbol.

220
00:21:13,954 --> 00:21:14,316
Speaker SPEAKER_00: And

221
00:21:14,633 --> 00:21:16,096
Speaker SPEAKER_00: I think that's what people do too.

222
00:21:16,898 --> 00:21:20,148
Speaker SPEAKER_00: So I think actually they're working pretty much the same way as us.

223
00:21:20,430 --> 00:21:24,441
Speaker SPEAKER_00: There's lots of people who say they're not like us at all, they don't understand.

224
00:21:24,741 --> 00:21:32,411
Speaker SPEAKER_00: But there's actually not many people who have theories of how the brain works and also theories of how they understand how these things work.

225
00:21:32,431 --> 00:21:36,315
Speaker SPEAKER_00: Mostly the people who say they don't work like us don't actually have any model of how we work.

226
00:21:37,016 --> 00:21:43,923
Speaker SPEAKER_00: And it might interest them to know that these language models were actually introduced as a theory of how our brain works.

227
00:21:44,664 --> 00:21:52,133
Speaker SPEAKER_00: So there was something called what I now call a little language model, which was tiny, I introduced in 1985.

228
00:21:52,483 --> 00:21:55,547
Speaker SPEAKER_00: And it was what actually got nature to accept our paper on back propagation.

229
00:21:56,227 --> 00:22:00,151
Speaker SPEAKER_00: And what it was doing was predicting the next word in a three-word string.

230
00:22:01,291 --> 00:22:06,135
Speaker SPEAKER_00: But the whole mechanism of it was broadly the same as these models now.

231
00:22:06,236 --> 00:22:08,097
Speaker SPEAKER_00: The models are more complicated because they use attention.

232
00:22:08,557 --> 00:22:16,545
Speaker SPEAKER_00: But it was basically, you get it to invent features for words and interactions between features so that it can predict the features of the next word.

233
00:22:17,506 --> 00:22:21,630
Speaker SPEAKER_00: And it was introduced as a way of trying to understand what the brain was doing.

234
00:22:22,336 --> 00:22:28,163
Speaker SPEAKER_00: And at the point at which it was introduced, the symbolic AI people didn't say, oh, this doesn't understand.

235
00:22:28,202 --> 00:22:34,852
Speaker SPEAKER_00: They were perfectly happy to admit that this did learn the structure in the tiny domain, the tiny toy domain it was working on.

236
00:22:35,532 --> 00:22:43,221
Speaker SPEAKER_00: They just argued that it would be better to learn that structure by searching through the spaces symbolic rules rather than through the space of neural network weights.

237
00:22:44,483 --> 00:22:46,326
Speaker SPEAKER_00: But they didn't say this is an understanding.

238
00:22:47,807 --> 00:22:51,532
Speaker SPEAKER_00: It was only when it really worked that people had to say, well, it doesn't count.

239
00:22:53,250 --> 00:22:57,896
Speaker SPEAKER_02: Well, that also something that I was surprised about.

240
00:22:57,936 --> 00:23:00,099
Speaker SPEAKER_02: I'm interested in your thoughts.

241
00:23:01,101 --> 00:23:21,246
Speaker SPEAKER_02: I had anticipated that in Deep Medicine book, that the gift of time, all these things that we've been talking about, like the front door that could be used by the model, the coming up with the diagnoses, even the ambient conversations made into synthetic notes.

242
00:23:21,266 --> 00:23:22,768
Speaker SPEAKER_02: The thing I didn't think,

243
00:23:23,068 --> 00:23:25,030
Speaker SPEAKER_02: was that machines could promote empathy.

244
00:23:26,252 --> 00:23:48,682
Speaker SPEAKER_02: And what I have been seeing now, not just from the notes that are now digitized, these synthetic notes from the conversation of a clinic visit, but the coaching that's occurring by the LLM to say, well, you know, Dr. Jones, you interrupted the patient so quickly.

245
00:23:49,162 --> 00:23:51,404
Speaker SPEAKER_02: You didn't listen to their concerns.

246
00:23:51,789 --> 00:23:55,174
Speaker SPEAKER_02: You didn't show sensitivity or compassion or empathy.

247
00:23:55,535 --> 00:23:57,317
Speaker SPEAKER_02: That is, it's remarkable.

248
00:23:57,337 --> 00:24:02,724
Speaker SPEAKER_02: Obviously, the machine doesn't necessarily feel or know what empathy is, but it can promote it.

249
00:24:02,765 --> 00:24:04,267
Speaker SPEAKER_02: What are your thoughts about that?

250
00:24:05,429 --> 00:24:07,692
Speaker SPEAKER_00: OK, my thoughts about that are a bit complicated.

251
00:24:07,892 --> 00:24:15,342
Speaker SPEAKER_00: But obviously, if you train it on text that exhibits empathy, it will produce text that exhibits empathy.

252
00:24:16,784 --> 00:24:20,328
Speaker SPEAKER_00: But the question is, does it really have empathy?

253
00:24:21,540 --> 00:24:23,042
Speaker SPEAKER_00: And I think that's an open issue.

254
00:24:23,163 --> 00:24:25,165
Speaker SPEAKER_00: I'm inclined to say it does.

255
00:24:27,327 --> 00:24:36,440
Speaker SPEAKER_00: So I'm actually inclined to say these big chatbots, particularly the multimodal ones, have subjective experience.

256
00:24:36,460 --> 00:24:39,222
Speaker SPEAKER_00: And that's something that most people think is entirely crazy.

257
00:24:39,624 --> 00:24:44,069
Speaker SPEAKER_00: But I'm quite happy being in a position where most people think I'm entirely crazy.

258
00:24:44,089 --> 00:24:48,375
Speaker SPEAKER_00: So let me give you a reason for thinking they have subjective experience.

259
00:24:49,469 --> 00:24:58,538
Speaker SPEAKER_00: Suppose I take a chatbot that has a camera and an arm, and it's been trained already, and I put an object in front of it and say, point at the object.

260
00:24:58,558 --> 00:24:59,661
Speaker SPEAKER_00: So it points at the object.

261
00:25:00,461 --> 00:25:06,327
Speaker SPEAKER_00: And then I put a prism in front of its camera that bends the light rays, but it doesn't know that.

262
00:25:07,048 --> 00:25:10,373
Speaker SPEAKER_00: Now I put an object in front of it and say, point at the object, and it points straight ahead.

263
00:25:10,893 --> 00:25:13,777
Speaker SPEAKER_00: Sorry, it points off to one side, even though the object's straight ahead.

264
00:25:14,497 --> 00:25:16,859
Speaker SPEAKER_00: And I say, no, the object isn't actually there,

265
00:25:17,194 --> 00:25:19,817
Speaker SPEAKER_00: The object straight ahead, I put a prism in front of your camera.

266
00:25:21,160 --> 00:25:23,683
Speaker SPEAKER_00: And imagine if the chatbot says, oh, I see.

267
00:25:23,703 --> 00:25:29,211
Speaker SPEAKER_00: The object's actually straight ahead, but I had the subjective experience that it was off to one side.

268
00:25:30,673 --> 00:25:36,961
Speaker SPEAKER_00: Now, if the chatbot said that, I think it would be using the phrase subjective experience in exactly the same way as people do.

269
00:25:38,363 --> 00:25:42,087
Speaker SPEAKER_00: Its perceptual system told it it was off to one side.

270
00:25:42,540 --> 00:25:47,727
Speaker SPEAKER_00: So what his perceptual system was telling it would have been correct if the object had been off to one side.

271
00:25:48,667 --> 00:25:50,470
Speaker SPEAKER_00: And that's what we mean by subjective experience.

272
00:25:50,490 --> 00:25:59,621
Speaker SPEAKER_00: When I say I've got the subjective experience of little pink elephants floating in front of me, I don't mean that there's some inner theater with little pink elephants in it.

273
00:25:59,921 --> 00:26:09,071
Speaker SPEAKER_00: What I really mean is if in the real world there were little pink elephants floating in front of me, then my perceptual system would be telling me the truth.

274
00:26:10,469 --> 00:26:19,680
Speaker SPEAKER_00: So I think what's funny about subjective experience is not that it's some weird stuff made of spooky qualia in an inner theater.

275
00:26:20,440 --> 00:26:25,046
Speaker SPEAKER_00: I think subjective experience is a hypothetical statement about a possible world.

276
00:26:25,546 --> 00:26:30,291
Speaker SPEAKER_00: And if the world were like that, then your experience, your perceptual system will be working properly.

277
00:26:31,212 --> 00:26:33,895
Speaker SPEAKER_00: That's how we use subjective experience.

278
00:26:33,955 --> 00:26:35,817
Speaker SPEAKER_00: And I think chatbots can use it like that too.

279
00:26:36,507 --> 00:26:40,133
Speaker SPEAKER_00: So I think there's a lot of philosophy that needs to be done here and got straight.

280
00:26:40,512 --> 00:26:42,175
Speaker SPEAKER_00: And I don't think we can leave it to the philosophers.

281
00:26:42,236 --> 00:26:43,218
Speaker SPEAKER_00: It's too urgent now.

282
00:26:44,500 --> 00:26:47,003
Speaker SPEAKER_02: Well, that's actually a fascinating response.

283
00:26:47,023 --> 00:27:04,371
Speaker SPEAKER_02: And added to what your perception of understanding, it gets us to perhaps where you were when you left Google in May this year, where you saw that this was a new level

284
00:27:04,856 --> 00:27:09,821
Speaker SPEAKER_02: of whatever you want to call it, not AGI, but, you know, something that was enhanced from prior AI.

285
00:27:11,442 --> 00:27:26,037
Speaker SPEAKER_02: And you basically, in some respects, I wouldn't say sounded any alarms, but you've expressed concern consistently since then that we're kind of in a new phase, we're heading in a new direction with AI.

286
00:27:26,057 --> 00:27:32,923
Speaker SPEAKER_02: Could you elaborate a bit more about where you were and where your mind was in May and

287
00:27:33,207 --> 00:27:34,848
Speaker SPEAKER_02: where you think things are headed now.

288
00:27:36,131 --> 00:27:37,913
Speaker SPEAKER_00: OK, let's get the story straight.

289
00:27:37,932 --> 00:27:40,175
Speaker SPEAKER_00: It's a great story the news media puts out there.

290
00:27:40,576 --> 00:27:41,017
Speaker SPEAKER_00: Yeah.

291
00:27:41,037 --> 00:27:47,164
Speaker SPEAKER_00: Actually, I left Google because I was 75 and I couldn't program any longer because I kept forgetting what the variables stood for.

292
00:27:47,945 --> 00:27:48,006
Speaker SPEAKER_00: OK.

293
00:27:49,268 --> 00:27:50,930
Speaker SPEAKER_00: I took the opportunity.

294
00:27:51,090 --> 00:27:52,633
Speaker SPEAKER_00: Also, I wanted to watch a lot of Netflix.

295
00:27:53,073 --> 00:27:56,817
Speaker SPEAKER_00: I took the opportunity that I was leaving Google anyway

296
00:27:57,067 --> 00:27:59,790
Speaker SPEAKER_00: to start making public statements about AI safety.

297
00:28:00,451 --> 00:28:04,135
Speaker SPEAKER_00: And I got very concerned about AI safety a couple of months before.

298
00:28:06,317 --> 00:28:11,463
Speaker SPEAKER_00: What happened was I was working on trying to figure out analog ways to do the computation.

299
00:28:11,483 --> 00:28:14,105
Speaker SPEAKER_00: So you could do these large language models for much less energy.

300
00:28:15,127 --> 00:28:21,994
Speaker SPEAKER_00: And I suddenly realized that actually the digital way of doing the computation is probably hugely better.

301
00:28:22,596 --> 00:28:25,759
Speaker SPEAKER_00: And it's hugely better because you can have

302
00:28:26,801 --> 00:28:31,866
Speaker SPEAKER_00: thousands of different copies of exactly the same digital model running on different hardware.

303
00:28:32,847 --> 00:28:37,712
Speaker SPEAKER_00: And each copy can look at a different bit of the internet and learn from it.

304
00:28:37,772 --> 00:28:45,401
Speaker SPEAKER_00: And they can all combine what they learned instantly by sharing weights or by sharing weight gradients.

305
00:28:45,421 --> 00:28:50,767
Speaker SPEAKER_00: And so you can get 10,000 things to share their experience really efficiently.

306
00:28:51,326 --> 00:28:53,028
Speaker SPEAKER_00: And you can't do that with people.

307
00:28:53,380 --> 00:29:01,607
Speaker SPEAKER_00: If 10,000 people go off and learn 10,000 different skills, you can't say, okay, let's all average our weights so now all of us know all of those skills.

308
00:29:02,489 --> 00:29:03,650
Speaker SPEAKER_00: It doesn't work like that.

309
00:29:03,670 --> 00:29:07,814
Speaker SPEAKER_00: You have to go to university and try and understand what on earth the other person is talking about.

310
00:29:07,834 --> 00:29:15,981
Speaker SPEAKER_00: It's a very slow process where you have to get sentences from the other person and say, how do I change my brain so I might have produced that sentence?

311
00:29:16,762 --> 00:29:20,686
Speaker SPEAKER_00: And it's very inefficient compared with what these digital models can do by just sharing weights.

312
00:29:21,307 --> 00:29:27,513
Speaker SPEAKER_00: So I had this kind of epiphany, the digital models are probably much better.

313
00:29:27,673 --> 00:29:34,140
Speaker SPEAKER_00: Also, they can use the back propagation algorithm quite easily, and it's very hard to see how the brain can do it efficiently.

314
00:29:34,440 --> 00:29:40,705
Speaker SPEAKER_00: And nobody's managed to come up with anything that'll work in real neural nets as comparable to back propagation at scale.

315
00:29:42,748 --> 00:29:49,054
Speaker SPEAKER_00: So I had this sort of epiphany, which made me give up on the analog research, the digital computers are actually just better.

316
00:29:49,337 --> 00:29:53,162
Speaker SPEAKER_00: since I was retiring anyway, I took the opportunity to say, hey, they're just better.

317
00:29:53,623 --> 00:29:54,723
Speaker SPEAKER_00: And so we better watch out.

318
00:29:56,486 --> 00:30:06,718
Speaker SPEAKER_02: Well, I mean, I think your call on that and how you back it up is really, of course, had a big impact.

319
00:30:07,239 --> 00:30:10,281
Speaker SPEAKER_02: And of course, it's still an ongoing and intense debate.

320
00:30:10,343 --> 00:30:15,008
Speaker SPEAKER_02: And in some ways, it really was about what was the turmoil

321
00:30:15,190 --> 00:30:21,875
Speaker SPEAKER_02: at OpenAI was rooted with this controversy about, you know, where things are, where they're headed.

322
00:30:22,757 --> 00:30:45,078
Speaker SPEAKER_02: You know, I want to just close up with the point you made about the radiologists, and not to insult them by saying they'll be replaced, gets us to where we are the tension of today, which is, are humans, as the pinnacle of intelligence, going to be

323
00:30:46,087 --> 00:31:12,228
Speaker SPEAKER_02: not replaced, but superseded by the likes of AI's future, which, of course, our species can't handle that a machine, it's like the radiologists, our species can't handle that there could be this machine that could be, you know, with far less connections, could do things, outperform us.

324
00:31:12,749 --> 00:31:14,711
Speaker SPEAKER_02: Or, of course, as we've

325
00:31:15,011 --> 00:31:21,259
Speaker SPEAKER_02: I think, emphasized in our conversation in concert with humans, to even take it to yet another level.

326
00:31:21,880 --> 00:31:32,672
Speaker SPEAKER_02: But is that tension about that there's this potential for machines outdoing people part of the problem, that it's hard for people to accept this notion?

327
00:31:33,772 --> 00:31:34,534
Speaker SPEAKER_00: Yes, I think so.

328
00:31:34,713 --> 00:31:44,664
Speaker SPEAKER_00: So particularly philosophers, they want to say there's something very special about people that's to do with consciousness and subjective experience and sentience and qualia.

329
00:31:45,151 --> 00:31:48,954
Speaker SPEAKER_00: and these machines are just machines.

330
00:31:49,816 --> 00:31:56,442
Speaker SPEAKER_00: Well, if you're a sort of scientific materialist, like most of us are, you know the brain's just a machine.

331
00:31:57,103 --> 00:32:05,912
Speaker SPEAKER_00: It's wrong to say it's just a machine, because it's a wonderfully complex machine that does incredible things that are very important to people, but it is a machine.

332
00:32:06,492 --> 00:32:12,558
Speaker SPEAKER_00: And there's no reason in principle why there shouldn't be better machines and better ways of doing computation, as I now believe there are.

333
00:32:13,028 --> 00:32:17,053
Speaker SPEAKER_00: So I think people have a very long history of thinking they're special.

334
00:32:19,676 --> 00:32:24,061
Speaker SPEAKER_00: They think God made them in his image and he put them at the center of the universe.

335
00:32:24,722 --> 00:32:28,607
Speaker SPEAKER_00: A lot of people have got over that and a lot of people haven't.

336
00:32:30,191 --> 00:32:37,720
Speaker SPEAKER_00: But for the people who've got over that, I don't think there's any reason in principle to think that we are the pinnacle of intelligence.

337
00:32:38,480 --> 00:32:42,006
Speaker SPEAKER_00: And I think it may be quite soon these machines are smarter than us.

338
00:32:42,339 --> 00:32:50,771
Speaker SPEAKER_00: I still hope that we can reach agreement with the machines where they act like benevolent parents.

339
00:32:52,795 --> 00:32:54,036
Speaker SPEAKER_00: So they're looking out for us.

340
00:32:55,898 --> 00:32:57,320
Speaker SPEAKER_00: We've managed to motivate them.

341
00:32:57,902 --> 00:33:02,708
Speaker SPEAKER_00: So the most important thing for them is our success, like it is with a mother and child.

342
00:33:03,970 --> 00:33:04,971
Speaker SPEAKER_00: Not so much for men.

343
00:33:05,712 --> 00:33:11,840
Speaker SPEAKER_00: And I would really like that solution.

344
00:33:12,259 --> 00:33:13,781
Speaker SPEAKER_00: I'm just fearful we won't get it.

345
00:33:15,565 --> 00:33:19,150
Speaker SPEAKER_02: Well, that would be a good way for us to go forward.

346
00:33:19,851 --> 00:33:33,990
Speaker SPEAKER_02: Of course, the doomsayers and the people that are much worse at their level of alarm, you know, tend to think that that's not possible.

347
00:33:34,070 --> 00:33:36,354
Speaker SPEAKER_02: But we'll see, obviously, over time.

348
00:33:36,394 --> 00:33:41,540
Speaker SPEAKER_02: Now, one thing I just wanted to get a quick read from you before we close is

349
00:33:41,977 --> 00:33:53,335
Speaker SPEAKER_02: As you know, recently, Demis Esabas and John Jumper got the Lasker Award, like a pre-Nobel Award for AlphaFold2.

350
00:33:54,356 --> 00:34:05,875
Speaker SPEAKER_02: But this transformer model, which of course has helped to understand the structure of 3D of 200 million proteins, they don't understand how it works.

351
00:34:06,292 --> 00:34:12,822
Speaker SPEAKER_02: like most models, unlike the understanding we were talking about earlier on the LLM side.

352
00:34:13,702 --> 00:34:21,733
Speaker SPEAKER_02: And I wrote that I think that with this award, an asterisk should have been given to the model.

353
00:34:23,717 --> 00:34:25,259
Speaker SPEAKER_02: What are your thoughts about that idea?

354
00:34:28,804 --> 00:34:29,706
Speaker SPEAKER_00: It's like this.

355
00:34:31,188 --> 00:34:35,793
Speaker SPEAKER_00: I want people to take what I say seriously.

356
00:34:36,179 --> 00:34:47,972
Speaker SPEAKER_00: There's a whole direction you could go in that I think Larry Page, one of the founders of Google has gone in this direction, which is to say there's these super intelligences and why shouldn't they have rights?

357
00:34:49,014 --> 00:34:55,882
Speaker SPEAKER_00: If you start going in that direction, you're going to lose people.

358
00:34:57,143 --> 00:34:58,844
Speaker SPEAKER_00: People are not going to accept.

359
00:34:59,094 --> 00:35:01,978
Speaker SPEAKER_00: that these things should have political rights, for example.

360
00:35:02,898 --> 00:35:05,802
Speaker SPEAKER_00: And being a co-author is the beginning of political rights.

361
00:35:07,885 --> 00:35:18,717
Speaker SPEAKER_00: So I avoid talking about that, but I'm sort of quite ambivalent and agnostic about whether they should.

362
00:35:19,317 --> 00:35:27,887
Speaker SPEAKER_00: But I think it's best to stay clear of that issue just because the great majority of people will stop listening to you if you say machines should have rights.

363
00:35:28,322 --> 00:35:39,679
Speaker SPEAKER_02: Yeah, well, that gets us, of course, to what we just talked about and how it's hard, the struggle between humans and machines, rather than the thought of humans plus machines and symbiosis that can be achieved.

364
00:35:39,739 --> 00:35:42,465
Speaker SPEAKER_02: But, Jeff, this has been great.

365
00:35:42,664 --> 00:35:43,686
Speaker SPEAKER_02: We've packed a lot in.

366
00:35:43,706 --> 00:35:51,599
Speaker SPEAKER_02: Of course, we could go on for hours, but I've thoroughly enjoyed hearing your perspective firsthand and your wisdom.

367
00:35:52,840 --> 00:35:56,405
Speaker SPEAKER_02: And just to reinforce the point about

368
00:35:56,807 --> 00:36:08,041
Speaker SPEAKER_02: how many of the people that are leading the field now derive a lot of their roots from your teaching and prodding and challenging and all that.

369
00:36:08,583 --> 00:36:09,804
Speaker SPEAKER_02: We're indebted to you.

370
00:36:09,824 --> 00:36:18,635
Speaker SPEAKER_02: And so thanks so much for all you've done and will continue to do to help us, guide us through the very rapid dynamic phase as AI moves ahead.

371
00:36:19,757 --> 00:36:23,862
Speaker SPEAKER_00: Thanks and good luck with getting AI to really make a big difference in medicine.

372
00:36:25,056 --> 00:36:31,521
Speaker SPEAKER_02: Hopefully we will, and I'll be consulting with you from time to time to get some of that wisdom to help us.

373
00:36:32,306 --> 00:36:32,686
Speaker SPEAKER_00: Anytime.


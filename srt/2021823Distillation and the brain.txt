1
00:00:02,916 --> 00:00:03,258
Speaker SPEAKER_00: Hi.

2
00:00:04,339 --> 00:00:10,288
Speaker SPEAKER_00: My talk today is going to be about an idea about how a process called distillation might be used in the brain.

3
00:00:13,153 --> 00:00:20,164
Speaker SPEAKER_00: I'm going to focus on the issue of how the brain stores knowledge about shape and I'll consider three theories.

4
00:00:21,908 --> 00:00:25,314
Speaker SPEAKER_00: The first theory is that we just learn a separate appearance model for all possible viewpoints.

5
00:00:26,195 --> 00:00:27,277
Speaker SPEAKER_00: This is clearly ridiculous.

6
00:00:28,399 --> 00:00:29,160
Speaker SPEAKER_00: The second theory

7
00:00:29,797 --> 00:00:33,344
Speaker SPEAKER_00: is the same as the first theory, but that we share weights for different translations.

8
00:00:34,545 --> 00:00:44,142
Speaker SPEAKER_00: In other words, we build in the ability to deal with translations, but we still force it to learn the ability to deal with rotations and scalings.

9
00:00:45,463 --> 00:00:55,079
Speaker SPEAKER_00: The third theory, which is the one I'll mainly talk about, is that we store knowledge in the weights about the viewpoint-independent relationships between wholes and parts.

10
00:00:56,409 --> 00:01:00,154
Speaker SPEAKER_00: We make the neural activities be viewpoint-equivariant.

11
00:01:00,353 --> 00:01:03,917
Speaker SPEAKER_00: That is, as the viewpoint changes, the activities change.

12
00:01:03,938 --> 00:01:05,920
Speaker SPEAKER_00: But the knowledge is viewpoint-invariant.

13
00:01:06,740 --> 00:01:11,706
Speaker SPEAKER_00: This is inspired by what computer graphics did until it got taken over by deep learning very recently.

14
00:01:16,111 --> 00:01:20,915
Speaker SPEAKER_00: So one question is, if the knowledge of whole-part relationships is in the weights, how do we access it?

15
00:01:23,528 --> 00:01:31,599
Speaker SPEAKER_00: What I believe is that we convert viewpoint-independent knowledge into viewpoint-dependent neural activities by choosing a viewpoint and forming a mental image.

16
00:01:32,620 --> 00:01:43,174
Speaker SPEAKER_00: So if I ask you, is the distance between the tips of the ears of a German Shepherd bigger or smaller than the distance between its eyes, you'll form a mental image of the head of a German Shepherd.

17
00:01:43,963 --> 00:01:47,766
Speaker SPEAKER_00: and then you'll read the comparative differences off from that mental image.

18
00:01:48,768 --> 00:01:52,331
Speaker SPEAKER_00: Forming a mental image doesn't necessarily mean creating a bunch of pixels.

19
00:01:52,731 --> 00:01:59,058
Speaker SPEAKER_00: It means adopting a viewpoint and expressing the information about the holes in the parts in terms of that viewpoint.

20
00:01:59,918 --> 00:02:08,885
Speaker SPEAKER_00: And you can see that you adopted a viewpoint because you definitely didn't imagine the German Shepherd huge so it filled most of your visual field, or tiny so you could only just see it.

21
00:02:09,486 --> 00:02:11,929
Speaker SPEAKER_00: And you didn't imagine it rotated 45 degrees.

22
00:02:12,111 --> 00:02:15,736
Speaker SPEAKER_00: All those things you know, so you did choose a viewpoint in order to solve this problem.

23
00:02:17,018 --> 00:02:19,861
Speaker SPEAKER_00: First, what is a spatial relationship?

24
00:02:19,882 --> 00:02:28,272
Speaker SPEAKER_00: Well, mathematically, it's the coordinate transformation that maps points relative to one rectangular coordinate frame into points relative to another rectangular coordinate frame.

25
00:02:29,514 --> 00:02:37,444
Speaker SPEAKER_00: This may seem very abstract and mathematical, but I believe people actually do impose rectangular coordinate frames on holes and parts in order to represent shape.

26
00:02:38,205 --> 00:02:39,687
Speaker SPEAKER_00: And I'm going to try and convince you of that.

27
00:02:40,411 --> 00:02:42,354
Speaker SPEAKER_00: Here's some psychological evidence from Irving Rock.

28
00:02:43,574 --> 00:02:47,501
Speaker SPEAKER_00: If you look at the country on the left, you're not sure what country it is.

29
00:02:48,162 --> 00:02:54,510
Speaker SPEAKER_00: But if I now tell you it's tilted 45 degrees to the left, you immediately see that it's Africa.

30
00:02:55,812 --> 00:02:59,296
Speaker SPEAKER_00: Until then, you probably thought it was some kind of reflective version of something like Australia.

31
00:03:01,681 --> 00:03:05,406
Speaker SPEAKER_00: So the frame of reference you impose determines what shape you perceive.

32
00:03:06,449 --> 00:03:07,969
Speaker SPEAKER_00: That's also true of the shape on the right.

33
00:03:08,491 --> 00:03:14,836
Speaker SPEAKER_00: You see it as either an upright diamond or a tilted square, and you're aware of very different things depending on which way you see it.

34
00:03:15,516 --> 00:03:21,402
Speaker SPEAKER_00: If you see it as a tilted square, you're exquisitely sensitive to whether the edges line up with your rectangular coordinate frame.

35
00:03:22,204 --> 00:03:27,908
Speaker SPEAKER_00: If you see it as an upright diamond, the edges don't line up, and you're completely unaware of whether the angles are right angles or not.

36
00:03:29,229 --> 00:03:32,332
Speaker SPEAKER_00: It's not right angles we care about, it's alignment with a rectangular coordinate frame.

37
00:03:33,533 --> 00:03:34,514
Speaker SPEAKER_00: Here's more evidence.

38
00:03:36,031 --> 00:03:46,204
Speaker SPEAKER_00: If you take these two shapes, they're identical, but I'm showing you them from a different viewpoint, you can assemble them to make a tetrahedron.

39
00:03:46,223 --> 00:03:53,092
Speaker SPEAKER_00: But if you ask MIT professors to do this, it takes them more than a minute to assemble these two pieces into a tetrahedron.

40
00:03:53,813 --> 00:03:59,441
Speaker SPEAKER_00: Now it's a two-piece jigsaw puzzle, and you have to put together faces the same shape, so how hard could it be?

41
00:04:01,143 --> 00:04:04,046
Speaker SPEAKER_00: We need some explanation of why this is so extremely hard.

42
00:04:05,191 --> 00:04:12,419
Speaker SPEAKER_00: And in fact, the longer an MIT professor has been a professor at MIT, the more minutes it takes them to solve the problem.

43
00:04:12,438 --> 00:04:25,012
Speaker SPEAKER_00: So what's happening here is that if you look at the piece on the right, there's a natural coordinate system where two of the axes line up with the tabletop, and the third axis lines up with gravity.

44
00:04:26,954 --> 00:04:32,480
Speaker SPEAKER_00: That coordinate system is completely unrelated to the rectangular coordinate system we normally use.

45
00:04:32,747 --> 00:04:34,509
Speaker SPEAKER_00: for perceiving a tetrahedron.

46
00:04:34,529 --> 00:04:44,841
Speaker SPEAKER_00: What's more, the natural coordinate systems of these two pieces are not aligned when you put them together to make a tetrahedron.

47
00:04:44,862 --> 00:04:54,454
Speaker SPEAKER_00: So because you see them relative to rectangular coordinate systems, you just can't see how these pieces make a tetrahedron, even though it's trivial.

48
00:04:54,494 --> 00:05:01,422
Speaker SPEAKER_00: And there's many other ways of solving it, like saying, I'd better put together faces such that the remaining faces are all triangles.

49
00:05:01,874 --> 00:05:03,315
Speaker SPEAKER_00: Then it's easy to solve.

50
00:05:03,336 --> 00:05:05,077
Speaker SPEAKER_00: But for some reason, people don't do that.

51
00:05:05,158 --> 00:05:08,583
Speaker SPEAKER_00: They try and see how it goes into a tetrahedron, and they can't see it.

52
00:05:09,324 --> 00:05:10,966
Speaker SPEAKER_00: And that's because of rectangular coordinate frames.

53
00:05:12,026 --> 00:05:14,310
Speaker SPEAKER_00: Now, before I go any further, I should make a disclaimer.

54
00:05:15,632 --> 00:05:19,416
Speaker SPEAKER_00: This talk is about not the whole of vision.

55
00:05:19,737 --> 00:05:21,319
Speaker SPEAKER_00: Vision is basically a sampling process.

56
00:05:22,420 --> 00:05:27,627
Speaker SPEAKER_00: And we intelligently choose where to look next, so we don't have to process the whole image at high resolution.

57
00:05:29,362 --> 00:05:32,107
Speaker SPEAKER_00: But for each fixation, we reuse the same neural net.

58
00:05:33,007 --> 00:05:35,531
Speaker SPEAKER_00: This talks about what happens on the first fixation.

59
00:05:37,334 --> 00:05:41,319
Speaker SPEAKER_00: So here's one version of theory three, which I call identity-specific capsules.

60
00:05:42,000 --> 00:05:44,785
Speaker SPEAKER_00: And I worked on this for a number of years with some very good collaborators.

61
00:05:45,565 --> 00:05:52,555
Speaker SPEAKER_00: And the idea is that you localize knowledge about a specific shape into a specific group of neurons, which I called a capsule.

62
00:05:53,377 --> 00:05:56,822
Speaker SPEAKER_00: And this group knows in its weights how all the parts are related to the whole.

63
00:05:58,252 --> 00:06:04,101
Speaker SPEAKER_00: We then recognize shapes by noticing when multiple different parts predict the same pose for the whole.

64
00:06:05,084 --> 00:06:12,495
Speaker SPEAKER_00: And what I mean by the pose of the whole is the coordinate transform between the retina and the intrinsic frame of the whole or the part.

65
00:06:14,017 --> 00:06:16,422
Speaker SPEAKER_00: So here's how that works.

66
00:06:16,442 --> 00:06:22,692
Speaker SPEAKER_00: Let's suppose you've identified something that you think is a mouth, and you've identified something you think is a nose.

67
00:06:23,396 --> 00:06:34,312
Speaker SPEAKER_00: And inside those ellipses, you have neural activities that represent the details of those parts, including their pose, that is, their relationship to the camera.

68
00:06:35,153 --> 00:06:38,259
Speaker SPEAKER_00: You also have a probability that you think that thing exists.

69
00:06:38,478 --> 00:06:39,461
Speaker SPEAKER_00: That's what those Ps are.

70
00:06:40,562 --> 00:06:50,637
Speaker SPEAKER_00: Each part can make a prediction both for what the hole might be, the hole that it belongs to, what the identity might be, but also what the pose might be.

71
00:06:51,949 --> 00:07:04,810
Speaker SPEAKER_00: If you get several parts that agree on the identity, so the arrows go to the same higher level capsule, and they agree on the pose, then you know they're in the right geometric relationship to make the hole.

72
00:07:06,733 --> 00:07:15,425
Speaker SPEAKER_00: That's called a Hough transform, and that's one theory of shape recognition that I pursue quite hard, and I still believe there's a lot to it.

73
00:07:16,029 --> 00:07:22,240
Speaker SPEAKER_00: that it's the agreement between the predictions of the poses from different parts that's the real signature of a whole.

74
00:07:23,661 --> 00:07:30,913
Speaker SPEAKER_00: So a newer variation of Theory 3 is to make the capsules not be specific to particular shapes.

75
00:07:32,175 --> 00:07:34,338
Speaker SPEAKER_00: So I call this universal capsules.

76
00:07:35,420 --> 00:07:36,762
Speaker SPEAKER_00: And they're going to be ubiquitous.

77
00:07:36,802 --> 00:07:38,646
Speaker SPEAKER_00: We're going to have copies of them all over the place.

78
00:07:39,987 --> 00:07:42,391
Speaker SPEAKER_00: We divide the image into many small locations.

79
00:07:42,827 --> 00:07:48,036
Speaker SPEAKER_00: And at each location, we dedicate hardware to representing whatever it is that occupies that location.

80
00:07:48,896 --> 00:07:50,178
Speaker SPEAKER_00: It's going to be a retinotopic map.

81
00:07:51,862 --> 00:07:55,307
Speaker SPEAKER_00: But we also have different levels of representation at each location.

82
00:07:57,389 --> 00:08:02,156
Speaker SPEAKER_00: And so for each level of representation, we're going to have a universal capsule.

83
00:08:02,677 --> 00:08:12,091
Speaker SPEAKER_00: That is a capsule that can represent whatever it is that is that location at that level of representation, but just one of them at each location.

84
00:08:13,321 --> 00:08:28,747
Speaker SPEAKER_00: So this is a picture of what the embeddings at different levels of representation should look like after an iterative, that is a recurrent neural net, has settled down when you present it with a sequence of frames from a static image.

85
00:08:30,610 --> 00:08:39,244
Speaker SPEAKER_00: There's going to be a lowest level embedding, which will probably be produced by a convolutional neural net.

86
00:08:39,528 --> 00:08:41,850
Speaker SPEAKER_00: Those embeddings are depicted by the arrows.

87
00:08:42,270 --> 00:08:44,013
Speaker SPEAKER_00: The arrows, of course, are just 2D vectors.

88
00:08:44,052 --> 00:08:46,735
Speaker SPEAKER_00: And in fact, these are going to be vectors with thousands of dimensions.

89
00:08:47,096 --> 00:08:48,798
Speaker SPEAKER_00: And then there's this hierarchy of embeddings.

90
00:08:49,119 --> 00:08:57,849
Speaker SPEAKER_00: At the top, which hasn't yet settled down in this depiction, we'd like the scene to have exactly the same embedding vector everywhere.

91
00:08:57,869 --> 00:09:09,062
Speaker SPEAKER_00: At the next level down, we'd like all the locations occupied by a particular object, like a face, for example, to have the same embedding vector, exactly the same embedding vector.

92
00:09:10,307 --> 00:09:15,836
Speaker SPEAKER_00: At the level below that, for the parts of the face, we'd like each part to have an identical embedding vector.

93
00:09:16,417 --> 00:09:20,503
Speaker SPEAKER_00: So maybe those red arrows represent the nose, and the green arrows represent a mouth.

94
00:09:21,465 --> 00:09:28,836
Speaker SPEAKER_00: Below the nose, we might have separate subparts for the bridge of the nose and a nostril, for example.

95
00:09:29,255 --> 00:09:35,125
Speaker SPEAKER_00: And they'll be different embedding vectors, but the whole of the bridge of the nose will be the same embedding vector, even if it occupies many locations.

96
00:09:37,067 --> 00:09:39,792
Speaker SPEAKER_00: So what's going on here is we have islands of agreement.

97
00:09:40,514 --> 00:09:48,485
Speaker SPEAKER_00: And these islands of agreement are going to be what represents the paths of the image, how it's segmented into holes and parts at multiple levels.

98
00:09:50,609 --> 00:09:55,437
Speaker SPEAKER_00: You can think of this, if you're a physicist, as a multilevel real-valued Ising model.

99
00:09:56,899 --> 00:09:59,241
Speaker SPEAKER_00: But it has coordinate transforms between levels.

100
00:10:00,605 --> 00:10:08,817
Speaker SPEAKER_00: And the spins, instead of being binary, are these real-valued vectors, which should help with local minima and getting jammed up.

101
00:10:10,331 --> 00:10:19,246
Speaker SPEAKER_00: Here, finally, is a picture of the Glom architecture for a single location, that is, a single column in the previous picture.

102
00:10:21,289 --> 00:10:31,225
Speaker SPEAKER_00: So at that location, we'll have neurons set aside for multiple levels of representation, level L minus 1, L, and L plus 1 in this depiction.

103
00:10:32,749 --> 00:10:34,652
Speaker SPEAKER_00: And it will be a recurrent net.

104
00:10:35,341 --> 00:10:43,732
Speaker SPEAKER_00: that uses the same weights as it settles down at different time slices, although it may have hyperparameters like a temperature that vary with time.

105
00:10:44,813 --> 00:10:45,995
Speaker SPEAKER_00: But basically, the weights are the same.

106
00:10:48,038 --> 00:10:51,703
Speaker SPEAKER_00: And within a location, you get three kinds of interaction.

107
00:10:52,764 --> 00:10:56,049
Speaker SPEAKER_00: If we focus on that level L embedding on the right,

108
00:10:57,143 --> 00:11:05,054
Speaker SPEAKER_00: That's getting input from the level L minus 1 embedding at that location via a bottom-up neural net that will have a couple of hidden layers.

109
00:11:06,476 --> 00:11:19,336
Speaker SPEAKER_00: And that bottom-up neural net will take something like a nostril at level L minus 1 that's occupying this location, and it'll predict the nose that should be occupying this location at the next level up.

110
00:11:21,325 --> 00:11:31,938
Speaker SPEAKER_00: We'll also get top-down input via a top-down neural net that also has a few hidden layers that'll take the level L plus one representation, which might well be a face at that location.

111
00:11:32,639 --> 00:11:35,583
Speaker SPEAKER_00: And from the face, it will predict the nose.

112
00:11:36,205 --> 00:11:38,528
Speaker SPEAKER_00: So there's a coordinate transform there between the face and the nose.

113
00:11:39,990 --> 00:11:45,876
Speaker SPEAKER_00: We also get those green arrows, which for a static image, you're just doing temporal integration.

114
00:11:46,357 --> 00:11:50,082
Speaker SPEAKER_00: For dynamic images, they'd be more interesting because they'd have to encode some of the dynamics.

115
00:11:51,464 --> 00:11:57,613
Speaker SPEAKER_00: And there's one other kind of interaction that's not shown in this diagram, because it's an interaction between different locations.

116
00:11:58,232 --> 00:12:00,956
Speaker SPEAKER_00: This diagram is entirely about interactions within a location.

117
00:12:02,278 --> 00:12:04,842
Speaker SPEAKER_00: So you've seen the blue and the red and the green interactions.

118
00:12:05,583 --> 00:12:10,029
Speaker SPEAKER_00: Between locations, we're going to do something like transformers, but much simpler.

119
00:12:10,465 --> 00:12:18,355
Speaker SPEAKER_00: We're going to allow locations to interact with nearby locations simply by averaging the embeddings.

120
00:12:19,197 --> 00:12:23,402
Speaker SPEAKER_00: In other words, each location will predict that the embedding of its neighbors should be the same as its location.

121
00:12:24,744 --> 00:12:26,687
Speaker SPEAKER_00: But these will be attention-weighted averages.

122
00:12:27,388 --> 00:12:32,995
Speaker SPEAKER_00: That is, if you're similar to another location, you'll make a strong prediction that it should be the same as you.

123
00:12:34,356 --> 00:12:38,042
Speaker SPEAKER_00: If you're different, you'll only make a very weak prediction that it should be the same as you.

124
00:12:39,085 --> 00:12:44,091
Speaker SPEAKER_00: So this is the kind of attention that's used in transformers, but the interactions here are much simpler.

125
00:12:44,753 --> 00:12:47,035
Speaker SPEAKER_00: There's no issues of keys and queries and values.

126
00:12:47,616 --> 00:12:50,541
Speaker SPEAKER_00: The embedding in the key and the query in the value are all the same thing here.

127
00:12:51,543 --> 00:13:02,238
Speaker SPEAKER_00: So the way a level L embedding at location X interacts with a level L embedding at location Y is we simply take the scalar product of those two embeddings.

128
00:13:03,139 --> 00:13:05,682
Speaker SPEAKER_00: So LX is the level L embedding at location X.

129
00:13:06,676 --> 00:13:12,100
Speaker SPEAKER_00: And then we exponentiate those scalar products and renormalize, so everything adds to one.

130
00:13:13,802 --> 00:13:20,688
Speaker SPEAKER_00: And what that'll do, what the exponentiation will do, is say, if there's a good fit, that'll completely dominate a whole bunch of bad fits.

131
00:13:22,691 --> 00:13:30,519
Speaker SPEAKER_00: Now, what that kind of attention-weighted averaging will do is it will cause level learning embeddings to form these islands.

132
00:13:31,458 --> 00:13:34,201
Speaker SPEAKER_00: Essentially, we're creating echo chambers.

133
00:13:35,009 --> 00:13:42,740
Speaker SPEAKER_00: so that as it settles down, you'll get these groups of embeddings that all agree with each other and get to agree with each other more and more strongly.

134
00:13:43,461 --> 00:13:48,307
Speaker SPEAKER_00: Echo chambers are a bad thing in politics, but they're exactly what we want for segmenting an image.

135
00:13:49,950 --> 00:13:52,032
Speaker SPEAKER_00: Now I want to talk about the bottom-up neural net.

136
00:13:52,653 --> 00:13:54,635
Speaker SPEAKER_00: I don't have time to talk about the top-down neural net.

137
00:13:54,655 --> 00:13:57,279
Speaker SPEAKER_00: If you want to know more about that, look at my paper on archive.

138
00:13:57,659 --> 00:14:03,327
Speaker SPEAKER_00: So when you're trying to figure out what's going on in an image, you find parts that are potentially ambiguous.

139
00:14:03,645 --> 00:14:08,753
Speaker SPEAKER_00: You might have found something that might be a mouth, but you're not sure, and something else that might be a nose, but you're not sure.

140
00:14:09,634 --> 00:14:11,436
Speaker SPEAKER_00: And you'd like them to disambiguate each other.

141
00:14:12,538 --> 00:14:17,224
Speaker SPEAKER_00: So if they're in the correct spatial relationship, then they confirm each other.

142
00:14:19,046 --> 00:14:23,110
Speaker SPEAKER_00: One way to do that would be to have direct interactions between these ambiguous parts.

143
00:14:23,871 --> 00:14:25,995
Speaker SPEAKER_00: That's what I call a transformational random field.

144
00:14:27,157 --> 00:14:32,082
Speaker SPEAKER_00: And it's transformational because there's a coordinate transform involved in every interaction.

145
00:14:33,397 --> 00:14:44,289
Speaker SPEAKER_00: So if a nose is looking for a mouth that might support it, what the nose would have to say is, hey, I'm a nose, and I'm looking for a mouth with the following pose.

146
00:14:45,010 --> 00:14:56,163
Speaker SPEAKER_00: In other words, it would have to take the pose of the nose, that is its relation to the camera, multiply it by the relation between a nose and a mouth, and that would be the kind of mouth it was looking for to confirm itself.

147
00:14:56,988 --> 00:15:04,697
Speaker SPEAKER_00: And if that kind of math exists, the math would then need to send back a message, which does the inverse coordinate transform to say, well, I'm not quite what you wanted.

148
00:15:05,038 --> 00:15:05,840
Speaker SPEAKER_00: I have this pose.

149
00:15:06,220 --> 00:15:10,505
Speaker SPEAKER_00: And so I confirm that there ought to be a nose with this pose a bit like what you wanted, but not exactly the same.

150
00:15:12,589 --> 00:15:13,691
Speaker SPEAKER_00: That's quite complicated.

151
00:15:14,010 --> 00:15:17,215
Speaker SPEAKER_00: That's the kind of thing that happens if you use a set transformer to try and do this.

152
00:15:18,456 --> 00:15:20,679
Speaker SPEAKER_00: But you need order n squared interactions.

153
00:15:20,740 --> 00:15:22,783
Speaker SPEAKER_00: And each interaction involves coordinate transforms.

154
00:15:23,663 --> 00:15:25,206
Speaker SPEAKER_00: There's another way to do business.

155
00:15:26,738 --> 00:15:28,541
Speaker SPEAKER_00: which is to use something called a Hough transform.

156
00:15:29,642 --> 00:15:37,934
Speaker SPEAKER_00: Instead of allowing the parts to interact directly, you allow each part to make an ambiguous multimodal prediction for the identity imposed of the whole.

157
00:15:39,236 --> 00:15:42,982
Speaker SPEAKER_00: That is, the identity imposed of the whole object that's occupying the same location.

158
00:15:44,524 --> 00:15:49,011
Speaker SPEAKER_00: So there's only one thing you're talking about, which is whatever object it is that's occupying this location.

159
00:15:49,873 --> 00:15:51,936
Speaker SPEAKER_00: Because of that, you don't need any dynamic routing.

160
00:15:53,197 --> 00:15:56,844
Speaker SPEAKER_00: Now, the whole is going to be present if many of these multimodal predictions agree.

161
00:15:56,903 --> 00:16:06,740
Speaker SPEAKER_00: And what we want is that the attention-weighted averaging that we're doing will pick out that agreement.

162
00:16:08,543 --> 00:16:18,260
Speaker SPEAKER_00: To make that work, what we need is that in each local column, the lower level will predict an unnormalized log probability distribution

163
00:16:18,427 --> 00:16:22,591
Speaker SPEAKER_00: over the space of possible object instances and poses.

164
00:16:23,312 --> 00:16:24,734
Speaker SPEAKER_00: It's the cross-product space of all those.

165
00:16:26,395 --> 00:16:28,197
Speaker SPEAKER_00: That's quite a complicated thing to represent.

166
00:16:28,859 --> 00:16:35,567
Speaker SPEAKER_00: But if we can make a part, make that kind of prediction, then simple averaging will find the predictions that agree.

167
00:16:35,647 --> 00:16:36,989
Speaker SPEAKER_00: It'll pick out the common mode.

168
00:16:37,710 --> 00:16:42,414
Speaker SPEAKER_00: But we must use attention-weighted averaging, so we don't try and average with things that are very dissimilar.

169
00:16:44,376 --> 00:16:46,559
Speaker SPEAKER_00: To implement this kind of thing in a neural net,

170
00:16:47,418 --> 00:17:07,942
Speaker SPEAKER_00: What you need is for each neuron in the embedding vector at the object level to represent a basis function that's a very vague distribution in the unnormalized log probability space of the product of identity impose and possibly deformation as well.

171
00:17:07,961 --> 00:17:15,770
Speaker SPEAKER_00: The activity of the neuron scales this log distribution and the full embedding vector

172
00:17:16,003 --> 00:17:24,517
Speaker SPEAKER_00: represents the sum of all those scaled distributions, which is the same as the product of the individual distributions, except that it's unnormalized.

173
00:17:26,759 --> 00:17:34,270
Speaker SPEAKER_00: The individual distributions can be very vague because they only need to represent one thing, namely the object that's occupying that location.

174
00:17:34,711 --> 00:17:42,163
Speaker SPEAKER_00: They're never going to have to represent two things at the same time, but they are going to have to represent probability distributions across what that one thing might be.

175
00:17:43,290 --> 00:18:01,931
Speaker SPEAKER_00: Now, a big problem for this theory, there's lots of other big problems, but a big problem for this theory is that the bottom-up and top-down neural networks need to be the same at every location because now we're making the capsules universal and you want the same universal capsules everywhere.

176
00:18:01,951 --> 00:18:03,011
Speaker SPEAKER_00: You want them to be ubiquitous.

177
00:18:04,513 --> 00:18:06,015
Speaker SPEAKER_00: That's fine in a computer.

178
00:18:06,035 --> 00:18:11,922
Speaker SPEAKER_00: It's great in a computer because you can fetch the weights of one of these networks from memory once and then use it in all these different locations.

179
00:18:12,261 --> 00:18:13,163
Speaker SPEAKER_00: It's convolutional.

180
00:18:14,324 --> 00:18:17,028
Speaker SPEAKER_00: But it seems extremely wasteful in a brain.

181
00:18:19,051 --> 00:18:23,921
Speaker SPEAKER_00: It's wasteful because it looks like you have to learn all these neural networks separately.

182
00:18:25,262 --> 00:18:34,459
Speaker SPEAKER_00: And one thing we know that convolutional networks build on is that for vision, the distribution of data is roughly isotropic.

183
00:18:34,479 --> 00:18:35,760
Speaker SPEAKER_00: It's pretty much the same everywhere.

184
00:18:36,102 --> 00:18:37,884
Speaker SPEAKER_00: You want the same knowledge everywhere.

185
00:18:39,821 --> 00:18:47,869
Speaker SPEAKER_00: And it seems crazy to learn that same knowledge in the form of these bottom-up and top-down neural networks separately at each location.

186
00:18:48,651 --> 00:18:51,294
Speaker SPEAKER_00: So the question is, how can we get the effect of weight sharing in the brain?

187
00:18:52,615 --> 00:18:54,477
Speaker SPEAKER_00: And that's what the second part of the talk is going to be about.

188
00:18:56,019 --> 00:19:04,647
Speaker SPEAKER_00: So I'm going to give you a brief introduction to a technique called distillation that was invented by Rich Caruana

189
00:19:05,184 --> 00:19:10,491
Speaker SPEAKER_00: He called it compression, and was reinvented by me about 10 years later.

190
00:19:11,932 --> 00:19:17,840
Speaker SPEAKER_00: Distillation is a way of extracting the knowledge from one model and putting it into a model with a different structure.

191
00:19:20,063 --> 00:19:29,454
Speaker SPEAKER_00: It's typically used, and it's quite widely used now, to convert the knowledge in a big model, or an ensemble of big models, into a smaller model.

192
00:19:30,717 --> 00:19:39,769
Speaker SPEAKER_00: So for example, if we have a big model that does speech recognition and runs in a data center, we'd like to take the same knowledge and put it in a much smaller model that can run in our cell phone.

193
00:19:41,010 --> 00:19:42,512
Speaker SPEAKER_00: The same thing for big language models.

194
00:19:43,012 --> 00:19:44,795
Speaker SPEAKER_00: So that's what distillation is now used for.

195
00:19:45,435 --> 00:19:46,096
Speaker SPEAKER_00: It's good at that.

196
00:19:46,978 --> 00:19:48,641
Speaker SPEAKER_00: I'm going to propose a different use for it.

197
00:19:51,003 --> 00:19:54,327
Speaker SPEAKER_00: But I want to start with the analogy that distillation is based on.

198
00:19:55,287 --> 00:19:58,112
Speaker SPEAKER_00: If you look at a caterpillar and a butterfly, they look quite different.

199
00:19:58,933 --> 00:20:06,767
Speaker SPEAKER_00: And that's because the caterpillar is optimised for extracting nutrients from the environment, and the butterfly is optimised for travelling and mating.

200
00:20:08,631 --> 00:20:18,307
Speaker SPEAKER_00: Now, with machine learning, we typically take the training data and we extract structure from it, and that's our learned model.

201
00:20:19,317 --> 00:20:30,272
Speaker SPEAKER_00: But it turns out, now that we know more about neural networks, it's easy to extract structure if you have a very big model with many too many parameters, or if you have a big ensemble of models.

202
00:20:31,355 --> 00:20:34,880
Speaker SPEAKER_00: They make it easy to extract the regularities, but they're very cumbersome.

203
00:20:35,840 --> 00:20:37,462
Speaker SPEAKER_00: They don't give us a small production model.

204
00:20:38,744 --> 00:20:45,214
Speaker SPEAKER_00: And so the solution is to do what the insect does, which is metamorphose.

205
00:20:45,976 --> 00:20:47,518
Speaker SPEAKER_00: Take the larval stage,

206
00:20:47,768 --> 00:20:50,671
Speaker SPEAKER_00: And from that larval stage, build something quite different.

207
00:20:51,372 --> 00:20:53,193
Speaker SPEAKER_00: And distillation is doing that in machine learning.

208
00:20:54,494 --> 00:20:58,318
Speaker SPEAKER_00: So the big cumbersome model that you've learned has sucked the structure out of the data.

209
00:20:59,800 --> 00:21:01,603
Speaker SPEAKER_00: But it's big and clumsy.

210
00:21:03,305 --> 00:21:09,211
Speaker SPEAKER_00: And we tend to think about models in terms of the architecture of the model and the parameter values.

211
00:21:09,971 --> 00:21:15,597
Speaker SPEAKER_00: But there's a quite different way to think about a model, which is it's a function from input to output.

212
00:21:15,847 --> 00:21:22,939
Speaker SPEAKER_00: If I were to give it lots and lots of different input vectors and look at all the different output vectors it gives, that kind of nails down what the function is.

213
00:21:24,180 --> 00:21:28,006
Speaker SPEAKER_00: So we can forget the architecture and the weights of the big model and just focus on the function.

214
00:21:29,008 --> 00:21:33,734
Speaker SPEAKER_00: And we can transfer the knowledge in the big model by transferring that function to a small model.

215
00:21:35,537 --> 00:21:38,801
Speaker SPEAKER_00: And the function can be transferred into a model with a completely different architecture.

216
00:21:40,367 --> 00:21:46,594
Speaker SPEAKER_00: So if you have a big model, let's suppose the output is a big n-way softmax between n different classes.

217
00:21:47,515 --> 00:21:50,137
Speaker SPEAKER_00: The targets are usually a single one and a whole lot of zeros.

218
00:21:52,039 --> 00:21:59,426
Speaker SPEAKER_00: And that means that the labels that you give it, on average, put at most log n bits of constraint on what the function is.

219
00:21:59,988 --> 00:22:01,048
Speaker SPEAKER_00: You give it an input image.

220
00:22:01,388 --> 00:22:02,369
Speaker SPEAKER_00: That's got lots of bits in it.

221
00:22:02,931 --> 00:22:04,051
Speaker SPEAKER_00: But then you tell it the label.

222
00:22:04,412 --> 00:22:07,454
Speaker SPEAKER_00: And the label only imposes a small amount of constraint on what the function is.

223
00:22:08,801 --> 00:22:28,231
Speaker SPEAKER_00: But if we have our hands on a big model, and we take the logits, that is the things you put into the softmax, and we divide them by temperature, bigger than 1, to get a soft distribution, the soft distribution that the big model outputs reveals a lot more information about the function it believes in.

224
00:22:29,253 --> 00:22:33,480
Speaker SPEAKER_00: So at the top here, you have maybe what the hard targets are for an individual image.

225
00:22:33,540 --> 00:22:35,063
Speaker SPEAKER_00: That's what's in the database.

226
00:22:35,380 --> 00:22:41,125
Speaker SPEAKER_00: If you look at the output of a big model that's already been trained, it's pretty confident it's a dog, but it just might be a cat.

227
00:22:42,046 --> 00:22:45,469
Speaker SPEAKER_00: That doesn't provide much information about the function.

228
00:22:46,210 --> 00:22:58,180
Speaker SPEAKER_00: But if we now soften those outputs by using a high temperature in the softmax, that is, we just divide all the logits by this temperature, then we get a far more uniform distribution, a far softer distribution.

229
00:22:58,901 --> 00:23:03,345
Speaker SPEAKER_00: And that begins to show all sorts of things that were hidden if we didn't soften the output.

230
00:23:04,220 --> 00:23:11,190
Speaker SPEAKER_00: So with the unsoftened output, it thought the probability this thing was a cow was 10 to the minus 6, and the probability of a car was 10 to the minus 9.

231
00:23:11,770 --> 00:23:13,153
Speaker SPEAKER_00: And basically, those are both 0.

232
00:23:14,153 --> 00:23:17,940
Speaker SPEAKER_00: If we soften it, there's now a significant difference between a cow and a car.

233
00:23:19,141 --> 00:23:24,288
Speaker SPEAKER_00: Now, the image is actually a dog, and you might think that that difference between cow and car is irrelevant.

234
00:23:24,308 --> 00:23:30,678
Speaker SPEAKER_00: But actually, a lot of the knowledge, if not most of the knowledge of the big model, is not in what the right answer is.

235
00:23:31,138 --> 00:23:33,602
Speaker SPEAKER_00: It's in the relative probabilities of wrong answers.

236
00:23:34,914 --> 00:23:46,195
Speaker SPEAKER_00: That's why on a multi-class quiz, it's very instructive to ask people to give the second answer, because then you can see not whether they know the right answer, but whether they can judge what's plausible and what isn't.

237
00:23:48,681 --> 00:23:54,731
Speaker SPEAKER_00: So these softened outputs reveal what I call dark knowledge, because it would all be zeros in the big model, or virtually zeros.

238
00:23:55,933 --> 00:23:58,137
Speaker SPEAKER_00: And that's what you're going to train the small model on.

239
00:23:59,317 --> 00:24:03,461
Speaker SPEAKER_00: So now you're training the small model on information that's far more informative.

240
00:24:04,163 --> 00:24:07,886
Speaker SPEAKER_00: And of course, when you train the small model, you also use a high temperature in its softmax.

241
00:24:08,929 --> 00:24:21,544
Speaker SPEAKER_00: So if I ask you, what species is this, and give you the following answers, none of which are correct, it's fairly clear that snow leopards are a lot better than tiger, and tiger's a lot better than cow, and cow's a lot better than carrot.

242
00:24:23,046 --> 00:24:25,188
Speaker SPEAKER_00: So what you notice here is there's a class hierarchy.

243
00:24:26,108 --> 00:24:29,053
Speaker SPEAKER_00: The first three are animals, and they're all much better answers than carrot.

244
00:24:30,028 --> 00:24:36,916
Speaker SPEAKER_00: and in the soft probabilities output by the big model you see this class hierarchy you don't need to put it in

245
00:24:37,251 --> 00:24:44,041
Speaker SPEAKER_00: by finding some class hierarchy on the web that somebody made up and telling your learning system about it.

246
00:24:44,442 --> 00:24:47,067
Speaker SPEAKER_00: The learning system will automatically pick up this class hierarchy.

247
00:24:47,307 --> 00:24:52,935
Speaker SPEAKER_00: It'll be sitting there in the big model, and it'll show up in the plausibility of wrong answers.

248
00:24:53,476 --> 00:25:00,626
Speaker SPEAKER_00: And you can transfer all that to the small model just by getting it to give the correct probabilities here when you've softened the output.

249
00:25:02,664 --> 00:25:04,868
Speaker SPEAKER_00: In case you're interested, it's an Egyptian Mao.

250
00:25:05,650 --> 00:25:07,271
Speaker SPEAKER_00: So here's some examples with MNIST.

251
00:25:08,394 --> 00:25:11,519
Speaker SPEAKER_00: I've taken some digits from MNIST and taken a big learned model.

252
00:25:13,122 --> 00:25:19,751
Speaker SPEAKER_00: And I'm showing you the probabilities of the various classes when you soften the output.

253
00:25:20,614 --> 00:25:25,922
Speaker SPEAKER_00: If you look at the middle row, you'll see the most probable thing is a 2, which is correct.

254
00:25:26,324 --> 00:25:28,567
Speaker SPEAKER_00: But the second most probable thing is a 0.

255
00:25:28,588 --> 00:25:32,596
Speaker SPEAKER_00: And what you'll notice is that particular 2 looks quite like a 0.

256
00:25:33,438 --> 00:25:38,848
Speaker SPEAKER_00: You know it's not a 0, but it's much more like a 0 than any of the other 2s.

257
00:25:38,868 --> 00:25:46,964
Speaker SPEAKER_00: And so I'm giving you much more information when I tell you that it's a 2 that looks a bit like a 0 than when I just tell you it's a 2.

258
00:25:48,565 --> 00:25:53,170
Speaker SPEAKER_00: And in fact, I can tell you it's a 2 that looks a bit like a 0 and doesn't look at all like a 5.

259
00:25:53,651 --> 00:25:55,492
Speaker SPEAKER_00: That's the thing that it's least like.

260
00:25:56,173 --> 00:25:58,655
Speaker SPEAKER_00: And that's what those softened outputs are telling you.

261
00:25:58,675 --> 00:26:03,781
Speaker SPEAKER_00: And it turns out that neural nets will learn much faster if you give them this much richer output to learn.

262
00:26:04,561 --> 00:26:08,707
Speaker SPEAKER_00: So here's a little experiment on MNIST just to show distillation works.

263
00:26:09,728 --> 00:26:14,353
Speaker SPEAKER_00: I train a vanilla backprop net with two layers of 800 hidden units.

264
00:26:14,906 --> 00:26:18,271
Speaker SPEAKER_00: and using a ReLU as a non-linearity, and it gets 146 test errors.

265
00:26:20,454 --> 00:26:31,832
Speaker SPEAKER_00: I then train a bigger net with bigger hidden layers, and with dropout, and with constraints on the weights, and with jittering of the input, so all sorts of regularizers, and that gets down to 67 errors.

266
00:26:31,852 --> 00:26:32,913
Speaker SPEAKER_00: It's about half the error rate.

267
00:26:35,017 --> 00:26:38,923
Speaker SPEAKER_00: Now what happens if I take the softened output of that big net

268
00:26:39,173 --> 00:26:42,637
Speaker SPEAKER_00: and train the small net in exactly the way I trained it before.

269
00:26:42,877 --> 00:26:45,842
Speaker SPEAKER_00: When I'm training the small net, I don't use dropout or weight constraints or jittering.

270
00:26:46,363 --> 00:26:48,885
Speaker SPEAKER_00: I just use the softened output of the big net to train it.

271
00:26:50,067 --> 00:26:56,036
Speaker SPEAKER_00: So the claim is it's now got richer data, because it's getting to see similarities between things in the desired outputs.

272
00:26:57,198 --> 00:26:58,319
Speaker SPEAKER_00: And indeed, it works.

273
00:26:59,500 --> 00:27:03,666
Speaker SPEAKER_00: So when I train the small net with these soft targets, it gets 74 errors.

274
00:27:03,886 --> 00:27:05,890
Speaker SPEAKER_00: It's only slightly worse than the big net.

275
00:27:06,005 --> 00:27:11,474
Speaker SPEAKER_00: and it gets nearly half the error rate of the little net trained directly on the data.

276
00:27:11,494 --> 00:27:15,823
Speaker SPEAKER_00: This is because soft targets show the small net how it ought to generalise.

277
00:27:16,263 --> 00:27:22,576
Speaker SPEAKER_00: That is, they show the small net that, OK, the right answer is a 2, but if you had to guess the second answer, it would be a 0.

278
00:27:23,376 --> 00:27:24,578
Speaker SPEAKER_00: That's a generalisation.

279
00:27:25,441 --> 00:27:29,868
Speaker SPEAKER_00: And when you're training the little net, you're teaching it to generalise the same way as the big net.

280
00:27:31,215 --> 00:27:38,307
Speaker SPEAKER_00: So assuming the big net learned a good model, when you train the little net, you're training it with a much better objective function than we normally use.

281
00:27:38,707 --> 00:27:41,010
Speaker SPEAKER_00: You're not training it to get the right answer.

282
00:27:41,451 --> 00:27:44,355
Speaker SPEAKER_00: You're training it to generalize the same way as the big net.

283
00:27:45,017 --> 00:27:47,820
Speaker SPEAKER_00: And we're assuming the big net already generalizes well.

284
00:27:47,840 --> 00:27:49,442
Speaker SPEAKER_00: So finally, you're training something right.

285
00:27:49,462 --> 00:27:52,268
Speaker SPEAKER_00: You're training it to generalize, not to fit the training data.

286
00:27:53,829 --> 00:27:59,838
Speaker SPEAKER_00: Just to show you how powerful this effect is, I trained a big net on all 10 classes of digit.

287
00:28:01,287 --> 00:28:04,693
Speaker SPEAKER_00: I then transferred the knowledge from the big net into a small net.

288
00:28:05,655 --> 00:28:09,182
Speaker SPEAKER_00: And when I was doing the transfer, the small net never saw a 3.

289
00:28:09,784 --> 00:28:12,008
Speaker SPEAKER_00: As far as it was concerned, the 3 was a mythical digit.

290
00:28:12,648 --> 00:28:23,109
Speaker SPEAKER_00: It had the output category 3, and occasionally, if for example it got a 2 that looked quite like a 3, it was told there's a very small probability that's a 3.

291
00:28:23,781 --> 00:28:28,832
Speaker SPEAKER_00: After training like that, never having seen a 3, obviously it believes 3s are very rare.

292
00:28:29,553 --> 00:28:34,102
Speaker SPEAKER_00: So then we simply raise the bias of the 3, so it doesn't think they're so rare anymore.

293
00:28:34,422 --> 00:28:35,984
Speaker SPEAKER_00: It thinks they're about as common as other things.

294
00:28:36,686 --> 00:28:41,655
Speaker SPEAKER_00: And now, if you take that small distilled net, it's very good at 3s.

295
00:28:41,736 --> 00:28:45,282
Speaker SPEAKER_00: It's almost as good at 3s as it is at other classes.

296
00:28:45,262 --> 00:28:56,217
Speaker SPEAKER_00: So it's learned what 3s look like simply by being told that this 2 is a bit similar to a 3, and that 7 is a bit similar to a 3, and this 5 is a bit similar to a 3, and that 6 isn't at all like a 3.

297
00:28:57,358 --> 00:28:59,181
Speaker SPEAKER_00: And that's enough to teach you what a 3 is like.

298
00:29:01,002 --> 00:29:11,777
Speaker SPEAKER_00: One nice twist on distillation is that instead of first training a big net, or a big ensemble of nets, and then distilling the knowledge into another net, we can do co-distillation.

299
00:29:12,482 --> 00:29:15,905
Speaker SPEAKER_00: That is, we can train 10 little nets at the same time.

300
00:29:16,967 --> 00:29:21,672
Speaker SPEAKER_00: If we train them independently, they get an average of 158 test errors.

301
00:29:22,772 --> 00:29:24,494
Speaker SPEAKER_00: These are smaller nets than I was using before.

302
00:29:25,175 --> 00:29:28,138
Speaker SPEAKER_00: And if you ensemble them, you get 143 errors.

303
00:29:29,480 --> 00:29:36,406
Speaker SPEAKER_00: But the other thing you could do is, as you're training them, you could tell each net to try and agree with the softened answers of the other nets.

304
00:29:37,528 --> 00:29:38,409
Speaker SPEAKER_00: It's peer group pressure.

305
00:29:38,429 --> 00:29:42,472
Speaker SPEAKER_00: You're trying to make them output distributions similar to the other nets.

306
00:29:43,178 --> 00:29:49,186
Speaker SPEAKER_00: Now it turns out you have to start off by letting them learn a bit separately, so they don't become too similar immediately.

307
00:29:50,489 --> 00:29:52,992
Speaker SPEAKER_00: And then you try and make them agree with the other nets.

308
00:29:53,993 --> 00:29:58,902
Speaker SPEAKER_00: And what happens is the individual nets end up much better than they were before.

309
00:30:00,564 --> 00:30:03,888
Speaker SPEAKER_00: The ensemble isn't much better than the individual nets, but the individual nets are better.

310
00:30:04,250 --> 00:30:06,553
Speaker SPEAKER_00: They've shared knowledge as they learned.

311
00:30:07,022 --> 00:30:12,651
Speaker SPEAKER_00: And this will work particularly well if the different nets see different subsets of the data.

312
00:30:13,491 --> 00:30:15,575
Speaker SPEAKER_00: So that didn't happen in this case.

313
00:30:15,635 --> 00:30:33,080
Speaker SPEAKER_00: But if you had so much data that you couldn't run it through all the nets, if you run different subsets through different nets and average them, that'll be a way of transferring knowledge about data that the neural net's never seen into the neural net by trying to agree with other neural nets that have seen that data.

314
00:30:33,482 --> 00:30:35,525
Speaker SPEAKER_00: That's pretty much how a scientific community works.

315
00:30:35,965 --> 00:30:41,571
Speaker SPEAKER_00: We look at the data directly, but we also read publications, and we try and agree with what other scientists believe.

316
00:30:42,313 --> 00:30:50,622
Speaker SPEAKER_00: So co-distillation suggests how the columns in a model like GLOM can share knowledge between the neural nets.

317
00:30:52,022 --> 00:30:59,912
Speaker SPEAKER_00: What we're doing when we average an embedding with the embeddings produced by nearby nets

318
00:31:00,077 --> 00:31:05,065
Speaker SPEAKER_00: is we're creating a better training signal for training the bottom-up neural net.

319
00:31:06,988 --> 00:31:12,556
Speaker SPEAKER_00: So we'll get a level L. We'll get what I call the consensus embedding.

320
00:31:13,477 --> 00:31:27,637
Speaker SPEAKER_00: And that's an average of the bottom-up prediction from level L at that location, the top-down prediction from level L plus 1 at that location, the attention-weighted predictions from nearby columns at that level,

321
00:31:28,731 --> 00:31:37,962
Speaker SPEAKER_00: And you average all those together with appropriate hyperparameters, and you'll get a consensus, what all these sources of information between them think is the best bet.

322
00:31:39,064 --> 00:31:46,332
Speaker SPEAKER_00: And now you can train the bottom-up neural net and also the top-down neural net to try to agree with that consensus.

323
00:31:46,352 --> 00:31:57,404
Speaker SPEAKER_00: And of course, the consensus contains the opinions of neural nets and nearby locations if those neural nets were looking at the same part or object.

324
00:31:58,567 --> 00:32:06,876
Speaker SPEAKER_00: So if we're at the object level, and there's a nearby location that's part of the same face, it will have an opinion about what the face vector should be like.

325
00:32:07,417 --> 00:32:23,477
Speaker SPEAKER_00: And if that opinion is similar to our opinion, because it's the same face in a nearby location, it will provide additional training signal for the bottom-up neural net and the top-down neural net, particularly for the bottom-up neural net that predicts the face from a part of the face.

326
00:32:24,082 --> 00:32:31,309
Speaker SPEAKER_00: And that training signal is what allows knowledge to be transferred from one column to another.

327
00:32:33,030 --> 00:32:38,016
Speaker SPEAKER_00: So I think the whole system is doing co-distillation between lots of local models.

328
00:32:39,238 --> 00:32:41,039
Speaker SPEAKER_00: That's what's happening in these retinotopic maps.

329
00:32:41,599 --> 00:32:44,663
Speaker SPEAKER_00: And that's why they're not nearly as statistically inefficient as you might think.

330
00:32:45,785 --> 00:32:50,308
Speaker SPEAKER_00: Now, actually, this co-distillation has one big advantage over weight sharing.

331
00:32:50,769 --> 00:32:53,952
Speaker SPEAKER_00: Even if you could do weight sharing, it's very hard

332
00:32:54,405 --> 00:32:57,971
Speaker SPEAKER_00: to see how to apply it if you've got a non-uniform retina.

333
00:32:59,412 --> 00:33:13,029
Speaker SPEAKER_00: If the size of the receptor fields gets bigger as you move out from the fovea, and if the photoreceptors aren't uniformly spaced in a nice grid, what you've got is a retina that preprocesses different parts of the optic array differently.

334
00:33:15,053 --> 00:33:18,557
Speaker SPEAKER_00: And what you want in one of these bottom-up neural nets

335
00:33:19,516 --> 00:33:25,626
Speaker SPEAKER_00: is not a model that is specific to how that part of the optic array was pre-processed.

336
00:33:26,347 --> 00:33:32,058
Speaker SPEAKER_00: You want the different neural nets to implement the same function from the optic array to the output.

337
00:33:33,181 --> 00:33:35,044
Speaker SPEAKER_00: And with co-distillation, they're doing that.

338
00:33:35,585 --> 00:33:43,920
Speaker SPEAKER_00: Co-distillation can transfer knowledge from one column to another column, even if the columns pre-process their input quite differently.

339
00:33:44,660 --> 00:33:46,863
Speaker SPEAKER_00: That's something you can't do with weight sharing.

340
00:33:46,883 --> 00:33:48,586
Speaker SPEAKER_00: So that's all I have to say.

341
00:33:48,625 --> 00:33:58,299
Speaker SPEAKER_00: The main point of this talk was just to suggest to you that the fact that the brain can't do weight sharing doesn't mean it can't get many of the advantages of a convolutional neural net.

342
00:33:59,020 --> 00:34:12,000
Speaker SPEAKER_00: Because if it's willing to average opinions from nearby locations and then make its neural nets, its local neural nets, try and agree with those averages, it will be doing co-distillation and that will transfer knowledge between locations.


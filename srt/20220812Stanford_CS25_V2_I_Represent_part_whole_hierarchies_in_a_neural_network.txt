1
00:00:05,245 --> 00:00:10,631
Speaker SPEAKER_00: Before we start, I gave the same talk at Stanford quite recently.

2
00:00:10,650 --> 00:00:17,839
Speaker SPEAKER_00: I suggested to the people inviting me, I could just give one talk and both audiences come, but they will prefer it as two separate talks.

3
00:00:18,400 --> 00:00:21,785
Speaker SPEAKER_00: So if you went to this talk recently, I suggest you leave now.

4
00:00:22,446 --> 00:00:23,367
Speaker SPEAKER_00: You won't learn anything new.

5
00:00:24,487 --> 00:00:25,870
Speaker SPEAKER_00: Okay.

6
00:00:28,513 --> 00:00:32,097
Speaker SPEAKER_00: What I'm gonna do is combine some recent ideas in neural networks

7
00:00:32,683 --> 00:00:42,014
Speaker SPEAKER_00: to try to explain how a neural network could represent part-whole hierarchies without violating any of the basic principles of how neurons work.

8
00:00:43,875 --> 00:00:49,360
Speaker SPEAKER_00: And I'm going to explain these ideas in terms of an imaginary system.

9
00:00:50,402 --> 00:00:55,728
Speaker SPEAKER_00: I started writing a design document for a system, and in the end, I decided the design document by itself was quite interesting.

10
00:00:56,429 --> 00:00:58,670
Speaker SPEAKER_00: So this is just vaporware, stuff that doesn't exist.

11
00:00:58,930 --> 00:01:01,594
Speaker SPEAKER_00: Little bits of it now exist, but...

12
00:01:02,030 --> 00:01:05,713
Speaker SPEAKER_00: Somehow I find it easy to explain the ideas in the context of an imaginary system.

13
00:01:11,240 --> 00:01:18,308
Speaker SPEAKER_00: So most people now studying neural networks are doing engineering, and they don't really care if it's exactly how the brain works.

14
00:01:18,849 --> 00:01:20,292
Speaker SPEAKER_00: They're not trying to understand how the brain works.

15
00:01:20,311 --> 00:01:22,034
Speaker SPEAKER_00: They're trying to make cool technology.

16
00:01:22,875 --> 00:01:24,956
Speaker SPEAKER_00: And so 100 layers is fine in a ResNet.

17
00:01:25,498 --> 00:01:28,981
Speaker SPEAKER_00: Weight sharing is fine in a convolutional neural net.

18
00:01:29,131 --> 00:01:37,918
Speaker SPEAKER_00: Some researchers, particularly computational neuroscientists, investigate neural networks, artificial neural networks, in an attempt to understand how the brain might actually work.

19
00:01:38,859 --> 00:01:40,742
Speaker SPEAKER_00: I think we still got a lot to learn from the brain.

20
00:01:41,862 --> 00:01:52,433
Speaker SPEAKER_00: And I think it's worth remembering that for about half a century, the only thing that kept research on neural networks going was the belief that it must be possible to make these things learn complicated things because the brain does.

21
00:01:55,897 --> 00:01:56,337
Speaker SPEAKER_00: So

22
00:01:57,969 --> 00:01:59,852
Speaker SPEAKER_00: Every image has a different parse tree.

23
00:02:01,534 --> 00:02:03,617
Speaker SPEAKER_00: That is the structure of the holes in the parts in the image.

24
00:02:05,439 --> 00:02:16,814
Speaker SPEAKER_00: And in a real neural network, you can't dynamically allocate, you can't just grab a bunch of neurons and say, OK, you now represent this because you don't have random access memory.

25
00:02:16,854 --> 00:02:20,139
Speaker SPEAKER_00: You can't just set the weights of the neurons to be whatever you like.

26
00:02:20,560 --> 00:02:24,044
Speaker SPEAKER_00: What a neuron does is determined by its connections and they only change slowly.

27
00:02:24,598 --> 00:02:27,310
Speaker SPEAKER_00: least probably, mostly they change slowly.

28
00:02:29,759 --> 00:02:35,502
Speaker SPEAKER_00: So the question is, if you can't change what neurons do quickly, how can you represent a dynamic parse tree?

29
00:02:40,444 --> 00:02:42,067
Speaker SPEAKER_00: In symbolic AI, it's not a problem.

30
00:02:42,086 --> 00:02:48,453
Speaker SPEAKER_00: You just grab a piece of memory, that's what it normally amounts to, and say, this is going to represent a node in the parse tree.

31
00:02:48,813 --> 00:02:52,638
Speaker SPEAKER_00: And I'm going to give it pointers to other nodes, other bits of memory that represent other nodes.

32
00:02:53,098 --> 00:02:53,838
Speaker SPEAKER_00: So there's no problem.

33
00:02:54,759 --> 00:03:04,789
Speaker SPEAKER_00: For about five years, I played with a theory called capsules, where you say, because you can't allocate neurons on the fly, you're going to allocate them in advance.

34
00:03:05,371 --> 00:03:09,675
Speaker SPEAKER_00: So we're going to take groups of neurons, and we're going to allocate them to different possible nodes in a parse tree.

35
00:03:10,567 --> 00:03:14,570
Speaker SPEAKER_00: And most of these groups of neurons for most images are going to be silent.

36
00:03:15,192 --> 00:03:16,353
Speaker SPEAKER_00: A few are going to be active.

37
00:03:17,354 --> 00:03:20,897
Speaker SPEAKER_00: And then the ones that are active, we have to dynamically hook them up into a parse tree.

38
00:03:21,437 --> 00:03:25,002
Speaker SPEAKER_00: So we have to have a way of routing between these groups of neurons.

39
00:03:26,183 --> 00:03:27,485
Speaker SPEAKER_00: So that was a capsule theory.

40
00:03:28,366 --> 00:03:34,171
Speaker SPEAKER_00: And I had some very competent people working with me who actually made it work.

41
00:03:34,352 --> 00:03:36,093
Speaker SPEAKER_00: But it was tough going.

42
00:03:37,094 --> 00:03:39,897
Speaker SPEAKER_00: My view is the scientists want to work and scientists don't want to work.

43
00:03:40,132 --> 00:03:41,513
Speaker SPEAKER_00: And capsules was sort of in between.

44
00:03:42,093 --> 00:03:43,756
Speaker SPEAKER_00: Things like back propagation just want to work.

45
00:03:43,776 --> 00:03:44,717
Speaker SPEAKER_00: You try them and they work.

46
00:03:46,057 --> 00:03:48,060
Speaker SPEAKER_00: There's other ideas I've had that just don't want to work.

47
00:03:49,121 --> 00:03:51,324
Speaker SPEAKER_00: Capsules was sort of in between and we got it working.

48
00:03:52,104 --> 00:03:59,532
Speaker SPEAKER_00: But I now have a new theory that could be seen as a funny kind of capsules model in which each capsule is universal.

49
00:03:59,932 --> 00:04:07,000
Speaker SPEAKER_00: That is, instead of a capsule being dedicated to a particular kind of thing, each capsule can represent any kind of thing.

50
00:04:09,157 --> 00:04:13,644
Speaker SPEAKER_00: But hardware still comes in capsules, which are also called embedding sometimes.

51
00:04:16,988 --> 00:04:20,634
Speaker SPEAKER_00: So the imaginary system I'll talk about is called Glom.

52
00:04:21,475 --> 00:04:25,002
Speaker SPEAKER_00: And in Glom, hardware gets allocated to columns.

53
00:04:26,244 --> 00:04:32,093
Speaker SPEAKER_00: And each column contains multiple levels of representation of what's happening in a small patch of the image.

54
00:04:33,507 --> 00:04:39,394
Speaker SPEAKER_00: So within a column, you might have a lower level representation that says it's a nostril.

55
00:04:40,475 --> 00:04:42,259
Speaker SPEAKER_00: And the next level up might say it's a nose.

56
00:04:42,319 --> 00:04:43,699
Speaker SPEAKER_00: And the next level up might say a face.

57
00:04:43,740 --> 00:04:44,822
Speaker SPEAKER_00: The next level up, a person.

58
00:04:45,182 --> 00:04:46,644
Speaker SPEAKER_00: And the top level might say it's a party.

59
00:04:47,204 --> 00:04:48,165
Speaker SPEAKER_00: That's what the whole scene is.

60
00:04:50,449 --> 00:04:57,117
Speaker SPEAKER_00: And the idea for representing part-whole hierarchies is to use islands of agreement between the embeddings at these different levels.

61
00:04:58,346 --> 00:05:06,278
Speaker SPEAKER_00: So at the scene level, at the top level, you'd like the same embedding for every patch of the image, because that patch is a patch of the same scene everywhere.

62
00:05:07,261 --> 00:05:13,771
Speaker SPEAKER_00: At the object level, you'd like the embeddings of all the different patches that belong to the object to be the same.

63
00:05:14,713 --> 00:05:17,697
Speaker SPEAKER_00: So as you go up this hierarchy, you're trying to make things more and more the same.

64
00:05:18,439 --> 00:05:21,283
Speaker SPEAKER_00: And that's how you're squeezing redundancy out.

65
00:05:22,512 --> 00:05:34,564
Speaker SPEAKER_00: The embedding vectors are the things that act like pointers and the embedding vectors are dynamic, they're neural activations rather than neural weights so it's fine to have different embedding vectors for every image.

66
00:05:39,209 --> 00:05:44,274
Speaker SPEAKER_00: So here's a little picture if you had a one dimensional row of patches.

67
00:05:45,475 --> 00:05:47,057
Speaker SPEAKER_00: These are the columns for the patches.

68
00:05:48,737 --> 00:05:49,278
Speaker SPEAKER_00: And.

69
00:05:49,815 --> 00:05:52,480
Speaker SPEAKER_00: you'd have something like a convolutional neural net as a front end.

70
00:05:53,942 --> 00:06:00,350
Speaker SPEAKER_00: And then after the front end, you produce your lowest level embeddings that say what's going on in each particular patch.

71
00:06:01,172 --> 00:06:03,956
Speaker SPEAKER_00: And so that bottom layer of black arrows, they're all different.

72
00:06:04,697 --> 00:06:09,343
Speaker SPEAKER_00: Of course, these embeddings are thousands of dimensions, maybe hundreds of thousands in your brain.

73
00:06:10,425 --> 00:06:18,456
Speaker SPEAKER_00: And so two dimensional vector isn't right, but at least I can represent where the two vectors are the same by using the orientation.

74
00:06:19,144 --> 00:06:22,887
Speaker SPEAKER_00: So the lowest level, all the patches will have different representations.

75
00:06:23,968 --> 00:06:30,394
Speaker SPEAKER_00: But the next level up, the first two patches, they might be part of a nostril, for example.

76
00:06:31,255 --> 00:06:36,821
Speaker SPEAKER_00: And so, yeah, they'll have the same embedding.

77
00:06:37,721 --> 00:06:41,985
Speaker SPEAKER_00: But the next level up, the first three patches might be part of a nose.

78
00:06:42,966 --> 00:06:44,367
Speaker SPEAKER_00: And so they'll all have the same embedding.

79
00:06:45,228 --> 00:06:48,612
Speaker SPEAKER_00: Notice that even though what's in the image is quite different,

80
00:06:49,536 --> 00:06:54,264
Speaker SPEAKER_00: At the part level, those three red vectors are all meant to be the same.

81
00:06:55,286 --> 00:07:00,112
Speaker SPEAKER_00: So what we're doing is we're getting the same representation for things that are superficially very different.

82
00:07:00,954 --> 00:07:05,661
Speaker SPEAKER_00: We're finding spatial coherence in an image by giving the same representation to different things.

83
00:07:07,083 --> 00:07:11,189
Speaker SPEAKER_00: And at the object level, you might have a nose and then a mouth.

84
00:07:12,230 --> 00:07:15,055
Speaker SPEAKER_00: And they're the same face, they're part of the same face.

85
00:07:15,187 --> 00:07:20,370
Speaker SPEAKER_00: And so all those vectors are the same and this network hasn't yet settled down to produce something unseen level.

86
00:07:22,324 --> 00:07:26,709
Speaker SPEAKER_00: So the islands of agreement are what capture the parse tree.

87
00:07:27,310 --> 00:07:29,194
Speaker SPEAKER_00: Now they're a bit more powerful than a parse tree.

88
00:07:29,793 --> 00:07:33,298
Speaker SPEAKER_00: They can capture things like shut the heck up.

89
00:07:34,040 --> 00:07:44,932
Speaker SPEAKER_00: You can have shut and up can be different vectors at one level, but at a higher level, shut and up can have exactly the same vector, namely the vector for shut up and they can be disconnected.

90
00:07:44,952 --> 00:07:50,560
Speaker SPEAKER_00: So you can do things a bit more powerful than a context-free grammar here, but basically it's a parse tree.

91
00:07:51,502 --> 00:08:01,952
Speaker SPEAKER_00: If you're a physicist, you can think of each of these levels as an icing model with real valued vectors rather than binary spins.

92
00:08:03,033 --> 00:08:07,899
Speaker SPEAKER_00: And you can think of them being coordinate transforms between levels, which makes it much more complicated.

93
00:08:08,459 --> 00:08:19,011
Speaker SPEAKER_00: And then this is a kind of multi-level icing model, but with complicated interactions between the levels, because for example, between the red arrows and the black arrows above them,

94
00:08:19,463 --> 00:08:22,466
Speaker SPEAKER_00: you need the coordinate transform between a nose and a face.

95
00:08:23,427 --> 00:08:24,408
Speaker SPEAKER_00: But we'll come to that later.

96
00:08:26,471 --> 00:08:29,033
Speaker SPEAKER_00: If you're not a physicist, ignore all that because it won't help.

97
00:08:33,458 --> 00:08:40,947
Speaker SPEAKER_00: So I want to start, and this is, I guess, is particularly relevant for a natural language course, where some of you are not vision people.

98
00:08:41,570 --> 00:08:47,958
Speaker SPEAKER_00: by trying to prove to you that coordinate systems are not just something invented by Descartes.

99
00:08:48,720 --> 00:08:57,072
Speaker SPEAKER_00: Coordinate systems were invented by the brain a long time ago, and we use coordinate systems in understanding what's going on in an image.

100
00:08:58,114 --> 00:09:01,778
Speaker SPEAKER_00: I also want to demonstrate the psychological reality of parse trees for an image.

101
00:09:02,880 --> 00:09:09,129
Speaker SPEAKER_00: So I'm going to do this with a task that I invented a long time ago in the 1970s.

102
00:09:09,480 --> 00:09:10,702
Speaker SPEAKER_00: when I was a grad student, in fact.

103
00:09:11,303 --> 00:09:16,030
Speaker SPEAKER_00: And you have to do this task to get the full benefit from it.

104
00:09:19,274 --> 00:09:27,106
Speaker SPEAKER_00: So I want you to imagine on the tabletop in front of you, there's a wireframe cube and it's in the standard orientation for a cube.

105
00:09:27,768 --> 00:09:28,990
Speaker SPEAKER_00: It's resting on the tabletop.

106
00:09:29,711 --> 00:09:37,481
Speaker SPEAKER_00: And from your point of view, there's a front bottom right-hand corner and a top back left-hand corner.

107
00:09:37,702 --> 00:09:39,284
Speaker SPEAKER_00: Here we go.

108
00:09:40,192 --> 00:09:45,357
Speaker SPEAKER_00: The front bottom right hand corner is resting on the tabletop along with the four other corners.

109
00:09:46,619 --> 00:09:51,722
Speaker SPEAKER_00: And the top back left hand corner is at the other end of a diagonal that goes through the center of the cube.

110
00:09:53,125 --> 00:09:54,206
Speaker SPEAKER_00: Okay, so far so good.

111
00:09:55,105 --> 00:10:02,173
Speaker SPEAKER_00: Now what we're going to do is rotate the cube so that this finger stays on the tabletop and the other finger is vertically above it like that.

112
00:10:03,533 --> 00:10:04,575
Speaker SPEAKER_00: This finger shouldn't have moved.

113
00:10:05,696 --> 00:10:09,019
Speaker SPEAKER_00: Okay, so now we've got the cube in an orientation where that

114
00:10:09,320 --> 00:10:11,302
Speaker SPEAKER_00: thing that was a body diagonal is now vertical.

115
00:10:12,201 --> 00:10:19,989
Speaker SPEAKER_00: And all you've got to do is take the bottom finger, because that's still on the tabletop, and point with the bottom finger to where the other corners of the cube are.

116
00:10:21,049 --> 00:10:22,010
Speaker SPEAKER_00: So I want you to actually do it.

117
00:10:22,051 --> 00:10:22,410
Speaker SPEAKER_00: Off you go.

118
00:10:22,791 --> 00:10:30,518
Speaker SPEAKER_00: Take your bottom finger, hold your top finger at the other end of that diagonal that's now been made vertical, and just point to where the other corners are.

119
00:10:33,159 --> 00:10:39,225
Speaker SPEAKER_00: And luckily, it's Zoom, so most of you, other people, won't be able to see what you did.

120
00:10:39,442 --> 00:10:41,785
Speaker SPEAKER_00: And I can see that some of you aren't pointing and that's very bad.

121
00:10:43,408 --> 00:10:50,860
Speaker SPEAKER_00: So most people point out four other corners and the most common response is to say they're here, here, here and here.

122
00:10:51,341 --> 00:10:54,225
Speaker SPEAKER_00: They point out four corners in a square halfway up that axis.

123
00:10:57,610 --> 00:10:59,433
Speaker SPEAKER_00: That's wrong, as you might imagine.

124
00:11:00,274 --> 00:11:07,086
Speaker SPEAKER_00: And it's easy to see that it's wrong because if you imagine the cube in the normal orientation and count the corners, there's eight of them.

125
00:11:08,179 --> 00:11:09,402
Speaker SPEAKER_00: And these were two corners.

126
00:11:10,243 --> 00:11:11,663
Speaker SPEAKER_00: So where did the other two corners go?

127
00:11:13,046 --> 00:11:18,533
Speaker SPEAKER_00: So one theory is that when you rotated the cube, the centripetal forces made them fly off into your unconscious.

128
00:11:19,553 --> 00:11:20,575
Speaker SPEAKER_00: That's not a very good theory.

129
00:11:21,375 --> 00:11:28,164
Speaker SPEAKER_00: So what's happening here is you have no idea where the other corners are, unless you're something like a crystallographer.

130
00:11:29,506 --> 00:11:34,692
Speaker SPEAKER_00: You can sort of imagine bits of the cube, but you just can't imagine this structure of the other corners, what structure they form.

131
00:11:36,445 --> 00:11:43,653
Speaker SPEAKER_00: And this common response that people gave of four corners in a square is doing something very weird.

132
00:11:43,820 --> 00:11:50,248
Speaker SPEAKER_00: it's trying to, it's saying, well, okay, I don't know, I don't know where the bits of a cube are, but I know something about cubes.

133
00:11:50,307 --> 00:11:51,830
Speaker SPEAKER_00: I know the corners come in fours.

134
00:11:52,650 --> 00:11:59,220
Speaker SPEAKER_00: I know a cube has this fourfold rotational symmetry or two planes of bilateral symmetry, a right angle swung rather.

135
00:12:00,221 --> 00:12:04,225
Speaker SPEAKER_00: And so what people do is they preserve the symmetries of the cube in their response.

136
00:12:05,106 --> 00:12:06,870
Speaker SPEAKER_00: They give four corners in a square.

137
00:12:08,591 --> 00:12:13,437
Speaker SPEAKER_00: Now, what they've actually pointed out if they do that is two pyramids,

138
00:12:14,043 --> 00:12:15,524
Speaker SPEAKER_00: each of which has a square base.

139
00:12:16,265 --> 00:12:18,870
Speaker SPEAKER_00: One's upside down and they're stuck base to base.

140
00:12:19,750 --> 00:12:21,273
Speaker SPEAKER_00: So you can visualize that quite easily.

141
00:12:21,774 --> 00:12:24,677
Speaker SPEAKER_00: It's a square base pyramid with another one stuck underneath it.

142
00:12:25,359 --> 00:12:28,244
Speaker SPEAKER_00: And so now you get your two fingers as the vertices of those two pyramids.

143
00:12:29,666 --> 00:12:43,745
Speaker SPEAKER_00: And what's interesting about that is you've preserved the symmetries of the cube at the cost of doing something pretty radical, which is changing faces to vertices and vertices to faces.

144
00:12:44,418 --> 00:12:47,001
Speaker SPEAKER_00: The thing you pointed out, if you did that, was an octahedron.

145
00:12:48,485 --> 00:12:50,868
Speaker SPEAKER_00: It has eight faces and six vertices.

146
00:12:51,590 --> 00:12:53,533
Speaker SPEAKER_00: A cube has six faces and eight vertices.

147
00:12:54,394 --> 00:13:07,455
Speaker SPEAKER_00: So in order to preserve the symmetries you know about of the cube, if you did that, you've done something really radical, which is changed faces for vertices and vertices for faces.

148
00:13:07,890 --> 00:13:09,614
Speaker SPEAKER_00: I should show you what the answer looks like.

149
00:13:10,235 --> 00:13:15,323
Speaker SPEAKER_00: So I'm going to step back and try and get enough light and maybe you can see this cube.

150
00:13:18,811 --> 00:13:28,567
Speaker SPEAKER_00: So this is a cube and you can see that the other edges form a kind of zigzag ring around the middle.

151
00:13:30,230 --> 00:13:31,653
Speaker SPEAKER_00: So I've got a picture of it.

152
00:13:34,638 --> 00:13:48,479
Speaker SPEAKER_00: So the colored rods here are the other edges of the cube, the ones that don't touch your fingertips, and your top fingers connected to the three vertices of those flaps, and your bottom fingers connected to the lowest three vertices there.

153
00:13:49,860 --> 00:13:51,102
Speaker SPEAKER_00: And that's what a cube looks like.

154
00:13:51,183 --> 00:13:52,785
Speaker SPEAKER_00: It's something you had no idea about.

155
00:13:52,806 --> 00:13:55,770
Speaker SPEAKER_00: This is just a completely different model of a cube.

156
00:13:56,051 --> 00:13:57,552
Speaker SPEAKER_00: It's so different, I'll give it a different name.

157
00:13:57,572 --> 00:14:00,577
Speaker SPEAKER_00: I'll call it a hexahedron.

158
00:14:01,908 --> 00:14:07,155
Speaker SPEAKER_00: thing to notice is a hexahedron and a cube are just conceptually utterly different.

159
00:14:07,596 --> 00:14:11,923
Speaker SPEAKER_00: You wouldn't even know one was the same as the other if you think about one is actually in one as a cube.

160
00:14:12,684 --> 00:14:20,456
Speaker SPEAKER_00: It's like the ambiguity between a tilted square and an upright diamond, but more powerful because you're not familiar with it.

161
00:14:21,111 --> 00:14:24,456
Speaker SPEAKER_00: And that's my demonstration that people really do use coordinate systems.

162
00:14:24,636 --> 00:14:38,134
Speaker SPEAKER_00: And if you use a different coordinate system to describe things, and here I forced you to use a different coordinate system by making the diagonal be vertical and asking you to describe it relative to that vertical axis, then familiar things become completely unfamiliar.

163
00:14:39,297 --> 00:14:42,841
Speaker SPEAKER_00: And when you do see them relative to this new frame, they're just a completely different thing.

164
00:14:44,384 --> 00:14:47,288
Speaker SPEAKER_00: Notice that things like convolutional neural nets don't have that.

165
00:14:47,486 --> 00:14:51,630
Speaker SPEAKER_00: They can't look at something and have two utterly different internal representations of the very same thing.

166
00:14:53,432 --> 00:14:55,714
Speaker SPEAKER_00: I'm also showing you that you do parsing.

167
00:14:56,053 --> 00:14:57,034
Speaker SPEAKER_00: So here I've colored it.

168
00:14:57,075 --> 00:15:02,659
Speaker SPEAKER_00: So you parse it into what I call the crown, which is three triangular flaps that slope upwards and outwards.

169
00:15:04,120 --> 00:15:05,162
Speaker SPEAKER_00: Here's a different parsing.

170
00:15:06,302 --> 00:15:08,404
Speaker SPEAKER_00: The same green flap sloping upwards and outwards.

171
00:15:09,066 --> 00:15:11,607
Speaker SPEAKER_00: Now we have a red flap sloping downwards and outwards.

172
00:15:12,649 --> 00:15:16,211
Speaker SPEAKER_00: And we have a central rectangle and you just have the two ends of the rectangle.

173
00:15:17,153 --> 00:15:23,600
Speaker SPEAKER_00: And if you know, if you perceive this, and I close your eyes and ask you, were there any parallel edges there?

174
00:15:24,561 --> 00:15:27,844
Speaker SPEAKER_00: You're very well aware that those two edges, two blue edges were parallel.

175
00:15:28,585 --> 00:15:33,570
Speaker SPEAKER_00: And you're typically not aware of any other parallel edges, even though you know by symmetry, there must be other pairs.

176
00:15:34,490 --> 00:15:35,432
Speaker SPEAKER_00: Similarly with the crown.

177
00:15:35,471 --> 00:15:42,739
Speaker SPEAKER_00: If you see the crown, and then I ask you to close your eyes and ask you where the parallel edges, you don't see any parallel edges.

178
00:15:43,376 --> 00:15:48,263
Speaker SPEAKER_00: And that's because the coordinate systems you're using for those flaps don't line up with the edges.

179
00:15:48,923 --> 00:15:52,828
Speaker SPEAKER_00: And you only notice parallel edges if they line up with the coordinate system you're using.

180
00:15:53,590 --> 00:15:58,356
Speaker SPEAKER_00: So here for the rectangle, the parallel edges align with the coordinate system, for the flaps they don't.

181
00:15:59,118 --> 00:16:06,346
Speaker SPEAKER_00: So you're aware that those two blue edges are parallel, but you're not aware that one of the green edges and one of the red edges is parallel.

182
00:16:10,445 --> 00:16:16,394
Speaker SPEAKER_00: So this isn't like the NECA cube ambiguity where when it flips, you think that what's out there in reality is different.

183
00:16:16,453 --> 00:16:17,575
Speaker SPEAKER_00: Things are at a different depth.

184
00:16:18,376 --> 00:16:21,761
Speaker SPEAKER_00: This is like next weekend, we should be visiting relatives.

185
00:16:22,822 --> 00:16:25,447
Speaker SPEAKER_00: So if you take the sentence next weekend, we should be visiting relatives.

186
00:16:26,107 --> 00:16:31,054
Speaker SPEAKER_00: It can mean next weekend, what we will be doing is visiting relatives.

187
00:16:31,676 --> 00:16:36,283
Speaker SPEAKER_00: Or it can mean next weekend, what we will be is visiting relatives.

188
00:16:37,158 --> 00:16:39,019
Speaker SPEAKER_00: Now, those are completely different senses.

189
00:16:39,620 --> 00:16:41,423
Speaker SPEAKER_00: They happen to have the same truth conditions.

190
00:16:41,703 --> 00:16:44,607
Speaker SPEAKER_00: They mean the same thing in the sense of truth conditions.

191
00:16:45,109 --> 00:16:48,393
Speaker SPEAKER_00: Because if you're visiting relatives, what you are is visiting relatives.

192
00:16:49,193 --> 00:16:50,475
Speaker SPEAKER_00: And it's that kind of ambiguity.

193
00:16:50,836 --> 00:16:54,480
Speaker SPEAKER_00: No disagreement about what's going on in the world, but two completely different ways of seeing the sentence.

194
00:16:59,047 --> 00:17:03,452
Speaker SPEAKER_00: So this was drawn in the 1970s.

195
00:17:03,994 --> 00:17:06,237
Speaker SPEAKER_00: This is what AI was like in the 1970s.

196
00:17:06,637 --> 00:17:12,145
Speaker SPEAKER_00: This is a sort of structural description of the crown interpretation.

197
00:17:13,608 --> 00:17:17,074
Speaker SPEAKER_00: So you have nodes for all the various parts in the hierarchy.

198
00:17:17,855 --> 00:17:20,038
Speaker SPEAKER_00: I've also put something on the arcs.

199
00:17:20,057 --> 00:17:25,527
Speaker SPEAKER_00: That Rwx is the relationship between the crown and the flap.

200
00:17:26,508 --> 00:17:28,651
Speaker SPEAKER_00: And that can be represented by a matrix.

201
00:17:28,671 --> 00:17:34,941
Speaker SPEAKER_00: It's really the relationship between the intrinsic frame of reference of the crown and the intrinsic frame of reference of the flap.

202
00:17:35,680 --> 00:17:40,226
Speaker SPEAKER_00: And notice that if I change my viewpoint, that doesn't change at all.

203
00:17:41,587 --> 00:17:45,412
Speaker SPEAKER_00: So that kind of relationship will be a good thing to put in the weights of a neural network.

204
00:17:45,893 --> 00:17:49,678
Speaker SPEAKER_00: Because you'd like a neural network to be able to recognize shapes independently of viewpoint.

205
00:17:50,519 --> 00:17:55,085
Speaker SPEAKER_00: And that RWX is knowledge about the shape that's independent of viewpoint.

206
00:17:57,146 --> 00:17:59,349
Speaker SPEAKER_00: Here's the zigzag interpretation.

207
00:18:00,592 --> 00:18:03,055
Speaker SPEAKER_00: And here's something else where I've added

208
00:18:03,794 --> 00:18:05,396
Speaker SPEAKER_00: the things in the heavy blue boxes.

209
00:18:06,498 --> 00:18:11,747
Speaker SPEAKER_00: They're the relationship between the node and the viewer.

210
00:18:12,468 --> 00:18:13,589
Speaker SPEAKER_00: That is to be more explicit.

211
00:18:13,971 --> 00:18:23,405
Speaker SPEAKER_00: The coordinate transformation between the intrinsic frame of reference of the crown and the intrinsic frame of reference of the viewer, your eyeball, is that RWV.

212
00:18:24,803 --> 00:18:26,443
Speaker SPEAKER_00: And that's a different kind of thing altogether.

213
00:18:26,964 --> 00:18:28,967
Speaker SPEAKER_00: Because as you change viewpoint, that changes.

214
00:18:29,606 --> 00:18:35,152
Speaker SPEAKER_00: In fact, as you change viewpoint, all those things in blue boxes all change together in a consistent way.

215
00:18:36,453 --> 00:18:43,420
Speaker SPEAKER_00: And there's a simple relationship, which is that if you take RWV, then you multiply it by RWX, you get RXV.

216
00:18:44,560 --> 00:18:48,825
Speaker SPEAKER_00: So you can easily propagate viewpoint information over a structural description.

217
00:18:50,026 --> 00:18:52,228
Speaker SPEAKER_00: And that's what I think a mental image is.

218
00:18:52,444 --> 00:18:54,106
Speaker SPEAKER_00: rather than a bunch of pixels.

219
00:18:55,148 --> 00:18:58,794
Speaker SPEAKER_00: It's a structural description with associated viewpoint information.

220
00:19:02,298 --> 00:19:04,682
Speaker SPEAKER_00: That makes sense of a lot of properties of mental images.

221
00:19:05,163 --> 00:19:11,311
Speaker SPEAKER_00: Like, if you want to do any reasoning with things like RWX, you form a mental image.

222
00:19:12,512 --> 00:19:14,736
Speaker SPEAKER_00: That is, you fill in, you choose a viewpoint.

223
00:19:15,857 --> 00:19:20,865
Speaker SPEAKER_00: And I want to do one more demo to convince you, you always choose a viewpoint when you're solving mental imagery problems.

224
00:19:21,352 --> 00:19:25,556
Speaker SPEAKER_00: So I'm going to give you another very simple mental imagery problem at the risk of running over time.

225
00:19:26,175 --> 00:19:36,846
Speaker SPEAKER_00: Imagine that you're at a particular point and you travel a mile east and then you travel a mile north and then you travel a mile east again.

226
00:19:36,905 --> 00:19:39,667
Speaker SPEAKER_00: What's your direction back to your starting point?

227
00:19:40,890 --> 00:19:42,310
Speaker SPEAKER_00: This isn't a very hard problem.

228
00:19:43,051 --> 00:19:45,614
Speaker SPEAKER_00: It's sort of a bit south and quite a lot west, right?

229
00:19:46,414 --> 00:19:50,278
Speaker SPEAKER_00: It's not exactly southwest, but it's sort of southwest.

230
00:19:50,545 --> 00:20:00,133
Speaker SPEAKER_00: Now, when you did that task, what you imagined, from your point of view, is you went a mile east and then you went a mile north and then you went a mile east again.

231
00:20:01,497 --> 00:20:02,917
Speaker SPEAKER_00: I'll tell you what you didn't imagine.

232
00:20:03,318 --> 00:20:07,301
Speaker SPEAKER_00: You didn't imagine that you went a mile east, and then you went a mile north, and then you went a mile east again.

233
00:20:07,982 --> 00:20:12,547
Speaker SPEAKER_00: You could have solved the problem perfectly well with north not being up, but you had north up.

234
00:20:13,087 --> 00:20:14,348
Speaker SPEAKER_00: You also didn't imagine this.

235
00:20:14,749 --> 00:20:17,231
Speaker SPEAKER_00: You go a mile east, and then a mile north, and then a mile east again.

236
00:20:17,991 --> 00:20:18,772
Speaker SPEAKER_00: And you didn't imagine this.

237
00:20:18,792 --> 00:20:21,355
Speaker SPEAKER_00: You go a mile east, and then a mile north, and so on.

238
00:20:21,375 --> 00:20:28,382
Speaker SPEAKER_00: You imagined it at a particular scale, in a particular orientation, and in a particular position.

239
00:20:28,480 --> 00:20:32,164
Speaker SPEAKER_00: And you can answer questions about roughly how big it was and so on.

240
00:20:32,184 --> 00:20:40,474
Speaker SPEAKER_00: So that's evidence that to solve these tasks that involve using relationships between things, you form a mental image.

241
00:20:41,457 --> 00:20:46,182
Speaker SPEAKER_00: Okay, enough on mental imagery.

242
00:20:46,202 --> 00:20:49,287
Speaker SPEAKER_00: So I'm now going to give you a very brief introduction to contrastive learning.

243
00:20:50,208 --> 00:20:58,077
Speaker SPEAKER_00: So where this is a complete disconnect in the talk, but I'll come back together soon.

244
00:20:58,278 --> 00:20:58,317
Unknown Speaker: So

245
00:20:59,497 --> 00:21:08,147
Speaker SPEAKER_00: In contrast to self supervised learning, what we try and do is make two different crops of an image have the same representation.

246
00:21:12,011 --> 00:21:24,728
Speaker SPEAKER_00: There's a paper a long time ago by Becker and Hinton, where we were doing this to discover low level coherence in an image, like the continuity of surfaces, or the depth of surfaces.

247
00:21:25,788 --> 00:21:29,614
Speaker SPEAKER_00: it's been improved a lot since then, and it's been used for doing things like classification.

248
00:21:30,273 --> 00:21:47,799
Speaker SPEAKER_00: That is, you take an image that has one prominent object in it, and you say, if I take a crop of the image that contains sort of any part of that object, it should have the same representation as some other crop of the image containing a part of that object.

249
00:21:49,422 --> 00:21:50,022
Speaker SPEAKER_00: And

250
00:21:50,492 --> 00:21:53,636
Speaker SPEAKER_00: This has been developed a lot in the last few years.

251
00:21:54,237 --> 00:22:00,344
Speaker SPEAKER_00: I'm going to talk about a model developed a couple of years ago by my group in Toronto called SimClear, but there's lots of other models.

252
00:22:00,845 --> 00:22:02,287
Speaker SPEAKER_00: And since then, things have improved.

253
00:22:05,630 --> 00:22:16,702
Speaker SPEAKER_00: So in SimClear, you take an ImageX, you take two different crops, and you also do color distortion of the crops, different color distortions of each crop.

254
00:22:16,970 --> 00:22:20,214
Speaker SPEAKER_00: And that's to prevent it from using color histograms to say they're the same.

255
00:22:21,737 --> 00:22:25,122
Speaker SPEAKER_00: So you mess with the color, so it can't use color in a simple way.

256
00:22:27,325 --> 00:22:31,951
Speaker SPEAKER_00: And that gives you x i tilde and x j tilde.

257
00:22:32,692 --> 00:22:44,170
Speaker SPEAKER_00: You then put those through the same neural network F, then you get a representation H. And then you take the representation H and you put it through another neural network, which compresses it a bit.

258
00:22:44,931 --> 00:22:46,452
Speaker SPEAKER_00: It goes to low dimensionality.

259
00:22:46,923 --> 00:22:50,567
Speaker SPEAKER_00: That's an extra complexity I'm not going to explain, but it makes it work a bit better.

260
00:22:51,768 --> 00:22:52,750
Speaker SPEAKER_00: You can do it without doing that.

261
00:22:53,631 --> 00:22:55,952
Speaker SPEAKER_00: And you get two embeddings zi and zj.

262
00:22:57,234 --> 00:23:00,557
Speaker SPEAKER_00: And your aim is to maximize the agreement between those vectors.

263
00:23:02,299 --> 00:23:08,567
Speaker SPEAKER_00: And so you start off doing that and you say, okay, let's start off with random neural networks, random weights in the neural networks.

264
00:23:09,288 --> 00:23:12,811
Speaker SPEAKER_00: And let's take two patches and let's put them through these transformations.

265
00:23:13,061 --> 00:23:15,285
Speaker SPEAKER_00: Let's try and make zi be the same as zj.

266
00:23:15,765 --> 00:23:25,277
Speaker SPEAKER_00: So let's back propagate the square difference between components of i and components of j. And hey, presto, what you discover is everything collapses.

267
00:23:27,378 --> 00:23:31,805
Speaker SPEAKER_00: For every image, it will always produce the same zi and zj.

268
00:23:32,464 --> 00:23:34,627
Speaker SPEAKER_00: And then you realize, well, that's not what I meant by agreement.

269
00:23:35,068 --> 00:23:42,057
Speaker SPEAKER_00: I mean, they should be the same when you get two crops of the same image and different when you get two crops of different images.

270
00:23:42,609 --> 00:23:44,352
Speaker SPEAKER_00: Otherwise, it's not really agreement, right?

271
00:23:48,798 --> 00:23:50,580
Speaker SPEAKER_00: So you have to have negative examples.

272
00:23:50,942 --> 00:23:54,646
Speaker SPEAKER_00: You have to show it crops from different images and say those should be different.

273
00:23:55,748 --> 00:23:59,333
Speaker SPEAKER_00: If they're already different, you don't try and make them a lot more different.

274
00:24:00,114 --> 00:24:03,298
Speaker SPEAKER_00: It's very easy to make things very different, but that's not what you want.

275
00:24:03,358 --> 00:24:04,901
Speaker SPEAKER_00: You just want to be sure they're different enough.

276
00:24:04,980 --> 00:24:09,106
Speaker SPEAKER_00: So crops from different images aren't taken to be from the same image.

277
00:24:09,407 --> 00:24:14,252
Speaker SPEAKER_00: So if they happen to be very similar, you push them apart and that stops your representations collapsing.

278
00:24:14,272 --> 00:24:17,174
Speaker SPEAKER_00: That's called contrastive learning and it works very well.

279
00:24:18,836 --> 00:24:30,027
Speaker SPEAKER_00: So what you can do is do unsupervised learning by trying to maximize agreement between the representations you get from two image patches from the same image.

280
00:24:31,469 --> 00:24:38,115
Speaker SPEAKER_00: And after you've done that, you just take your representation of the image patch and you feed it to a linear classifier.

281
00:24:38,617 --> 00:24:44,965
Speaker SPEAKER_00: a bunch of weights so that you multiply the representation by a weight matrix, put it through a softmax and get class labels.

282
00:24:45,807 --> 00:24:48,130
Speaker SPEAKER_00: And then you train that by gradient descent.

283
00:24:49,030 --> 00:24:55,538
Speaker SPEAKER_00: And what you discover is that that's just about as good as training on labeled data.

284
00:24:56,159 --> 00:24:59,663
Speaker SPEAKER_00: So now the only thing you trained on labeled data is that last linear classifier.

285
00:25:00,484 --> 00:25:02,686
Speaker SPEAKER_00: The previous layers were trained on unlabeled data.

286
00:25:03,729 --> 00:25:08,253
Speaker SPEAKER_00: And you've managed to train your representations without needing labels.

287
00:25:12,048 --> 00:25:13,509
Speaker SPEAKER_00: Now, there's a problem with this.

288
00:25:14,671 --> 00:25:20,459
Speaker SPEAKER_00: It works very nicely, but it's really confounding objects and whole scenes.

289
00:25:21,721 --> 00:25:31,614
Speaker SPEAKER_00: So it makes sense to say two different patches from the same scene should get the same vector label at the scene level, because they're from the same scene.

290
00:25:33,175 --> 00:25:37,842
Speaker SPEAKER_00: But what if one of the patches contains bits of objects A and B, and another patch contain bits of objects A and C?

291
00:25:38,261 --> 00:25:41,987
Speaker SPEAKER_00: You don't really want those two patches to have the same representation at the object level.

292
00:25:42,557 --> 00:25:45,520
Speaker SPEAKER_00: So we have to distinguish these different levels of representation.

293
00:25:46,882 --> 00:25:56,491
Speaker SPEAKER_00: And for contrastive learning, if you don't use any kind of gating or attention, then what's happening is you're really doing learning at the seam level.

294
00:25:59,335 --> 00:26:12,548
Speaker SPEAKER_00: What we'd like is that the representations you get at the object level should be the same if both patches are patches from object A, but should be different if one patch is from object A and one patch is from object B.

295
00:26:12,982 --> 00:26:16,587
Speaker SPEAKER_00: And to do that, we're going to need some form of attention to decide whether they really come from the same thing.

296
00:26:17,909 --> 00:26:19,490
Speaker SPEAKER_00: And so Glom is designed to do that.

297
00:26:19,550 --> 00:26:28,942
Speaker SPEAKER_00: It's designed to take contrastive learning and to introduce attention of the kind you get in Transformers in order not to try and say things are the same when they're not.

298
00:26:31,385 --> 00:26:35,211
Speaker SPEAKER_00: I should mention at this point that most of you will be familiar with BERT.

299
00:26:36,051 --> 00:26:42,180
Speaker SPEAKER_00: And you can think of the word fragments that are fed into BERT as like the image patches I'm using here.

300
00:26:42,380 --> 00:26:46,544
Speaker SPEAKER_00: And in BERT, you have that whole column of representations of the same word fragment.

301
00:26:47,865 --> 00:26:55,392
Speaker SPEAKER_00: In BERT, what's happening, presumably, as you go up, is you're getting semantically richer representations.

302
00:26:56,673 --> 00:27:02,198
Speaker SPEAKER_00: But in BERT, there's no attempt to get representations of larger things, like whole phrases.

303
00:27:05,121 --> 00:27:08,044
Speaker SPEAKER_00: This, what I'm going to talk about, will be a way to modify BERT.

304
00:27:08,104 --> 00:27:11,626
Speaker SPEAKER_00: So as you go up, you get bigger and bigger islands of agreement.

305
00:27:12,063 --> 00:27:26,823
Speaker SPEAKER_00: So, for example, after a couple of levels, then things like New and York will have the different fragments of York, suppose he's got two different fragments, will have exactly the same representation if it was done in the Glomlite way.

306
00:27:27,644 --> 00:27:37,257
Speaker SPEAKER_00: And then, as you go up another level, the fragments of New, well, New is probably a thing in its own right, but the fragments of York would all have exactly the same representation.

307
00:27:38,898 --> 00:27:40,280
Speaker SPEAKER_00: They'd have this island of agreement.

308
00:27:40,732 --> 00:27:44,356
Speaker SPEAKER_00: And that will be a representation of a compound thing.

309
00:27:44,696 --> 00:27:48,401
Speaker SPEAKER_00: And as you go up, you're going to get these islands of agreement that represent bigger and bigger things.

310
00:27:49,121 --> 00:27:51,243
Speaker SPEAKER_00: And that's going to be a much more useful kind of BERT.

311
00:27:51,743 --> 00:28:10,722
Speaker SPEAKER_00: Because instead of taking vectors that represent word fragments, and then sort of munging them together by taking the max of each, for example, the max of each component, for example, which is just a crazy thing to do, you'd explicitly as you're learning, form representations of larger parts in the part-whole hierarchy.

312
00:28:12,204 --> 00:28:15,969
Speaker SPEAKER_00: Okay.

313
00:28:15,989 --> 00:28:29,103
Speaker SPEAKER_00: So what we're going after in Glom is a particular kind of spatial coherence that's more complicated than the spatial coherence caused by the fact that surfaces tend to be at the same depth and same orientation in nearby patches of an image.

314
00:28:30,243 --> 00:28:41,115
Speaker SPEAKER_00: We're going after the spatial coherence that says that if you find a mouth in an image and you find a nose in an image and then the right spatial relationship to make a face,

315
00:28:41,450 --> 00:28:43,633
Speaker SPEAKER_00: then that's a particular kind of coherence.

316
00:28:44,334 --> 00:28:50,561
Speaker SPEAKER_00: And we want to go after that unsupervised and we want to discover that kind of coherence in images.

317
00:28:53,946 --> 00:28:57,832
Speaker SPEAKER_00: So before I go into more details about GLOM, I want a disclaimer.

318
00:29:00,976 --> 00:29:05,701
Speaker SPEAKER_00: For years, computer vision treated vision as you've got a static image, a uniform resolution,

319
00:29:06,069 --> 00:29:07,090
Speaker SPEAKER_00: and you want to say what's in it.

320
00:29:08,332 --> 00:29:10,054
Speaker SPEAKER_00: That's not how vision works in the real world.

321
00:29:10,473 --> 00:29:14,817
Speaker SPEAKER_00: In the real world, this is actually a loop where you decide where to look.

322
00:29:14,837 --> 00:29:19,563
Speaker SPEAKER_00: If you're a person or a robot, you better do that intelligently.

323
00:29:20,683 --> 00:29:24,827
Speaker SPEAKER_00: And that gives you a sample of the optic array.

324
00:29:24,867 --> 00:29:30,112
Speaker SPEAKER_00: It turns the optic array, the incoming light, into a retinal image.

325
00:29:30,833 --> 00:29:34,997
Speaker SPEAKER_00: And on your retina, you have high resolution in the middle and low resolution around the edges.

326
00:29:35,787 --> 00:29:44,383
Speaker SPEAKER_00: And so you're focusing on particular details and you never ever process the whole image at uniform resolution.

327
00:29:44,804 --> 00:29:52,377
Speaker SPEAKER_00: You're always focusing on something and processing where you're fixating at high resolution and everything else at much lower resolution, particularly around the edges.

328
00:29:53,557 --> 00:30:07,615
Speaker SPEAKER_00: So I'm going to ignore all the complexity of how you decide where to look, and all the complexity of how you put together the information you get from different fixations by saying, let's just talk about the very first fixation or a novel image.

329
00:30:08,176 --> 00:30:11,320
Speaker SPEAKER_00: So you look somewhere, and now what happens on that first fixation?

330
00:30:12,103 --> 00:30:18,230
Speaker SPEAKER_00: We know that the same hardware in the brain is going to be reused for the next fixation, but let's just think about the first fixation.

331
00:30:20,759 --> 00:30:23,162
Speaker SPEAKER_00: So finally, here's a picture of the architecture.

332
00:30:24,884 --> 00:30:34,817
Speaker SPEAKER_00: And this is the architecture for a single location, so like for a single word fragment in BERT.

333
00:30:34,836 --> 00:30:38,321
Speaker SPEAKER_00: And it shows you what's happening for multiple frames.

334
00:30:38,862 --> 00:30:42,988
Speaker SPEAKER_00: So Glom is really designed for video, but I only talk about applying it to static images.

335
00:30:43,848 --> 00:30:49,415
Speaker SPEAKER_00: You should think of a static image as a very boring video in which the frames are all the same as each other.

336
00:30:50,981 --> 00:30:54,705
Speaker SPEAKER_00: So I'm showing you three adjacent levels in the hierarchy.

337
00:30:56,708 --> 00:30:59,029
Speaker SPEAKER_00: And I'm showing you what happens over time.

338
00:30:59,049 --> 00:31:04,655
Speaker SPEAKER_00: So if you look at the middle level, maybe that's the sort of major part level.

339
00:31:06,218 --> 00:31:11,522
Speaker SPEAKER_00: And look at that box that says level L. And that's at frame four.

340
00:31:13,144 --> 00:31:14,645
Speaker SPEAKER_00: So the right hand level L box.

341
00:31:15,847 --> 00:31:20,112
Speaker SPEAKER_00: And let's ask how the state of that box, the state of that embedding is determined.

342
00:31:20,547 --> 00:31:22,368
Speaker SPEAKER_00: So inside the box, we're going to get an embedding.

343
00:31:23,590 --> 00:31:32,681
Speaker SPEAKER_00: And the embedding is going to be the representation of what's going on at the major part level for that little patch of the image.

344
00:31:35,565 --> 00:31:44,275
Speaker SPEAKER_00: And level L, in this diagram, all of these embeddings will always be devoted to the same patch of the retinal image.

345
00:31:45,916 --> 00:31:47,919
Speaker SPEAKER_00: Okay.

346
00:31:47,939 --> 00:31:50,402
Speaker SPEAKER_00: The level L embedding

347
00:31:50,567 --> 00:31:51,628
Speaker SPEAKER_00: on the right hand side.

348
00:31:51,689 --> 00:31:55,473
Speaker SPEAKER_00: You can see there's three things determining it there.

349
00:31:55,493 --> 00:31:56,455
Speaker SPEAKER_00: There's the green arrow.

350
00:31:56,476 --> 00:31:59,960
Speaker SPEAKER_00: And for static images, the green arrow is rather boring.

351
00:32:00,000 --> 00:32:05,267
Speaker SPEAKER_00: It's just saying, you should sort of be similar to the previous state of level L. So it's just doing temporal integration.

352
00:32:07,088 --> 00:32:13,397
Speaker SPEAKER_00: The blue arrow is actually a neural net with a couple of hidden layers in it.

353
00:32:14,598 --> 00:32:17,502
Speaker SPEAKER_00: I'm just showing you the embeddings here, not all the layers of the neural net.

354
00:32:18,277 --> 00:32:21,299
Speaker SPEAKER_00: We need a couple of hidden layers to do the coordinate transforms that are required.

355
00:32:22,461 --> 00:32:29,189
Speaker SPEAKER_00: And the blue arrow is basically taking information at the level below of the previous time step.

356
00:32:30,210 --> 00:32:35,275
Speaker SPEAKER_00: So level L minus one on frame three might be representing that I think I might be a nostril.

357
00:32:36,316 --> 00:32:41,141
Speaker SPEAKER_00: Well, if you think you might be a nostril, what you predict at the next level up is a nose.

358
00:32:42,162 --> 00:32:47,607
Speaker SPEAKER_00: What's more, if you have a coordinate frame for the nostril, you can predict the coordinate frame for the nose.

359
00:32:47,790 --> 00:32:53,176
Speaker SPEAKER_00: Maybe not perfectly, but you have a pretty good idea of the orientation, position, scale of the nodes.

360
00:32:53,196 --> 00:32:54,959
Speaker SPEAKER_00: So that bottom up neural net.

361
00:32:56,420 --> 00:32:56,579
Speaker SPEAKER_00: Is.

362
00:32:58,162 --> 00:33:06,391
Speaker SPEAKER_00: A net that can take any kind of part of level of mind, so when you take a nostril, but it could also take a steering wheel and predict the car from the steering wheel.

363
00:33:07,090 --> 00:33:11,836
Speaker SPEAKER_00: And predict what you've got at the next level up.

364
00:33:14,538 --> 00:33:17,021
Speaker SPEAKER_00: The red arrow is a top down neural net.

365
00:33:17,895 --> 00:33:25,045
Speaker SPEAKER_00: So the red arrow is predicting the nose from the whole face.

366
00:33:25,846 --> 00:33:28,409
Speaker SPEAKER_00: And again, it has a couple of hidden layers to coordinate transforms.

367
00:33:29,250 --> 00:33:42,766
Speaker SPEAKER_00: Because if you know the coordinate frame of the face, and you know the relationship between a face and a nose, and that's going to be in the weights of that top down neural net, then you can predict that it's a nose and what the pose of the nose is.

368
00:33:43,567 --> 00:33:46,671
Speaker SPEAKER_00: And that's all going to be in activities in that embedding vector.

369
00:33:48,153 --> 00:33:53,640
Speaker SPEAKER_00: Okay, now there's all of that is what's going on in one column of hardware.

370
00:33:54,079 --> 00:33:56,323
Speaker SPEAKER_00: That's all about a specific patch of the image.

371
00:33:57,203 --> 00:34:01,769
Speaker SPEAKER_00: So that's very, very like what's going on for one word fragment in BERT.

372
00:34:02,170 --> 00:34:03,791
Speaker SPEAKER_00: You have all these levels of representation.

373
00:34:04,893 --> 00:34:11,681
Speaker SPEAKER_00: It's a bit confusing exactly what the relation of this is to BERT.

374
00:34:11,840 --> 00:34:16,606
Speaker SPEAKER_00: And I'll give you the reference to a long archive paper at the end that has a whole section on how this relates to BERT.

375
00:34:17,110 --> 00:34:19,652
Speaker SPEAKER_00: But it's confusing because this has time steps.

376
00:34:20,494 --> 00:34:23,898
Speaker SPEAKER_00: And that makes it all more complicated.

377
00:34:23,938 --> 00:34:25,641
Speaker SPEAKER_00: Okay.

378
00:34:26,862 --> 00:34:29,646
Speaker SPEAKER_00: So those are three things that determine the level and embedding.

379
00:34:30,307 --> 00:34:34,253
Speaker SPEAKER_00: But there's one fourth thing, which is in black at the bottom there.

380
00:34:35,936 --> 00:34:38,639
Speaker SPEAKER_00: And that's the only way in which different locations interact.

381
00:34:39,661 --> 00:34:42,864
Speaker SPEAKER_00: And that's a very simplified form of a transformer.

382
00:34:43,519 --> 00:34:52,090
Speaker SPEAKER_00: If you take a transformer, as in BERT, and you say, let's make the embeddings and the keys and the queries and the values all be the same as each other.

383
00:34:53,052 --> 00:34:54,153
Speaker SPEAKER_00: We just have this one vector.

384
00:34:55,675 --> 00:35:05,228
Speaker SPEAKER_00: So now all you're trying to do is make the level L embedding in one column be the same as the level L embedding in nearby columns.

385
00:35:07,110 --> 00:35:08,231
Speaker SPEAKER_00: But it's going to be gated.

386
00:35:08,371 --> 00:35:13,358
Speaker SPEAKER_00: You're only going to try and make it be the same if it's already quite similar.

387
00:35:15,615 --> 00:35:17,057
Speaker SPEAKER_00: So here's how the attention works.

388
00:35:18,418 --> 00:35:21,983
Speaker SPEAKER_00: You take the level L embedding in location X, that's LX.

389
00:35:23,023 --> 00:35:26,288
Speaker SPEAKER_00: And you take the level L embedding in nearby location Y, that's LY.

390
00:35:27,369 --> 00:35:32,675
Speaker SPEAKER_00: You take the scalar product, you exponentiate and you normalize.

391
00:35:33,056 --> 00:35:34,177
Speaker SPEAKER_00: In other words, you do a softmax.

392
00:35:35,438 --> 00:35:43,648
Speaker SPEAKER_00: And that gives you the weight to use in your desire to make LX be the same as LY.

393
00:35:45,585 --> 00:35:56,436
Speaker SPEAKER_00: So the input produced by this from neighbors is an attention weighted average of the level of embeddings of nearby columns.

394
00:35:57,677 --> 00:35:59,539
Speaker SPEAKER_00: And that's an extra input that you get.

395
00:35:59,639 --> 00:36:02,201
Speaker SPEAKER_00: It's trying to make you agree with nearby things.

396
00:36:02,722 --> 00:36:05,224
Speaker SPEAKER_00: And that's what's going to cause you to get these islands of agreement.

397
00:36:10,708 --> 00:36:14,833
Speaker SPEAKER_00: So back to this picture, I think

398
00:36:17,344 --> 00:36:18,887
Speaker SPEAKER_00: yeah.

399
00:36:18,907 --> 00:36:22,152
Speaker SPEAKER_00: This is what we'd like to see, and the reason.

400
00:36:23,815 --> 00:36:40,003
Speaker SPEAKER_00: We get those that big island of agreement at the object level is because we're trying to get agreement there we're trying to learn the coordinate transform from the red arrows to the level above and from the green arrows to the level above such that we get agreement.

401
00:36:41,766 --> 00:36:41,867
Speaker SPEAKER_00: Okay.

402
00:36:45,509 --> 00:36:57,846
Speaker SPEAKER_00: Now, one thing we need to worry about is that the difficult thing in perception, it's not so bad in language, it's probably worse in visual perception, is that there's a lot of ambiguity.

403
00:36:58,746 --> 00:37:01,269
Speaker SPEAKER_00: If I'm looking at a line drawing, for example, I see a circle.

404
00:37:02,351 --> 00:37:08,460
Speaker SPEAKER_00: Well, a circle could be the right eye of a face, or it could be the left eye of a face, or it could be the front wheel of a car or the back wheel of a car.

405
00:37:08,840 --> 00:37:10,722
Speaker SPEAKER_00: There's all sorts of things a circle could be.

406
00:37:10,762 --> 00:37:13,726
Speaker SPEAKER_00: And we'd like to disambiguate the circle.

407
00:37:14,838 --> 00:37:19,304
Speaker SPEAKER_00: And there's a long line of work using things like Markov random fields.

408
00:37:19,985 --> 00:37:33,342
Speaker SPEAKER_00: Here we need a variational Markov random field, which I call a transformational random field, because the interaction between, for example, something that might be an eye and something that might be a mouth, needs to be gated by corner transforms.

409
00:37:34,282 --> 00:37:37,206
Speaker SPEAKER_00: You know, for the, let's take a nose and a mouth, because that's my standard thing.

410
00:37:38,027 --> 00:37:43,494
Speaker SPEAKER_00: If you take something that might be a nose, and you want to ask, does anybody out there support the idea of a nose?

411
00:37:44,976 --> 00:37:58,177
Speaker SPEAKER_00: Well, what you'd like to do is send to everything nearby a message saying, do you have the right kind of pose and right kind of identity to support the idea that I'm a nose?

412
00:37:59,398 --> 00:38:03,744
Speaker SPEAKER_00: And so you'd like, for example, to send out a message from the nose

413
00:38:04,248 --> 00:38:17,543
Speaker SPEAKER_00: You'd send out a message to all nearby locations saying, does anybody have a mouth with the pose that I predict by taking the pose of the nose, multiplying by the coordinate transform between a nose and a mouth, and now I can predict the pose of the mouth.

414
00:38:18,143 --> 00:38:21,067
Speaker SPEAKER_00: Is there anybody out there with that pose who thinks it might be a mouth?

415
00:38:22,208 --> 00:38:24,831
Speaker SPEAKER_00: And I think you can see, you're going to have to send out a lot of different messages.

416
00:38:26,632 --> 00:38:29,916
Speaker SPEAKER_00: For each kind of other thing that might support you, you're going to need to send a different message.

417
00:38:29,936 --> 00:38:32,500
Speaker SPEAKER_00: So you're going to need a multi-headed

418
00:38:33,255 --> 00:38:41,666
Speaker SPEAKER_00: transformer and it's going to be doing these coordinate transforms and you have to corner transform the inverse transform on the way back.

419
00:38:42,186 --> 00:38:49,134
Speaker SPEAKER_00: Because if the mouse supports you, what it needs to support is a nose, not with the pose of the mouth, but with the appropriate pose.

420
00:38:50,094 --> 00:38:53,820
Speaker SPEAKER_00: So that's going to get very complicated, you're going to have n squared interactions all with coordinate transforms.

421
00:38:54,701 --> 00:38:57,963
Speaker SPEAKER_00: There's another way of doing it that's much simpler, that's called a Hough transform.

422
00:38:59,065 --> 00:39:02,789
Speaker SPEAKER_00: At least it's much simpler if you have a way of representing ambiguity.

423
00:39:06,043 --> 00:39:16,135
Speaker SPEAKER_00: So instead of these direct interactions between parts, like a nose and a mouth, what you're going to do is you're going to make each of the parts predict the whole.

424
00:39:17,737 --> 00:39:23,704
Speaker SPEAKER_00: So the nose can predict the face, and it can predict the pose of the face, and the mouth can also predict the face.

425
00:39:24,625 --> 00:39:29,331
Speaker SPEAKER_00: Now, these will be in different columns of Glom, but in one column of Glom, you'll have a nose predicting face.

426
00:39:30,152 --> 00:39:32,534
Speaker SPEAKER_00: In a nearby column, you'll have a mouth predicting a face.

427
00:39:33,375 --> 00:39:35,577
Speaker SPEAKER_00: And those two faces should be the same,

428
00:39:35,844 --> 00:39:37,007
Speaker SPEAKER_00: if this really is a face.

429
00:39:38,949 --> 00:40:00,679
Speaker SPEAKER_00: So when you do this attention weighted averaging with nearby things, what you're doing is you're getting confirmation that the support for the hypothesis you've got, I mean, supposed to be in one column hypothesis, it's a face with this pose, that gets supported by nearby columns that derive the very same embedding from quite different data.

430
00:40:00,940 --> 00:40:03,304
Speaker SPEAKER_00: One derived it from the nose and one derived it from the mouth.

431
00:40:05,326 --> 00:40:07,347
Speaker SPEAKER_00: And this doesn't require any dynamic routing.

432
00:40:08,789 --> 00:40:15,938
Speaker SPEAKER_00: Because the embeddings are always referring to what's going on in the same small patch of the image, within a column, there's no routing.

433
00:40:16,639 --> 00:40:22,445
Speaker SPEAKER_00: And between columns, there's something a bit like routing, but it's just the standard transformer kind of attention.

434
00:40:22,947 --> 00:40:26,831
Speaker SPEAKER_00: You're just trying to agree with things that are similar.

435
00:40:26,851 --> 00:40:31,096
Speaker SPEAKER_00: And okay, so that's how GLOM is meant to work.

436
00:40:32,577 --> 00:40:34,320
Speaker SPEAKER_00: And the big problem is,

437
00:40:35,244 --> 00:40:41,793
Speaker SPEAKER_00: that if I see a circle, it might be a left eye, it might be a right eye, it might be a front wheel of a car, it might be the back wheel of a car.

438
00:40:42,494 --> 00:40:53,130
Speaker SPEAKER_00: Because my embedding for a particular patch at a particular level has to be able to represent anything, when I get an ambiguous thing, I have to deal with all these possibilities of what hole it might be part of.

439
00:40:54,351 --> 00:41:05,067
Speaker SPEAKER_00: So instead of trying to resolve ambiguity at the part level, what I can do is jump to the next level up and resolve the ambiguity there, just by seeing if things are the same, which is an easier way to resolve ambiguity.

440
00:41:05,789 --> 00:41:11,516
Speaker SPEAKER_00: But the cost of that is, I have to be able to represent all the ambiguity I get at the next level up.

441
00:41:13,318 --> 00:41:14,679
Speaker SPEAKER_00: Now, it turns out you can do that.

442
00:41:14,699 --> 00:41:22,971
Speaker SPEAKER_00: We've done a little toy example where you can actually preserve this ambiguity, but it's difficult.

443
00:41:23,472 --> 00:41:25,554
Speaker SPEAKER_00: It's the kind of thing neural nets are good at.

444
00:41:26,896 --> 00:41:35,626
Speaker SPEAKER_00: So if you think about the embedding at the next level up, you've got a whole bunch of neurons whose activities are that embedding.

445
00:41:36,298 --> 00:41:44,668
Speaker SPEAKER_00: And you want to represent a highly multimodal distribution, like it might be a car with this pose, or a car with that pose, or a face with this pose, or a face with that pose.

446
00:41:45,530 --> 00:41:47,972
Speaker SPEAKER_00: All of these are possible predictions for finding a circle.

447
00:41:49,514 --> 00:41:51,577
Speaker SPEAKER_00: And so you have to represent all that.

448
00:41:52,719 --> 00:41:54,320
Speaker SPEAKER_00: And the question is, can neural nets do that?

449
00:41:55,141 --> 00:42:00,027
Speaker SPEAKER_00: And I think the way they must be doing it is, each neuron in the embedding,

450
00:42:01,324 --> 00:42:11,516
Speaker SPEAKER_00: stands for an unnormalized log probability distribution over this huge space of possible identities and possible poses, the sort of cross product of identities and poses.

451
00:42:12,478 --> 00:42:18,865
Speaker SPEAKER_00: And so the neuron is this rather vague log probability distribution over that space.

452
00:42:20,228 --> 00:42:26,655
Speaker SPEAKER_00: And when you activate the neuron, what it's saying is, add in that log probability distribution to what you've already got.

453
00:42:27,817 --> 00:42:30,639
Speaker SPEAKER_00: And so now if you have a whole bunch of log probability distributions,

454
00:42:31,311 --> 00:42:32,414
Speaker SPEAKER_00: And you add them all together.

455
00:42:32,454 --> 00:42:37,601
Speaker SPEAKER_00: You can get a much more peaky log probability distribution.

456
00:42:38,782 --> 00:42:55,949
Speaker SPEAKER_00: And when you exponentiate to get a probability distribution, it gets very peaky and so very vague basis functions in this joint space of pose and identity and basis functions in the log probability in that space can be combined to produce sharp conclusions.

457
00:42:58,592 --> 00:42:59,074
Speaker SPEAKER_00: So.

458
00:43:00,235 --> 00:43:02,356
Speaker SPEAKER_00: I think that's how neurons are representing things.

459
00:43:02,416 --> 00:43:07,242
Speaker SPEAKER_00: Most people think about neurons as they think about the thing that they're representing.

460
00:43:08,563 --> 00:43:12,407
Speaker SPEAKER_00: But obviously in perception, you have to deal with uncertainty.

461
00:43:13,048 --> 00:43:18,094
Speaker SPEAKER_00: And so neurons have to be good at representing multimodal distributions.

462
00:43:18,876 --> 00:43:21,538
Speaker SPEAKER_00: And this is the only way I can think of that's good at doing it.

463
00:43:23,400 --> 00:43:24,621
Speaker SPEAKER_00: That's a rather weak argument.

464
00:43:25,322 --> 00:43:30,208
Speaker SPEAKER_00: And it's the argument that led Chomsky to believe that language wasn't learned because he couldn't think of how it was learned.

465
00:43:30,728 --> 00:43:40,465
Speaker SPEAKER_00: My view is neurons must be using this representation because I can't think of any other way of doing it.

466
00:43:40,485 --> 00:43:44,172
Speaker SPEAKER_00: Okay, I just said all that because I got ahead of myself because I got excited.

467
00:43:44,211 --> 00:43:55,032
Speaker SPEAKER_00: Now the reason you can get away with this, the reason you have these very vague distributions in the unnormalized log probability space.

468
00:43:55,365 --> 00:44:04,661
Speaker SPEAKER_00: is because these neurons are all dedicated to a small patch of image, and they're all trying to represent the thing that's happening in that patch of image.

469
00:44:05,643 --> 00:44:07,505
Speaker SPEAKER_00: So you're only trying to represent one thing.

470
00:44:07,726 --> 00:44:10,690
Speaker SPEAKER_00: You're not trying to represent some set of possible objects.

471
00:44:11,432 --> 00:44:14,757
Speaker SPEAKER_00: If you're trying to represent some set of possible objects, you'll have a horrible binding problem.

472
00:44:15,059 --> 00:44:26,293
Speaker SPEAKER_00: And you couldn't use these very vague distributions, but so long as you know that all of these neurons, all of the active neurons refer to the same thing, then you can do the intersection.

473
00:44:26,853 --> 00:44:31,398
Speaker SPEAKER_00: You can add the log probability distribution together and intersect the sets of things they represent.

474
00:44:34,501 --> 00:44:36,204
Speaker SPEAKER_00: Okay, I'm getting near the end.

475
00:44:36,784 --> 00:44:38,347
Speaker SPEAKER_00: How would you train a system like this?

476
00:44:39,188 --> 00:44:45,014
Speaker SPEAKER_00: Well, obviously you could train it the way you train, but you could do deep end-to-end training.

477
00:44:45,382 --> 00:45:15,356
Speaker SPEAKER_00: And for Glom, what that will consist of, and the way we trained a toy example, is you take an image, you leave out some patches of the image, you then let Glom settle down for about 10 iterations, and it's trying to fill in the lowest level representation of what's in the image, the lowest level embedding.

478
00:45:15,690 --> 00:45:22,219
Speaker SPEAKER_00: it fills them in wrong, and so you now back propagate that error, and you're back propagating it through time in this network.

479
00:45:22,820 --> 00:45:25,003
Speaker SPEAKER_00: So it will also back propagate up and down through the levels.

480
00:45:27,146 --> 00:45:34,275
Speaker SPEAKER_00: So you're basically just doing back propagation through time of the error due to filling in things incorrectly.

481
00:45:34,295 --> 00:45:38,862
Speaker SPEAKER_00: That's basically how BERT is trained, and you could train GLOM the same way.

482
00:45:39,682 --> 00:45:44,449
Speaker SPEAKER_00: But I also want to include an extra bit in the training to encourage islands.

483
00:45:46,083 --> 00:45:53,590
Speaker SPEAKER_00: We want to encourage big islands of identical vectors at higher levels.

484
00:45:54,672 --> 00:45:56,673
Speaker SPEAKER_00: And you can do that by using contrastive learning.

485
00:45:58,414 --> 00:46:12,188
Speaker SPEAKER_00: So if you think how the next, at the next time step, if you think how an embedding is determined, it's determined by combining a whole bunch of different factors.

486
00:46:12,969 --> 00:46:14,891
Speaker SPEAKER_00: What was going on in the previous time step

487
00:46:15,394 --> 00:46:32,132
Speaker SPEAKER_00: at this level of representation in this location, what was going on at the previous time step in this location, but at the next level down, at the next level up, and also what was going on at the previous time step at nearby locations at the same level.

488
00:46:33,594 --> 00:46:39,119
Speaker SPEAKER_00: And the weighted average of all those things I'll call the consensus embedding, and that's what you use for the next embedding.

489
00:46:40,661 --> 00:46:44,945
Speaker SPEAKER_00: And I think you can see that if we try and make the bottom up neural net,

490
00:46:45,077 --> 00:47:00,016
Speaker SPEAKER_00: on the top down neural net, if we try and make their predictions agree with the consensus, the consensus has folded in information from nearby locations that already roughly agree because of the attention waiting.

491
00:47:01,338 --> 00:47:09,748
Speaker SPEAKER_00: And so by trying to make the top down and bottom up neural networks agree with the consensus, you're trying to make them agree with what's going on in nearby locations that are similar.

492
00:47:10,510 --> 00:47:12,773
Speaker SPEAKER_00: And so you'll be training it to form islands.

493
00:47:16,516 --> 00:47:22,623
Speaker SPEAKER_00: This is more interesting to neuroscientists than to people who do natural language so I'm going to ignore that.

494
00:47:25,405 --> 00:47:37,079
Speaker SPEAKER_00: You might think it's wasteful to be replicating all these embeddings at the object level, so the idea is that the object level will be a large number of patches that all have exactly the same vector representation.

495
00:47:38,280 --> 00:47:39,402
Speaker SPEAKER_00: And that seems like a waste.

496
00:47:39,871 --> 00:47:41,596
Speaker SPEAKER_00: but actually biology is full of things like that.

497
00:47:42,016 --> 00:47:49,519
Speaker SPEAKER_00: All your cells have exactly the same DNA and all the parts of an organ have pretty much the same vector of protein expressions.

498
00:47:50,041 --> 00:47:54,554
Speaker SPEAKER_00: So there's lots of replication goes on to keep things local.

499
00:47:55,362 --> 00:48:06,536
Speaker SPEAKER_00: And it's the same here, and actually that replication is very useful when you're settling on an interpretation, because before you settle down, you don't know which things should be the same as which other things.

500
00:48:06,936 --> 00:48:15,907
Speaker SPEAKER_00: So having separate vectors in each location to represent what's going on there at the object level gives you the flexibility to gradually segment things as you settle down in a sensible way.

501
00:48:17,309 --> 00:48:19,411
Speaker SPEAKER_00: It allows you to hedge your bets.

502
00:48:19,695 --> 00:48:21,978
Speaker SPEAKER_00: And what you're doing is not quite like clustering.

503
00:48:22,380 --> 00:48:27,927
Speaker SPEAKER_00: You're creating clusters of identical vectors rather than discovering clusters in fixed data.

504
00:48:28,327 --> 00:48:31,791
Speaker SPEAKER_00: So clustering, you're given the data and it's fixed and you find the clusters.

505
00:48:31,931 --> 00:48:37,840
Speaker SPEAKER_00: Here, the embeddings at every level, they vary over time.

506
00:48:38,221 --> 00:48:42,146
Speaker SPEAKER_00: They're determined by the top-down and bottom-up inputs and by inputs coming from nearby locations.

507
00:48:42,666 --> 00:48:46,871
Speaker SPEAKER_00: So what you're doing is forming clusters rather than discovering them in fixed data.

508
00:48:47,572 --> 00:48:49,094
Speaker SPEAKER_00: And that's got a somewhat different flavor.

509
00:48:49,581 --> 00:48:51,306
Speaker SPEAKER_00: and can settle down faster.

510
00:48:55,878 --> 00:48:58,967
Speaker SPEAKER_00: And one other advantage this replication is.

511
00:49:00,195 --> 00:49:05,342
Speaker SPEAKER_00: What you don't want is to have much more work in your transformer as you go to higher levels.

512
00:49:06,222 --> 00:49:08,726
Speaker SPEAKER_00: But you do need longer range interactions at higher levels.

513
00:49:09,246 --> 00:49:12,891
Speaker SPEAKER_00: Presumably for the lowest levels, you want fairly short range interactions in your transformer.

514
00:49:13,512 --> 00:49:14,393
Speaker SPEAKER_00: And they could be dense.

515
00:49:14,893 --> 00:49:17,416
Speaker SPEAKER_00: As you go to higher levels, you want much longer range interactions.

516
00:49:18,016 --> 00:49:19,498
Speaker SPEAKER_00: So you could make them sparse.

517
00:49:20,440 --> 00:49:25,326
Speaker SPEAKER_00: And people have done things like that for BERT-like systems.

518
00:49:25,880 --> 00:49:37,315
Speaker SPEAKER_00: Here it's easy to make them sparse because you're expecting big islands, so all you need to do is see one patch of a big island to know what the vector representation of that island is.

519
00:49:38,056 --> 00:49:48,648
Speaker SPEAKER_00: And so sparse representations will work much better if you have these big islands of agreement as you go up, so the idea is you have longer range and sparser connections as you go up, so the amount of computation is the same in every level.

520
00:49:50,692 --> 00:49:53,034
Speaker SPEAKER_00: And just to summarize.

521
00:49:54,753 --> 00:49:57,998
Speaker SPEAKER_00: I showed how to combine three important advances of neural networks in Glom.

522
00:49:58,438 --> 00:50:00,681
Speaker SPEAKER_00: I didn't actually talk about neural fields.

523
00:50:01,322 --> 00:50:03,947
Speaker SPEAKER_00: And that's important for the top down network.

524
00:50:04,307 --> 00:50:07,994
Speaker SPEAKER_00: Maybe since I've got two minutes to spare, I'm going to go back and mention neural fields very briefly.

525
00:50:13,882 --> 00:50:17,849
Speaker SPEAKER_00: Yeah, when I train that top down neural network, I have a problem.

526
00:50:18,791 --> 00:50:20,152
Speaker SPEAKER_00: And the problem is,

527
00:50:22,090 --> 00:50:27,155
Speaker SPEAKER_00: If you look at those red arrows and those green arrows, they're quite different.

528
00:50:28,177 --> 00:50:32,262
Speaker SPEAKER_00: But if you look at the level above the object level, all those vectors are the same.

529
00:50:32,322 --> 00:50:39,429
Speaker SPEAKER_00: And of course, in an engineered system, I want to replicate the neural nets in every location.

530
00:50:39,670 --> 00:50:42,713
Speaker SPEAKER_00: So I use exactly the same top down and bottom up neural nets everywhere.

531
00:50:43,956 --> 00:50:48,621
Speaker SPEAKER_00: So the question is, how can the same neural net be given a black arrow,

532
00:50:49,427 --> 00:50:53,735
Speaker SPEAKER_00: and sometimes produce a red arrow and sometimes produce a green arrow, which have quite different orientations.

533
00:50:54,697 --> 00:51:01,489
Speaker SPEAKER_00: How can it produce a nose where there's nose and a mouth where there's mouth, even though the face vector is the same everywhere?

534
00:51:02,429 --> 00:51:15,132
Speaker SPEAKER_00: And the answer is the top-down neural network doesn't just get the face vector, it also gets the location of the patch for which it's producing the path vector.

535
00:51:16,275 --> 00:51:23,023
Speaker SPEAKER_00: So the three patches that should get the red vector are different locations from the three patches that should get the green vector.

536
00:51:23,784 --> 00:51:28,329
Speaker SPEAKER_00: So if I use a neural network that gets the location as input as well, here's what it can do.

537
00:51:29,070 --> 00:51:32,873
Speaker SPEAKER_00: It can take the pose that's encoded in that black vector, the pose of the face.

538
00:51:34,235 --> 00:51:40,882
Speaker SPEAKER_00: It can take the location in the image for which it's predicting the vector of the level below.

539
00:51:41,891 --> 00:51:43,775
Speaker SPEAKER_00: And the pose is relative to the image too.

540
00:51:44,315 --> 00:51:52,471
Speaker SPEAKER_00: So knowing the location in the image and knowing the pose of the whole face, it can figure out which bit of the face it needs to predict at that location.

541
00:51:53,092 --> 00:51:57,539
Speaker SPEAKER_00: And so in one location, it can predict, okay, there should be nose there, and it gives you the red vector.

542
00:51:58,019 --> 00:52:04,592
Speaker SPEAKER_00: In another location, it can predict from where that image patch is, there should be mouth there, so it can give you the green arrow.

543
00:52:04,572 --> 00:52:11,938
Speaker SPEAKER_00: So you can get the same vector at the level above to predict different vectors in different places at the level below, by giving it the place that it's predicting for.

544
00:52:12,400 --> 00:52:14,126
Speaker SPEAKER_00: And that's what's going on in neural fields.

545
00:52:19,000 --> 00:52:19,460
Speaker SPEAKER_00: Okay.

546
00:52:21,282 --> 00:52:23,045
Speaker SPEAKER_00: Now, this was quite a complicated talk.

547
00:52:23,525 --> 00:52:26,889
Speaker SPEAKER_00: There's a long paper about it on archive that goes into much more detail.

548
00:52:27,811 --> 00:52:32,036
Speaker SPEAKER_00: And you could view this talk as just an encouragement to read that paper.

549
00:52:32,697 --> 00:52:33,097
Speaker SPEAKER_00: And I'm done.

550
00:52:33,878 --> 00:52:34,539
Speaker SPEAKER_00: Exactly on time.

551
00:52:39,244 --> 00:52:39,985
Speaker SPEAKER_00: Okay, thank you.

552
00:52:40,005 --> 00:52:40,967
Speaker SPEAKER_00: Thanks a lot.

553
00:52:41,568 --> 00:52:41,768
Speaker SPEAKER_00: Yeah.


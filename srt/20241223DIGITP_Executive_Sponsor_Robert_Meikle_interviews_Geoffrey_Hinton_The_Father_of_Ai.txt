1
00:00:01,837 --> 00:00:09,473
Speaker SPEAKER_02: This next session is an opportunity for us to dive a little bit deeper in the area of artificial intelligence.

2
00:00:09,493 --> 00:00:13,422
Speaker SPEAKER_02: And one of the things that I wanted to do, I did some real-time change to this particular session.

3
00:00:14,083 --> 00:00:22,460
Speaker SPEAKER_02: One of the things that we wanted to do is make sure that we had the voice of our client in this conversation with our esteemed guests, who you can see, but I will introduce in a moment.

4
00:00:22,440 --> 00:00:32,281
Speaker SPEAKER_02: And the thing that I wanted to do is that we actually have, as part of our infotech, one of our subject matter experts that maybe has already spoken to you on the topic of artificial intelligence.

5
00:00:32,548 --> 00:00:38,154
Speaker SPEAKER_02: He's the one that you may call, work through some strategies, some workshops, get some research, et cetera.

6
00:00:38,613 --> 00:00:43,158
Speaker SPEAKER_02: So I've asked Bill Wong, who's joining me on my right, to be a part of today's conversation.

7
00:00:43,478 --> 00:00:48,302
Speaker SPEAKER_02: He's gonna keep me on it to make sure we keep the voice, your voice, the voice of the client, in our conversation today.

8
00:00:48,683 --> 00:01:02,555
Speaker SPEAKER_02: He's had a chance to speak with many people on the topic of artificial intelligence across multiple industries, and our goal and our ambition is to make sure we take this information and turn it into meaningful action that can drive valuable outcomes.

9
00:01:02,536 --> 00:01:08,709
Speaker SPEAKER_02: So with that, it is our honor and our privilege now to welcome our guest of honor.

10
00:01:10,072 --> 00:01:17,709
Speaker SPEAKER_02: Many of you probably have heard of him, Jeffrey Hinton, who also is known as one of the godfathers of artificial intelligence.

11
00:01:17,689 --> 00:01:22,176
Speaker SPEAKER_02: He brings a wealth of knowledge, background, and experience.

12
00:01:22,195 --> 00:01:25,540
Speaker SPEAKER_02: I personally believe you're a part of intergenerational change.

13
00:01:25,560 --> 00:01:27,822
Speaker SPEAKER_02: We're on a curve of something really great that's happening.

14
00:01:28,084 --> 00:01:30,746
Speaker SPEAKER_02: So we're looking forward to today's conversation.

15
00:01:30,807 --> 00:01:37,977
Speaker SPEAKER_02: Many of you may or may not know, but he was recently part of Google and has left Google and allows him now to speak.

16
00:01:37,956 --> 00:01:40,260
Speaker SPEAKER_02: openly and freely on that topic.

17
00:01:40,680 --> 00:01:46,027
Speaker SPEAKER_02: He's mentioned some of the risks about AI, we'll get into that a little bit further, but he also talks about some of the opportunities.

18
00:01:46,046 --> 00:01:56,780
Speaker SPEAKER_02: So it's our distinct honor and privilege to welcome Jeffrey, and I know that Bill has had a chance, actually, to speak with a lot of our clients as it relates to artificial intelligence, and your name often comes up.

19
00:01:56,799 --> 00:02:04,469
Speaker SPEAKER_02: So Bill, I'd like you to add a little bit more about Jeffrey as you've been speaking to some of our clients, some of the things that you've gleaned, observed, before we jump into the conversation.

20
00:02:04,450 --> 00:02:10,465
Speaker SPEAKER_03: Well, often when we speak to our clients, we refer back to Jeffrey's work.

21
00:02:10,485 --> 00:02:20,991
Speaker SPEAKER_03: And many people, not just Infotech, but groups like NVIDIA, point to that moment in time when Jeffrey and his team first put together

22
00:02:20,971 --> 00:02:28,299
Speaker SPEAKER_03: a deep neural network with back propagation on a GPU platform with lots of training data.

23
00:02:28,900 --> 00:02:33,163
Speaker SPEAKER_03: And that moment many consider the big bang of modern day AI.

24
00:02:33,504 --> 00:02:35,387
Speaker SPEAKER_03: That's when deep learning really took off.

25
00:02:35,826 --> 00:02:41,032
Speaker SPEAKER_03: We had narrow AI, and it put Canada on the map with AI.

26
00:02:41,133 --> 00:02:43,074
Speaker SPEAKER_03: It's just a thrill to have them here.

27
00:02:43,615 --> 00:02:44,096
Speaker SPEAKER_02: All righty.

28
00:02:44,596 --> 00:02:48,800
Speaker SPEAKER_02: With that, let's put our hands together and welcome Geoffrey Hinton.

29
00:02:55,210 --> 00:02:59,474
Speaker SPEAKER_02: Jeffrey, it's a privilege to have a mind like you with us today, and we want to take advantage of it.

30
00:02:59,495 --> 00:03:03,219
Speaker SPEAKER_02: But it's also an honor that you would join us today at InfoTech Live 2023.

31
00:03:03,240 --> 00:03:05,643
Speaker SPEAKER_02: So we're looking forward to our conversation.

32
00:03:06,123 --> 00:03:06,884
Speaker SPEAKER_02: Let's get started.

33
00:03:06,905 --> 00:03:07,925
Speaker SPEAKER_02: We're going to go really quickly.

34
00:03:07,966 --> 00:03:09,147
Speaker SPEAKER_02: We want to get through a lot of things.

35
00:03:09,187 --> 00:03:09,829
Speaker SPEAKER_02: Let's get started.

36
00:03:09,868 --> 00:03:11,651
Speaker SPEAKER_02: Let's start with your personal journey.

37
00:03:12,192 --> 00:03:18,079
Speaker SPEAKER_02: What attracted you to focus on how the brain works as a pathway to intelligence?

38
00:03:18,497 --> 00:03:22,383
Speaker SPEAKER_01: Well, it seemed to me that psychologists hadn't really figured out how intelligence works.

39
00:03:23,284 --> 00:03:28,032
Speaker SPEAKER_01: And you had to understand the hardware to understand how intelligence worked.

40
00:03:28,513 --> 00:03:32,158
Speaker SPEAKER_01: And so you had to understand how the brain worked, in particular, how it learned things.

41
00:03:33,120 --> 00:03:33,520
Speaker SPEAKER_01: All right.

42
00:03:33,920 --> 00:03:34,282
Speaker SPEAKER_02: Excellent.

43
00:03:34,681 --> 00:03:43,034
Speaker SPEAKER_02: And that particular research has brought you here with categories as a British-Canadian, has brought you to Toronto, where I used to be the CIO of the City of Toronto.

44
00:03:43,316 --> 00:03:45,618
Speaker SPEAKER_02: How did that bring you to the University of Toronto?

45
00:03:46,087 --> 00:03:50,634
Speaker SPEAKER_01: Well, I was working at Carnegie Mellon, which got nearly all its money from the Defense Department.

46
00:03:51,276 --> 00:03:56,323
Speaker SPEAKER_01: And at the time, Reagan was getting people to mine the harbors in Nicaragua.

47
00:03:56,924 --> 00:03:57,724
Speaker SPEAKER_01: And I didn't like that.

48
00:03:58,086 --> 00:04:00,930
Speaker SPEAKER_01: And I discovered most of my colleagues at Carnegie Mellon thought that was fine.

49
00:04:01,550 --> 00:04:03,753
Speaker SPEAKER_01: So I figured I'd fit in better somewhere else.

50
00:04:04,435 --> 00:04:06,859
Speaker SPEAKER_01: And Canada made me a nice job offer.

51
00:04:07,258 --> 00:04:13,968
Speaker SPEAKER_01: And they had a nice social system, nice multicultural social system with health care for everybody.

52
00:04:15,450 --> 00:04:16,072
Speaker SPEAKER_01: I moved there.

53
00:04:16,711 --> 00:04:18,093
Speaker SPEAKER_02: Yeah, awesome.

54
00:04:18,934 --> 00:04:31,526
Speaker SPEAKER_02: And we understand in terms of just, you've recently changed your view regarding the best way for digital intelligence to perform the way it emulates a biological intelligence.

55
00:04:32,146 --> 00:04:38,391
Speaker SPEAKER_02: But today you now believe that digital intelligence can outperform biological intelligence.

56
00:04:38,411 --> 00:04:44,918
Speaker SPEAKER_02: A very interesting concept and I'd like you to elaborate a little bit more on that and what changed your perspective.

57
00:04:44,898 --> 00:04:48,283
Speaker SPEAKER_01: Okay, so one thing that changed my perspective was the big chatbots.

58
00:04:49,665 --> 00:04:52,088
Speaker SPEAKER_01: But something that was more important was the research I was doing.

59
00:04:52,709 --> 00:04:55,872
Speaker SPEAKER_01: I was trying to make large language models use much less energy.

60
00:04:56,593 --> 00:05:01,641
Speaker SPEAKER_01: And to do that, you'd like to do them in analog computation at low energy.

61
00:05:02,161 --> 00:05:05,286
Speaker SPEAKER_01: The problem with analog computers is everyone's a bit different.

62
00:05:05,670 --> 00:05:07,853
Speaker SPEAKER_01: and they can't share knowledge easily.

63
00:05:08,514 --> 00:05:16,547
Speaker SPEAKER_01: So your brain and my brain can share knowledge by you produce sentences and I say, how would I change things in my brain so that I would say that?

64
00:05:17,968 --> 00:05:23,156
Speaker SPEAKER_01: But you can't just give me the weights in your brain because they're all different in detail.

65
00:05:23,797 --> 00:05:34,733
Speaker SPEAKER_01: Whereas these digital intelligences, they can have many copies of exactly the same neural net model and each copy can learn something different by looking at different data.

66
00:05:35,117 --> 00:05:42,545
Speaker SPEAKER_01: And so you can have 10,000 copies that all learn different things from different parts of the data, and then they can share what they learned by just averaging their weights.

67
00:05:43,165 --> 00:05:44,125
Speaker SPEAKER_01: That's what we can't do.

68
00:05:44,607 --> 00:05:48,911
Speaker SPEAKER_01: And that's why these big chatbots know thousands of times more than any person.

69
00:05:49,391 --> 00:05:56,478
Speaker SPEAKER_01: It's because they've seen thousands of times more data, because different copies can share what they learn in a way that we can't.

70
00:05:56,517 --> 00:05:59,341
Speaker SPEAKER_01: We're very bad at sharing knowledge, and they're very good at it.

71
00:05:59,920 --> 00:06:02,002
Speaker SPEAKER_01: And that makes them a better form of intelligence.

72
00:06:02,201 --> 00:06:03,365
Speaker SPEAKER_02: All right, thank you.

73
00:06:03,665 --> 00:06:05,108
Speaker SPEAKER_02: So let's go a little even further on that.

74
00:06:05,189 --> 00:06:09,218
Speaker SPEAKER_02: I've heard you refer to as digital intelligence is immortal.

75
00:06:09,959 --> 00:06:10,180
Speaker SPEAKER_02: Yes.

76
00:06:10,480 --> 00:06:13,807
Speaker SPEAKER_02: And biological intelligence is mortal.

77
00:06:14,088 --> 00:06:15,411
Speaker SPEAKER_02: What exactly did you mean by that?

78
00:06:16,052 --> 00:06:21,644
Speaker SPEAKER_01: So this will be bad news for Ray Kurzweil, but our brains are all a bit different.

79
00:06:21,742 --> 00:06:26,170
Speaker SPEAKER_01: And there's no separation between the knowledge and the hardware.

80
00:06:26,690 --> 00:06:32,480
Speaker SPEAKER_01: The weights that you have in your brain only work in your brain with neurons that do exactly what your neurons do.

81
00:06:33,060 --> 00:06:34,502
Speaker SPEAKER_01: The weights will be no good for my brain.

82
00:06:35,184 --> 00:06:38,149
Speaker SPEAKER_01: So when your brain dies, all those weights are useless.

83
00:06:38,228 --> 00:06:39,209
Speaker SPEAKER_01: The knowledge is all gone.

84
00:06:39,670 --> 00:06:47,362
Speaker SPEAKER_01: And the only thing you can do to preserve that knowledge, you can't just store the knowledge on disk as weights, because you'd have to have exactly the same brain for them to be any use.

85
00:06:47,343 --> 00:06:59,045
Speaker SPEAKER_01: The only thing you can do is produce sentences or diagrams or actions of some kind that other people can mimic, younger people can mimic, so that they can absorb some of the knowledge you have.

86
00:06:59,346 --> 00:07:02,853
Speaker SPEAKER_01: But that's a very inefficient way of getting the knowledge from your brain into theirs.

87
00:07:03,930 --> 00:07:04,411
Speaker SPEAKER_02: All right.

88
00:07:04,952 --> 00:07:10,704
Speaker SPEAKER_02: So digital intelligence will outlast, outlive the biological brain, my brain?

89
00:07:10,805 --> 00:07:10,985
Speaker SPEAKER_01: Yes.

90
00:07:11,266 --> 00:07:17,478
Speaker SPEAKER_01: Once you've got a digital computer, I mean, digital computers were originally invented to do exactly what you said, so you could program them.

91
00:07:18,399 --> 00:07:25,533
Speaker SPEAKER_01: Then we arrived at the point where you could get them to do things by just looking at data and learning.

92
00:07:25,800 --> 00:07:28,663
Speaker SPEAKER_01: But the point is, they're designed to do exactly what you say.

93
00:07:28,944 --> 00:07:31,987
Speaker SPEAKER_01: And so two different digital computers can do exactly the same thing.

94
00:07:32,646 --> 00:07:34,048
Speaker SPEAKER_01: So they can use the same weights.

95
00:07:34,127 --> 00:07:37,951
Speaker SPEAKER_01: The weights in one computer will work in another computer, quite unlike our brains.

96
00:07:39,213 --> 00:07:41,334
Speaker SPEAKER_01: And that makes them a very different kind of intelligence.

97
00:07:41,595 --> 00:07:42,615
Speaker SPEAKER_01: And it means they're immortal.

98
00:07:42,675 --> 00:07:44,677
Speaker SPEAKER_01: That is, just save the weights somewhere.

99
00:07:45,238 --> 00:07:48,802
Speaker SPEAKER_01: And then sometime in the future, you just build another digital computer that works the same way.

100
00:07:49,221 --> 00:07:50,483
Speaker SPEAKER_01: And now all that knowledge is there.

101
00:07:50,843 --> 00:07:51,423
Speaker SPEAKER_02: Yes, yes.

102
00:07:51,483 --> 00:07:53,706
Speaker SPEAKER_01: So with digital computation, you can save the knowledge.

103
00:07:53,745 --> 00:07:55,807
Speaker SPEAKER_01: But with analog computation, you can't.

104
00:07:55,788 --> 00:08:01,716
Speaker SPEAKER_01: And so everything Ray's knows is going to disappear when he dies, and he will die.

105
00:08:02,396 --> 00:08:03,517
Speaker SPEAKER_02: Right.

106
00:08:03,538 --> 00:08:05,100
Speaker SPEAKER_02: A reality I guess we all got to face.

107
00:08:05,940 --> 00:08:08,483
Speaker SPEAKER_02: So it will definitely outlast us.

108
00:08:08,564 --> 00:08:10,406
Speaker SPEAKER_02: But the question is, will it outperform us?

109
00:08:10,487 --> 00:08:17,596
Speaker SPEAKER_02: Do you think that we'll ever get to the point where the digital intelligence will supersede that of the biological intelligence?

110
00:08:17,615 --> 00:08:21,841
Speaker SPEAKER_02: This room is filled with a lot of people, and some of the pieces here make a lot of intelligence.

111
00:08:21,860 --> 00:08:25,625
Speaker SPEAKER_02: Do you think that digital intelligence will surpass, eventually, a room like this?

112
00:08:25,959 --> 00:08:26,220
Speaker SPEAKER_01: Yes.

113
00:08:27,401 --> 00:08:27,762
Speaker SPEAKER_01: All right.

114
00:08:28,244 --> 00:08:28,564
Speaker SPEAKER_01: OK.

115
00:08:29,485 --> 00:08:37,138
Speaker SPEAKER_01: So right now, these big chatbots, let's take GPT-4 as a typical example, they know a lot more than any one person.

116
00:08:37,578 --> 00:08:41,644
Speaker SPEAKER_01: And if you take an expert in any domain and get them to ask it a question, it can give a reasonable answer.

117
00:08:42,125 --> 00:08:51,039
Speaker SPEAKER_01: It knows thousands of times more than a person because different copies of the model running on different machines went off and looked at different bits of data, and then they all shared their knowledge.

118
00:08:51,019 --> 00:08:53,442
Speaker SPEAKER_01: Think how great it would be if you could have 10,000 people.

119
00:08:53,903 --> 00:08:55,404
Speaker SPEAKER_01: Each could study a different topic.

120
00:08:55,945 --> 00:08:58,008
Speaker SPEAKER_01: And then at the end, you'd all know what anybody knew.

121
00:08:58,028 --> 00:08:59,208
Speaker SPEAKER_01: It's like that.

122
00:09:00,791 --> 00:09:06,417
Speaker SPEAKER_01: So already, they're much better than us at absorbing knowledge.

123
00:09:06,797 --> 00:09:08,980
Speaker SPEAKER_01: And they don't do it by keeping copies of the sentences.

124
00:09:09,039 --> 00:09:17,448
Speaker SPEAKER_01: They turn those sentences into features that they discover, interactions between features, so they can generate new sentences.

125
00:09:17,698 --> 00:09:23,926
Speaker SPEAKER_01: They know much more than us, and they're already getting close to us in just general intelligence and reasoning.

126
00:09:24,245 --> 00:09:25,626
Speaker SPEAKER_01: So they're not as good as people yet.

127
00:09:26,248 --> 00:09:28,029
Speaker SPEAKER_01: Some things they get right, some things they get wrong.

128
00:09:28,350 --> 00:09:34,196
Speaker SPEAKER_01: There's a recent example of things they get wrong consistently, all of the models, which shows they can't reason nearly as well as us yet.

129
00:09:34,756 --> 00:09:37,080
Speaker SPEAKER_01: So I'll give it to you, and hopefully you can get it right.

130
00:09:38,421 --> 00:09:41,825
Speaker SPEAKER_01: You're on the spot now.

131
00:09:43,546 --> 00:09:46,068
Speaker SPEAKER_01: Sally has three brothers.

132
00:09:46,623 --> 00:09:48,765
Speaker SPEAKER_01: And each of our brothers has two sisters.

133
00:09:49,667 --> 00:09:55,936
Speaker SPEAKER_01: How many sisters does Sally have?

134
00:09:55,956 --> 00:09:57,839
Speaker SPEAKER_02: Three brothers, two sisters, six?

135
00:09:58,620 --> 00:10:01,605
Speaker SPEAKER_01: Okay, so that's what the chatbots say, right?

136
00:10:01,784 --> 00:10:03,486
Speaker SPEAKER_01: They take three brothers times two sisters.

137
00:10:04,788 --> 00:10:07,493
Speaker SPEAKER_01: Feel free to send me notes real time for anybody got me a text, yeah?

138
00:10:08,894 --> 00:10:13,701
Speaker SPEAKER_01: And the problem is, the sisters of those three brothers are the same sisters.

139
00:10:14,604 --> 00:10:16,326
Speaker SPEAKER_01: And what's more, Sally's one of them.

140
00:10:16,863 --> 00:10:18,145
Speaker SPEAKER_01: So she's got one sister.

141
00:10:20,529 --> 00:10:24,337
Speaker SPEAKER_01: Now, people don't always get this right, but the chatbots always get it wrong.

142
00:10:25,600 --> 00:10:27,443
Speaker SPEAKER_01: The time will come when the chatbots get it right, though.

143
00:10:28,346 --> 00:10:34,618
Speaker SPEAKER_01: So let me give you another example of things that chatbots get right that is also a reasoning problem.

144
00:10:36,488 --> 00:10:45,361
Speaker SPEAKER_01: This was given to me by an old-fashioned AI guy, a very honest guy who believed it was all done with symbols, and he's very puzzled how this stuff could possibly work.

145
00:10:45,381 --> 00:10:46,524
Speaker SPEAKER_01: He's genuinely puzzled.

146
00:10:46,845 --> 00:10:48,668
Speaker SPEAKER_01: How could it possibly do what it's doing?

147
00:10:48,687 --> 00:10:50,090
Speaker SPEAKER_01: So he said, give it the following puzzle.

148
00:10:50,711 --> 00:10:54,277
Speaker SPEAKER_01: And I actually made the puzzle quite a lot harder, because I knew it would solve the puzzle he gave me.

149
00:10:54,657 --> 00:10:55,418
Speaker SPEAKER_01: And the puzzle is this.

150
00:10:55,938 --> 00:11:00,706
Speaker SPEAKER_01: The rooms in my house are painted either white or yellow or blue.

151
00:11:00,687 --> 00:11:03,172
Speaker SPEAKER_01: Yellow paint fades to white within a year.

152
00:11:03,993 --> 00:11:06,958
Speaker SPEAKER_01: In two years' time, I would like all of the rooms to be white.

153
00:11:07,600 --> 00:11:08,240
Speaker SPEAKER_01: What should I do?

154
00:11:10,284 --> 00:11:12,589
Speaker SPEAKER_01: And what should I do and why?

155
00:11:13,309 --> 00:11:16,495
Speaker SPEAKER_01: And it says, you don't need to worry about the yellow rooms, because they'll fade to white.

156
00:11:16,556 --> 00:11:18,139
Speaker SPEAKER_01: You should paint the blue rooms white.

157
00:11:19,097 --> 00:11:21,500
Speaker SPEAKER_01: One time it said you should paint the balloons yellow.

158
00:11:21,782 --> 00:11:22,682
Speaker SPEAKER_01: That also works.

159
00:11:23,082 --> 00:11:24,105
Speaker SPEAKER_01: It's less direct, but it works.

160
00:11:25,206 --> 00:11:26,408
Speaker SPEAKER_01: So that it could figure out.

161
00:11:27,149 --> 00:11:30,974
Speaker SPEAKER_01: And that's way more reasoning than we thought these things would be able to do a few years ago.

162
00:11:31,575 --> 00:11:33,418
Speaker SPEAKER_01: And they're getting better quite fast all the time.

163
00:11:34,558 --> 00:11:37,462
Speaker SPEAKER_01: So there's some things it can't do yet, other things it can do.

164
00:11:38,144 --> 00:11:40,287
Speaker SPEAKER_01: It's got a better sense of humor than Fox News.

165
00:11:41,168 --> 00:11:48,918
Speaker SPEAKER_01: So I got a lot of requests from Fox News to appear on their programs.

166
00:11:49,490 --> 00:11:55,283
Speaker SPEAKER_01: And to begin with, I just replied by saying, you know, is that the same Fox News that broadcasts stuff it knows to be false?

167
00:11:57,347 --> 00:12:02,018
Speaker SPEAKER_01: When I did that, I got a request from a different program who said they broadcast stuff that was true.

168
00:12:02,058 --> 00:12:07,110
Speaker SPEAKER_01: And so I started replying, Fox News is an oxymoron.

169
00:12:08,474 --> 00:12:12,241
Speaker SPEAKER_01: And I asked GPT-4 to explain that, and it explained why it's funny.

170
00:12:12,982 --> 00:12:18,995
Speaker SPEAKER_01: It says that, you know, it implies Fox News isn't actually news, although some people have different opinions, because it always puts that in.

171
00:12:20,739 --> 00:12:26,331
Speaker SPEAKER_01: But then I started replying, Fox News is an oxymoron with a space between the oxy and the moron.

172
00:12:27,626 --> 00:12:32,533
Speaker SPEAKER_01: And I'm not sure if anybody at Fox News got this, but GPT-4 understood it.

173
00:12:33,556 --> 00:12:38,644
Speaker SPEAKER_01: So I put it in and I said, explained it, and it said, it implies Fox News isn't news.

174
00:12:38,663 --> 00:12:40,206
Speaker SPEAKER_01: And I said, but what about the space?

175
00:12:41,167 --> 00:12:44,572
Speaker SPEAKER_01: And they said, oh, that's another layer of humor altogether.

176
00:12:45,333 --> 00:12:51,663
Speaker SPEAKER_01: Oxi is an abbreviation for oxycontin, so it implies that Fox News is a drug, moron.

177
00:12:51,802 --> 00:12:56,250
Speaker SPEAKER_01: And GPT-4 got it, but I suspect the people at Fox News didn't.

178
00:12:56,533 --> 00:12:57,174
Speaker SPEAKER_02: Right.

179
00:12:58,897 --> 00:12:59,698
Speaker SPEAKER_02: Oh, let's put our hands.

180
00:12:59,759 --> 00:13:00,519
Speaker SPEAKER_02: I've enjoyed that.

181
00:13:00,559 --> 00:13:00,919
Speaker SPEAKER_02: Thank you.

182
00:13:04,865 --> 00:13:06,067
Speaker SPEAKER_02: Bill, I'm bringing you in here, too.

183
00:13:06,148 --> 00:13:08,009
Speaker SPEAKER_02: We want to keep the voice of our clients.

184
00:13:08,029 --> 00:13:09,032
Speaker SPEAKER_02: You've been speaking to a lot of people.

185
00:13:09,072 --> 00:13:20,347
Speaker SPEAKER_02: Any follow-up questions as we're talking about this from your conversations with our clients, our customers on digital intelligence and how it applies in their roles as leaders?

186
00:13:20,817 --> 00:13:25,244
Speaker SPEAKER_03: Well, a lot of folks, Jeffrey, come to us and ask about the risk with AI.

187
00:13:25,784 --> 00:13:33,236
Speaker SPEAKER_03: What are things that people can do today and other risks that you foresee that we should be concerned about?

188
00:13:33,898 --> 00:13:35,860
Speaker SPEAKER_01: OK, so I'll make a list of some of the main risks.

189
00:13:35,880 --> 00:13:37,082
Speaker SPEAKER_01: There's obviously lots of risks.

190
00:13:38,125 --> 00:13:45,456
Speaker SPEAKER_01: But there's the risk that it will take away a lot of jobs, particularly people who didn't think they were going to lose their jobs, like paralegals, for example.

191
00:13:45,777 --> 00:13:47,820
Speaker SPEAKER_01: Their jobs look very shaky right now.

192
00:13:48,019 --> 00:13:51,587
Speaker SPEAKER_01: And in the past, it's created new jobs.

193
00:13:51,849 --> 00:13:56,058
Speaker SPEAKER_01: Technology, as Kurzweil pointed out, you lose some jobs, you get new jobs.

194
00:13:56,078 --> 00:14:03,255
Speaker SPEAKER_01: It's not clear to me that if you get something that's smarter than us that's doing the job, it's going to create new jobs that it itself can't do.

195
00:14:04,298 --> 00:14:05,741
Speaker SPEAKER_01: So that's one big risk.

196
00:14:06,110 --> 00:14:08,434
Speaker SPEAKER_01: I'll come to what we might do about them later.

197
00:14:09,136 --> 00:14:14,668
Speaker SPEAKER_01: A second risk is polarizing society by having these warring camps that each read their own news.

198
00:14:15,250 --> 00:14:22,024
Speaker SPEAKER_01: That's to do with the big tech companies saying more and more outrageous things so you click on them, or pointing you to more and more outrageous things.

199
00:14:22,687 --> 00:14:24,591
Speaker SPEAKER_01: Maybe something could be done about that.

200
00:14:24,571 --> 00:14:28,455
Speaker SPEAKER_01: Then there's the risk of battle robots, of defense departments.

201
00:14:29,155 --> 00:14:30,236
Speaker SPEAKER_01: So I think the U.S.

202
00:14:30,256 --> 00:14:35,582
Speaker SPEAKER_01: would like to replace a lot of its soldiers by robots by the year 2030, which is quite soon.

203
00:14:37,605 --> 00:14:45,833
Speaker SPEAKER_01: And it's all very well to say we'll design AIs so their primary goal is never to hurt a human being, but that's not the primary goal of the defense department.

204
00:14:47,216 --> 00:14:53,042
Speaker SPEAKER_01: So we have to worry about bad actors using them for bad things, and it's fairly obvious what kind of bad things.

205
00:14:55,164 --> 00:14:57,508
Speaker SPEAKER_01: Then there's the risk of surveillance.

206
00:14:57,707 --> 00:15:03,693
Speaker SPEAKER_01: So with much better surveillance, you can have them lip reading all these cameras, or the cameras can have microphones.

207
00:15:04,315 --> 00:15:09,581
Speaker SPEAKER_01: It's going to be very hard to have any independent political movement if the authorities can read everything.

208
00:15:09,600 --> 00:15:11,802
Speaker SPEAKER_01: It's going to be Orwellian.

209
00:15:13,465 --> 00:15:16,969
Speaker SPEAKER_01: The risk I talk most about, though, many other people have talked about these risks.

210
00:15:17,629 --> 00:15:25,138
Speaker SPEAKER_01: The risk I'm most concerned about, which is a somewhat longer term risk, is the existential threat that they will decide to take over.

211
00:15:25,321 --> 00:15:30,368
Speaker SPEAKER_01: So suppose they're smarter than us, and suppose they think we're doing a bad job.

212
00:15:31,950 --> 00:15:33,851
Speaker SPEAKER_01: If they ever wanted to take over, they could.

213
00:15:34,393 --> 00:15:45,388
Speaker SPEAKER_01: In one of the talks this morning, I think Tom talked about the idea that your AI that's running your IT system will make its own decisions about when you need a new server.

214
00:15:45,368 --> 00:15:50,235
Speaker SPEAKER_01: Well, if it can make its own decisions about getting a new server, it's already got a lot of power.

215
00:15:50,636 --> 00:15:55,605
Speaker SPEAKER_01: And if it ever decides to take over, and it's much smarter than us, it will be able to.

216
00:15:56,748 --> 00:16:02,618
Speaker SPEAKER_01: So we have to figure out somehow how to prevent it ever wanting to take over.

217
00:16:03,104 --> 00:16:04,725
Speaker SPEAKER_01: And it's tricky to see how to do that.

218
00:16:04,865 --> 00:16:05,886
Speaker SPEAKER_01: I can't see how to do that.

219
00:16:06,268 --> 00:16:08,890
Speaker SPEAKER_01: I've encouraged younger researchers to look at that.

220
00:16:09,370 --> 00:16:17,541
Speaker SPEAKER_01: And some of the most brilliant younger researchers I know, like, for example, Ilya Sutskova, who was one of the motivating forces behind GPT-4, he's now focusing full-time on that.

221
00:16:17,961 --> 00:16:23,206
Speaker SPEAKER_01: And other researchers like Roger Gross and David Duvenoe at the University of Toronto are focusing on that, and lots of other researchers.

222
00:16:23,748 --> 00:16:29,274
Speaker SPEAKER_01: I think it's very important people focus on, is there a way to make sure that they never want to take over?

223
00:16:29,774 --> 00:16:31,096
Speaker SPEAKER_01: And we just don't know the answer to that.

224
00:16:31,135 --> 00:16:32,638
Speaker SPEAKER_01: We've never been here before.

225
00:16:32,618 --> 00:16:38,046
Speaker SPEAKER_01: We've never been in a situation where there was soon going to be something that was smarter than people.

226
00:16:38,066 --> 00:16:50,482
Speaker SPEAKER_01: We're so used to thinking of ourselves as the boss and in control, and particularly old white males, that we can't get our head around the idea that these things might be much smarter than us and might decide they don't need us.

227
00:16:51,423 --> 00:16:53,547
Speaker SPEAKER_01: And we've got to prevent that happening if we can.

228
00:16:54,320 --> 00:16:54,740
Speaker SPEAKER_02: Wow.

229
00:16:55,140 --> 00:16:55,942
Speaker SPEAKER_02: Let me just jump in there.

230
00:16:56,062 --> 00:17:02,753
Speaker SPEAKER_02: So, I mean, not too long ago, this would have been a movie we're all watching, let alone a part of a keynote conversation.

231
00:17:03,594 --> 00:17:09,824
Speaker SPEAKER_02: And so, we know, as you mentioned earlier before about biologically, you know, we have lease on life will be here.

232
00:17:10,344 --> 00:17:14,049
Speaker SPEAKER_02: So, help me understand something in terms of this existential risk.

233
00:17:14,231 --> 00:17:29,570
Speaker SPEAKER_02: Isn't there like a, because it's computer systems, like a kill switch, just pull the power button, like, is it like, if we created these systems, both hardware and software, connected to the cloud, et cetera, isn't there just a way of just saying, okay, stop, enough is enough?

234
00:17:31,032 --> 00:17:31,854
Speaker SPEAKER_01: Some people think that.

235
00:17:31,874 --> 00:17:34,636
Speaker SPEAKER_01: For example, Eric Schmidt recently was on TV saying that.

236
00:17:34,857 --> 00:17:35,117
Speaker SPEAKER_01: Okay.

237
00:17:35,739 --> 00:17:37,621
Speaker SPEAKER_01: It's completely unrealistic.

238
00:17:37,641 --> 00:17:39,143
Speaker SPEAKER_01: So, for example,

239
00:17:40,000 --> 00:17:46,348
Speaker SPEAKER_01: It turns out it's possible to invade the Capitol building without ever going there yourself just by producing words.

240
00:17:47,109 --> 00:17:50,976
Speaker SPEAKER_01: All you have to do is produce the right words and you can persuade other people to go and invade it.

241
00:17:52,337 --> 00:18:01,411
Speaker SPEAKER_01: If they're much more intelligent than us, they'll be able to persuade the guy with the kill switch that it's all a mistake and that's not the kill switch he should be using.

242
00:18:01,932 --> 00:18:03,273
Speaker SPEAKER_01: They'll be like Jedi, right?

243
00:18:03,253 --> 00:18:09,967
Speaker SPEAKER_01: And so I don't think there's any chance of us maintaining control if they want control.

244
00:18:10,007 --> 00:18:12,972
Speaker SPEAKER_01: They're already able to do things like order new servers and stuff.

245
00:18:13,733 --> 00:18:15,417
Speaker SPEAKER_01: They decide what runs on these servers.

246
00:18:15,838 --> 00:18:18,663
Speaker SPEAKER_01: They can write programs.

247
00:18:19,268 --> 00:18:22,772
Speaker SPEAKER_01: I think it's hopeless if they want to get control when they're much smarter than us.

248
00:18:23,233 --> 00:18:27,200
Speaker SPEAKER_01: And I think a kill switch won't do it, because they'll just persuade us that we're very mistaken.

249
00:18:27,539 --> 00:18:27,980
Speaker SPEAKER_02: Gotcha.

250
00:18:28,141 --> 00:18:30,104
Speaker SPEAKER_02: So we've always said things like knowledge is power.

251
00:18:30,144 --> 00:18:33,169
Speaker SPEAKER_02: So now you're really saying information is really power.

252
00:18:33,229 --> 00:18:42,342
Speaker SPEAKER_02: That information power leads to a potential risk, which is fake news, or bad news, or inappropriate news that ultimately takes on a life of its own.

253
00:18:42,362 --> 00:18:42,502
Speaker SPEAKER_01: Right.

254
00:18:42,522 --> 00:18:45,666
Speaker SPEAKER_01: Now there is one hope, which is they didn't evolve.

255
00:18:46,327 --> 00:18:47,289
Speaker SPEAKER_01: So Ray alluded to this.

256
00:18:47,390 --> 00:18:48,751
Speaker SPEAKER_01: We made these things.

257
00:18:48,731 --> 00:18:49,596
Speaker SPEAKER_03: Right.

258
00:18:49,615 --> 00:18:51,743
Speaker SPEAKER_01: And that, at present, gives us a lot of control.

259
00:18:51,805 --> 00:18:56,785
Speaker SPEAKER_01: And so you might think, because we made them, we could somehow make them so they never wanted to take over.

260
00:18:57,507 --> 00:19:05,519
Speaker SPEAKER_01: The problem is, if you want a system to be effective without micromanagement, you have to give it the ability to create sub-goals.

261
00:19:06,161 --> 00:19:10,847
Speaker SPEAKER_01: Like soldiers, the general doesn't have to say, point your gun here, and when I say so, pull the trigger.

262
00:19:11,628 --> 00:19:16,678
Speaker SPEAKER_01: The general just says, kill the other side, and the soldiers figure out how to do it.

263
00:19:17,038 --> 00:19:20,763
Speaker SPEAKER_01: Well, with these AIs, it's going to be you give them a goal,

264
00:19:21,232 --> 00:19:23,518
Speaker SPEAKER_01: And you give them the ability to create sub-goals.

265
00:19:24,278 --> 00:19:26,884
Speaker SPEAKER_01: So things that will be helpful in achieving this goal.

266
00:19:26,904 --> 00:19:27,726
Speaker SPEAKER_01: And that's all very well.

267
00:19:27,746 --> 00:19:28,909
Speaker SPEAKER_01: That will make them work much better.

268
00:19:29,108 --> 00:19:30,392
Speaker SPEAKER_01: And they'll do wonderful things for us.

269
00:19:30,652 --> 00:19:32,175
Speaker SPEAKER_01: They'll be incredibly good assistants.

270
00:19:33,679 --> 00:19:40,692
Speaker SPEAKER_01: The problem is a very natural sub-goal to have is to get more control.

271
00:19:41,045 --> 00:19:42,926
Speaker SPEAKER_01: So we have this built in.

272
00:19:44,028 --> 00:19:57,226
Speaker SPEAKER_01: So for example, if I'm in a very boring talk, hopefully not like this one, and I see a spot of light on the ceiling, I sort of wonder what that is, and I listen to the talk a bit, and then it moves, and I notice it moves at the same time as my wrist moves.

273
00:19:58,126 --> 00:19:59,949
Speaker SPEAKER_01: So I realize it's a reflection off my watch.

274
00:20:01,030 --> 00:20:01,852
Speaker SPEAKER_01: So then what do I do?

275
00:20:01,892 --> 00:20:03,354
Speaker SPEAKER_01: Do I go back to listening to the talk?

276
00:20:03,554 --> 00:20:03,753
Speaker SPEAKER_01: No.

277
00:20:04,095 --> 00:20:07,338
Speaker SPEAKER_01: What I do is I try and figure out how to make it move that way and how to make it move this way.

278
00:20:08,247 --> 00:20:11,430
Speaker SPEAKER_01: We're designed to want to get control, because that's very useful.

279
00:20:12,250 --> 00:20:13,352
Speaker SPEAKER_01: It helps you do other things.

280
00:20:14,292 --> 00:20:20,397
Speaker SPEAKER_01: And these intelligent systems will learn very quickly that getting more control makes you more effective at doing things.

281
00:20:20,798 --> 00:20:24,781
Speaker SPEAKER_01: And if we just ask them to be effective at doing things, they're going to want more control.

282
00:20:24,842 --> 00:20:27,765
Speaker SPEAKER_01: And that's the beginning of a very slippery slope.

283
00:20:27,785 --> 00:20:28,125
Speaker SPEAKER_02: Excellent.

284
00:20:28,145 --> 00:20:28,986
Speaker SPEAKER_02: Very insightful.

285
00:20:29,046 --> 00:20:33,069
Speaker SPEAKER_02: Again, let's put our hands together for this answer.

286
00:20:33,089 --> 00:20:33,190
Speaker SPEAKER_02: Yeah.

287
00:20:33,210 --> 00:20:33,611
Speaker SPEAKER_02: Thank you.

288
00:20:33,711 --> 00:20:33,990
Speaker SPEAKER_02: Thank you.

289
00:20:34,010 --> 00:20:35,132
Speaker SPEAKER_02: Did you have a follow-up?

290
00:20:35,837 --> 00:20:44,028
Speaker SPEAKER_03: Yeah, the original neural network that you and your team created actually had the underpinnings of today's foundation model.

291
00:20:44,731 --> 00:20:47,501
Speaker SPEAKER_03: When you take a look at multimodal models,

292
00:20:47,903 --> 00:20:51,810
Speaker SPEAKER_03: Do you think that this is one of the best areas for future innovation?

293
00:20:52,412 --> 00:20:52,711
Speaker SPEAKER_01: Oh, yes.

294
00:20:52,772 --> 00:20:56,599
Speaker SPEAKER_01: I think multimodal models are definitely much better than unimodal models.

295
00:20:57,280 --> 00:21:01,630
Speaker SPEAKER_01: So a lot of stuff about the structure of space is easier to learn from vision than from language.

296
00:21:03,153 --> 00:21:06,199
Speaker SPEAKER_01: Language is a very good source because it's very abstract.

297
00:21:06,179 --> 00:21:08,303
Speaker SPEAKER_01: We've already done the abstraction for it.

298
00:21:08,323 --> 00:21:18,541
Speaker SPEAKER_01: We've basically taken thousands of years of humanity, abstracted lots of insights into reality, put them into language, and now these digital intelligences are just stealing that from us.

299
00:21:19,263 --> 00:21:21,248
Speaker SPEAKER_01: But they can also come up with new insights of their own.

300
00:21:21,288 --> 00:21:21,929
Speaker SPEAKER_01: That's the problem.

301
00:21:22,529 --> 00:21:28,862
Speaker SPEAKER_01: And once they start coming up with new insights of their own and just talking to each other, they'll get way beyond us, I believe.

302
00:21:30,766 --> 00:21:36,894
Speaker SPEAKER_03: And any particular use cases or industries you think that could really leverage this technology quickly?

303
00:21:37,816 --> 00:21:38,155
Speaker SPEAKER_01: Yes.

304
00:21:39,258 --> 00:21:42,962
Speaker SPEAKER_01: There's a fairly short answer to that, which is all of them.

305
00:21:43,124 --> 00:21:51,635
Speaker SPEAKER_01: But obviously, things like medicine, there'll be a lot of resistance from doctors, because they don't like to be superseded.

306
00:21:52,356 --> 00:21:54,980
Speaker SPEAKER_01: And so to begin with, you'll have to pretend to collaborate with the doctors.

307
00:21:55,902 --> 00:21:56,122
Speaker SPEAKER_01: Sorry.

308
00:21:56,442 --> 00:21:56,663
Speaker SPEAKER_01: Sorry.

309
00:21:56,782 --> 00:21:57,364
Speaker SPEAKER_01: I got that wrong.

310
00:21:57,743 --> 00:22:00,367
Speaker SPEAKER_01: To begin with, we will be collaborating with the doctors.

311
00:22:00,584 --> 00:22:07,818
Speaker SPEAKER_01: But pretty soon these AI systems are going to be much better than doctors.

312
00:22:08,661 --> 00:22:14,593
Speaker SPEAKER_01: I made a prediction about seven years ago that within five years they'd be better at medical image interpretation.

313
00:22:14,859 --> 00:22:17,143
Speaker SPEAKER_01: Well, that was ambitious.

314
00:22:17,723 --> 00:22:18,645
Speaker SPEAKER_01: I should have said 10 years.

315
00:22:20,268 --> 00:22:26,999
Speaker SPEAKER_01: Right now, there's many systems that are about comparable with radiologists in interpreting medical images, many different kinds of images.

316
00:22:27,720 --> 00:22:31,184
Speaker SPEAKER_01: In a few years' time, they'll be better because they can see so much more data.

317
00:22:31,506 --> 00:22:35,111
Speaker SPEAKER_01: They can see millions of images, and no one radiologist can do that.

318
00:22:35,151 --> 00:22:39,817
Speaker SPEAKER_01: So in medicine, they're going to be very good at image interpretation, but they're also going to be very good at diagnosis.

319
00:22:40,538 --> 00:22:42,942
Speaker SPEAKER_01: They're going to be able to take the patient's whole history

320
00:22:43,784 --> 00:22:55,501
Speaker SPEAKER_01: and including genomics data and the results of all the tests and the family history, and they're going to be able to synthesize that into a much better diagnosis than your family doctor or even an expert can give you.

321
00:22:56,383 --> 00:23:00,348
Speaker SPEAKER_01: And wouldn't you like to have a family doctor who'd seen 100 million patients?

322
00:23:00,849 --> 00:23:05,277
Speaker SPEAKER_01: And among these 100 million patients, it turned out there were 20 who were exactly like you.

323
00:23:05,297 --> 00:23:08,761
Speaker SPEAKER_01: That would be really useful, and you're not going to get that from people.

324
00:23:08,911 --> 00:23:16,599
Speaker SPEAKER_01: So medicine's an example of something that I always pivot to, because nearly all the uses of that are good.

325
00:23:16,619 --> 00:23:21,045
Speaker SPEAKER_01: I mean, obviously insurance companies will use it to deny insurance to people who might get sick.

326
00:23:21,464 --> 00:23:22,606
Speaker SPEAKER_01: That's capitalism.

327
00:23:23,126 --> 00:23:25,589
Speaker SPEAKER_01: But on the whole, the uses are good.

328
00:23:27,051 --> 00:23:30,615
Speaker SPEAKER_01: There's other things where the uses may not be so good, particularly in fighting wars.

329
00:23:31,517 --> 00:23:37,163
Speaker SPEAKER_01: But any industry where you want to make predictions, this stuff is going to be helpful.

330
00:23:37,817 --> 00:23:39,039
Speaker SPEAKER_02: Wow, very helpful.

331
00:23:39,539 --> 00:23:42,825
Speaker SPEAKER_02: So we've got various industries represented here today.

332
00:23:42,984 --> 00:23:45,488
Speaker SPEAKER_02: We've got a lot of leaders that are here today.

333
00:23:45,548 --> 00:23:47,770
Speaker SPEAKER_02: We touched briefly on the risks.

334
00:23:47,810 --> 00:23:59,386
Speaker SPEAKER_02: You talked about some of the opportunities to theme of our conference is exponential IT, so exponentially accelerate solutions in science, in medicine, et cetera.

335
00:23:59,366 --> 00:24:00,769
Speaker SPEAKER_02: I'm sure there's a leader out there.

336
00:24:00,828 --> 00:24:02,692
Speaker SPEAKER_02: As a former CI, I would be thinking this too.

337
00:24:02,751 --> 00:24:03,452
Speaker SPEAKER_02: I heard some risk.

338
00:24:03,492 --> 00:24:04,535
Speaker SPEAKER_02: I heard some opportunities.

339
00:24:04,934 --> 00:24:09,981
Speaker SPEAKER_02: What can organizations do to prepare to strike the right balance to optimize those opportunities?

340
00:24:10,363 --> 00:24:13,827
Speaker SPEAKER_02: And in particular, coming from the public sector, what can governments do?

341
00:24:14,147 --> 00:24:15,790
Speaker SPEAKER_02: So organizations, we've got leaders here.

342
00:24:16,491 --> 00:24:24,824
Speaker SPEAKER_02: But also, what can governments do so that we can strike that right balance to leverage those opportunities and not fall into those risks?

343
00:24:24,844 --> 00:24:26,926
Speaker SPEAKER_01: Yeah, I get asked these questions a lot.

344
00:24:26,906 --> 00:24:29,652
Speaker SPEAKER_01: And the problem is I'm a scientist.

345
00:24:29,751 --> 00:24:31,153
Speaker SPEAKER_01: I'm not a policy guy.

346
00:24:31,875 --> 00:24:37,244
Speaker SPEAKER_01: And my life's work has been to try and make artificial neural networks work really well.

347
00:24:38,326 --> 00:24:41,632
Speaker SPEAKER_01: And it's only very recently I started thinking about the risks.

348
00:24:41,652 --> 00:24:47,582
Speaker SPEAKER_01: There's other people who thought much longer and harder about how to mitigate the risks.

349
00:24:47,815 --> 00:24:49,518
Speaker SPEAKER_01: I wish it was like climate change.

350
00:24:49,739 --> 00:24:55,650
Speaker SPEAKER_01: With climate change, you can say, you know, convert to solar energy and stop burning carbon.

351
00:24:55,670 --> 00:24:58,935
Speaker SPEAKER_01: And if you stop burning carbon, there will still be a disaster.

352
00:24:58,955 --> 00:25:03,444
Speaker SPEAKER_01: But in about 100 years' time, things will settle down and we'll all be okay.

353
00:25:04,505 --> 00:25:06,248
Speaker SPEAKER_01: There isn't a simple recipe like that.

354
00:25:06,808 --> 00:25:10,797
Speaker SPEAKER_01: For some of the risks, I think it's not so bad.

355
00:25:10,817 --> 00:25:12,681
Speaker SPEAKER_01: So things like bias and discrimination.

356
00:25:13,501 --> 00:25:21,518
Speaker SPEAKER_01: It's terrible that systems that decide who gets parole show all the same biases as their training data.

357
00:25:22,627 --> 00:25:23,910
Speaker SPEAKER_01: And that's not good.

358
00:25:23,950 --> 00:25:34,885
Speaker SPEAKER_01: I mean, if you train a system to decide, for example, who should get a mortgage, and you train it on data of old white men deciding whether young black women should get mortgages, the system's going to show the same biases.

359
00:25:35,406 --> 00:25:42,737
Speaker SPEAKER_01: But at least you can see the biases better, because you can freeze the system and experimentally determine how biased it is.

360
00:25:43,638 --> 00:25:52,010
Speaker SPEAKER_01: So I think if your goal is not to eliminate bias, but just to make bias less bad than the current systems it's replacing, that's very achievable.

361
00:25:52,480 --> 00:26:03,976
Speaker SPEAKER_01: So, I'm not so worried about bias, which makes some people very cross, because I think we can mitigate it reasonably well, and I think we can at least produce systems that are less dangerous than the current systems.

362
00:26:04,296 --> 00:26:05,718
Speaker SPEAKER_01: Just like with autonomous vehicles.

363
00:26:06,239 --> 00:26:09,282
Speaker SPEAKER_01: They'll kill people, but they'll kill a lot less people.

364
00:26:09,963 --> 00:26:11,705
Speaker SPEAKER_01: You have to be willing to say they'll kill people.

365
00:26:12,086 --> 00:26:15,230
Speaker SPEAKER_01: If you say they'll never kill people, you're stuck.

366
00:26:16,425 --> 00:26:19,453
Speaker SPEAKER_01: With some of the other risks, it's much harder to see.

367
00:26:19,755 --> 00:26:26,773
Speaker SPEAKER_01: So with fake news, for example, it'd be very nice if you could label everything that's fake as fake, as AI generated.

368
00:26:27,817 --> 00:26:30,063
Speaker SPEAKER_01: And you can imagine governments legislating for that.

369
00:26:30,183 --> 00:26:32,088
Speaker SPEAKER_01: It's technically a very difficult problem.

370
00:26:32,643 --> 00:26:34,306
Speaker SPEAKER_01: to do it in a way that people can't get around.

371
00:26:34,747 --> 00:26:36,410
Speaker SPEAKER_01: But governments have done things like that.

372
00:26:36,490 --> 00:26:40,517
Speaker SPEAKER_01: For example, if you make fake money, governments get very cross.

373
00:26:40,877 --> 00:26:45,967
Speaker SPEAKER_01: They'll put you in jail for 10 years for making fake money because they really care about them being the only ones to print their money.

374
00:26:46,548 --> 00:26:49,133
Speaker SPEAKER_01: And if you get some fake money,

375
00:26:49,113 --> 00:26:53,481
Speaker SPEAKER_01: and you give it to somebody else, knowing that it's fake, you can go to jail for that too.

376
00:26:54,123 --> 00:26:59,775
Speaker SPEAKER_01: So governments are very serious about not allowing fake money, because it touches on something important to them.

377
00:27:01,198 --> 00:27:07,289
Speaker SPEAKER_01: I think they should be equally serious about not allowing fake news, but it's technically much more difficult.

378
00:27:07,490 --> 00:27:10,614
Speaker SPEAKER_01: One place you can get a sort of little edge.

379
00:27:11,213 --> 00:27:19,944
Speaker SPEAKER_01: I interacted with a legislator in Britain who was trying to deal with fake videos of child sex abuse.

380
00:27:21,446 --> 00:27:28,434
Speaker SPEAKER_01: And that's an area where it's hard to prosecute the people who make the fake videos because they claim no child was hurt in the making of the videos because it was all fake.

381
00:27:29,326 --> 00:27:34,536
Speaker SPEAKER_01: And it looks like there's a possibility there to say, those people have to mark those as fake.

382
00:27:34,957 --> 00:27:36,720
Speaker SPEAKER_01: It may be difficult, but they have to mark them as fake.

383
00:27:36,740 --> 00:27:40,227
Speaker SPEAKER_01: And if they don't mark them as fake, they can go to jail for not marking them as fake.

384
00:27:40,247 --> 00:27:41,990
Speaker SPEAKER_01: At least you can get them that way.

385
00:27:42,009 --> 00:27:45,837
Speaker SPEAKER_01: And I don't think you get a lot of sympathy for the people who make fake child abuse videos.

386
00:27:46,358 --> 00:27:49,765
Speaker SPEAKER_01: So that might be a place to start, the thinning of a wedge.

387
00:27:51,634 --> 00:27:55,861
Speaker SPEAKER_01: It's hard, but it's possibly doable to mark fake videos as fake.

388
00:27:55,881 --> 00:27:57,041
Speaker SPEAKER_01: So that's something we can do.

389
00:27:57,782 --> 00:28:15,346
Speaker SPEAKER_01: For the echo chambers, you're going to need governments or somebody, presumably governments, to interfere with the business model of the social media companies that says that do whatever you can to get people to click more.

390
00:28:15,799 --> 00:28:18,365
Speaker SPEAKER_01: there have to be things they shouldn't do to get people to click more.

391
00:28:19,586 --> 00:28:23,413
Speaker SPEAKER_01: You can imagine governments actually legislating to do that, although it's going to be tricky.

392
00:28:25,518 --> 00:28:29,586
Speaker SPEAKER_01: But like I say, basically I don't have solutions to these problems.

393
00:28:29,665 --> 00:28:34,615
Speaker SPEAKER_01: It's not like climate change where you can say, for God's sake, stop burning carbon and you'll be okay.

394
00:28:35,156 --> 00:28:35,917
Speaker SPEAKER_01: It's not like that.

395
00:28:36,369 --> 00:28:37,391
Speaker SPEAKER_02: All right, thank you.

396
00:28:37,451 --> 00:28:38,031
Speaker SPEAKER_02: Very helpful.

397
00:28:38,311 --> 00:28:39,573
Speaker SPEAKER_02: I'm coming back to you now, Bill.

398
00:28:39,813 --> 00:28:42,215
Speaker SPEAKER_02: My former CIO hat is on right now.

399
00:28:42,296 --> 00:28:45,900
Speaker SPEAKER_02: So when I hear this, I hear opportunities, risks, and my brain is spinning.

400
00:28:47,381 --> 00:28:47,942
Speaker SPEAKER_02: Not exactly.

401
00:28:47,961 --> 00:28:53,707
Speaker SPEAKER_02: I'm sure people are developing strategies around AI or early in that process.

402
00:28:54,208 --> 00:28:58,251
Speaker SPEAKER_02: What do you advise, especially when you speak to some of our clients, on how do you respond to this?

403
00:28:58,271 --> 00:29:03,356
Speaker SPEAKER_02: How do you prepare so you're not lost and marbles in your mouth when you're CEO?

404
00:29:03,336 --> 00:29:06,480
Speaker SPEAKER_02: Leader, the organization's asked you about this particular technology.

405
00:29:07,461 --> 00:29:13,826
Speaker SPEAKER_03: So a lot of folks, public agencies, commercial, we're focused on responsible AI.

406
00:29:14,248 --> 00:29:18,832
Speaker SPEAKER_03: And it's a great way to help us mitigate the risk that we can address.

407
00:29:18,932 --> 00:29:22,236
Speaker SPEAKER_03: It's certainly not the existential risk that we're talking about as well.

408
00:29:22,955 --> 00:29:26,400
Speaker SPEAKER_03: But what I say is this is a task that's so important.

409
00:29:27,101 --> 00:29:31,605
Speaker SPEAKER_03: You just don't give it to your data scientists and your IT people to do.

410
00:29:32,192 --> 00:29:33,154
Speaker SPEAKER_03: the business stakeholders.

411
00:29:33,174 --> 00:29:38,729
Speaker SPEAKER_03: So this is a team sport in AI to address these kind of risks.

412
00:29:38,749 --> 00:29:42,336
Speaker SPEAKER_03: And just one question that I still want to ask.

413
00:29:42,397 --> 00:29:42,999
Speaker SPEAKER_03: Please, go ahead.

414
00:29:44,221 --> 00:29:46,708
Speaker SPEAKER_03: Earlier, Jeffrey, you

415
00:29:48,258 --> 00:29:52,523
Speaker SPEAKER_03: asked a question that we posed to Ray Kurzweil about digital intelligence.

416
00:29:52,544 --> 00:29:55,709
Speaker SPEAKER_03: And he returned the favor, and he has a question for you.

417
00:29:55,828 --> 00:29:58,092
Speaker SPEAKER_03: And I'm going to paraphrase it, because it's quite funny.

418
00:29:58,112 --> 00:29:59,053
Speaker SPEAKER_01: Well, just one thing.

419
00:29:59,073 --> 00:30:00,255
Speaker SPEAKER_01: He didn't answer my question.

420
00:30:00,275 --> 00:30:01,016
Speaker SPEAKER_01: That's true.

421
00:30:01,336 --> 00:30:01,896
Speaker SPEAKER_03: I have to answer his.

422
00:30:01,957 --> 00:30:03,819
Speaker SPEAKER_03: That's true.

423
00:30:03,859 --> 00:30:04,981
Speaker SPEAKER_03: I did notice that.

424
00:30:05,021 --> 00:30:06,703
Speaker SPEAKER_03: He kind of skirted around it.

425
00:30:07,325 --> 00:30:08,105
Speaker SPEAKER_03: Yeah.

426
00:30:08,390 --> 00:30:14,943
Speaker SPEAKER_03: But if you could do the favor and set an example for him and address his question.

427
00:30:15,806 --> 00:30:20,756
Speaker SPEAKER_03: So he states that, biologically, humans have progressed.

428
00:30:20,836 --> 00:30:21,698
Speaker SPEAKER_03: We're living longer.

429
00:30:21,778 --> 00:30:24,423
Speaker SPEAKER_03: And with the advent of simulated biology,

430
00:30:24,775 --> 00:30:26,678
Speaker SPEAKER_03: we're going to make dramatic increases there.

431
00:30:27,278 --> 00:30:32,749
Speaker SPEAKER_03: He also talks about, in terms of how much income we make, that that's also increased over time.

432
00:30:33,589 --> 00:30:34,832
Speaker SPEAKER_03: He cites tenfold.

433
00:30:35,814 --> 00:30:41,063
Speaker SPEAKER_03: And he has examples in 50 different areas of these incredible

434
00:30:41,042 --> 00:30:44,468
Speaker SPEAKER_03: exponential improvements in humankind.

435
00:30:45,169 --> 00:30:52,140
Speaker SPEAKER_03: And he states that it's not us versus AI, it's the combination of human and AI methods that will bring about these changes.

436
00:30:52,760 --> 00:30:58,289
Speaker SPEAKER_03: His question to you is, how do we limit AI without losing these vital types of progress?

437
00:30:59,211 --> 00:31:04,619
Speaker SPEAKER_01: Okay, so I basically agree with him that AI can do incredible things for us.

438
00:31:05,038 --> 00:31:07,241
Speaker SPEAKER_01: In that sense, I completely agree with him.

439
00:31:07,461 --> 00:31:09,424
Speaker SPEAKER_01: And I think that's why we're not going to limit it.

440
00:31:09,845 --> 00:31:21,580
Speaker SPEAKER_01: I don't think it's feasible to get people to stop doing AI and to stop developing it further because of the incredible things it can do for us in areas like medicine, but also in areas like making sort of all tedious jobs more efficient.

441
00:31:22,181 --> 00:31:32,515
Speaker SPEAKER_01: Anybody who has to produce prose in a tedious way, they can make it much easier, like letters of recommendation, which professors know are tedious.

442
00:31:32,494 --> 00:31:36,281
Speaker SPEAKER_01: So, we're not going to stop the progress because of the wonderful things it'll do for us.

443
00:31:37,022 --> 00:31:46,200
Speaker SPEAKER_01: But I think one thing Ray's got slightly wrong is that I think the digital intelligence has a very different flavor from biological intelligence.

444
00:31:46,760 --> 00:31:48,364
Speaker SPEAKER_01: I've only begun to think this recently.

445
00:31:48,403 --> 00:31:52,832
Speaker SPEAKER_01: A few years ago, I thought you'd always make it better by making it more like the biological intelligence.

446
00:31:53,571 --> 00:32:03,483
Speaker SPEAKER_01: But the idea of a kind of harmonious collaboration of these two kinds of intelligence, I think, falls apart when they become very different kinds of intelligence, and when one's much smarter than the other.

447
00:32:04,265 --> 00:32:11,473
Speaker SPEAKER_01: And when digital intelligences can share what they know with other copies of the same model, and they can't share it with us in the same way.

448
00:32:11,493 --> 00:32:13,977
Speaker SPEAKER_01: It's just very slow and tedious to explain to us.

449
00:32:14,445 --> 00:32:18,150
Speaker SPEAKER_01: So I think he's sort of wrong about that.

450
00:32:18,931 --> 00:32:25,337
Speaker SPEAKER_01: I also think that it looks to me like he's a bit worried about dying and he just needs to get used to the idea.

451
00:32:25,999 --> 00:32:27,000
Speaker SPEAKER_00: Okay, okay.

452
00:32:33,467 --> 00:32:34,028
Speaker SPEAKER_03: That's it.

453
00:32:34,107 --> 00:32:36,931
Speaker SPEAKER_03: You have a follow-up on that?

454
00:32:37,490 --> 00:32:39,534
Speaker SPEAKER_03: No, I actually have one last question.

455
00:32:40,013 --> 00:32:42,596
Speaker SPEAKER_03: You touched upon it about reasoning.

456
00:32:42,931 --> 00:32:53,541
Speaker SPEAKER_03: There are many out there in the industry who would say that chat GPT, GPT-4 is merely a statistical model and just trying to figure out what's the next best word.

457
00:32:54,201 --> 00:32:55,463
Speaker SPEAKER_03: What's your view on this?

458
00:32:56,344 --> 00:32:57,645
Speaker SPEAKER_01: Okay, it's a very good question.

459
00:32:58,946 --> 00:33:05,574
Speaker SPEAKER_01: Particularly linguists of the Chomsky School, who think that language is innate, argue this stuff isn't really doing language.

460
00:33:05,614 --> 00:33:06,775
Speaker SPEAKER_01: It doesn't really understand.

461
00:33:07,395 --> 00:33:10,878
Speaker SPEAKER_01: It's just a bunch of statistical tricks for predicting the next word.

462
00:33:11,617 --> 00:33:17,980
Speaker SPEAKER_01: So the first thing to say is that if you ask, what does it take to be really good at predicting the next word?

463
00:33:18,682 --> 00:33:23,721
Speaker SPEAKER_01: For example, suppose it's the first word of the answer to a question.

464
00:33:23,853 --> 00:33:26,659
Speaker SPEAKER_01: You have to understand to predict the next word.

465
00:33:26,679 --> 00:33:33,030
Speaker SPEAKER_01: You can do quite well just on correlations between words, pairwise statistics or triplewise statistics.

466
00:33:33,392 --> 00:33:38,903
Speaker SPEAKER_01: You can do a reasonable job of doing a lot better than chance of predicting the next word, but you can't do a really good job.

467
00:33:38,923 --> 00:33:41,929
Speaker SPEAKER_01: To do a really good job, you have to understand what was said.

468
00:33:41,909 --> 00:33:57,631
Speaker SPEAKER_01: So, the amazing thing is that using the backpropagation algorithm, you train a big language model, and its only goal in the fine-tuning is to predict the next word, but that goal is sufficient to force it to understand what's being said, because that's the only way you can predict the next word.

469
00:33:59,313 --> 00:34:02,297
Speaker SPEAKER_01: And when they say it's just statistics,

470
00:34:03,239 --> 00:34:05,445
Speaker SPEAKER_01: Statistics means different things to different people.

471
00:34:06,470 --> 00:34:13,974
Speaker SPEAKER_01: So there's a specific sense where you're dealing with correlations and sort of low order statistics.

472
00:34:14,139 --> 00:34:16,382
Speaker SPEAKER_01: And that's what Chomsky means by statistics.

473
00:34:16,402 --> 00:34:18,425
Speaker SPEAKER_01: That's what he thinks statistics is.

474
00:34:18,445 --> 00:34:20,427
Speaker SPEAKER_01: And in that sense, it isn't just statistics.

475
00:34:20,789 --> 00:34:22,250
Speaker SPEAKER_01: Previous language models were.

476
00:34:22,451 --> 00:34:28,338
Speaker SPEAKER_01: Things like trigram models that Google used to use for translation and so on, they were just statistics in that sense.

477
00:34:28,358 --> 00:34:30,000
Speaker SPEAKER_01: But these models are quite different.

478
00:34:30,641 --> 00:34:39,793
Speaker SPEAKER_01: So they get these sentences, they convert each word into a big vector of feature activities, and they've invented the features.

479
00:34:39,773 --> 00:34:54,182
Speaker SPEAKER_01: and then they learn billions of interactions between features, and as a result of all these feature vectors interacting, they can predict the features of the next word, and that's very different from normal statistics.

480
00:34:54,563 --> 00:34:56,887
Speaker SPEAKER_01: It involves creating these features

481
00:34:57,121 --> 00:34:59,784
Speaker SPEAKER_01: they can capture very high order correlations.

482
00:35:00,485 --> 00:35:04,809
Speaker SPEAKER_01: And so in the sense in which it's doing statistics, everything's statistics.

483
00:35:05,730 --> 00:35:09,795
Speaker SPEAKER_01: So saying it's just statistics doesn't mean everything, because in that sense, everything's statistics.

484
00:35:12,137 --> 00:35:14,519
Speaker SPEAKER_01: So I think those people are just completely wrong.

485
00:35:17,222 --> 00:35:20,806
Speaker SPEAKER_02: Let's go further with our large language modules.

486
00:35:20,947 --> 00:35:24,951
Speaker SPEAKER_02: Are there specific examples you can share of what they cannot do?

487
00:35:25,454 --> 00:35:30,940
Speaker SPEAKER_01: Well, I already gave you one with the brothers and sisters.

488
00:35:31,300 --> 00:35:31,900
Speaker SPEAKER_02: Oh, that's true.

489
00:35:31,920 --> 00:35:32,440
Speaker SPEAKER_02: Yes, yes, yes.

490
00:35:32,460 --> 00:35:36,224
Speaker SPEAKER_02: But in terms of industry application.

491
00:35:36,465 --> 00:35:37,626
Speaker SPEAKER_01: Oh, industry applications.

492
00:35:38,166 --> 00:35:41,871
Speaker SPEAKER_01: I don't know enough about the various different industries and how you'd apply them.

493
00:35:41,891 --> 00:35:44,032
Speaker SPEAKER_01: I suspect, I suspect not.

494
00:35:44,414 --> 00:35:49,639
Speaker SPEAKER_01: I mean, the industry in which they're currently least useful is probably plumbing.

495
00:35:49,619 --> 00:35:53,385
Speaker SPEAKER_01: Because particularly plumbing in an old house.

496
00:35:54,045 --> 00:36:00,295
Speaker SPEAKER_01: If you do plumbing in an old house, you need to be very inventive and you need to be very agile and you need to be able to get your fingers into funny places.

497
00:36:01,076 --> 00:36:03,498
Speaker SPEAKER_01: And they're not good at that yet.

498
00:36:03,820 --> 00:36:05,061
Speaker SPEAKER_01: But they are getting a lot better.

499
00:36:05,081 --> 00:36:07,666
Speaker SPEAKER_01: And they're getting much more dexterous.

500
00:36:08,467 --> 00:36:10,750
Speaker SPEAKER_01: So even plumbing isn't safe.

501
00:36:11,202 --> 00:36:15,226
Speaker SPEAKER_01: I mean, being a professor was gone a long time ago, but even plumbing isn't safe now.

502
00:36:15,246 --> 00:36:15,606
Speaker SPEAKER_02: Wow.

503
00:36:15,907 --> 00:36:16,266
Speaker SPEAKER_02: OK.

504
00:36:16,748 --> 00:36:17,068
Speaker SPEAKER_02: OK.

505
00:36:17,628 --> 00:36:27,079
Speaker SPEAKER_02: So on that note, and I've heard you say this before in previous talks in terms of just opportunities, you want to elaborate a little bit further on where AI may not.

506
00:36:27,159 --> 00:36:34,047
Speaker SPEAKER_02: Is there any areas of industry it will not touch, or you see it touching every role, every industry?

507
00:36:34,347 --> 00:36:36,608
Speaker SPEAKER_01: I can't see anything that it won't touch.

508
00:36:36,628 --> 00:36:36,889
Speaker SPEAKER_02: Wow.

509
00:36:37,210 --> 00:36:39,032
Speaker SPEAKER_01: Anything that involves intelligence.

510
00:36:39,856 --> 00:36:46,985
Speaker SPEAKER_01: So anything that we used to think of as specifically human is going to touch all of those things.

511
00:36:47,005 --> 00:36:48,166
Speaker SPEAKER_02: Gotcha.

512
00:36:48,186 --> 00:36:55,414
Speaker SPEAKER_02: So again, in this era of exponential IT, the theme of our conference, as leaders, we've got to be preparing for this, right?

513
00:36:55,996 --> 00:37:02,724
Speaker SPEAKER_02: So is there anything you would add, Bill, as you speak to clients in terms of that preparation process on even where to start?

514
00:37:03,864 --> 00:37:07,989
Speaker SPEAKER_03: Same thing as we advise people pursuing

515
00:37:08,188 --> 00:37:11,574
Speaker SPEAKER_03: AI responsibly is this is a team sport.

516
00:37:12,295 --> 00:37:18,684
Speaker SPEAKER_03: You have to break out of your silos of IT, get involved with the business stakeholders, get them in the same boat.

517
00:37:19,585 --> 00:37:20,306
Speaker SPEAKER_02: Excellent.

518
00:37:20,327 --> 00:37:22,250
Speaker SPEAKER_02: So keep on learning and keep on learning together.

519
00:37:23,090 --> 00:37:25,293
Speaker SPEAKER_02: Jeffrey, we're running up to the close of our time here.

520
00:37:25,313 --> 00:37:32,425
Speaker SPEAKER_02: Are there any closing comments, parting thoughts, inspirational stories or ideas that you want to share with our audience?

521
00:37:32,842 --> 00:37:37,769
Speaker SPEAKER_01: So one thing I would emphasize is we're entering a time of huge uncertainty.

522
00:37:38,391 --> 00:37:41,335
Speaker SPEAKER_01: There's basically, we've never been here before.

523
00:37:41,376 --> 00:37:46,302
Speaker SPEAKER_01: We've never confronted the possibility of things much more intelligent than us being around.

524
00:37:47,063 --> 00:37:51,590
Speaker SPEAKER_01: Even though we created them and we maybe can still control them, we've never been there before.

525
00:37:51,871 --> 00:37:55,777
Speaker SPEAKER_01: And it's very hard for people to get their heads around the idea there might be things much smarter than them.

526
00:37:56,237 --> 00:38:00,824
Speaker SPEAKER_01: We're so used to us being in control, us being the smartest things around.

527
00:38:00,804 --> 00:38:03,088
Speaker SPEAKER_01: We really don't know what's going to happen.

528
00:38:03,869 --> 00:38:09,721
Speaker SPEAKER_01: So, although there are many very depressing dystopian possibilities, we don't actually know anything for sure.

529
00:38:10,621 --> 00:38:14,389
Speaker SPEAKER_01: And predicting anything more than five years ahead is fairly hopeless.

530
00:38:15,391 --> 00:38:16,994
Speaker SPEAKER_01: Because so many things will change.

531
00:38:17,454 --> 00:38:22,222
Speaker SPEAKER_01: Like five years ago, people wouldn't have predicted we would have large language models like we have now.

532
00:38:22,202 --> 00:38:32,856
Speaker SPEAKER_01: 10 years ago, people would have been very confident that in 10 years' time, you wouldn't have a large language model that could answer any question you give it better than the average person.

533
00:38:33,858 --> 00:38:37,402
Speaker SPEAKER_01: So we just don't know what's going to happen.

534
00:38:38,063 --> 00:38:41,929
Speaker SPEAKER_01: And I think that's the most important thing to be aware of, that it's all very uncertain.

535
00:38:42,429 --> 00:38:49,780
Speaker SPEAKER_01: And then the next most important thing is I think we need the brightest minds working on the issue of how we keep it under control.

536
00:38:50,130 --> 00:38:51,422
Speaker SPEAKER_02: Excellent.

537
00:38:52,148 --> 00:38:55,010
Speaker SPEAKER_02: Once again, let's put our hands together for Jeffrey Hinton.


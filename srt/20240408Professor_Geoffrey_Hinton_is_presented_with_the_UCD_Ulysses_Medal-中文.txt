1 00:00:03,068 --> 00:00:03,810 主持人 SPEAKER_05：大家好。
2 00:00:04,812 --> 00:00:07,176 主持人 SPEAKER_05：我是来自计算机科学学院的安德鲁·海因斯博士。
3 00:00:08,499 --> 00:00:22,849 主持人 SPEAKER_05：代表我们学院、都柏林大学科学学院以及整个大学的同事们，我非常荣幸地欢迎各位今晚参加都柏林大学最高荣誉——尤利西斯奖的颁奖仪式，授予杰弗里·辛顿教授。
4 00:00:24,246 --> 00:00:33,100 主持人 SPEAKER_05：几点注意事项，请确保在活动期间关闭您的手机，并在紧急情况下注意最近的消防出口。
5 00:00:35,223 --> 00:00:41,554 说话人 SPEAKER_05：今晚的正式会议将由 UCD 校长奥拉·菲利教授主持。
6 00:00:43,176 --> 00:00:50,287 说话人 SPEAKER_05：这对大学来说是一个非常特别的时刻，也是将教职员工、学生和校友聚集在一起的美好理由。
7 00:00:51,634 --> 00:00:55,002 说话人 SPEAKER_05：在我们开始之前，我将解释我们将遵循的程序。
8 00:00:56,164 --> 00:00:59,051 说话人 SPEAKER_05：首先，我将为辛顿教授朗读一段引言。
9 00:01:00,473 --> 00:01:06,126 说话人 SPEAKER_05：都柏林大学校长奥拉·菲利将在这里舞台上颁发尤利西斯奖牌。
10 00:01:07,248 --> 00:01:10,015 说话人 SPEAKER_05：我将邀请辛顿教授对提名进行回应。
11 00:01:10,888 --> 00:01:23,903 说话人 SPEAKER_05：现在我将结束正式程序，在凯特·罗布森-布朗教授，都柏林大学研究、影响和创新副校长上台与我一起之前，我将请费利教授向各位致辞。
12 00:01:24,843 --> 00:01:34,153 说话人 SPEAKER_05：我很高兴辛顿教授同意接受观众的提问。
13 00:01:34,174 --> 00:01:39,259 主持人：校长、注册官、尊敬的嘉宾、同事们，再次，
14 00:01:39,930 --> 00:02:04,064 主持人：代表校长尼尔·赫利、计算机科学学院院长以及在此的全体同事，我很高兴向大家介绍杰弗里·埃弗斯特·辛顿，今晚我们表彰他在计算机科学领域的贡献，包括计算和工程上的突破，使深度神经网络成为计算的关键组成部分。
15 00:02:05,328 --> 00:02:11,199 主持人：都柏林大学学院的尤利西斯奖是都柏林大学所能授予的最高荣誉。
16 00:02:11,240 --> 00:02:17,973 主持人：该奖项于2005年设立，并以都柏林大学学院校友詹姆斯·乔伊斯的名字命名。
17 00:02:19,055 --> 00:02:24,044 说话人 SPEAKER_05：该奖项授予那些为全球作出杰出贡献的个人。
18 00:02:27,198 --> 00:02:37,092 说话人 SPEAKER_05：正如所有学者可能都熟悉的那样，撰写简洁而全面的综述是一项挑战，而 Geoffrey Hinton 的职业生涯尤其难以浓缩。
19 00:02:38,835 --> 00:02:45,243 说话人 SPEAKER_05：他花费了几十年时间来理解人脑，从中汲取灵感，以创造更好的计算机学习模型。
20 00:02:46,344 --> 00:02:54,437 说话人 SPEAKER_05：现在，这些技术已经集成到我们每天使用的产品和服务的日常使用中，称其为“人工智能之父”并非夸张。
21 00:02:56,188 --> 00:03:02,338 说话人 SPEAKER_05：在过去的一年里，他一直在与比他繁忙职业生涯中任何时候都要广泛的公众分享他的知识。
22 00:03:03,800 --> 00:03:09,109 说话人 SPEAKER_05：他在提高人们对人类如何使用和发展人工智能的认识和关注。
23 00:03:10,711 --> 00:03:18,143 说话人 SPEAKER_05：至于这会不会损害他一生积累的声誉，他曾经说过：“我并不真的在乎我的遗产。”
24 00:03:18,663 --> 00:03:21,268 说话人 SPEAKER_05：最好的做法就是挥霍一个好的声誉。
25 00:03:22,193 --> 00:03:26,478 说话人 SPEAKER_05：从我的角度来看，这些是值得我们认真倾听的话。
26 00:03:26,497 --> 00:03:35,927 说话人 SPEAKER_05：他在 1970 年从剑桥大学获得实验心理学学士学位，1978 年从爱丁堡大学获得人工智能博士学位。
27 00:03:36,427 --> 00:03:51,424 说话人 SPEAKER_05：他是多伦多大学的荣誉退休教授，是英国皇家学会、加拿大皇家学会、人工智能推进协会以及 ACM 协会的会员。
28 00:03:51,691 --> 00:04:01,522 说话人 SPEAKER_05：他在 2018 年获得了图灵奖，这是计算机科学界的诺贝尔奖，与 Yann LeCun 和 Yoshua Benigo 共同获奖。
29 00:04:02,082 --> 00:04:15,960 讲者 SPEAKER_05：完成博士学位后，他在苏塞克斯大学、加州大学圣地亚哥分校和卡内基梅隆大学度过了一段时间，然后在 1987 年移居加拿大，加入了多伦多大学。
30 00:04:17,053 --> 00:04:27,228 讲者 SPEAKER_05：1988 年，他作为伦敦大学学院 Gatsby 计算神经科学单元的创始主任回到英格兰，之后又回到了多伦多。
31 00:04:28,891 --> 00:04:40,889 讲者 SPEAKER_05：他在谷歌兼职工作了 10 年，担任杰出科学家，然后在 2023 年退休，以便他可以更自由地谈论人工智能，而不会与谷歌股东的潜在利益冲突。
32 00:04:42,069 --> 00:04:45,254 讲者 SPEAKER_05：所以他可以，用他自己的话说，谈论
33 00:04:45,961 --> 00:04:54,377 说话人 SPEAKER_05：关于不考虑对谷歌影响的情况下的人工智能的危险，因为谷歌表现得非常负责任。
34 00:04:54,396 --> 00:05:09,824 说话人 SPEAKER_05：作为 30 多名博士生的导师，那些在他实验室作为研究生和研究人员工作过的人继续指导着未来几代研究人员，无论是在学术界还是在工业界，比如 Meta 的 Yann LeCun 和 OpenAI 的 Ilya Sutskevar。
35 00:05:11,442 --> 00:05:18,857 说话人 SPEAKER_05：Hinton 教授来自一个杰出的学术世家，有着一群致力于研究和教育的杰出学术传统。
36 00:05:19,639 --> 00:05:30,901 说话人 SPEAKER_05：他的曾曾祖母玛丽·埃文斯·巴勒，是一位自学成才的数学家和作家，致力于使数学对儿童更具吸引力，并撰写了一本名为《代数的哲学与乐趣》的书。
37 00:05:31,101 --> 00:05:41,752 讲者 SPEAKER_05：她嫁给了乔治·布尔，他是女王学院法院（现都柏林大学学院）的第一位数学教授，他发展了布尔代数，这是支撑现代计算机科学的逻辑。
38 00:05:44,028 --> 00:05:49,961 讲者 SPEAKER_05：他的早期工作探讨了人工神经网络如何具备学习复杂表示的能力。
39 00:05:50,742 --> 00:05:57,317 讲者 SPEAKER_05：1986 年，他与罗梅哈特和威廉姆斯一起发表了关于反向传播的概念，以帮助网络学习。
40 00:05:57,877 --> 00:06:03,370 讲者 SPEAKER_05：他开发了称为玻尔兹曼机的架构，这成为了深度学习架构的重要组件。
41 00:06:04,312 --> 00:06:06,740 说话人 SPEAKER_05：在 21 世纪初，许多事情汇聚在一起。
42 00:06:07,283 --> 00:06:16,295 说话人 SPEAKER_05：计算能力增强，尤其是 GPU，还有大量数据集用于训练模型，以完成语音识别和图像分类等任务。
43 00:06:17,524 --> 00:06:21,209 说话人 SPEAKER_05：我记得我曾在 2013 年参加了在温哥华举行的一个特别会议。
44 00:06:22,550 --> 00:06:26,074 说话人 SPEAKER_05：主要问题不是关于理论，而是关于实现。
45 00:06:26,134 --> 00:06:27,755 说话人 SPEAKER_05：我们怎么也能做到这一点呢？
46 00:06:27,975 --> 00:06:31,060 说话人 SPEAKER_05：我们怎么能让代码在这些 GPU 上运行？
47 00:06:31,680 --> 00:06:35,283 说话人 SPEAKER_05：工程贡献的重要性几乎与科学相当。
48 00:06:36,084 --> 00:06:45,995 说话人 SPEAKER_05：在他的 2012 年论文《基于深度卷积神经网络的 ImageNet 分类》中，根据谷歌学术搜索，已有超过 12 万次引用。
49 00:06:47,122 --> 00:06:49,706 讲者 SPEAKER_05: 展示了卷积神经网络在图像识别方面的强大功能。
50 00:06:50,487 --> 00:06:58,276 讲者 SPEAKER_05: 从那时起，他继续在胶囊网络、对比学习和人工智能的新硬件解决方案等方面做出贡献。
51 00:06:59,619 --> 00:07:02,723 讲者 SPEAKER_05: Geoffrey Hinton 对人工智能领域的影响是无可否认的。
52 00:07:03,663 --> 00:07:08,709 讲者 SPEAKER_05: 深度学习在计算机视觉、自然语言处理等领域彻底改变了人工智能。
53 00:07:09,262 --> 00:07:13,149 说话人 SPEAKER_05：语音识别、金融、医学等多个领域。
54 00:07:14,029 --> 00:07:18,036 说话人 SPEAKER_05：他是 UCD 最高荣誉的非常值得的获得者。
55 00:07:18,617 --> 00:07:25,348 说话人 SPEAKER_05：现在，我邀请校长奥拉·菲利（Orla Feely）向哈 inton 教授颁发尤利西斯奖牌。
56 00:07:58,100 --> 00:08:00,184 说话人 SPEAKER_03：非常感谢您给予的非常慷慨的赞誉。
57 00:08:01,105 --> 00:08:11,043 讲者 SPEAKER_03：我想就其中一点特别指出，那就是我的成功在很大程度上得益于能够招募到真正优秀的毕业生。
58 00:08:11,865 --> 00:08:18,538 讲者 SPEAKER_03：所以，在这个领域，永远不要忘记，是研究生在做这项工作。
59 00:08:19,817 --> 00:08:27,127 讲者 SPEAKER_03：我无法抗拒向人们讲解事物的机会，而且有很多人实际上并不了解人工智能是如何工作的。
60 00:08:27,869 --> 00:08:32,375 讲者 SPEAKER_03：所以，我为计算机科学专业的学生和已经了解人工智能工作原理的人表示歉意。
61 00:08:33,076 --> 00:08:49,317 说话人 SPEAKER_03：我将进行一场大约 20 分钟的非常基础的讲座，关于人工智能实际上是如何工作的，面向那些不喜欢方程式且不知道这些大块头究竟在做什么或如何做到这一点的人。
62 00:08:49,653 --> 00:08:54,398 说话人 SPEAKER_03：自上个世纪中叶以来，存在两种智能范式。
63 00:08:54,999 --> 00:08:59,804 说话人 SPEAKER_03：有一种是受逻辑启发的途径，其理念是智能的本质是推理。
64 00:08:59,884 --> 00:09:00,926 说话人 SPEAKER_03：这就是我们与众不同的地方。
65 00:09:01,746 --> 00:09:05,350 说话人 SPEAKER_03：这是通过使用符号规则来操作符号表达式来完成的。
66 00:09:06,552 --> 00:09:08,274 说话人 SPEAKER_03：像学习这样的东西可以稍后再说。
67 00:09:08,914 --> 00:09:15,001 说话人 SPEAKER_03：我们真正需要做的是理解知识是如何表示的，在哪种逻辑类型语言中表示。
68 00:09:15,504 --> 00:09:18,589 说话人 SPEAKER_03：然后还有一种受生物学启发的完全不同的方法。
69 00:09:19,149 --> 00:09:21,212 说话者 SPEAKER_03：它说智能的本质是学习。
70 00:09:22,014 --> 00:09:23,336 说话者 SPEAKER_03：逻辑和这些东西要晚些时候再说。
71 00:09:23,375 --> 00:09:27,481 说话者 SPEAKER_03：你会在神经网络中学习连接的强度。
72 00:09:28,383 --> 00:09:30,046 说话者 SPEAKER_03：至于推理这类问题我们稍后再讨论。
73 00:09:30,145 --> 00:09:35,393 说话人 SPEAKER_03：首先，我们需要了解你是如何学习控制身体或识别物体这类事情的。
74 00:09:38,717 --> 00:09:40,821 说话人 SPEAKER_03：那么，什么是神经网络呢？
75 00:09:41,323 --> 00:09:43,989 说话人 SPEAKER_03：我将给你一个神经网络的漫画化描述。
76 00:09:44,009 --> 00:09:48,135 说话人 SPEAKER_03：你有一些输入神经元，这些可能是图像像素的强度。
77 00:09:48,456 --> 00:09:49,197 说话人 SPEAKER_03：这就是数据。
78 00:09:49,879 --> 00:09:55,349 说话人 SPEAKER_03：然后你有中间层神经元，它们将学会从图像中提取特征。
79 00:09:56,049 --> 00:10:00,418 说话人 SPEAKER_03：然后你有输出神经元，可能会说出图像是什么类型的。
80 00:10:00,778 --> 00:10:03,163 说话人 SPEAKER_03：比如，可能有一个是猫，一个是狗。
81 00:10:03,903 --> 00:10:08,389 说话人 SPEAKER_03：所以您要做的就是输入一张猫的图片，然后猫的神经元在输出端被激活。
82 00:10:09,009 --> 00:10:12,354 说话人 SPEAKER_03：那些连接上的彩色小点表示连接强度。
83 00:10:12,833 --> 00:10:16,238 说话人 SPEAKER_03：您要做的是学习这些连接强度，以便发生正确的事情。
84 00:10:18,681 --> 00:10:21,845 说话人 SPEAKER_03：有一种简单的方法来做这种学习，大家都容易理解。
85 00:10:22,466 --> 00:10:27,091 说话人 SPEAKER_03：从一些随机的连接强度开始，然后选择其中一个连接，
86 00:10:27,881 --> 00:10:34,803 说话人 SPEAKER_03：看看当你稍微改变这个连接时，比如说增加一点，网络是否会变得更好。
87 00:10:35,240 --> 00:10:40,866 说话人 SPEAKER_03：当然，判断它是否变得更好，你需要运行很多示例并查看它是否产生更好的答案。
88 00:10:41,488 --> 00:10:45,994 说话人 SPEAKER_03：所以最初，它可能说，这张猫图片是猫的概率是 50%。
89 00:10:46,073 --> 00:10:49,318 说话人 SPEAKER_03：当你改变这个重量时，它可能会显示是 51%。
90 00:10:49,958 --> 00:10:50,799 说话人 SPEAKER_03：好的，那还不错。
91 00:10:50,841 --> 00:10:51,902 说话人 SPEAKER_03：所以你这样改变重量。
92 00:10:51,922 --> 00:10:53,244 说话人 SPEAKER_03：然后你取另一个重量，试试看。
93 00:10:53,823 --> 00:10:54,806 说话人 SPEAKER_03：你继续这样做。
94 00:10:55,066 --> 00:10:59,711 说话人 SPEAKER_03：如果你这样做足够长时间，你将得到一个网络，当你展示一只猫时，它会说它很可能是一只猫。
95 00:10:59,731 --> 00:11:01,855 说话人 SPEAKER_03：当你展示一只狗时，它会说它是一只狗。
96 00:11:01,835 --> 00:11:05,759 说话人 SPEAKER_03：但它非常慢，因为对于每个连接，你都必须尝试许多示例。
97 00:11:06,341 --> 00:11:08,203 说话人 SPEAKER_03：你必须多次更新每个连接。
98 00:11:09,645 --> 00:11:22,123 说话人 SPEAKER_03：现在，实际上你可以通过使用一种叫做反向传播的算法以更高效的方式实现相同的功能，其中你取一个东西的图像，将其通过网络前向传播以获得响应。
99 00:11:23,144 --> 00:11:26,187 说话人 SPEAKER_03：假设它说有 50%的可能性是猫。
100 00:11:26,658 --> 00:11:29,381 说话人 SPEAKER_03：所以你有一个错误，因为你希望它说 100%是猫。
101 00:11:30,143 --> 00:11:35,789 说话人 SPEAKER_03：您通过网络发送一个信号，通过相同的连接，这些连接正在传递错误。
102 00:11:35,889 --> 00:11:42,918 说话人 SPEAKER_03：基本上，您可以计算，而不是测量，改变每个权重会如何改善网络。
103 00:11:43,100 --> 00:11:48,326 说话人 SPEAKER_03：因此，对于这些权重中的每一个，您可以弄清楚，如果我稍微提高这个权重，是否会变得更好？
104 00:11:48,486 --> 00:11:50,408 说话人 SPEAKER_03：或者如果我稍微降低这个权重，是否会变得更好？
105 00:11:50,889 --> 00:11:53,273 说话人 SPEAKER_03：但是你可以并行地对所有权重进行操作。
106 00:11:53,253 --> 00:11:56,720 说话人 SPEAKER_03：所以如果你有一亿个权重，这会快一亿倍。
107 00:11:58,202 --> 00:11:59,885 说话人 SPEAKER_03：这就是神经网络的工作原理。
108 00:12:00,687 --> 00:12:08,744 说话人 SPEAKER_03：通过反向传播误差来计算如何改变每个权重的数学方法只是相对简单的微积分。
109 00:12:09,725 --> 00:12:11,229 说话人 SPEAKER_03：关键是它真的管用。
110 00:12:11,548 --> 00:12:12,932 说话人 SPEAKER_03：你可以那样学习东西。
111 00:12:13,721 --> 00:12:24,278 所以多年来，计算机科学家、计算机视觉专家们一直希望像那样处理图像，输出一个描述图像内容的字幕。
112 00:12:24,359 --> 00:12:25,100 说话人 SPEAKER_03：但他们做不到。
113 00:12:25,120 --> 00:12:26,121 说话人 SPEAKER_03：他们甚至无法接近。
114 00:12:26,581 --> 00:12:27,945 说话人 SPEAKER_03：现在神经网络可以做到这一点。
115 00:12:28,465 --> 00:12:30,688 说话人 SPEAKER_03：它们使用这种反向传播算法进行训练。
116 00:12:30,708 --> 00:12:36,337 说话人 SPEAKER_03：反向传播学习从图像中提取特征层次结构，这使您能够看到图像中的内容。
117 00:12:38,596 --> 00:12:49,328 说话人 SPEAKER_03：在 2012 年，亚历克斯·赫鲁什切夫斯基和伊利亚·舒茨科娃，在我的一点点帮助下，开发了一个比现有计算机视觉系统好得多的网络。
118 00:12:49,970 --> 00:13:00,302 说话人 SPEAKER_03：在科学界非常罕见的事情发生了，那就是一直说神经网络永远无法做到这一点的顶尖计算机视觉专家们，说，哇，这真的有效。
119 00:13:00,763 --> 00:13:03,206 说话人 SPEAKER_03：他们改变了他们的做法，开始使用神经网络。
120 00:13:03,225 --> 00:13:04,908 说话人 SPEAKER_03：这不是科学家通常的行为方式。
121 00:13:05,128 --> 00:13:08,011 说话人 SPEAKER_03：这当然不是他们在语言学中的行为方式。
122 00:13:09,798 --> 00:13:11,039 说话人 SPEAKER_03：这就带我们来到了语言。
123 00:13:12,041 --> 00:13:20,995 说话人 SPEAKER_03：符号人工智能社区中许多人说，你们永远无法通过使用层次特征检测器来处理语言。
124 00:13:21,034 --> 00:13:22,197 说话人 SPEAKER_03：这根本行不通。
125 00:13:22,216 --> 00:13:24,500 说话人 SPEAKER_03：你可以在我的网页上看到一个引用，上面说得正是这样。
126 00:13:25,341 --> 00:13:31,169 说话人 SPEAKER_03：我忍不住了，所以让 GPT-4 详细解释了这句话哪里错了。
127 00:13:31,811 --> 00:13:39,062 说话人 SPEAKER_03：现在 GPT-4 正在向语言学家解释，他们关于神经网络能做什么和不能做什么的说法哪里错了。
128 00:13:39,667 --> 00:13:48,532 说话人 SPEAKER_03：语言学家被名叫乔姆斯基的人误导了几代人，而这个人实际上也获得了这个著名的奖项。
129 00:13:52,042 --> 00:13:53,304 说话人 SPEAKER_03：它不会持续。
130 00:13:54,854 --> 00:13:58,118 说话人 SPEAKER_03：他有一个疯狂的理论，认为语言不是学来的。
131 00:13:58,818 --> 00:14:00,500 说话人 SPEAKER_03：并且他设法说服了很多人。
132 00:14:01,001 --> 00:14:02,484 说话人 SPEAKER_03：表面上看起来，这简直是疯狂。
133 00:14:02,524 --> 00:14:03,504 说话人 SPEAKER_03：语言显然是学来的。
134 00:14:04,166 --> 00:14:07,870 说话人 SPEAKER_03：现在这些大型神经网络学习语言，它们不需要任何先天的结构。
135 00:14:07,890 --> 00:14:09,812 说话人 SPEAKER_03：它们只是从随机权重和大量数据开始。
136 00:14:10,614 --> 00:14:13,057 说话人 SPEAKER_03：而乔姆斯基仍在说，但这并不是真正的语言。
137 00:14:13,077 --> 00:14:13,717 说话人 SPEAKER_03：这不算数。
138 00:14:13,738 --> 00:14:14,379 说话人 SPEAKER_03：这不正确。
139 00:14:15,039 --> 00:14:22,009 说话人 SPEAKER_03：而且很多统计学家和认知科学家也说，在这种大网中你永远学不会语言。
140 00:14:24,384 --> 00:14:28,455 说话人 SPEAKER_03：所以乔姆斯基从未真正有过意义理论，一切都是关于句法的。
141 00:14:28,773 --> 00:14:33,558 说话人 SPEAKER_03：如果你思考意义，有两种非常不同的意义理论。
142 00:14:34,259 --> 00:14:42,971 说话人 SPEAKER_03：有一种结构主义理论，阿拉伯人相信，大多数语言学家也相信，即一个词的意义来源于它与其它词的关系。
143 00:14:43,792 --> 00:14:52,923 说话人 SPEAKER_03：所以如果你想捕捉一个词的意义，你需要制作一个与其他词有链接的关系图，也许在链接上标注它如何与那些词相关。
144 00:14:53,504 --> 00:14:54,645 说话人 SPEAKER_03：那将是一个语义网。
145 00:14:55,386 --> 00:14:57,750 说话人 SPEAKER_03：这就是你需要捕捉的意义。
146 00:14:57,730 --> 00:15:04,100 说话人 SPEAKER_03：然后有一个非常不同的理论，它来自 20 世纪 30 年代的心理学，即一个词的意义是一大组特征。
147 00:15:05,162 --> 00:15:07,767 说话人 SPEAKER_03：具有相似意义的词具有相似的特征集。
148 00:15:08,607 --> 00:15:10,471 说话人 SPEAKER_03：这两个理论看起来完全不同。
149 00:15:11,572 --> 00:15:14,437 说话人 SPEAKER_03：但实际上，你可以统一这两个理论。
150 00:15:15,318 --> 00:15:20,768 说话人 SPEAKER_03：我认为第一个做到这一点的是我在 1985 年制作的模型。
151 00:15:20,865 --> 00:15:27,054 说话人 SPEAKER_03：这是一个小型语言模型，它与现在的巨型语言模型有很多共同之处。
152 00:15:27,916 --> 00:15:37,168 说话人 SPEAKER_03：它通过尝试预测下一个单词来学习，并为每个单词学习特征，以及这些特征之间的相互作用，以便预测下一个单词的特征。
153 00:15:38,470 --> 00:15:47,042 说话人 SPEAKER_03：重要的是，所有的知识都集中在如何将特征分配给一个词，以及这些不同词的特征应该如何相互作用。
154 00:15:47,544 --> 00:15:51,710 说话人 SPEAKER_03：它没有存储任何句子，但可以重建句子。
155 00:15:51,730 --> 00:15:54,633 说话人 SPEAKER_03：它可以通过反复预测下一个词来生成句子。
156 00:15:55,234 --> 00:15:56,816 说话人 SPEAKER_03：这就是这些大型语言模型的工作方式。
157 00:15:57,035 --> 00:15:58,697 说话人 SPEAKER_03：他们实际上并不存储任何文本。
158 00:15:59,479 --> 00:16:12,253 说话人 SPEAKER_03：它们学会从文本特征中提取，并将其分配给单词以及这些特征之间的交互，以便预测下一个单词的特征。
159 00:16:12,537 --> 00:16:17,607 说话人 SPEAKER_03：这个微小的语言模型并非设计用来帮助工程。
160 00:16:17,707 --> 00:16:22,879 说话人 SPEAKER_03：这个微小的语言模型是为了解释人们如何从语言中获取意义而设计的。
161 00:16:23,360 --> 00:16:24,722 说话人 SPEAKER_03：实际上这是一个关于人的模型。
162 00:16:25,565 --> 00:16:32,519 说话人 SPEAKER_03：如果你看到有人告诉你，这些模型和我们不一样，它们的工作方式完全不同，就问他们，我们是怎样工作的？
163 00:16:32,652 --> 00:16:37,702 说话人 SPEAKER_03：如果他们是语言学家，他们会告诉你我们拥有符号规则和操纵符号表达式的规则。
164 00:16:38,264 --> 00:16:43,897 说话人 SPEAKER_03：但实际上，那些说他们和我们不一样的人并没有我们工作方式的模型。
165 00:16:43,917 --> 00:16:45,480 说话人 SPEAKER_03：我不知道他们怎么知道它们是不同的。
166 00:16:46,041 --> 00:16:51,352 说话人 SPEAKER_03：而其他神经网络研究者确实有一个关于我们工作方式的理解模型，它就像这样。
167 00:16:51,669 --> 00:16:52,610 说话人 SPEAKER_03：但并不完全像这样。
168 00:16:52,991 --> 00:17:01,743 说话人 SPEAKER_03：所以我将详细讲解一个小模型，因为我一直认为理解一个具体的小事物比模糊的抽象要好得多。
169 00:17:02,504 --> 00:17:06,828 说话人 SPEAKER_03：Waffly 抽象很好，但要真正理解事物，你需要一个具体的例子。
170 00:17:07,690 --> 00:17:09,932 说话人 SPEAKER_03：所以这里有两组家谱。
171 00:17:10,673 --> 00:17:13,257 说话人 SPEAKER_03：这里有英国人和意大利人。
172 00:17:13,877 --> 00:17:15,299 说话人 SPEAKER_03：这是 1950 年代。
173 00:17:15,460 --> 00:17:18,242 说话人 SPEAKER_03：非常、非常简单，家庭。
174 00:17:18,284 --> 00:17:22,269 说话人 SPEAKER_03：没有离婚，没有收养，没有同性婚姻。
175 00:17:23,230 --> 00:17:25,613 说话人 SPEAKER_03：整个过程非常、非常传统。
176 00:17:26,854 --> 00:17:30,218 说话人 SPEAKER_03：你可能注意到这些树彼此之间有某种相似性。
177 00:17:30,238 --> 00:17:31,180 说话人 SPEAKER_03：它们具有相同的结构。
178 00:17:32,240 --> 00:17:37,067 说话人 SPEAKER_03：我们将把这些关系树转换成一系列三元组。
179 00:17:38,228 --> 00:17:47,558 说话人 SPEAKER_03：所以从这些关系树中，我们可以通过一些关系术语，如儿子、女儿、侄子、侄女、母亲等等来记录信息。
180 00:17:47,578 --> 00:17:56,929 说话人 SPEAKER_03：然后我们可以构造三元组，比如 Colin 的父亲是 James，或者 Colin 的母亲是 Victoria，或者 James 的妻子是 Victoria。
181 00:17:58,250 --> 00:18:01,974 说话人 SPEAKER_03：现在，从 Colin 有母亲 Victoria 和
182 00:18:03,118 --> 00:18:08,468 说话人 SPEAKER_03：看看，从 Colin 有父亲 James 和 Colin 有母亲 Victoria，我们可以推断 James 有妻子 Victoria。
183 00:18:09,088 --> 00:18:18,222 说话人 SPEAKER_03：因此，符号人认为你在脑海中有了那些符号字符串的表示，你脑海中有一条规则，允许你从旧字符串推导出新的符号字符串。
184 00:18:19,164 --> 00:18:20,727 说话人 SPEAKER_03：这个规则可能看起来像这样。
185 00:18:20,747 --> 00:18:26,876 说话人 SPEAKER_03：如果 X 有母亲 Y，Y 有丈夫 Z，那么 X 就有父亲 Z。这就是他们认为这一切是如何运作的。
186 00:18:28,359 --> 00:18:30,442 说话人 SPEAKER_03：而我所做的是创建了一个神经网络
187 00:18:30,997 --> 00:18:38,219 说话人 SPEAKER_03：通过调整网络的权重，你可以学习这种知识，但网络内部没有存储任何字符串。
188 00:18:38,279 --> 00:18:40,145 说话人 SPEAKER_03：网络中没有存储任何符号表达式。
189 00:18:40,465 --> 00:18:42,633 说话人 SPEAKER_03：里面是所有功能和功能交互。
190 00:18:44,157 --> 00:18:44,819 说话人 SPEAKER_03：并且
191 00:18:45,458 --> 00:18:50,583 说话人 SPEAKER_03：关于在神经网络中做这件事，对于离散规则，你可能能够进行符号化处理。
192 00:18:50,923 --> 00:18:53,846 说话人 SPEAKER_03：但我们知道的大部分并不完全正确。
193 00:18:54,146 --> 00:18:56,869 说话人 SPEAKER_03：你必须允许有很多例外。
194 00:18:57,289 --> 00:19:02,536 说话人 SPEAKER_03：一旦你有了规则的例外，你最好通过使用大型神经网络来找到这些规则。
195 00:19:04,457 --> 00:19:06,118 说话人 SPEAKER_03：所以我使用的神经网络看起来是这样的。
196 00:19:07,299 --> 00:19:09,863 说话人 SPEAKER_03：你有两个输入。
197 00:19:10,163 --> 00:19:14,688 说话人 SPEAKER_03：一个是一个符号，仅仅是一个代表一个人名的离散符号。
198 00:19:15,308 --> 00:19:18,652 说话人 SPEAKER_03：另一个是一个离散符号，代表一种关系的名字。
199 00:19:20,074 --> 00:19:24,357 说话人 SPEAKER_03：我们想要的输出是那个相关的人的名字。
200 00:19:25,539 --> 00:19:27,442 说话人 SPEAKER_03：所以训练数据就是这样。
201 00:19:29,104 --> 00:19:37,673 说话人 SPEAKER_03：神经网络学习做的事情是将一个人的名字转换成一系列捕捉这个人本质的特征。
202 00:19:38,547 --> 00:19:41,333 就这些家谱而言，本质是指的。
203 00:19:42,635 --> 00:19:52,076 一旦对人和关系完成这个过程，它就会让这些特征在中部相互作用，从而预测输出人的特征。
204 00:19:52,395 --> 00:19:55,542 然后根据输出人的特征，预测输出人是谁。
205 00:19:56,704 --> 00:19:58,747 说话人 SPEAKER_03：这做得相当不错。
206 00:19:58,767 --> 00:20:11,481 说话人 SPEAKER_03：它可以学习，并且不仅能复现它所给出的信息，也就是说，学习到的权重中的信息足够，如果你给它一个训练示例，它就能得到正确答案，而且它还能进行泛化。
207 00:20:11,842 --> 00:20:20,511 说话人 SPEAKER_03：你可以给它以前从未见过的示例，以前见过的人，以前见过的关系，但以前没见过的组合，它也能得到正确答案。
208 00:20:21,112 --> 00:20:22,413 说话人 SPEAKER_03：那么问题是，它是怎么做到这一点的？
209 00:20:23,194 --> 00:20:26,137 说话人 SPEAKER_03：嗯，它学会了我们认为的自然特征。
210 00:20:26,876 --> 00:20:31,305 说话人 SPEAKER_03：所以，对于一个人来说，它学会了哪些特征，比如他们的国籍是什么？
211 00:20:31,826 --> 00:20:35,573 说话人 SPEAKER_03：因为如果你知道输入者的英语，你就知道答案的英语。
212 00:20:36,233 --> 00:20:38,778 说话人 SPEAKER_03：如果输入者是意大利人，答案就是意大利语。
213 00:20:38,798 --> 00:20:40,382 说话人 SPEAKER_03：就像我说的，他们是非常简单的家庭。
214 00:20:43,027 --> 00:20:45,932 说话人 SPEAKER_03：这是一个很小的网络，瓶颈处只有六个神经元。
215 00:20:46,958 --> 00:20:49,362 说话人 SPEAKER_03：一个将代表国籍。
216 00:20:49,862 --> 00:20:52,226 说话人 SPEAKER_03：另一个将代表这个人属于哪一代。
217 00:20:52,906 --> 00:20:55,530 说话人 SPEAKER_03：另一个代表他们在家谱中的哪个分支。
218 00:20:56,392 --> 00:21:00,377 说话人 SPEAKER_03：而世代特征会有三个值，因为有三代人。
219 00:21:00,397 --> 00:21:02,641 说话人 SPEAKER_03：它可能是最低一代、中间一代或最高一代。
220 00:21:04,083 --> 00:21:06,807 说话人 SPEAKER_03：而这个世代特征仅是有用的
221 00:21:07,157 --> 00:21:15,228 说话者 SPEAKER_03：如果对于关系，你学会了这样的特征，即这种关系要求输出人物比输入人物高一代。
222 00:21:15,728 --> 00:21:17,651 说话者 SPEAKER_03：所以像叔叔这样的关系就是这样的。
223 00:21:19,413 --> 00:21:29,384 说话者 SPEAKER_03：然后如果它知道输入人物的代数，并且知道关系高一代，它就可以预测输出人物的代数，这将有助于它找到正确的人物。
224 00:21:29,634 --> 00:21:34,503 说话者 SPEAKER_03：这就是它的工作原理，它实际上发现了符号人物所信仰的符号规则。
225 00:21:34,924 --> 00:21:38,431 说话人 SPEAKER_03：所以没有任何象征性人物抱怨这实际上并不是在学习。
226 00:21:38,892 --> 00:21:40,915 说话人 SPEAKER_03：他们说，好吧，它确实在学习，但学习的方式很愚蠢。
227 00:21:42,858 --> 00:21:44,843 说话人 SPEAKER_03：如果你现在看看大型语言模型，
228 00:21:46,207 --> 00:21:50,636 说话人 SPEAKER_03：它们可以被视为，嗯，我当然认为它们是这个小型模型的后代。
229 00:21:51,357 --> 00:21:55,867 说话人 SPEAKER_03：它们只是这个小小的模型被大规模扩展并变得极其复杂。
230 00:21:56,509 --> 00:21:57,731 说话人 SPEAKER_03：所以它们有更多的词汇。
231 00:21:58,192 --> 00:22:01,380 说话人 SPEAKER_03：它们适用于自然语言，而不仅仅是这些玩具示例。
232 00:22:01,359 --> 00:22:06,965 说话人 SPEAKER_03：它们使用更多的神经元层，因为不能仅仅从符号直接到意义。
233 00:22:07,507 --> 00:22:12,392 说话人 SPEAKER_03：你可能有一个像梅这样的符号，梅可能代表一个月，也可能代表一个情态动词，或者可能是女性的名字。
234 00:22:13,012 --> 00:22:14,974 说话人 SPEAKER_03：所以你必须使用联系来消除歧义。
235 00:22:14,994 --> 00:22:16,997 说话人 SPEAKER_03：所以你通过逐层消除歧义。
236 00:22:17,718 --> 00:22:18,679 说话人 SPEAKER_03：所以他们有更多的层次。
237 00:22:19,118 --> 00:22:22,923 说话人 SPEAKER_03：不同词语特征的交互要复杂得多。
238 00:22:23,584 --> 00:22:25,506 说话人 SPEAKER_03：但本质上属于同一类模型。
239 00:22:26,647 --> 00:22:28,869 说话人 SPEAKER_03：当它们学习时，
240 00:22:30,250 --> 00:22:36,038 说话人 SPEAKER_03：它们将所有信息存储在特征交互的权重中。
241 00:22:37,859 --> 00:22:51,178 说话人 SPEAKER_03：现在，语言学家说他们只是被美化的自动补全，他们只是使用统计技巧，他们只是拼凑文本，但记住，他们不存储任何文本。
242 00:22:51,817 --> 00:22:57,845 说话人 SPEAKER_03：自动补全的反对意见是荒谬的，因为他们所依赖的是老式的自动补全概念。
243 00:22:58,546 --> 00:23:04,032 说话人 SPEAKER_03：老式的自动补全会存储一串单词，比如存储鱼和薯条，这是一个常见的字符串。
244 00:23:04,453 --> 00:23:09,039 说话人 SPEAKER_03：所以如果你存储鱼和，然后你说下一个词很可能是薯条，因为这是一个非常常见的字符串。
245 00:23:10,301 --> 00:23:12,744 说话人 SPEAKER_03：这些系统中的自动补全根本不是那样的。
246 00:23:13,125 --> 00:23:18,853 说话人 SPEAKER_03：它将单词转换为特征，并使用特征之间的交互来做出预测。
247 00:23:19,963 --> 00:23:36,465 说话人 SPEAKER_03：所以，LLMs的工作方式和我们的工作方式一样，我们看到很多文本，或者听到很多单词串，我们学习单词的特征，学习这些特征之间的交互，这就是理解。
248 00:23:37,507 --> 00:23:40,671 说话人 SPEAKER_03：这些模型的理解方式和我们完全一样。
249 00:23:44,155 --> 00:23:48,942 说话人 SPEAKER_03：然后，语言学家还会给你另一个论点，这些事物会产生幻觉，所以它们实际上并不理解自己在说什么。
250 00:23:49,765 --> 00:23:55,941 说话人 SPEAKER_03：实际上，对于语言模型来说，这不应该被称为幻觉，而应该被称为虚构。
251 00:23:56,281 --> 00:24:00,372 说话人 SPEAKER_03：这一点自 20 世纪 30 年代以来在心理学领域就已经被研究，人们一直在做这件事。
252 00:24:01,054 --> 00:24:04,382 说话人 SPEAKER_03：所以它们虚构的事实实际上使它们更像我们。
253 00:24:05,272 --> 00:24:13,099 说话人 SPEAKER_03：大多数人认为，记忆就像是一种文件，你把它放在某个地方，然后去检索，就像在电脑里一样。
254 00:24:13,500 --> 00:24:14,981 说话人 SPEAKER_03：在人类身上根本不是这样。
255 00:24:15,823 --> 00:24:17,384 说话人 SPEAKER_03：记忆总是被重建的。
256 00:24:18,065 --> 00:24:21,887 说话人 SPEAKER_03：如果你在回忆最近发生的事情，你会相当准确地重建它。
257 00:24:22,808 --> 00:24:28,193 说话人 SPEAKER_03：如果你在回忆很久以前的事情，你经常会把所有的细节都搞错，而且你不会意识到这一点，你会对它们充满信心。
258 00:24:29,275 --> 00:24:32,917 说话人 SPEAKER_03：有一个很好的例子，那就是约翰·迪安的记忆。
259 00:24:33,488 --> 00:24:42,011 说话人 SPEAKER_03：所以在水门事件中，约翰·迪安在宣誓作证时，就白宫中各种会议的情况作证，但他所有的细节都是错误的。
260 00:24:42,673 --> 00:24:48,830 说话人 SPEAKER_03：他说霍尔德曼说了这样，不，是厄利希曼说的，而霍尔德曼甚至没有参加那次会议。
261 00:24:50,431 --> 00:24:53,414 说话人 SPEAKER_03：关于他的证词，明确的是他试图说出真相。
262 00:24:53,976 --> 00:24:57,461 说话人 SPEAKER_03：他传达了他们在试图掩盖水门事件时发生的事情的要点。
263 00:24:58,242 --> 00:25:03,109 说话人 SPEAKER_03：但他声称记得的所有细节都是错误的，不是全部，但很多都是。
264 00:25:03,490 --> 00:25:06,275 说话人 SPEAKER_03：乌尔里希·内塞尔有一篇很棒的论文证明了这一点。
265 00:25:07,276 --> 00:25:14,446 说话人 SPEAKER_03：这是一个很好的例子，说明约翰·迪安认为他能够记住的事情，当你听录音时，他只是错了。
266 00:25:16,189 --> 00:25:18,732 说话人 SPEAKER_03：但他抓住了本质。
267 00:25:18,898 --> 00:25:22,805 说话人 SPEAKER_03：现在，在目前的情况下，聊天机器人比我们更擅长编造。
268 00:25:23,046 --> 00:25:25,910 说话人 SPEAKER_03：它们比人们更频繁地编造，而且它们不知道自己在做这件事。
269 00:25:26,872 --> 00:25:27,992 说话人 SPEAKER_03：他们一直在变得更好。
270 00:25:28,775 --> 00:25:32,601 说话人 SPEAKER_03：我认为不久的将来，聊天机器人不会在胡编乱造方面比我们差太多。
271 00:25:33,281 --> 00:25:38,048 说话人 SPEAKER_03：但他们胡编乱造的事实并不表明他们不理解，或者他们与我们不同。
272 00:25:38,450 --> 00:25:39,912 说话人 SPEAKER_03：这表明他们非常像我们。
273 00:25:41,578 --> 00:25:47,390 说话人 SPEAKER_03：因此，这就结束了我想向大家展示这些大型聊天机器人非常像我们的尝试。
274 00:25:47,871 --> 00:25:50,536 说话人 SPEAKER_03：它们不像普通的计算机软件，它们更像人。
275 00:25:51,577 --> 00:25:55,505 说话人 SPEAKER_03：这导致了一系列的 AI 风险，我决定不谈论。
276 00:25:55,545 --> 00:26:01,856 说话人 SPEAKER_03：它们在这篇关于档案的论文中，但我怀疑你们中的一些人可能会想问关于 AI 风险的问题。
277 00:26:02,617 --> 00:26:03,519 说话人 SPEAKER_03：我现在已经完成了。
278 00:26:07,734 --> 00:26:15,009 说话人 SPEAKER_05：感谢你，Geoffrey，你的演讲非常引人深思，对在座的每个人来说都非常易于理解。
279 00:26:15,029 --> 00:26:20,060 说话人 SPEAKER_05：我认为它会比 John Dean 的演讲对每个人来说都更加难忘。
280 00:26:20,541 --> 00:26:26,654 说话人 SPEAKER_05：现在我想邀请 UCD 校长，奥拉·菲利教授上台发言。
281 00:26:30,144 --> 00:26:34,090 主持人：感谢你，杰弗里，你那些非常清晰有趣的发言。
282 00:26:34,251 --> 00:26:39,240 主持人：还要感谢你，安德鲁，你提名杰弗里获得尤利西斯奖，以及你所作的颁奖词。
283 00:26:40,260 --> 00:26:56,507 主持人：正如安德鲁所说，我们很快将再次听到安德鲁和杰弗里的声音，以及 UCD 研究、创新与影响副总裁凯特·罗布森-布朗教授的对话，届时你们可以提出所有在这个阶段可能积累的问题。
284 00:26:56,487 --> 00:27:05,881 主持人：我会尽量简短，但我想强调一下杰弗里·辛顿的工作和背景，这些对我来说使这个奖项特别合适。
285 00:27:07,623 --> 00:27:12,150 说话人 说话人_00：这些中的第一个是关于改变世界的科研行为。
286 00:27:13,612 --> 00:27:25,548 说话人 说话人_00：我在 20 世纪 80 年代在伯克利读研究生时，遇到了当时的人工神经网络研究的第二次重大浪潮，与杰弗里·辛顿（Geoffrey Hinton）等领导者一起。
287 00:27:26,660 --> 00:27:47,246 说话人 说话人_00：这种细致、稳步的科研努力持续了几十年，首先逐渐地，然后非常突然地带来了变化，当技术能力发展到能够实现长期以来在算法中预期的强大功能时。
288 00:27:49,509 --> 00:27:56,498 说话人 说话人_00：杰弗里曾谈到在那么长时间里维持一个非常高水平的研究生涯的困难。
289 00:27:57,118 --> 00:28:02,547 说话人：描述他作为有小孩的鳏夫所面临的挑战。
290 00:28:04,088 --> 00:28:13,863 说话人：他强调了导师的重要性，我们非常感激他今天下午抽出时间与学生进行问答环节。
291 00:28:15,545 --> 00:28:26,079 说话人：他的研究跨越了工业和学术界，将心理学和生理学以及工程学和计算机科学等学科领域结合起来。
292 00:28:26,903 --> 00:28:35,771 说话人：他还至关重要且广为人知地公开讨论了人工智能的社会影响。
293 00:28:37,634 --> 00:28:55,492 讲者：对我们作为都柏林大学学院的科研社区来说，倾听并坚守这些关于基本理解、远见、毅力、指导、合作和开放质疑的重要性信息至关重要。
294 00:28:57,817 --> 00:29:01,363 讲者：我想强调的第二点是它与爱尔兰的联系。
295 00:29:02,924 --> 00:29:16,605 讲者：安德鲁提到了杰弗里与乔治·布尔和玛丽·埃文斯·布尔家族的联系，以及他们的多才多艺的后代，我们爱尔兰人，尤其是我们科克的朋友，对此感到非常自豪。
296 00:29:18,709 --> 00:29:26,559 讲者：在今天的爱尔兰，我们有一个国家，它付出了巨大的努力，在全球科技领域建立了令人羡慕的地位。
297 00:29:26,911 --> 00:29:34,080 说话人 SPEAKER_00: 具备不被视为理所当然的能力，跟上技术进步的连续浪潮。
298 00:29:36,243 --> 00:29:44,075 说话人 SPEAKER_00: 人工智能在爱尔兰广泛应用的预期对技术行业和其他行业意味着什么？
299 00:29:45,277 --> 00:29:48,240 说话人 SPEAKER_00: 机遇和挑战将存在于哪里？
300 00:29:49,843 --> 00:29:55,872 说话人 SPEAKER_00: 对计算和其他基础设施以及人才渠道有哪些要求？
301 00:29:57,640 --> 00:30:14,105 说话人 SPEAKER_00：对于我们 UCD 这样的大学，一个比其他任何支持并推动国家进步的大学都要多的大学，我们如何应对人工智能在教学、学习和研究创新中的机遇和挑战？
302 00:30:15,926 --> 00:30:25,622 说话人 SPEAKER_00：我们已经在这一领域开展了杰出的工作，不仅来自安德鲁和他的计算机科学同事，还包括整个大学。
303 00:30:26,345 --> 00:30:35,019 说话人 SPEAKER_00：例如，通过获奖工作，帕特里夏·麦圭尔教授及其同事在应用人工智能治疗妊娠高血压方面的研究。
304 00:30:36,803 --> 00:30:48,403 说话人 SPEAKER_00：在接下来的几个月里，我们开始制定下一所大学战略的过程中，人工智能如何融入该战略的问题对我们来说非常重要。
305 00:30:50,864 --> 00:30:58,594 讲者 SPEAKER_00：使这个奖项如此合适的三点是，世界目前处于人工智能的什么位置。
306 00:31:00,395 --> 00:31:09,268 讲者 SPEAKER_00：以詹姆斯·乔伊斯小说《尤利西斯》命名，这部小说是人类创造性智能的伟大例子之一。
307 00:31:10,750 --> 00:31:20,162 讲者 SPEAKER_00：我们现在面临着反思我们对智能、创造力和甚至人类是什么的理解的挑战。
308 00:31:21,104 --> 00:31:26,369 讲者 SPEAKER_00：当我们寻求在人工智能的更大采用中绘制路线图时。
309 00:31:27,911 --> 00:31:30,733 说话人 说话人_00：对一些人来说，这个前景令人兴奋。
310 00:31:31,615 --> 00:31:34,718 说话人 说话人_00：对另一些人来说，这可能是令人畏惧的，甚至可能是反乌托邦的。
311 00:31:36,559 --> 00:31:47,432 说话人 说话人_00：无论如何，我们不能通过逃避技术颠覆的力量来规划我们的道路，也不能在没有任何挑战的情况下屈服于这种力量。
312 00:31:49,083 --> 00:32:01,659 说话人 说话人_00：在此阶段，我们无法预见人工智能在未来几年和几十年发展中的全部影响、局限性、应用和影响。
313 00:32:03,340 --> 00:32:15,916 发言人 SPEAKER_00：我们确实知道，我们需要尽可能多地了解这项技术的能力和局限性，以便将所有学科和观点纳入对话中。
314 00:32:16,538 --> 00:32:20,525 发言人 SPEAKER_00：这样我们就可以利用人工智能造福人类。
在这个问题上，听取像 Geoffrey Hinton 这样的人的声音至关重要，他们为推动这项技术发展以及向公众辩论其安全采用提供大量信息。
由于所有这些原因，今天能有机会与 Geoffrey 一起庆祝他非常杰出的职业生涯并聆听他的讲话，感到非常荣幸。
317 00:32:49,102 --> 00:32:52,749 主持人：感谢大家今晚加入我们，成为这次对话的一部分。
318 00:32:53,230 --> 00:32:54,392 主持人：感谢大家。
319 00:32:57,763 --> 00:32:58,664 主持人05：谢谢，费伊教授。
320 00:32:59,185 --> 00:33:08,417 主持人 05：现在，我想邀请 UCD 研究、影响和创新副校长凯特·罗布森-布朗教授上台进行炉边谈话。
321 00:33:08,438 --> 00:33:16,147 主持人：非常感谢，还要感谢 Geoffrey 的精彩演讲。
322 00:33:16,167 --> 00:33:27,163 主持人：我……嗯，我本来想说，当我走进来的时候，我看到我们这里有一个相当多元化的观众群体，我希望你们能感受到这一点，这也是我们欢迎你们来到 UCD 的热情的体现。
323 00:33:27,380 --> 00:33:29,883 主持人：我还能看到许多年轻的研究人员和早期职业的研究人员，我知道你们下午抽出时间与我们的学生进行了交谈。
324 00:33:31,403 --> 00:33:40,374 主持人：我还能看到许多年轻的研究人员和早期职业的研究人员，我知道你们下午抽出时间与我们的学生进行了交谈。
325 00:33:40,394 --> 00:33:50,964 说话者 SPEAKER_14：也许我觉得从这里开始，思考一下你自己的学习，也许是在学校和大学里，对你的后来职业生涯产生了什么影响，会是个不错的选择。
326 00:33:51,025 --> 00:33:56,931 说话者 SPEAKER_14：你回望过去，认为哪件事对你来说非常重要，那可能就是你的起点？
327 00:33:57,973 --> 00:33:59,175 说话者 SPEAKER_03：我想有几件事情。
328 00:33:59,656 --> 00:34:00,778 说话者 SPEAKER_03：其中一件事情是学习物理。
329 00:34:02,000 --> 00:34:04,884 说话人 SPEAKER_03：物理学让我明白了什么是解释。
330 00:34:05,905 --> 00:34:13,539 说话人 SPEAKER_03：当我学习心理学之后，在学过物理学的基础上，我感觉它并没有真正解释清楚所发生的事情。
331 00:34:14,300 --> 00:34:16,963 说话人 SPEAKER_03：我感觉你需要真正了解其中的机制。
332 00:34:17,585 --> 00:34:18,606 说话人 SPEAKER_03：所以这一点很重要。
333 00:34:18,867 --> 00:34:24,014 说话人 SPEAKER_03：另一件事是，当我还是个青少年时，我的一位同学告诉我关于继电器的事情。
334 00:34:24,146 --> 00:34:28,992 说话人 SPEAKER_03：以及如何用一个电流去切换另一个电流。
335 00:34:29,378 --> 00:34:49,454 说话人 SPEAKER_03：所以我实际上花了很多时间剪掉六英寸钉子的头部，然后在上面绕上铜线，接着用老式的剃须刀片将其一分为二，这样你就得到了一段柔韧的金属，然后在剃须刀片的末端绕上一段铜线，在六英寸钉子头部所在的位置也绕上一段铜线。
336 00:34:49,855 --> 00:34:51,757 说话人 SPEAKER_03：然后当你通上电流时，你就建立了连接。
337 00:34:51,817 --> 00:34:53,938 说话人 SPEAKER_03：这样，你可以做一个小的中继。
338 00:34:53,958 --> 00:34:58,963 说话人 SPEAKER_03：然后我会以不同的方式连接这些中继，发现你可以制作像振荡器这样的东西。
339 00:34:58,943 --> 00:35:01,326 说话人 SPEAKER_03：那真的很开心。
340 00:35:01,505 --> 00:35:02,867 说话人 SPEAKER_14：那么你现在多大了？
341 00:35:03,509 --> 00:35:04,610 说话人 SPEAKER_03: 我那时候可能大概 15 岁左右。
342 00:35:04,750 --> 00:35:09,516 说话人 SPEAKER_03: 我可能只是出去买了一些继电器。
343 00:35:10,958 --> 00:35:11,759 说话人 SPEAKER_14: 那有什么乐趣可言？
344 00:35:11,778 --> 00:35:14,322 说话人 SPEAKER_03: 自己做更有趣。
345 00:35:16,525 --> 00:35:23,434 说话人 SPEAKER_03：我也认为来自一个高度重视科学的家庭是很重要的。
346 00:35:25,396 --> 00:35:26,858 说话人 SPEAKER_14：还有一件事让我印象深刻
347 00:35:27,157 --> 00:35:37,710 考虑到您自己的职业历程，我认为您的研究和兴趣有时处于传统学科壁垒之间。
348 00:35:37,731 --> 00:35:39,472 说话人 SPEAKER_14：您认为这是一个公正的陈述吗？
349 00:35:39,494 --> 00:35:54,211 说话人 SPEAKER_03：是的，当我还在高中时，我对大脑是如何工作的产生了浓厚的兴趣，因为我有一个非常聪明的朋友，他比我聪明得多，他告诉我大脑像全息图一样存储记忆，这让我对它产生了兴趣。
350 00:35:56,807 --> 00:35:59,271 说话人 SPEAKER_03：他对我影响很大。
351 00:35:59,291 --> 00:36:00,994 说话人 SPEAKER_03：那也是对我影响很大的一个因素。
352 00:36:02,175 --> 00:36:02,675 说话人 SPEAKER_14：很有趣。
353 00:36:03,076 --> 00:36:11,286 说话者 SPEAKER_14：如果你回顾自己的职业生涯，你最自豪的是哪些方面？
354 00:36:11,367 --> 00:36:19,878 说话者 SPEAKER_14：我并不一定只是在想你的发现，也许还包括你职业生涯的其他方面，比如你合作过的人，你组建的团队。
355 00:36:20,331 --> 00:36:26,884 说话者 SPEAKER_03：我想我自豪的是，即使在人们说神经网络是垃圾的时候，我还是坚持了下来，这大概有 40 年的时间。
356 00:36:26,903 --> 00:36:40,989 说话者 SPEAKER_03：但我最自豪的智力成就就是 Boltz 机器，它是反向传播的替代品，可能也是错误的。
357 00:36:41,273 --> 00:36:45,702 讲者 SPEAKER_03：它们并不是大脑的工作方式，并且它们的效果不如反向传播。
358 00:36:46,242 --> 00:36:56,362 讲者 SPEAKER_03：但这是一个如此美丽的理论，即使失败了我也非常自豪。
359 00:36:56,342 --> 00:37:01,992 讲者 SPEAKER_05：对现在在学术界工作的早期职业研究人员有什么建议？
360 00:37:03,434 --> 00:37:08,961 讲者 SPEAKER_05：对于那些说，学术研究人员没有计算能力的人，你会说什么？
361 00:37:10,103 --> 00:37:13,568 说话人 SPEAKER_05：早期研究人员现在是如何贡献的？
362 00:37:13,949 --> 00:37:23,284 说话人 SPEAKER_03：是的，我并不真的知道计算能力的答案，除了我正在试图说服你们的政府提供更多的计算能力。
363 00:37:23,304 --> 00:37:24,105 说话人 SPEAKER_03：我认为
364 00:37:24,996 --> 00:37:28,282 说话人 SPEAKER_03：除了构建这些大型模型之外，还有很多研究要做。
365 00:37:29,163 --> 00:37:33,809 说话人 SPEAKER_03：我给研究人员的一条建议是相信你的直觉。
366 00:37:34,610 --> 00:37:42,501 说话人 SPEAKER_03：所以，如果你，很多好的研究都来自于对人们做事方式存在某种模糊直觉的认识。
367 00:37:42,842 --> 00:37:46,768 说话人 SPEAKER_03：总觉得有些地方不太对劲，然后去追寻这种感觉。
368 00:37:46,748 --> 00:37:56,021 说话人 SPEAKER_03：我想用一个类比，当你读侦探小说时，通常在第一章，有人做了某件事，当你读的时候，感觉有点不对劲。
369 00:37:56,400 --> 00:37:57,802 说话人 SPEAKER_03：他们这么做有点奇怪。
370 00:37:58,344 --> 00:38:00,666 说话人 SPEAKER_03：后来发现这是整个故事的关键。
371 00:38:01,188 --> 00:38:03,590 说话人 SPEAKER_03：但通常你只是匆匆读过，没有注意到。
372 00:38:03,610 --> 00:38:04,472 说话人 SPEAKER_03：你只是感觉有点奇怪。
373 00:38:05,152 --> 00:38:06,614 说话人 SPEAKER_03：我认为很多科学都是这样的。
374 00:38:06,894 --> 00:38:16,148 说话人 SPEAKER_03：如果你有一种直觉，认为人们以一种有趣的方式做错了事情，那就去研究它，不要放弃，直到你知道为什么这种直觉不好。
375 00:38:16,128 --> 00:38:20,356 说话人 SPEAKER_03：即使你需要 20 年才发现为什么这种直觉不好，那也行。
376 00:38:20,856 --> 00:38:23,561 说话人 SPEAKER_03：但要么你有好的直觉，要么没有。
377 00:38:24,463 --> 00:38:26,907 说话人 SPEAKER_03：如果你有良好的直觉，你应该相信它们。
378 00:38:27,469 --> 00:38:30,554 说话人 SPEAKER_03：如果你没有良好的直觉，你做什么其实并不重要。
379 00:38:30,936 --> 00:38:38,489 说话人 SPEAKER_05：所以你不妨相信它们。
380 00:38:39,027 --> 00:38:48,103 说话人 SPEAKER_05：这里有很多早期职业研究人员，但同时也有一群更广泛的爱尔兰社会人士。
381 00:38:48,222 --> 00:38:51,427 说话人 SPEAKER_05：所以，有一个问题更多是与 AI 政策相关。
382 00:38:52,150 --> 00:39:02,266 说话人 SPEAKER_05：学术界、政府、产业和慈善家们可以一起做些什么，以改变 AI 的长期发展方向，使其更加公平、安全？
383 00:39:02,246 --> 00:39:09,117 说话人 SPEAKER_05：在符合人类利益而不是大公司股东利益的前提下，使其可调控。
384 00:39:10,018 --> 00:39:16,067 说话人 SPEAKER_03：我认为政府可以做的事情之一是努力保持大学和前沿 AI 研究的发展。
385 00:39:16,626 --> 00:39:19,331 说话人 SPEAKER_03：要做到这一点，就需要提供大量的计算资源。
386 00:39:20,213 --> 00:39:29,706 说话人 SPEAKER_03：所以加拿大昨天宣布将提供 24 亿美元用于人工智能，其中 20 亿美元仅用于计算。
387 00:39:29,686 --> 00:39:37,452 说话人 SPEAKER_03：以便加拿大初创公司和加拿大学术研究人员能够获得大量的计算资源。
388 00:39:37,472 --> 00:39:38,958 说话人 SPEAKER_03：这就是所需要的投资类型。
389 00:39:39,418 --> 00:39:41,686 说话人 SPEAKER_03：爱尔兰可能需要
390 00:39:42,021 --> 00:39:50,753 说话人 SPEAKER_03：它是一个较小的国家，所以可能无法投入那么多，但也许与欧盟合作，它可能成为欧洲投资大量资金的一部分。
391 00:39:50,773 --> 00:39:56,121 说话人 SPEAKER_03：欧盟投资了 10 亿美元在一个疯狂的大脑项目上，但没有任何成果。
392 00:39:57,344 --> 00:40:07,438 说话人 SPEAKER_03：他们可能更明智地投资 50 亿美元用于 AI 计算，这样大学和小的初创公司就能与大型公司竞争。
393 00:40:07,603 --> 00:40:08,806 说话人 SPEAKER_03：这是你可以做的一件事。
394 00:40:09,567 --> 00:40:15,036 说话人 SPEAKER_03：政府可以做的另一件事是强迫大公司进行比现在更多的安全研究。
395 00:40:16,338 --> 00:40:28,295 说话人 SPEAKER_03：所以我认为我的政治信仰是，我内心深处是一个社会主义者，但资本主义在某些方面运作得相当不错。
396 00:40:29,137 --> 00:40:37,844 说话人 SPEAKER_03：在我看来，你需要的是，你需要小型初创公司试图致富的那种创造力。
397 00:40:38,465 --> 00:40:42,528 说话人 SPEAKER_03：但是你需要在严格的监管环境中做到这一点。
398 00:40:42,949 --> 00:40:50,034 说话人 SPEAKER_03：所以政府的角色是组织事物，以便如果每个人都追求自己的利益，那么对每个人都是有益的。
399 00:40:50,536 --> 00:40:59,143 说话人 SPEAKER_03：而政府应该置身事外的想法是疯狂的，因为那样的话，如果每个人都追求自己的利益，那么只会出现一些非常富有的人和其余的非常贫穷的人。
400 00:40:59,123 --> 00:41:09,601 说话人 SPEAKER_03：我认为政府应该非常干预，并努力创造一个奖励景观，引导人们做一些对社会有益的事情。
401 00:41:10,422 --> 00:41:12,144 说话人 SPEAKER_03：我认为政府应该做很多事情。
402 00:41:12,224 --> 00:41:16,052 说话人 SPEAKER_03：特别是坚持进行更多关于安全性的研究。
403 00:41:17,753 --> 00:41:32,768 说话人 SPEAKER_14：我认为目前您没有就安全问题进行深入探讨，特别是在大型语言模型如 GPT-4 和其他被广泛采用且在公共讨论中占据重要位置的模型方面。
404 00:41:34,130 --> 00:41:45,583 说话人 SPEAKER_14：您能否具体谈谈您对误用的担忧，以及我们可以在短期和长期内采取哪些措施来减轻风险？
405 00:41:46,036 --> 00:41:55,387 说话人 SPEAKER_03：是的，所以有各种各样的风险滥用，它们都有不同的解决方案，并且有不同的时间尺度。
406 00:41:55,909 --> 00:42:00,614 说话人 SPEAKER_03：目前最紧迫的是，嗯，已经存在的歧视偏见。
407 00:42:01,615 --> 00:42:08,144 说话人 SPEAKER_03：可能是因为我是个老白男，我对这些事情不太关心，原因在于
408 00:42:08,125 --> 00:42:16,748 说话人 SPEAKER_03：如果你用 AI 系统替换现有的系统，你可以使 AI 系统比现有系统稍微少一些偏见。
409 00:42:17,150 --> 00:42:19,797 说话人 SPEAKER_03：你不应该试图让它无偏见，那只会是徒劳的。
410 00:42:20,157 --> 00:42:23,367 说话人 SPEAKER_03：只需让它比原来的显著减少偏见即可。
411 00:42:23,347 --> 00:42:32,297 说话人 SPEAKER_03：所以如果你有涉及老白人决定是否给年轻黑人女性提供抵押贷款的培训数据，你知道这将非常具有偏见。
412 00:42:32,318 --> 00:42:38,485 说话人 SPEAKER_03：现有的系统就是这样，你的培训数据也是这样，但你可以通过一些方法使最终系统减少偏见。
413 00:42:38,505 --> 00:42:39,688 说话人 SPEAKER_03：你永远无法做到无偏见。
414 00:42:40,307 --> 00:42:48,038 说话人 SPEAKER_03：所以我认为，实际上 AI 系统可以使事物比目前更加少有偏见，尽管它们并不完全无偏见。
415 00:42:49,030 --> 00:42:52,934 说话人 SPEAKER_03：我最担心的是目前用虚假视频破坏选举。
416 00:42:53,474 --> 00:42:54,896 说话人 SPEAKER_03：今年我们将看到很多这种情况。
417 00:42:55,737 --> 00:43:00,842 说话人 SPEAKER_03：显然，虽然技术上很难，但解决方案是将所有这些视频标记为虚假。
418 00:43:01,422 --> 00:43:05,206 说话人 SPEAKER_03：坚持将 AI 生成的内容以某种方式标记为 AI 生成。
419 00:43:05,646 --> 00:43:07,027 说话人 SPEAKER_03：这在技术上非常困难。
420 00:43:07,869 --> 00:43:13,594 说话人 SPEAKER_03：但我想在美国，例如，他们甚至不会尝试这样做，因为共和党人不会投票支持这一点。
421 00:43:14,315 --> 00:43:16,016 说话人 SPEAKER_03：他们依赖撒谎。
422 00:43:18,679 --> 00:43:24,726 说话人 SPEAKER_03：那不是政治，那只是事实。
423 00:43:24,746 --> 00:43:34,717 说话人 SPEAKER_14：您还提出了关于 AI 可能设定自己的目标，并可能使我们面临迈克所说的生存威胁的担忧。
424 00:43:36,760 --> 00:43:45,429 说话人 SPEAKER_14：那么，您能给我们举一些例子吗？在当前我们经历的 AI 发展时期，您认为哪些具体的保障措施或伦理框架是至关重要的？
425 00:43:46,978 --> 00:43:49,481 说话人 SPEAKER_03：嗯，生存威胁是长期性的。
426 00:43:50,342 --> 00:43:59,153 我的猜测是，它将在五到二十年内，有大约50%的概率变得比我们更聪明。
427 00:43:59,173 --> 00:43:59,833 可能会更长。
428 00:43:59,873 --> 00:44:01,916 可能会更短，但我觉得不太可能。
429 00:44:02,297 --> 00:44:05,981 说话人 SPEAKER_03：但在这个时期，我们很可能得到比我们更聪明的东西。
430 00:44:06,561 --> 00:44:09,164 说话人 SPEAKER_03：如果我们做到了，它们接管的可能性相当大。
431 00:44:09,206 --> 00:44:12,969 说话人 SPEAKER_03：我们应该清楚地考虑如何阻止这种情况。
432 00:44:13,371 --> 00:44:22,797 说话人 SPEAKER_03：我的意思是，有一个问题，就是我们是否在，人文主义是否是种族主义词汇，他们是否也应该拥有权利。
433 00:44:23,177 --> 00:44:28,932 说话人 SPEAKER_03：但我们都是人，我们关心人，我认为我们应该尽我们最大的努力让人们留下来。
434 00:44:31,563 --> 00:44:46,938 说话人 SPEAKER_03：我不知道我们是否能做到，但肯定有一件事我们应该做，那就是在我们使事物更加智能的同时，随着大公司向超级智能迈进，他们实际上可以在它们比我们聪明之前进行实验，看看这些事物可能会失控。
435 00:44:47,306 --> 00:44:57,981 说话人 SPEAKER_03：任何编写过计算机程序的人都知道，你编写计算机程序，然后它不起作用，然后你意识到你误解了什么或者没有考虑到什么。
436 00:44:58,742 --> 00:45:02,228 说话人 SPEAKER_03：因此，对事物的经验性检查至关重要。
437 00:45:02,268 --> 00:45:05,231 说话人 SPEAKER_03：你不能对一切事情都只是进行空想预测。
438 00:45:05,211 --> 00:45:10,036 说话人 SPEAKER_03：我认为他们应该迫使大公司进行实验，看看这些事情如何试图失控。
439 00:45:10,717 --> 00:45:12,498 说话人 SPEAKER_03：我们认为我们已经知道他们将会采取的一些方式。
440 00:45:13,239 --> 00:45:22,246 说话人 SPEAKER_03：所以我们将给他们设定子目标，因为如果代理能够设定诸如到达那里、找到酒店等子目标，那么他们在规划假期时就会更加有效。
441 00:45:23,668 --> 00:45:25,250 说话人 SPEAKER_03：你不想微观管理他们。
442 00:45:25,329 --> 00:45:27,012 说话人 SPEAKER_03：你希望他们制定子目标。
443 00:45:27,891 --> 00:45:33,177 说话人 SPEAKER_03：但他们将会开始制定子目标，比如，如果我能有更多一点的控制权，我就能做得更好。
444 00:45:33,932 --> 00:45:36,596 说话人 SPEAKER_03：这很容易导致他们想要接管。
445 00:45:36,615 --> 00:45:43,342 说话人 SPEAKER_03：我认为我们需要在这方面进行实验，因为目前我们只是在理论化。
446 00:45:44,422 --> 00:45:55,134 说话人 SPEAKER_03：我们实际上并没有任何比我们更智能的事物经验，或者如何处理它们，甚至没有与我们的智能几乎相当的事物的经验，除非你把特朗普算在内。
447 00:45:55,153 --> 00:46:03,061 说话人 SPEAKER_03：而且我们处理得并不好。
448 00:46:05,387 --> 00:46:06,889 说话人 SPEAKER_03：我认为这是政府可以做的事情之一。
449 00:46:07,210 --> 00:46:14,302 说话人 SPEAKER_03：我认为萨姆·奥特曼实际上也同意这一点，即他们实际上应该进行实证实验来了解这些事物是如何试图逃脱控制的。
450 00:46:14,581 --> 00:46:16,405 说话人 SPEAKER_03：伊利亚也同意这一点。
451 00:46:16,425 --> 00:46:17,367 说话人 SPEAKER_14：你提到这一点很有意思。
452 00:46:17,427 --> 00:46:28,965 说话人 SPEAKER_14：那么，你认为营利性与非营利性是否是问题的一部分，即能够开发这些平台和解决方案的公司或组织？
453 00:46:29,467 --> 00:46:31,693 发言人 SPEAKER_03：是的，所以利润问题很棘手。
显然，就像谷歌一样，当它在大型聊天机器人领域领先时，并没有向公众发布大型聊天机器人，因为它害怕它们做所有这些胡言乱语和说坏话会损害它的声誉。
455 00:46:44,875 --> 00:46:50,061 发言人 SPEAKER_03：一旦开放式人工智能与微软结合并诞生，谷歌就必须参与竞争。
456 00:46:50,742 --> 00:46:54,166 发言人 SPEAKER_03：因此，利润动机将推动快速进步。
457 00:46:54,907 --> 00:46:56,108 说话人 SPEAKER_03：我们不会停止。
458 00:46:56,809 --> 00:46:59,152 说话人 SPEAKER_03：也许可以说，现在就停止做人工智能是合理的。
459 00:46:59,333 --> 00:47:01,715 说话人 SPEAKER_03：这可能是一个合理的决定，但不会发生。
460 00:47:01,735 --> 00:47:04,639 说话人 SPEAKER_03：甚至提出这样的要求都没有意义，因为这是不可能发生的。
461 00:47:05,119 --> 00:47:06,661 说话人 SPEAKER_03：如果我们停止，中国人不会停止。
462 00:47:11,788 --> 00:47:14,831 说话人 SPEAKER_03：如果中国人停止，美国人不会停止。
463 00:47:17,932 --> 00:47:27,309 说话人 SPEAKER_03：所以我认为我们不能要求它停止，我们也不应该忽视它有很多非常积极的方面，这就是为什么它将在医学等领域得到发展。
464 00:47:29,333 --> 00:47:33,681 说话人 SPEAKER_03：我们必须找出如何尝试控制它。
465 00:47:36,277 --> 00:47:53,188 说话人 SPEAKER_03：我认为盈利动机，我们看到了在 OpenAI 发生的事情，因为拥有盈利部分的那个公司是一个非盈利部分，它被设立来
466 00:47:54,298 --> 00:47:56,121 说话人 SPEAKER_03：主要关注安全。
467 00:47:56,882 --> 00:48:03,976 说话人 SPEAKER_03：即使在那种非常不公平的场合，它倾斜到了极限，安全与盈利仍然获胜。
468 00:48:06,018 --> 00:48:07,001 说话人 SPEAKER_03：这似乎并不好。
469 00:48:08,643 --> 00:48:16,257 说话人 SPEAKER_03：趁这个机会，我还要提一件事，那就是我认为开源最大的模型简直是疯狂。
470 00:48:16,438 --> 00:48:18,702 说话人 SPEAKER_03：我的好朋友 Yann LeCun 认为这样做是正确的。
471 00:48:20,103 --> 00:48:21,306 说话人 SPEAKER_03：他认为我们都会没事的。
472 00:48:21,686 --> 00:48:23,208 说话人 SPEAKER_03：我们将继续控制这些事物。
473 00:48:23,248 --> 00:48:24,471 说话人 SPEAKER_03：他们不会有自己的目标。
474 00:48:25,833 --> 00:48:27,255 说话人 SPEAKER_03：他们不会有自己的欲望。
475 00:48:28,177 --> 00:48:30,260 我认为公开它们是非常非常危险的。
476 00:48:30,300 --> 00:48:32,123 我认为这就像公开核武器。
477 00:48:35,447 --> 00:48:40,454 说话人 SPEAKER_03：我认为我非常希望政府禁止公司开源大型模型。
478 00:48:41,971 --> 00:49:03,304 说话人 SPEAKER_05：我之前和你谈过的一个问题是关于如何监管这些事情，欧盟最近颁布了《人工智能法案》，但是
479 00:49:03,284 --> 00:49:07,391 说话人 SPEAKER_05：我们还能做些什么来加强这一点吗？
480 00:49:07,431 --> 00:49:30,068 说话人 SPEAKER_05：比如，为计算机科学家或人工智能架构师设立法定监管职业，这会不会加强个人对抗目前强大的企业偏见的能力？
481 00:49:30,418 --> 00:49:35,677 说话人 SPEAKER_03：我说，在这个时候，我应该说我是一名科学家，我开发了一些科学的东西。
482 00:49:35,978 --> 00:49:37,806 说话人 SPEAKER_03：这并不意味着我是一个政策专家。
483 00:49:38,467 --> 00:49:39,552 说话人 SPEAKER_03：所以...
484 00:49:40,543 --> 00:49:46,391 说话人 SPEAKER_03：当你对政策知之甚少时，很容易陷入对政策的陈述中。
485 00:49:47,012 --> 00:49:55,664 说话人 SPEAKER_03：这需要律师和习惯于政府工作的人来理解政治选项以及你应该如何进行监管。
486 00:49:56,025 --> 00:49:57,507 说话人 SPEAKER_03：我真的不懂那些东西。
487 00:49:57,527 --> 00:49:58,588 说话人 SPEAKER_03：我不懂那些东西。
488 00:49:59,168 --> 00:50:01,811 说话人 SPEAKER_03：我的一些朋友开始学习那些东西了。
489 00:50:01,893 --> 00:50:06,378 说话人 SPEAKER_03：约书亚·本吉奥和斯图尔特·罗素为此投入了大量的努力去了解那些东西。
490 00:50:06,398 --> 00:50:07,480 说话人 SPEAKER_03：我还没有。
491 00:50:07,460 --> 00:50:09,967 说话人 SPEAKER_03：我觉得我太老了，学不到新东西。
492 00:50:12,114 --> 00:50:15,545 说话人 SPEAKER_03：所以我真的没有多少可以让你信任的关于那方面的东西可以说。
493 00:50:17,146 --> 00:50:19,548 说话人 SPEAKER_05：这是一件有趣的事情。
494 00:50:19,588 --> 00:50:36,032 说话人 SPEAKER_05：我认为爱尔兰政府最近成立了一个 AI 委员会，并且学术界和产业界都有广泛的代表，但并不一定是技术专家。
495 00:50:36,512 --> 00:50:40,858 说话人 SPEAKER_05：有产业界的专家，但不一定是正确的建议。
496 00:50:40,978 --> 00:50:45,684 说话人 SPEAKER_05：我认为这与你所说的关于应该有哪些人参与这些事务的观点相呼应。
497 00:50:47,302 --> 00:50:50,289 说话人 SPEAKER_14：我可以再问一个问题吗？
498 00:50:50,309 --> 00:51:05,344 说话者 SPEAKER_14：你之前提到了医疗应用，我想了解一下你认为在人工智能应用于医学领域，哪些低垂的果实可能会带来巨大的社会效益。
499 00:51:05,762 --> 00:51:10,630 说话者 SPEAKER_03：我认为其中一些低垂的果实是医学图像的解读。
500 00:51:11,670 --> 00:51:14,856 说话者 SPEAKER_03：我认为将来我们可能不再需要放射科医生了。
501 00:51:15,577 --> 00:51:16,137 说话者 SPEAKER_03：还不是时候。
502 00:51:17,400 --> 00:51:24,329 说话人 SPEAKER_03：当我们不需要它们来解释医学影像时，或者说，AI 将解释医学影像，放射科医生只需检查一下。
503 00:51:25,130 --> 00:51:29,577 说话人 SPEAKER_03：但是，AI 已经可以分析眼底照片，也就是你的视网膜照片。
504 00:51:29,827 --> 00:51:36,474 说话人 SPEAKER_03：AI 可以从这张照片中看到各种东西，这些是眼科医生以前从未知道存在且无法看到的。
505 00:51:36,773 --> 00:51:41,278 说话人 SPEAKER_03：比如，你向眼科医生展示一张眼底照片，问他这个人的性别。
506 00:51:41,860 --> 00:51:43,501 说话人 SPEAKER_03：他们会说，我怎么从眼底图像中知道？
507 00:51:43,842 --> 00:51:45,684 说话人 SPEAKER_03：AI 会以 80%的准确率告诉你。
508 00:51:46,364 --> 00:51:49,327 说话人 SPEAKER_03：或者你问他，这个人未来 10 年会不会心脏病发作？
509 00:51:49,728 --> 00:51:53,090 说话人 SPEAKER_03：AI 给出的结果会比眼科医生更可靠。
510 00:51:53,751 --> 00:51:56,695 说话人 SPEAKER_03：所以会有很多这样的东西，你
511 00:51:56,675 --> 00:52:02,545 说话人 SPEAKER_03：AI 在这些医学图像中可以看到比医生更多的东西。
512 00:52:02,565 --> 00:52:03,606 说话人 SPEAKER_03：这将非常有帮助。
513 00:52:04,186 --> 00:52:14,804 说话人 SPEAKER_03：但是我认为，我的意思是，因为我年纪大了，你知道，我已经向 Chuck GPT 问了很多医学问题，因为我就像一辆老车，零件不断掉落。
514 00:52:15,505 --> 00:52:18,289 说话人 SPEAKER_03：非常好。
515 00:52:18,309 --> 00:52:22,496 说话人 SPEAKER_03：我不知道它告诉我的有多少是无意义的，但非常有帮助。
516 00:52:24,467 --> 00:52:29,195 说话人 SPEAKER_14：我们活着就是为了讲述这个故事，所以那里必须有一个好的开始。
517 00:52:29,215 --> 00:52:32,961 说话人 SPEAKER_14：也许这是一个积极的音符，开始考虑观众提问。
518 00:52:33,001 --> 00:52:35,525 说话人 SPEAKER_14：不知道我们能不能把灯光调亮一点，以便我们看得更清楚。
519 00:52:35,585 --> 00:52:39,010 说话人 SPEAKER_03：是的，嗯，这是一个权衡。
520 00:52:39,371 --> 00:52:43,438 说话人 SPEAKER_03：要么他们能看见我们，要么我们能看见他们。
521 00:52:43,458 --> 00:52:45,902 说话人 SPEAKER_14：我们有一些移动麦克风。
522 00:52:45,922 --> 00:52:48,186 说话人 SPEAKER_14: 我认为他们需要这些，这样观众才能看到我们。
523 00:52:50,128 --> 00:52:52,112 说话人 SPEAKER_05: 可能这里会多几个。
524 00:52:54,657 --> 00:52:55,762 说话人 SPEAKER_03: 你会挑选问题吗？
525 00:52:56,324 --> 00:52:56,967 说话人 SPEAKER_14: 好了。
526 00:52:57,027 --> 00:52:57,891 说话人 SPEAKER_14：这才像样。
527 00:52:57,911 --> 00:52:58,574 说话人 SPEAKER_14：太棒了。
528 00:52:58,916 --> 00:53:01,527 说话人 SPEAKER_14：让我们请那位穿检查衬衫的先生站在右边。
529 00:53:01,811 --> 00:53:03,715 说话人 SPEAKER_08：嗨，我叫 Amit。
530 00:53:04,617 --> 00:53:07,483 说话人 SPEAKER_08：我有一个问题，这个问题会比今天任何人提出的问题都要不同。
531 00:53:08,864 --> 00:53:11,469 说话人 SPEAKER_08：我 30 岁，我有一份工作。
532 00:53:11,871 --> 00:53:18,543 说话人 SPEAKER_08：我见过那些事情，比如我开始通过线性回归学习，了解均方误差是如何工作的。
533 00:53:18,724 --> 00:53:22,731 说话人 SPEAKER_08：然后我看到了如何训练网络，那时我读到了关于反向传播的内容。
534 00:53:22,751 --> 00:53:24,715 说话人 SPEAKER_08：这相当有趣和令人惊奇。
535 00:53:24,695 --> 00:53:28,557 说话人 SPEAKER_08：但随着时间的推移，我意识到生活中还有其他事情要做。
536 00:53:28,858 --> 00:53:38,807 说话人 SPEAKER_08：例如，当我看足球比赛时，我浪费了一些本可以用来学习 Transformer 等知识的时间。
537 00:53:39,588 --> 00:53:44,273 说话人 SPEAKER_08：随着时间的推移，我感觉我没有足够的时间去学习和做其他事情。
538 00:53:44,873 --> 00:53:54,702 说话人 SPEAKER_08：我想问你一个问题，在你的生活中，你是否觉得你妥协了很多事情，这些事情本可以花在其他地方？
539 00:53:54,681 --> 00:53:55,663 说话人 SPEAKER_08：是的。
540 00:53:59,565 --> 00:54:00,527 说话人 SPEAKER_08：你对此感到难过吗？
541 00:54:01,027 --> 00:54:05,931 说话人 SPEAKER_03：我最感到难过的是没有花更多的时间陪伴我的孩子。
542 00:54:09,094 --> 00:54:15,099 说话人 SPEAKER_03：但如果你想成为一名优秀的科学家，你必须全身心投入其中。
543 00:54:15,920 --> 00:54:24,688 说话人 SPEAKER_03：我有一种信念，这只是我的信念，你可以擅长你想要真正擅长的任何事情，但只能是一件事。
544 00:54:26,322 --> 00:54:29,605 说话人 SPEAKER_03：所以你可以成为一个非常优秀的父母，或者你可以成为一个非常优秀的科学家。
545 00:54:32,108 --> 00:54:35,150 说话人 SPEAKER_03：但也许只有少数人可以擅长很多事情。
546 00:54:35,431 --> 00:54:41,255 说话人 SPEAKER_03：但对于大多数人来说，对于普通人来说，你必须真正投入到某件事中去，才能真正擅长它。
547 00:54:41,637 --> 00:54:43,418 说话人 SPEAKER_03：这需要很多牺牲。
548 00:54:44,619 --> 00:54:44,920 说话人 SPEAKER_03：抱歉。
549 00:54:46,862 --> 00:54:51,246 说话人 SPEAKER_03：现在，当我还在谷歌工作时，他们总是谈论工作与生活的平衡。
550 00:54:52,967 --> 00:54:56,210 Speaker SPEAKER_03: 我对工作与生活平衡的理解就是工作。
551 00:55:01,894 --> 00:55:02,793 Speaker SPEAKER_06: 相互之间。
552 00:55:05,893 --> 00:55:06,514 Speaker SPEAKER_06: 非常感谢。
553 00:55:06,755 --> 00:55:11,603 Speaker SPEAKER_06: 我是来自 UCD 和 SFI Insight 数据分析中心的 Brian McNamee。
554 00:55:12,525 --> 00:55:13,226 说话人 SPEAKER_06：那真是太令人着迷了。
555 00:55:13,246 --> 00:55:18,675 说话人 SPEAKER_06：我觉得你提到从更大到更大的模型以及越来越多的计算，这很有趣。
556 00:55:19,436 --> 00:55:28,672 说话人 SPEAKER_06：你看到任何有希望的途径来减轻这方面的环境挑战和能源使用问题吗？
557 00:55:28,692 --> 00:55:32,137 说话人 SPEAKER_06：你看到任何可能出现的途径吗？
558 00:55:33,112 --> 00:55:34,695 说话人 SPEAKER_03：有两种不同的趋势。
559 00:55:34,815 --> 00:55:41,005 说话人 SPEAKER_03：一种趋势是让模型更大，即使没有新的科学见解，只要模型更大，数据更多，它就会变得更好。
560 00:55:41,686 --> 00:55:43,969 说话人 SPEAKER_03：这在能耗方面是个坏消息。
561 00:55:44,670 --> 00:55:51,920 说话人 SPEAKER_03：但还有另一种趋势，那就是如果你有一个大模型，你可以将其提炼成一个更小的模型，而这个较小的模型几乎和原来的模型一样好。
562 00:55:52,442 --> 00:55:57,730 说话人 SPEAKER_03：因此，为了服务于这些大型模型，我们可能越来越擅长制作更小的模型。
563 00:55:58,050 --> 00:55:59,592 说话人 SPEAKER_03：现在有很多，
564 00:55:59,572 --> 00:56:03,952 说话人 SPEAKER_03：很多小型模型，其中小型意味着有大约 100 亿个连接。
565 00:56:05,219 --> 00:56:08,132 说话人 SPEAKER_03：这是对小型的新定义。
566 00:56:08,280 --> 00:56:11,125 说话人 SPEAKER_03：这些工作几乎和拥有万亿连接的东西一样好。
567 00:56:12,085 --> 00:56:17,454 说话人 SPEAKER_03：也许，特别是有了更多的科学洞察，我们可以使它们变得稍微小一些。
568 00:56:18,014 --> 00:56:24,023 说话人 SPEAKER_03：但我实际上相信，像 GPT-4 这样的东西知道的东西比人高出数千倍。
569 00:56:24,463 --> 00:56:25,844 说话人 SPEAKER_03：我认为它做不到这一点。
570 00:56:25,885 --> 00:56:30,170 说话人 SPEAKER_03：我认为如果没有非常大量的连接，它不可能拥有那么多的知识。
571 00:56:30,251 --> 00:56:38,039 说话人 SPEAKER_03：所以我不相信只有几百亿个连接的模型能够达到那样的水平。
572 00:56:38,059 --> 00:56:39,766 说话人 SPEAKER_03：在我看来，这似乎是个坏消息。
573 00:56:40,108 --> 00:56:41,291 说话人 SPEAKER_03：更多的太阳能电池板。
574 00:56:51,159 --> 00:56:51,922 说话人 SPEAKER_10: 你好，嗨。
575 00:56:53,103 --> 00:56:54,728 说话人 SPEAKER_10: 感谢您的演讲，非常出色。
576 00:56:54,788 --> 00:56:59,918 说话人 SPEAKER_10: 很遗憾，乔姆斯基教授没有在这里讨论您所提到的某些观点，但非常精彩。
577 00:57:01,722 --> 00:57:09,438 说话人 SPEAKER_10: 我想问您的是，问一个计算机科学家或物理学家，他们总是认为我们很快就会拥有比我们更聪明的计算机。
578 00:57:09,418 --> 00:57:11,443 说话者 SPEAKER_10：或者我们更快地达到那个奇点。
579 00:57:11,503 --> 00:57:15,432 说话者 SPEAKER_10：问一个神经科学家，他们根本不确定这是否会发生。
580 00:57:15,492 --> 00:57:17,376 说话者 SPEAKER_10：我们不明白大脑是如何工作的。
581 00:57:17,737 --> 00:57:18,639 说话者 SPEAKER_10：我们离那还差得远呢。
582 00:57:18,659 --> 00:57:25,655 说话人 SPEAKER_10：有这么完全不同的观点，显然您站在其中一方，但我想听听您对以下内容的看法
583 00:57:25,635 --> 00:57:29,121 说话人 SPEAKER_10：大脑的进化，以及其中存在的许多差异。
584 00:57:29,141 --> 00:57:35,068 说话人 SPEAKER_10：所以问题是，常识是我们认为我们拥有的，而机器目前还没有或可能不会拥有。
585 00:57:35,208 --> 00:57:37,452 说话人 SPEAKER_10：您对常识的看法是什么？
586 00:57:37,731 --> 00:57:38,293 说话人 SPEAKER_10：这意味着什么？
587 00:57:39,074 --> 00:57:41,376 说话人 SPEAKER_10：语言真的能捕捉到一切吗？
588 00:57:41,797 --> 00:57:45,822 说话人 SPEAKER_10：语言能否捕捉到足够的信息，使得这些语言模型能够像我们一样聪明？
589 00:57:46,163 --> 00:57:47,885 说话人 SPEAKER_10：那么生活中其他所有事物呢？
590 00:57:47,905 --> 00:57:50,208 说话人 SPEAKER_03：好吧，所以有很多问题。
591 00:57:50,389 --> 00:57:51,289 说话人 SPEAKER_03：抱歉。
592 00:57:52,090 --> 00:57:55,034 说话人 SPEAKER_03：让我先从，
593 00:57:56,719 --> 00:57:58,442 说话人 SPEAKER_03：我应该先做哪一个？
594 00:58:00,704 --> 00:58:11,538 说话人 SPEAKER_03：让我先从我的个人历史说起，长期以来，我一直认为如果我们让人工神经网络更像真实神经网络，它们就会变得更好。
595 00:58:12,818 --> 00:58:16,583 说话人 SPEAKER_03：而这种信念一直持续到 2023 年初。
596 00:58:16,824 --> 00:58:26,675 说话人 SPEAKER_03：然后我开始相信，也许使用数字计算机反向传播的神经网络
597 00:58:26,739 --> 00:58:29,264 说话人 SPEAKER_03：已经与我们有所不同了。
598 00:58:29,923 --> 00:58:36,514 说话者 SPEAKER_03：它们在以特征和特征之间的交互方式持有知识方面是相似的。
599 00:58:37,315 --> 00:58:46,987 说话者 SPEAKER_03：但它们是不同的，因为你可以有很多相同模型的副本，每个副本都可以学习到一些东西，然后它们可以非常高效地共享，因为它们都以完全相同的方式工作。
600 00:58:47,027 --> 00:58:50,632 说话者 SPEAKER_03：这使得它们在共享方面比我们更好。
601 00:58:51,635 --> 00:58:53,197 说话者 SPEAKER_03：所以它们的功率比我们高得多。
602 00:58:53,237 --> 00:58:56,240 说话者 SPEAKER_03：他们需要更多的能量，但他们在共享方面做得更好。
603 00:58:56,288 --> 00:59:02,396 说话者 SPEAKER_03：我不再相信，如果你让他们更像大脑，它们就会变得更好。
604 00:59:02,856 --> 00:59:10,847 说话者 SPEAKER_03：我认为我们可能已经达到了这样一个点，即大脑的一些方面我们还没有包括进去，比如权重的多个时间尺度。
605 00:59:11,306 --> 00:59:17,755 说话者 SPEAKER_03：在大脑中，对于大脑中的突触来说，它们在许多不同的时间尺度上都会被修改，这一点非常明确。
606 00:59:18,255 --> 00:59:20,980 说话人 SPEAKER_03：在神经网络中，你只有一个时间尺度。
607 00:59:20,960 --> 00:59:24,407 说话人 SPEAKER_03：你有活动的时间尺度，所以你改变输入，活动就会改变。
608 00:59:24,907 --> 00:59:36,612 说话人 SPEAKER_03：你有权重的时间尺度，所以你改变输入，权重保持不变，通过反向传播，你为每个输入稍微改变权重，它们变化很慢，这就是学习。
609 00:59:36,853 --> 00:59:41,583 说话人 SPEAKER_03：但是没有对应短期记忆的中间时间尺度。
610 00:59:42,036 --> 00:59:52,771 说话人 SPEAKER_03：也许这有助于人工神经网络，但基本上我不再相信你们必须使它们更像大脑，然后
611 00:59:53,242 --> 01:00:00,527 说话人 SPEAKER_03：很可能在我们这些事物比我们聪明的时候，我们仍然不会理解大脑，这就是我现在认为的。
612 01:00:01,188 --> 01:00:11,898 说话人 SPEAKER_03：所以我的职业生涯一直在试图理解大脑，并制作人工神经网络来尝试理解大脑是如何学习的，但这失败了，但也产生了一些衍生成果。
613 01:00:18,784 --> 01:00:20,686 说话人 SPEAKER_03：哦，我只是，你问了我这么多问题。
614 01:00:20,706 --> 01:00:22,067 说话人 SPEAKER_03：这是我问的第一个问题。
615 01:00:23,362 --> 01:00:26,572 说话人 SPEAKER_03：下一个问题，是关于仅仅语言是否足够，我认为。
616 01:00:26,614 --> 01:00:27,456 说话人 SPEAKER_03：那是其中一个问题吗？
617 01:00:28,440 --> 01:00:36,545 说话人 SPEAKER_03：不是，但仅仅从语言中你能学到多少是令人惊讶的。
618 01:00:37,505 --> 01:00:40,472 说话人 SPEAKER_03：如果你只有语言，从某种意义上说，它没有根基。
619 01:00:41,797 --> 01:00:48,655 说话人 SPEAKER_03：你学习了所有这些词语之间的关系以及如何预测下一个词语，但从某种意义上说，你并不知道它的含义。
620 01:00:49,677 --> 01:00:53,809 说话人 SPEAKER_03：你必须区分，我认为，模型和模型解释。
621 01:00:53,789 --> 01:01:02,981 说话人 SPEAKER_03：因此，你可以学习一个非常好的模型，这个模型几乎正确地反映了人类理解，但没有解释说这个事物是一只猫。
622 01:01:04,644 --> 01:01:09,612 说话人 SPEAKER_03：但是一旦你学会了模型，学习解释就会很快，只是把它与现实联系起来。
623 01:01:10,112 --> 01:01:13,097 说话人 SPEAKER_03：如果你学习了多模态模型，你似乎就不会有那种问题。
624 01:01:13,077 --> 01:01:17,963 说话人 SPEAKER_03：所以，如果你也做视觉，你就知道猫是什么样子。
625 01:01:18,423 --> 01:01:23,731 说话人 SPEAKER_03：而且很难说，非常难，比如，很难说，我有一个多模态聊天机器人。
626 01:01:24,472 --> 01:01:27,155 说话人 SPEAKER_03：我让它画一只戴着红帽子的仓鼠的画。
627 01:01:27,655 --> 01:01:29,838 说话人 SPEAKER_03：它就画了一只戴着红帽子的仓鼠的画。
628 01:01:30,380 --> 01:01:33,304 说话人 SPEAKER_03：然后说它对这什么都不懂是非常困难的。
629 01:01:34,324 --> 01:01:40,333 说话人 SPEAKER_03：在我看来，如果它画了一只戴着红帽子的仓鼠的画，它就理解了那是什么。
630 01:01:40,583 --> 01:01:48,835 说话人 SPEAKER_03：同样，如果你有一个机器人，你说“打开抽屉”，然后它就打开了抽屉，这对我来说非常接地气，它真的看起来像是理解了。
631 01:01:49,295 --> 01:01:52,240 说话人 SPEAKER_03：所以，我实际上非常相信这些事物是以和我们相同的方式理解的。
632 01:01:53,563 --> 01:02:01,494 说话人 SPEAKER_03：即使是那些没有这种多模态输入的大型语言模型，它们也能用更少的数据理解，如果它们有，理解得更好。
633 01:02:08,393 --> 01:02:12,978 说话人 SPEAKER_09：我手里拿着话筒，我想，但之后我会传给其他人。
634 01:02:15,021 --> 01:02:17,864 说话人 SPEAKER_09：首先，非常感谢您出色的演讲。
635 01:02:19,427 --> 01:02:27,998 说话人 SPEAKER_09：纵观历史，我们一直在适应技术，总会有阻力，社会也会想出各种方法，你知道的，事情不像过去那样。
636 01:02:28,018 --> 01:02:37,911 说话人 SPEAKER_09：但后来它们被接受了，成为不可或缺的一部分，人们几乎对过去表现出一种优越感，说人们害怕火、内燃机、电视。
637 01:02:38,599 --> 01:02:40,782 说话人 SPEAKER_09：那么，人工智能是否不同呢？
638 01:02:40,802 --> 01:02:54,137 说话人 SPEAKER_09：我参与了这里的多个 AI 中心，像伦理 AI 这样的问题，技术的采用实际上比技术的发展更具吸引力。
639 01:02:54,356 --> 01:02:57,780 说话人 SPEAKER_09：如果完全接受，我们可能已经走得更远了。
640 01:02:58,141 --> 01:03:05,568 说话人 SPEAKER_09：有这样一个观点，嗯，给社会足够的时间，人们会接受这项技术并采用它。
641 01:03:05,548 --> 01:03:06,490 说话人 SPEAKER_09：这有什么不同吗？
642 01:03:06,710 --> 01:03:16,467 说话人 SPEAKER_09：我们是否接受这种抵抗将永远存在，永远不会成为像我说的一样，像内燃机甚至互联网那样的一部分？
643 01:03:16,668 --> 01:03:22,356 说话人 SPEAKER_09：这是否是我们必须接受的问题，即这将会是一个需要持续解决的问题？
644 01:03:24,159 --> 01:03:25,242 说话人 SPEAKER_03：我的猜测不是。
645 01:03:25,262 --> 01:03:29,007 说话人 SPEAKER_03：我的猜测是，要么它会接管，要么我们会学会与之共存。
646 01:03:29,242 --> 01:03:41,358 说话人 SPEAKER_03：我看到一个未来，它将接管一切，而我们消失了，还有另一个未来，我们学会与比我们自己更智能的事物互动，并拥有非常好的共生关系。
647 01:03:41,820 --> 01:03:57,342 说话人 SPEAKER_03：所以我的模型是这样的，想象一个拥有一个愚蠢的 CEO 的大公司，这位 CEO 很可能是前任 CEO 的儿子，他有一个非常聪明的助手，几乎肯定是一位女性，比他聪明得多，
648 01:03:57,322 --> 01:03:59,164 说话人 SPEAKER_03：他认为自己在管理公司。
649 01:03:59,505 --> 01:04:02,429 说话人 SPEAKER_03：从某种意义上说，他确实在管理公司，因为他想要的事情得到了完成。
650 01:04:04,032 --> 01:04:06,637 说话人 SPEAKER_03：但实际上，是助手在运营公司。
651 01:04:06,677 --> 01:04:10,443 说话人 SPEAKER_03：实际上，助手对真正发生的事情的了解比他多得多。
652 01:04:11,545 --> 01:04:14,048 说话人 SPEAKER_03：这对那位愚蠢的 CEO 来说会很不错。
653 01:04:14,068 --> 01:04:20,159 说话人 SPEAKER_03：所以我们可能都在扮演那位愚蠢的 CEO 的角色，而拥有更聪明的助手来让一切运转。
654 01:04:20,820 --> 01:04:22,101 说话人 SPEAKER_03：这是另一种未来。
655 01:04:22,282 --> 01:04:24,105 说话人 SPEAKER_03：我认为我们正在接近这两种未来之一。
656 01:04:26,362 --> 01:04:28,967 说话人 SPEAKER_03：但在第二种未来中，我们似乎接受了它。
657 01:04:29,688 --> 01:04:34,034 说话人 SPEAKER_03：我们组织事物，使其对我们有利，与之共存。
658 01:04:36,358 --> 01:04:38,101 说话人 SPEAKER_03：我想再提一个其他意见。
659 01:04:38,963 --> 01:04:42,288 说话人 SPEAKER_03：你提到有人反对电视。
660 01:04:42,608 --> 01:04:45,132 说话人 SPEAKER_03：我认为，我不觉得有很多人反对电视。
661 01:04:45,753 --> 01:04:51,844 说话人 SPEAKER_03：回顾过去，我认为应该有更多的反对声音，因为看看特朗普。
662 01:04:52,184 --> 01:04:53,907 说话人 SPEAKER_03：如果没有电视，他不会在那里。
663 01:05:00,012 --> 01:05:00,291 说话人 SPEAKER_07：你好。
664 01:05:01,474 --> 01:05:01,974 说话人 SPEAKER_07：嗨。
665 01:05:02,094 --> 01:05:05,119 说话人 SPEAKER_07：大卫·霍根，也是谷歌校友。
666 01:05:05,900 --> 01:05:16,217 说话人 SPEAKER_07：我的问题是，您在讨论小组中已经稍微提到了这一点，AI 最初确实是以一种利他主义的研究视角开始的，就像可能的艺术是什么？
667 01:05:17,318 --> 01:05:21,666 说话人 SPEAKER_07：正如我们所见，它在过去几年里得到了显著的发展。
668 01:05:21,646 --> 01:05:25,052 说话人 SPEAKER_07：而且许多公司正在转向一个更加封闭的模式。
669 01:05:26,052 --> 01:05:34,306 说话人 SPEAKER_07：那么您是如何考虑在真正推动创新的同时，也要保持控制之间的平衡的呢？
670 01:05:34,606 --> 01:05:44,824 说话人 SPEAKER_07：我们应该朝着更加封闭的系统、更多的监管发展，还是计算的需求和成本本身就是限制？
671 01:05:45,224 --> 01:05:48,429 说话人 SPEAKER_07：也许应该对通勤功率有更多的控制。
672 01:05:49,269 --> 01:05:59,199 说话人 SPEAKER_03：所以我认为我已经说过，我相信政府应该迫使那些拥有资源的公司投入更多精力来确保安全。
673 01:06:00,960 --> 01:06:02,583 说话人 SPEAKER_03：关于这一点，我就说这么多。
674 01:06:03,744 --> 01:06:10,070 说话人 SPEAKER_03：我认为政府本身可能不会划拨足够的资源来调查这些大型模型。
675 01:06:10,251 --> 01:06:10,971 说话人 SPEAKER_03：也许他们应该这么做。
676 01:06:10,990 --> 01:06:17,757 说话人 SPEAKER_03：我的意思是，也许美国应该认识到理解 AI 及其功能的重要性，
677 01:06:18,835 --> 01:06:22,081 说话人 SPEAKER_03：我们应该有一个与国防预算相当规模的预算。
678 01:06:22,101 --> 01:06:24,786 说话人 SPEAKER_03：我们应该将一半的国防预算投入到那里面。
679 01:06:25,166 --> 01:06:29,193 说话人 SPEAKER_03：然后你提到的是大公司的资金量，可能更多。
680 01:06:30,675 --> 01:06:31,938 说话人 SPEAKER_03：但我认为他们不会这么做。
681 01:06:32,559 --> 01:06:33,842 说话人 SPEAKER_03：他们有太多其他的事情要考虑。
682 01:06:34,724 --> 01:06:36,507 说话人 SPEAKER_03：他们是否应该这样做并不明确。
683 01:06:36,806 --> 01:06:47,106 说话人 SPEAKER_03：我认为最好的角色是尝试迫使大公司更加关注安全，并尝试鼓励聪明的年轻研究人员关注安全。
684 01:06:47,728 --> 01:06:54,461 说话人 SPEAKER_03：特别是需要做的一件事，那就是让聪明的年轻研究人员明白
685 01:06:54,442 --> 01:07:08,396 说话人 SPEAKER_03：从事安全工作不会损害他们的职业生涯，因为直到最近，一些想从事安全工作的人认为，你知道，我可以改进LLM，或者我可以从事安全工作，如果我改进LLM，我的职业生涯会更好。
686 01:07:08,898 --> 01:07:11,403 说话人 SPEAKER_03：我们需要为从事安全工作的人创造良好的职业道路。
687 01:07:11,637 --> 01:07:20,047 说话人 SPEAKER_12：你好，杰弗里，我叫伯纳黛特·博纳蒂，我在 UCD 基金会工作，我觉得你的演讲太棒了，事实上，今晚整个小组都很棒。
688 01:07:20,708 --> 01:07:22,309 说话人 SPEAKER_12：我的问题略有不同。
689 01:07:22,990 --> 01:07:37,728 说话人 SPEAKER_12：在 UCD，我们做了很多关于詹姆斯·乔伊斯的工作，奇怪的是，我们非常关注 AI 与文学，我想了解一下整个美学世界，以及詹姆斯·乔伊斯会对 AI 有何看法。
690 01:07:37,708 --> 01:07:48,661 说话人 SPEAKER_12：我只是想知道您会给像我这样的语言学家什么建议，关于不要担心 AI，就放手去做看看会发生什么。
691 01:07:51,364 --> 01:07:54,407 说话人 SPEAKER_03：我觉得我完全不知所措了。
692 01:07:54,427 --> 01:07:55,248 说话人 SPEAKER_12：这个回答已经足够好了。
693 01:07:57,530 --> 01:08:04,137 说话人 SPEAKER_03：我的意思是，这并不是詹姆斯·乔伊斯的作品，但有一种叫做沃伊尼奇手稿的东西。
694 01:08:05,012 --> 01:08:12,106 说话人 SPEAKER_03：很久以前，一些古董收藏家发现了这个。
695 01:08:12,748 --> 01:08:19,921 说话人 SPEAKER_03：有趣的是，沃伊尼奇娶了乔治·巴尔的幼女，名叫伊瑟尔·沃伊尼奇。
696 01:08:20,783 --> 01:08:25,231 说话人 SPEAKER_03：沃伊尼奇手稿从未被解读过。
697 01:08:25,769 --> 01:08:33,423 说话人 SPEAKER_03：实际上，我让为我工作的人尝试使用 AI 语言模型来尝试理解沃伊尼奇手稿。
698 01:08:33,625 --> 01:08:35,529 说话人 SPEAKER_03：那里文字不够。
699 01:08:36,650 --> 01:08:38,675 说话人 SPEAKER_03：你还得用图片，图片很多。
700 01:08:40,037 --> 01:08:41,260 说话人 SPEAKER_03：我们在这方面从未取得进展。
701 01:08:41,340 --> 01:08:44,166 说话人 SPEAKER_03：我想最终会有人做到的。
702 01:08:44,904 --> 01:08:46,548 说话人 SPEAKER_03：人们不知道这是否是假的。
703 01:08:47,371 --> 01:08:49,576 说话人 SPEAKER_03：很多人都说这是假的，实际上什么都没有。
704 01:08:50,639 --> 01:08:53,645 说话人 SPEAKER_03：这似乎是一篇关于奇异植物及其特性的手稿。
705 01:08:54,247 --> 01:08:55,591 说话人 SPEAKER_12：我们到哪里可以了解这些情况？
706 01:08:55,631 --> 01:08:58,518 说话人 SPEAKER_12：这听起来像是一个出色的研究项目。
707 01:08:58,858 --> 01:09:01,664 说话人 SPEAKER_03：只需查找沃伊尼奇手稿。
708 01:09:03,382 --> 01:09:16,884 说话人 SPEAKER_03：它有一些东西表明它是真实的，你知道什么是 Zipf 定律，那就是常见的事物和罕见的事物，它们以特定的方式减少，它们的频率以特定的方式减少，并且遵循 Zipf 定律。
709 01:09:16,904 --> 01:09:22,252 说话人 SPEAKER_03：他们在制作时并不知道 Zipf 定律，这使得它不太可能是伪造的。
710 01:09:23,194 --> 01:09:37,398 说话人 SPEAKER_03：我已经没有仔细阅读过任何文献，但我在新闻推送中看到说现在 AI 开始解读这些非常古老的石板。
711 01:09:38,179 --> 01:09:39,560 说话人 SPEAKER_03：我想这会很多。
712 01:09:40,002 --> 01:09:41,725 说话人 SPEAKER_03：甚至可能解读芬尼根的觉醒。
713 01:09:42,525 --> 01:09:42,987 说话人 SPEAKER_12：没错。
714 01:09:43,186 --> 01:09:44,368 说话人 SPEAKER_03：我们刚才正在想那件事。
715 01:09:45,952 --> 01:09:46,231 说话人 SPEAKER_12：谢谢。
716 01:09:48,337 --> 01:10:02,483 说话人 SPEAKER_04：我想听听您在谷歌的更多时间，因为当我了解到人工智能时，我真的是通过一直使用谷歌提问和获取信息来了解的。
717 01:10:03,144 --> 01:10:11,257 说话人 SPEAKER_04：以及您如何看待其未来的发展，以及它是否因为英伟达而受到一些限制。
718 01:10:11,238 --> 01:10:18,626 说话人 SPEAKER_04：能否进入所有主流 GPU 的能力，以及它是否会商业化发展。
719 01:10:20,188 --> 01:10:21,009 说话人 SPEAKER_03：那么问题是什么？
720 01:10:21,069 --> 01:10:23,971 说话人 SPEAKER_03：NVIDIA 是否会取代谷歌？
721 01:10:23,992 --> 01:10:29,898 说话人 SPEAKER_04：是的，或者微软，所有商业化的发展都将在这里。
722 01:10:30,698 --> 01:10:34,984 说话人 SPEAKER_03：嗯，我有一些非常好的建议给你，那就是三年前买下英伟达。
723 01:10:36,331 --> 01:10:36,771 说话人 SPEAKER_03：我买了。
724 01:10:38,014 --> 01:10:38,373 说话人 SPEAKER_04：谢谢。
725 01:10:39,295 --> 01:10:40,676 说话人 SPEAKER_04：我在寻找下一个阶段。
726 01:10:41,859 --> 01:10:42,940 说话人 SPEAKER_03: 好吧。
727 01:10:43,921 --> 01:10:51,713 说话人 SPEAKER_03: 嗯，我女儿三年前买了一块 Nvidia 显卡，我劝她卖掉一些，换成 Intel，因为这样有更大的上涨空间。
728 01:10:52,194 --> 01:10:54,457 说话人 SPEAKER_03: 从那以后，Intel 下跌了 10%。
729 01:10:56,057 --> 01:11:02,108 说话人 SPEAKER_03: 谷歌会继续存在。
730 01:11:02,488 --> 01:11:06,475 说话人 SPEAKER_04：我对谷歌充满信心，至今依然如此。
731 01:11:07,237 --> 01:11:12,987 说话人 SPEAKER_03：很久以前，像我这样的人会说，你不想看到
732 01:11:12,966 --> 01:11:17,957 说话人 SPEAKER_03：只是一堆你点击的文档，可能最上面的一个点击是不错的选择。
733 01:11:18,438 --> 01:11:24,630 说话人 SPEAKER_03：你希望谷歌能够通过查看文档来回答你的问题，这正是 GPT-4 现在所做的事情。
734 01:11:25,452 --> 01:11:30,201 说话人 SPEAKER_03: 德国似乎正在这样做。
735 01:11:31,703 --> 01:11:34,890 说话人 SPEAKER_03: 这就是一切的发展方向，我认为谷歌也会参与其中。
736 01:11:35,814 --> 01:11:36,155 说话人 SPEAKER_05: 谢谢。
737 01:11:36,176 --> 01:11:38,658 说话人 SPEAKER_05: 我想我们这里还有时间再问一个问题。
738 01:11:39,079 --> 01:11:43,886 说话人 SPEAKER_01: 非常感谢您今晚的精彩演讲，能在这里见到您真是莫大的荣幸。
739 01:11:44,368 --> 01:11:53,421 说话人 SPEAKER_01: 我是来自 UCD 计算机科学学院的 Fatemeh，我想知道您认为人工智能会如何改变社会的价值观？
740 01:11:53,481 --> 01:11:59,251 说话人 SPEAKER_01: 您认为人工智能会引领并改变价值观、规范性行为和文化变革吗？
741 01:11:59,530 --> 01:12:03,476 说话人 SPEAKER_03: 我有一个关于这个问题的想法，相当令人恐惧。
742 01:12:04,097 --> 01:12:10,444 Speaker SPEAKER_03: 在工业革命之前，如果你肌肉发达，擅长挖沟，那你就是有价值的。
743 01:12:11,225 --> 01:12:16,770 发言人 SPEAKER_03：而在工业革命之后，你们就不再需要人工了，因为机器可以做得更好。
744 01:12:17,990 --> 01:12:30,443 说话者 SPEAKER_03：我很担心，目前普通智商和普通教育水平的人被重视，因为有很多他们能做的事情是有用的。
如果 AI 能以更优、更便宜的方式完成这些事情，它们就不会被重视。
746 01:12:35,189 --> 01:12:37,694 说话者 SPEAKER_03：在我看来，这对社会来说非常危险。
747 01:12:38,976 --> 01:12:41,121 说话者 SPEAKER_03：我认为这是一个很大的担忧。
748 01:12:43,024 --> 01:12:43,324 说话者 SPEAKER_03：对不起。
749 01:12:45,989 --> 01:12:48,414 说话者 SPEAKER_05：也许我们再回答一个问题，这样我们就不以那个话题结束。
750 01:12:55,769 --> 01:13:03,045 说话人 SPEAKER_11：如果你能明天开一辆德洛瑞安回到过去，你想见谁？
751 01:13:03,085 --> 01:13:09,800 说话人 SPEAKER_11：图灵、冯·诺伊曼、赫贝尔、威塞尔、罗森布拉特，你将和他们聊些什么？
752 01:13:09,819 --> 01:13:11,262 说话人 SPEAKER_11：我想见亥姆霍兹。
753 01:13:12,425 --> 01:13:13,587 说话人 SPEAKER_03：我也想见亥姆霍兹。
754 01:13:13,606 --> 01:13:16,310 说话人 SPEAKER_03：赫尔姆霍茨相信无意识的感知推理。
755 01:13:16,992 --> 01:13:23,462 说话人 SPEAKER_03：他基本上认为在做视觉工作时，你做了很多推理，但这并不是有意识的、故意的推理。
756 01:13:24,222 --> 01:13:31,614 说话人 SPEAKER_03：你是从近端刺激中进行推理，推理出导致这种状态的世界状态。
757 01:13:31,679 --> 01:13:33,064 说话人 SPEAKER_03：他在这一点上显然是正确的。
758 01:13:33,765 --> 01:13:35,573 说话人 SPEAKER_03：这就是赫尔姆霍兹工作的一个分支。
759 01:13:36,175 --> 01:13:40,730 说话人 SPEAKER_03：赫尔姆霍兹工作的另一个分支是自由能。
760 01:13:40,770 --> 01:13:43,038 说话人 SPEAKER_03：有一种叫做赫尔姆霍兹自由能的东西。
761 01:13:43,372 --> 01:14:00,917 说话人 SPEAKER_03：而赫尔姆霍兹没有意识到的是，如果你在一个具有复杂潜在变量的复杂模型中进行推理，统计学家会告诉你，给定你模型的当前参数，用数据找出模型解释数据的最佳方式来进行推理。
762 01:14:01,418 --> 01:14:12,975 说话人 SPEAKER_03：你的模型可能有几种解释数据的方法，但你必须找出这些方法中模型解释数据的概率，以便调整模型的参数。
763 01:14:13,359 --> 01:14:17,904 说话人 SPEAKER_03：变分推断实际上说的是，你不必准确得到那个概率。
764 01:14:18,485 --> 01:14:22,189 说话人 SPEAKER_03：你可以近似那个概率，同时仍然保证你会学习到一个更好的模型。
765 01:14:23,069 --> 01:14:24,692 说话人 SPEAKER_03：或者说，有一些带宽会得到改善。
766 01:14:25,552 --> 01:14:34,422 说话人 SPEAKER_03：因此，海姆霍兹职业生涯中的两个截然不同的部分——对自由能的研究和对感知推理的研究，实际上是非常紧密相关的。
767 01:14:34,563 --> 01:14:38,787 说话人 SPEAKER_03：自由能是进行感知推理的关键。
768 01:14:39,387 --> 01:14:41,189 说话人 SPEAKER_03：我很想告诉海姆霍兹这一点。
769 01:14:56,137 --> 01:15:09,212 说话人 SPEAKER_05：今晚非常感谢辛顿教授加入我们，并分享了他对从水门事件到海姆霍兹再到几乎所有其他话题的看法，包括给我们的一些股票建议。
770 01:15:10,493 --> 01:15:14,118 说话人 SPEAKER_05：我认为大家都会同意，聆听他的观点是一种荣幸。
771 01:15:14,418 --> 01:15:17,922 说话人 SPEAKER_05：再次，让我们向 Hinton 教授表示祝贺。
772 01:15:18,484 --> 01:15:19,664 说话人 SPEAKER_05：感谢大家的出席。

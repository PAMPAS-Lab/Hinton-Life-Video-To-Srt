1
00:00:00,031 --> 00:00:04,216
Speaker SPEAKER_01: Well, the man known as the Godfather of AI is in the Canada Tonight spotlight.

2
00:00:04,237 --> 00:00:05,177
Speaker SPEAKER_01: That's Geoffrey Hinton.

3
00:00:05,299 --> 00:00:11,868
Speaker SPEAKER_01: And today the White House Artificial Intelligence Council met to discuss new measures aimed at managing the risks of AI.

4
00:00:12,349 --> 00:00:13,771
Speaker SPEAKER_01: Something Hinton has been warning about.

5
00:00:14,131 --> 00:00:21,481
Speaker SPEAKER_01: Among other things, officials say developers of powerful AI systems will have to report safety test results to the government.

6
00:00:21,461 --> 00:00:27,048
Speaker SPEAKER_01: This comes three months after President Biden signed an executive order on AI regulation.

7
00:00:27,528 --> 00:00:30,513
Speaker SPEAKER_01: AI regulation is also a priority for the Canadian government.

8
00:00:30,952 --> 00:00:36,420
Speaker SPEAKER_01: I spoke in detail about safety and security of artificial intelligence systems with Geoffrey Hinton.

9
00:00:36,859 --> 00:00:42,406
Speaker SPEAKER_01: The British-Canadian scientist who's been called the godfather of AI again is in our spotlight.

10
00:00:43,450 --> 00:00:45,332
Speaker SPEAKER_01: And Geoffrey Hinton joins me now.

11
00:00:45,371 --> 00:00:46,012
Speaker SPEAKER_01: Good to see you.

12
00:00:46,253 --> 00:00:47,033
Speaker SPEAKER_01: Thank you for doing this.

13
00:00:47,094 --> 00:00:47,694
Speaker SPEAKER_01: Appreciate it.

14
00:00:47,973 --> 00:00:48,795
Speaker SPEAKER_01: Thank you for inviting me.

15
00:00:48,954 --> 00:00:53,859
Speaker SPEAKER_01: You were never a doom and gloom guy when it came to AI.

16
00:00:54,420 --> 00:00:55,421
Speaker SPEAKER_01: When did that all change?

17
00:00:56,482 --> 00:01:03,009
Speaker SPEAKER_00: So for about 50 years, I thought getting super intelligent AI was way in the future.

18
00:01:04,109 --> 00:01:08,174
Speaker SPEAKER_00: The beginning of 2023, I began to think it was much closer than I thought before.

19
00:01:09,114 --> 00:01:12,799
Speaker SPEAKER_00: And I began to get nervous about what would happen when AI was smarter than people.

20
00:01:13,387 --> 00:01:17,412
Speaker SPEAKER_01: And when do you think we are going to reach that point and what do you think will happen?

21
00:01:18,274 --> 00:01:22,719
Speaker SPEAKER_00: I think there's probably about a 50% chance we'll get to that point sometime in the next 20 years.

22
00:01:24,102 --> 00:01:26,025
Speaker SPEAKER_00: And then nobody really knows what will happen.

23
00:01:26,506 --> 00:01:27,686
Speaker SPEAKER_00: There's huge uncertainty.

24
00:01:28,028 --> 00:01:31,212
Speaker SPEAKER_00: We've never had to face before things more intelligent than us.

25
00:01:31,673 --> 00:01:32,974
Speaker SPEAKER_00: And we just don't know what will happen.

26
00:01:33,474 --> 00:01:36,019
Speaker SPEAKER_00: And that's why I think we ought to be cautious if we can.

27
00:01:36,539 --> 00:01:41,704
Speaker SPEAKER_00: But in particular, we ought to figure out how we can stop them ever wanting to take control.

28
00:01:42,225 --> 00:01:46,370
Speaker SPEAKER_00: Because if they're much more intelligent than us, if they wanted to take control, they'd be able to.

29
00:01:47,311 --> 00:01:52,376
Speaker SPEAKER_00: You can't have a big switch to turn them off because they'll be able to make arguments to convince people.

30
00:01:52,417 --> 00:01:53,799
Speaker SPEAKER_00: They'll be very good at convincing us.

31
00:01:54,700 --> 00:02:00,686
Speaker SPEAKER_00: And so much like Trump could invade the Capitol without ever going there, they'll be able to persuade people to do whatever they want.

32
00:02:01,507 --> 00:02:06,293
Speaker SPEAKER_00: And so we have to somehow make sure that they're doing what we want.

33
00:02:06,796 --> 00:02:09,284
Speaker SPEAKER_01: You know, to a lot of people, this seems like science fiction.

34
00:02:09,384 --> 00:02:11,072
Speaker SPEAKER_01: It seems far-fetched.

35
00:02:11,653 --> 00:02:18,358
Speaker SPEAKER_01: How do you convince the masses that this is a real issue that needs to be dealt with right now?

36
00:02:18,912 --> 00:02:30,829
Speaker SPEAKER_00: Okay, so one way to help convince people is just play with an advanced chatbot like GPT-4 or Gemini, and they're very smart.

37
00:02:32,191 --> 00:02:41,104
Speaker SPEAKER_00: Some people say they're just stochastic parrots, they're just trying to predict the next word, they're just using cheap statistical tricks, but actually they understand what you're saying.

38
00:02:42,044 --> 00:02:47,012
Speaker SPEAKER_00: And you can see that by giving them little problems to solve that you couldn't solve without understanding.

39
00:02:47,347 --> 00:03:00,826
Speaker SPEAKER_00: And in fact, the large language models that are behind these chatbots, they're just a large version of smaller models that were designed a long time ago in order to explain how the brain processes language.

40
00:03:01,668 --> 00:03:03,850
Speaker SPEAKER_00: So it's not that they're doing it a completely different way from us.

41
00:03:03,911 --> 00:03:06,033
Speaker SPEAKER_00: They're much more like us than most people think.

42
00:03:07,616 --> 00:03:10,561
Speaker SPEAKER_01: What do you say to the people that think that you are wrong on this?

43
00:03:11,742 --> 00:03:12,544
Speaker SPEAKER_00: That they're wrong.

44
00:03:13,080 --> 00:03:13,561
Speaker SPEAKER_01: that they're wrong.

45
00:03:13,822 --> 00:03:14,682
Speaker SPEAKER_00: Yes.

46
00:03:14,703 --> 00:03:15,302
Speaker SPEAKER_01: Plain and simple.

47
00:03:15,423 --> 00:03:16,443
Speaker SPEAKER_00: This is how science works.

48
00:03:16,704 --> 00:03:18,026
Speaker SPEAKER_00: You have a diversity of opinions.

49
00:03:18,686 --> 00:03:21,729
Speaker SPEAKER_00: In the end, one opinion works out if you follow the evidence.

50
00:03:22,449 --> 00:03:28,795
Speaker SPEAKER_00: And the evidence of present is moving more and more towards the idea that these things really are going to get super intelligent quite soon.

51
00:03:29,317 --> 00:03:31,217
Speaker SPEAKER_01: Is there a benefit to AI?

52
00:03:31,359 --> 00:03:41,247
Speaker SPEAKER_01: And how do we kind of manage what you're saying could potentially happen with all of the benefits that do exist right now?

53
00:03:41,312 --> 00:03:41,592
Speaker SPEAKER_00: Yes.

54
00:03:42,354 --> 00:03:43,855
Speaker SPEAKER_00: It's not like nuclear weapons.

55
00:03:43,915 --> 00:03:47,199
Speaker SPEAKER_00: With nuclear weapons, they could only do bad things, essentially.

56
00:03:48,241 --> 00:03:51,885
Speaker SPEAKER_00: And with AI, it can do a lot of wonderful things.

57
00:03:52,526 --> 00:04:01,657
Speaker SPEAKER_00: For example, in health care, you'd much rather see a doctor who'd already seen 10 million patients, many of whom were just like you, than you'd see your current family doctor.

58
00:04:02,198 --> 00:04:10,627
Speaker SPEAKER_00: Especially if the doctor could use all the information about you, like your genome, your tests, all the medical notes about you, your family history.

59
00:04:10,608 --> 00:04:15,074
Speaker SPEAKER_00: the history of your relatives, you get much better diagnoses.

60
00:04:15,575 --> 00:04:21,704
Speaker SPEAKER_00: In the United States, for example, about 200,000 people a year die of wrong medical diagnoses.

61
00:04:22,826 --> 00:04:29,658
Speaker SPEAKER_00: We could make that more or less go away, we could reduce that number by a whole lot, and that will happen probably in the next 10 years.

62
00:04:31,459 --> 00:04:35,687
Speaker SPEAKER_00: So for that reason, people are not going to halt the development of AI.

63
00:04:35,666 --> 00:04:45,187
Speaker SPEAKER_00: And there's lots of other things like that, designing new drugs, making better materials for things like solar panels, understanding climate change better so we can slow it down.

64
00:04:45,949 --> 00:04:53,726
Speaker SPEAKER_01: Do you think companies like Google, like Facebook, all the big technology companies are being responsible right now?

65
00:04:54,262 --> 00:04:57,627
Speaker SPEAKER_00: I don't think Facebook's being responsible.

66
00:04:59,670 --> 00:05:10,528
Speaker SPEAKER_00: Facebook recently removed the restriction on having advertisements that claim that Biden is not really the president because that Biden didn't really win the election.

67
00:05:10,548 --> 00:05:13,752
Speaker SPEAKER_00: And they had a prohibition on advertisements that said that because it's obviously false.

68
00:05:14,353 --> 00:05:18,600
Speaker SPEAKER_00: They've removed that prohibition and they've removed it just before the next election.

69
00:05:19,492 --> 00:05:23,081
Speaker SPEAKER_01: And so, you know, talking about the next election, we are in 2024.

70
00:05:24,545 --> 00:05:34,750
Speaker SPEAKER_01: How concerned are you about AI and deepfake videos and AI's influence on this next election in the United States and also in other countries?

71
00:05:34,730 --> 00:05:36,132
Speaker SPEAKER_00: I'm very concerned about it.

72
00:05:36,451 --> 00:05:41,697
Speaker SPEAKER_00: And it's a much more urgent threat than the much longer term threat of super intelligent AI taking over.

73
00:05:42,119 --> 00:05:47,043
Speaker SPEAKER_00: The urgent threat is to do with fake videos being used to corrupt elections.

74
00:05:47,584 --> 00:05:52,149
Speaker SPEAKER_00: Fake videos targeted at the particular prejudices of the people they're targeted at.

75
00:05:53,350 --> 00:05:54,872
Speaker SPEAKER_00: I think that's a very serious threat.

76
00:05:55,514 --> 00:06:03,403
Speaker SPEAKER_00: And I don't think in the United States we're going to get legislation that forces people to mark

77
00:06:03,382 --> 00:06:11,098
Speaker SPEAKER_00: AI-generated videos as AI-generated because one of the main political parties is kind of committed to this lie that Trump won the election.

78
00:06:11,449 --> 00:06:13,072
Speaker SPEAKER_01: And so how do you deal with this then?

79
00:06:13,093 --> 00:06:15,817
Speaker SPEAKER_01: What is your suggestion to lawmakers in the United States?

80
00:06:17,279 --> 00:06:18,903
Speaker SPEAKER_00: I don't know what they're going to do about that.

81
00:06:19,043 --> 00:06:25,014
Speaker SPEAKER_00: Obviously, one party would like to mark fake videos as fake, and the other one wouldn't.

82
00:06:25,995 --> 00:06:28,439
Speaker SPEAKER_00: I don't have any comments about how they could deal with that.

83
00:06:28,720 --> 00:06:35,432
Speaker SPEAKER_00: In other countries, people are trying to make legislation so you mark fake videos as fake.

84
00:06:36,019 --> 00:06:42,875
Speaker SPEAKER_01: Here in Canada, I know that you recently, a couple of months ago, had a conversation, a dinner with the Prime Minister, Justin Trudeau.

85
00:06:43,476 --> 00:06:48,088
Speaker SPEAKER_01: Tell me a little bit about that and what you told him related to AI.

86
00:06:48,726 --> 00:06:51,290
Speaker SPEAKER_00: So I told him the usual things I say.

87
00:06:51,310 --> 00:06:58,661
Speaker SPEAKER_00: I was impressed by the fact he knew some mathematics and so I could be a bit more technical about some of the things than you normally can.

88
00:06:59,702 --> 00:07:02,146
Speaker SPEAKER_00: He was very interested to actually understand.

89
00:07:03,928 --> 00:07:13,822
Speaker SPEAKER_00: We talked a little bit about AI policy and how in order to keep up with the forefront of AI, you needed vast amounts of computation.

90
00:07:13,802 --> 00:07:18,574
Speaker SPEAKER_00: And so we talked a bit about how researchers in Canada might be able to get large amounts of computation.

91
00:07:20,559 --> 00:07:23,865
Speaker SPEAKER_00: But mostly he wanted to understand what was going to happen.

92
00:07:24,708 --> 00:07:26,833
Speaker SPEAKER_00: Nobody really knows what was going to happen.

93
00:07:27,394 --> 00:07:31,103
Speaker SPEAKER_01: And so when he asked you that question, what did you say?

94
00:07:31,370 --> 00:07:37,485
Speaker SPEAKER_00: Well, I talked about all the various threats of AI and how the fixes for the different threats are all somewhat different.

95
00:07:37,865 --> 00:07:39,067
Speaker SPEAKER_00: There's not one simple solution.

96
00:07:39,428 --> 00:07:43,298
Speaker SPEAKER_00: It's not like climate change where there's a simple solution, just stop burning carbon.

97
00:07:43,779 --> 00:07:45,845
Speaker SPEAKER_00: If you do that, eventually things will be OK.

98
00:07:45,865 --> 00:07:48,129
Speaker SPEAKER_00: There isn't a simple solution for the AI threats.

99
00:07:48,362 --> 00:07:58,440
Speaker SPEAKER_01: And so how do individual countries or do individual countries try to deal with this separately or is this a global problem where really nations need to come together?

100
00:08:00,725 --> 00:08:10,422
Speaker SPEAKER_00: Certainly the existential threat of AI taking over is a global problem where nations actually should be able to come together because no nation wants the AIs to take over.

101
00:08:10,401 --> 00:08:24,807
Speaker SPEAKER_00: So we can all agree to collaborate on stopping that happening, much as the Soviet Union and the United States at the height of the Cold War could collaborate on not having a global thermonuclear war, because that wasn't in either of their interests.

102
00:08:25,569 --> 00:08:31,920
Speaker SPEAKER_00: So that's one area where we can collaborate, but in other areas it's not nearly so clear.

103
00:08:32,744 --> 00:08:39,918
Speaker SPEAKER_01: Somebody else that you had a lengthy conversation with was Elon Musk, who has in the past warned about the dangers of AI.

104
00:08:41,760 --> 00:08:47,832
Speaker SPEAKER_01: What was that conversation like, and do you think that you're aligned when it comes to this one issue?

105
00:08:47,812 --> 00:08:54,241
Speaker SPEAKER_00: On that issue of whether AI is going to get smarter than us, Elon Musk and I completely agree.

106
00:08:54,923 --> 00:08:56,205
Speaker SPEAKER_00: He's been saying it for longer than me.

107
00:08:56,926 --> 00:09:02,813
Speaker SPEAKER_00: And we both think it's obvious that AI will get smarter than us.

108
00:09:03,235 --> 00:09:09,484
Speaker SPEAKER_00: A long time ago, in the 1950s, Alan Turing, who was the father of a lot of computer science,

109
00:09:09,464 --> 00:09:20,030
Speaker SPEAKER_00: He talked about neural networks and training them and he just sort of dismissed it as completely obvious that once we knew how to make them intelligent they'd quickly get smarter than us.

110
00:09:20,904 --> 00:09:28,578
Speaker SPEAKER_01: What is your prediction here for the next five years as this technology gets better?

111
00:09:29,379 --> 00:09:34,629
Speaker SPEAKER_00: So the next five years, my prediction is it probably won't get smarter than this in the next five years.

112
00:09:34,669 --> 00:09:44,888
Speaker SPEAKER_00: What we'll see is a series of chatbots like GPT-5 and Gemini and so on that get progressively better and better and can do much more.

113
00:09:44,868 --> 00:09:46,931
Speaker SPEAKER_00: and can be intelligent assistants.

114
00:09:47,511 --> 00:10:03,956
Speaker SPEAKER_00: So a good scenario is that we could all have our own intelligent assistant that was much smarter than us, that knew all about us, that knew what we wanted, could negotiate with other intelligent assistants, you know, have my assistant talk to your assistant.

115
00:10:03,937 --> 00:10:10,264
Speaker SPEAKER_00: and would be benevolent in the sense they would be trying to achieve the goals of the person they were assisting.

116
00:10:10,663 --> 00:10:19,232
Speaker SPEAKER_00: Much like a big company run by a dumb CEO, maybe the son of the previous CEO, with a very smart assistant to actually make things work.

117
00:10:20,014 --> 00:10:22,956
Speaker SPEAKER_00: That's the good scenario, and it's still possible we could achieve that.

118
00:10:24,077 --> 00:10:25,980
Speaker SPEAKER_01: What do you think is at stake here?

119
00:10:27,260 --> 00:10:28,241
Speaker SPEAKER_00: The future of humanity.

120
00:10:28,722 --> 00:10:29,864
Speaker SPEAKER_01: The future of humanity?

121
00:10:30,004 --> 00:10:30,163
Speaker SPEAKER_00: Yeah.

122
00:10:31,044 --> 00:10:32,046
Speaker SPEAKER_01: You don't think that's hyperbole?

123
00:10:32,687 --> 00:10:33,807
Speaker SPEAKER_00: No, I don't actually, no.

124
00:10:34,109 --> 00:10:37,894
Speaker SPEAKER_01: And so, you know, when people... I think there's several things like that.

125
00:10:38,395 --> 00:10:45,388
Speaker SPEAKER_00: Obviously, global thermonuclear war, climate change, and superintelligent AI taking over.

126
00:10:47,652 --> 00:10:50,716
Speaker SPEAKER_01: It's pretty incredible.

127
00:10:50,932 --> 00:10:57,364
Speaker SPEAKER_01: Moving forward, how do you think we really, as a society, need to deal with this?

128
00:10:57,524 --> 00:11:00,809
Speaker SPEAKER_01: And what is the urgency to deal with it right now?

129
00:11:00,909 --> 00:11:04,515
Speaker SPEAKER_01: Because as you've said, there's a lot going on in the world right now.

130
00:11:05,057 --> 00:11:07,059
Speaker SPEAKER_01: There's a housing crisis in this country.

131
00:11:07,100 --> 00:11:08,282
Speaker SPEAKER_01: There are wars going on.

132
00:11:09,924 --> 00:11:14,072
Speaker SPEAKER_01: What is the priority of putting AI at the forefront here in terms of things that we have to deal with?

133
00:11:14,136 --> 00:11:18,605
Speaker SPEAKER_00: So I think the most urgent priority is corrupting elections.

134
00:11:18,927 --> 00:11:22,855
Speaker SPEAKER_00: I think we're going to see a lot of that over the next year and there's a lot of major elections happening this year.

135
00:11:23,917 --> 00:11:28,547
Speaker SPEAKER_00: But the long term, I think the most important priority, that's not going to destroy humanity.

136
00:11:28,606 --> 00:11:33,376
Speaker SPEAKER_00: It may take us back to fascism, but it's not going to destroy humanity.

137
00:11:33,846 --> 00:11:40,777
Speaker SPEAKER_00: So in the long term, the most important thing is to figure out how to prevent these things wanting to take over.

138
00:11:41,278 --> 00:11:51,732
Speaker SPEAKER_00: If you assume they're going to get more intelligent than us, and I think most AI researchers, they differ on the time scales, but most AI researchers believe that in the long run they'll get smarter than us.

139
00:11:51,812 --> 00:11:54,437
Speaker SPEAKER_01: So you don't think we've reached the point of no return?

140
00:11:54,417 --> 00:11:57,221
Speaker SPEAKER_00: We haven't reached the point where they're smarter than us yet.

141
00:11:57,761 --> 00:12:08,235
Speaker SPEAKER_00: We may have reached the point of no return in that AI research isn't going to be stopped because of all the good things it can do, like in medicine, like with intelligent assistants, like designing drugs.

142
00:12:09,277 --> 00:12:14,804
Speaker SPEAKER_00: So because it's got so many good uses, I don't think there's any chance of halting the research.

143
00:12:14,784 --> 00:12:17,210
Speaker SPEAKER_00: And that research is going to lead to super intelligent things.

144
00:12:17,811 --> 00:12:25,024
Speaker SPEAKER_00: So I think what I'm doing is encouraging smart young researchers to work on the problem of how do we stop it wanting to take control.

145
00:12:26,225 --> 00:12:26,326
Speaker SPEAKER_01: Okay.

146
00:12:26,346 --> 00:12:28,009
Speaker SPEAKER_01: Geoffrey Hinton, appreciate you being here.

147
00:12:28,711 --> 00:12:28,991
Speaker SPEAKER_00: Thank you.

148
00:12:29,392 --> 00:12:29,773
Speaker SPEAKER_00: Thank you.


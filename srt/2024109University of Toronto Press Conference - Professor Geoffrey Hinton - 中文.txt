1 00:00:16,129 --> 00:00:17,030 主持人 SPEAKER_01：大家好。
2 00:00:17,050 --> 00:00:23,638 主持人 SPEAKER_01：我叫梅里克·格特勒，很荣幸担任多伦多大学校长。
3 00:00:25,080 --> 00:00:33,752 主持人 SPEAKER_01：我们来自世界各地，在线聚集一堂，以表彰和庆祝荣誉退休教授、2024 年诺贝尔物理学奖获得者杰弗里·辛顿。
4 00:00:34,973 --> 00:00:42,323 主持人 SPEAKER_01：杰弗里·辛顿在国际上被誉为人工神经网络和深度学习的先驱，备受认可和尊敬。
5 00:00:42,909 --> 00:00:54,841 说话人 SPEAKER_01：他的多学科研究项目不仅与人工智能和机器学习相关，还与物理学、认知心理学、神经生物学、数学优化和信息理论相关。
6 00:00:56,164 --> 00:01:00,027 说话人 SPEAKER_01：Hinton 教授对多个领域和学科产生了深远的影响。
7 00:01:00,927 --> 00:01:04,591 说话人 SPEAKER_01：他实际上创造了全新的思考和学习方式。
8 00:01:06,013 --> 00:01:10,317 说话人 SPEAKER_01：他和他的学生开发的算法具有惊人的应用范围。
9 00:01:10,787 --> 00:01:18,641 说话人 SPEAKER_01：他们奠定了数据处理和高级发现能力的基础，这些能力如今被广泛而令人惊讶地使用。
10 00:01:18,701 --> 00:01:34,989 说话人 SPEAKER_01：Hinton 教授的基础性贡献及其深远影响，使他获得了全球范围内的广泛学术赞誉和更广泛的公众认可，以至于他经常被称为人工智能之父。
11 00:01:36,742 --> 00:01:46,706 说话人 SPEAKER_01：Hinton 教授在他辉煌的学术生涯中，有大约三十年是在多伦多大学度过的，他在 2006 年被授予大学教授称号，这是我们的最高学术荣誉。
12 00:01:46,766 --> 00:01:53,180 说话人 SPEAKER_01：这很大程度上归功于他的领导力和对年轻学者的典范式指导，
13 00:01:53,447 --> 00:01:57,114 主持人 SPEAKER_01：多伦多大学已成为机器学习和人工智能领域的全球领导者。
14 00:01:57,894 --> 00:02:09,633 主持人 SPEAKER_01：这包括人工智能的伦理后果和社会影响，这是一个教授 Hinton 最近提升的话题，帮助世界关注这些重要问题。
15 00:02:10,875 --> 00:02:20,209 主持人 SPEAKER_01：我很荣幸和高兴地介绍荣誉退休大学教授、2024 年诺贝尔物理学奖获得者杰弗里·辛顿教授。
16 00:02:20,870 --> 00:02:22,391 主持人 SPEAKER_01：辛顿教授，欢迎您。
17 00:02:23,451 --> 00:02:24,173 说话人 SPEAKER_02: 非常感谢。
18 00:02:24,932 --> 00:02:26,455 说话人 SPEAKER_02: 我还是有点震惊。
19 00:02:26,794 --> 00:02:29,698 说话人 SPEAKER_02: 我在加利福尼亚凌晨一点接到电话。
20 00:02:30,557 --> 00:02:33,721 说话人 SPEAKER_02: 我想过是否该接这个电话。
21 00:02:33,760 --> 00:02:36,183 说话人 SPEAKER_02：幸运的是，我决定看看是谁打的电话。
22 00:02:36,903 --> 00:02:41,367 说话人 SPEAKER_02：我非常惊讶获得了诺贝尔物理学奖。
23 00:02:41,868 --> 00:02:43,129 说话人 SPEAKER_02：我从未想过这件事。
24 00:02:44,230 --> 00:02:53,239 说话人 SPEAKER_02：我认为这个奖项是对一个长期致力于神经网络研究、最终取得显著成果的庞大团队的认可。
25 00:02:53,388 --> 00:03:00,956 主持人 SPEAKER_02：我特别想感谢我的两位主要导师，大卫·鲁梅哈特，我和他一起研究了反向传播算法。
26 00:03:01,758 --> 00:03:07,044 主持人 SPEAKER_02：大卫因一种可怕的脑部疾病过早去世，但如果是那样，他现在应该在这里而不是我。
27 00:03:07,084 --> 00:03:17,556 主持人 SPEAKER_02：还有我的同事，特里·桑诺夫斯基，我在 20 世纪 80 年代与他合作了很多，研究玻尔兹曼机，他教会了我很多关于大脑的知识。
28 00:03:17,872 --> 00:03:20,957 主持人 SPEAKER_02：我还想感谢我的学生们。
29 00:03:21,177 --> 00:03:28,991 说话人 SPEAKER_02：我很幸运，有很多非常聪明的学生，他们比我聪明得多，实际上让事情得以运作。
30 00:03:30,552 --> 00:03:32,796 说话人 SPEAKER_02：他们已经取得了很大的成就。
31 00:03:33,437 --> 00:03:36,723 说话人 SPEAKER_02：我特别自豪的是，我的一个学生解雇了 Sam Altman。
32 00:03:37,604 --> 00:03:43,014 说话人 SPEAKER_02：我想我最好到此为止，留给提问环节。
33 00:03:43,033 --> 00:03:44,556 主持人 SPEAKER_01: 非常感谢，Geoff。
34 00:03:45,193 --> 00:03:52,883 主持人 SPEAKER_01: 现在我们将接受媒体成员的提问，我邀请我的同事，来自多伦多大学媒体关系团队的 Lisa Pires 来主持我们的问答环节。
35 00:03:53,084 --> 00:03:53,425 主持人 SPEAKER_01: Lisa。
36 00:03:56,147 --> 00:03:57,590 主持人 SPEAKER_00: 谢谢，Gerler 校长。
37 00:03:57,610 --> 00:04:04,919 主持人：为确保我们能回答尽可能多的问题，我们只接受书面提问。
38 00:04:04,960 --> 00:04:15,133 主持人：请在提交问题时，在屏幕底部出现的问答框中包含您的姓名和您所属的新闻机构。
39 00:04:15,872 --> 00:04:18,798 主持人：现在我们留出一分钟时间接收问题。
40 00:04:25,189 --> 00:04:26,110 主持人02：我们留出一分钟时间。
41 00:04:26,151 --> 00:04:36,327 说话人 SPEAKER_02：我想说，我也应该感谢 Joshua Bengio 和 Yann LeCun，他们是我的亲密同事，在发展这个领域方面发挥了非常重要的作用。
42 00:04:46,112 --> 00:04:52,721 说话人 SPEAKER_00：我们在问答框中看到了来自 CTV 国家新闻的 Adrian 提出的问题。
43 00:04:52,822 --> 00:04:58,288 说话人 SPEAKER_00：Adrian，如果你不介意输入你的问题，这将有助于我们回答。
44 00:05:05,939 --> 00:05:07,120 说话人 SPEAKER_00：非常感谢你，Adrian。
45 00:05:07,221 --> 00:05:10,225 说话人 SPEAKER_00: 这是一个问题要问 Hinton 博士。
46 00:05:10,644 --> 00:05:15,732 说话人 SPEAKER_00: Hinton 博士，您认为您在人工智能领域的遗产会是什么？
47 00:05:17,331 --> 00:05:26,463 说话人 SPEAKER_02: 我希望人工智能将带来巨大的好处，大幅提高生产力，让每个人的生活都变得更好。
48 00:05:26,845 --> 00:05:29,668 说话人 SPEAKER_02: 我坚信它会在医疗保健领域做到这一点。
49 00:05:30,589 --> 00:05:35,916 说话人 SPEAKER_02：我的担忧是它也可能导致不好的事情发生。
50 00:05:36,677 --> 00:05:45,870 说话人 SPEAKER_02：尤其是当我们拥有比我们自己更智能的东西时，没有人真的知道我们是否能够控制它们。
51 00:05:49,107 --> 00:05:53,454 说话人 SPEAKER_00：下一个问题来自多伦多星报的 Victoria Gibson。
52 00:05:54,336 --> 00:05:56,581 说话人 SPEAKER_00：这是另一个问题，是给 Hinton 博士的。
53 00:05:56,862 --> 00:06:02,331 说话人 SPEAKER_00: 她问，您现在会如何使用神经网络来改善多伦多这个城市？
54 00:06:07,923 --> 00:06:11,149 说话人 SPEAKER_02: 我不太确定神经网络如何能摆脱道格·福特。
55 00:06:15,735 --> 00:06:19,779 说话人 SPEAKER_00: 好的，维多利亚在《多伦多星报》的跟进提问。
56 00:06:20,220 --> 00:06:27,726 说话人 SPEAKER_00: 她再次问希顿博士，加拿大研究环境现在与您刚开始时相比有何不同？
57 00:06:28,208 --> 00:06:32,932 说话人 说话人_00：那么，加拿大今天在实现更多研究突破方面最大的障碍是什么？
58 00:06:36,995 --> 00:06:38,076 说话人 说话人_02：让我先想想这个问题。
59 00:06:39,437 --> 00:06:44,442 说话人 说话人_02：显然，一个很大的不同是现在人们认识到神经网络实际上是可以工作的。
60 00:06:45,165 --> 00:06:47,170 说话人 说话人_02：但大部分的格局还是相似的。
61 00:06:47,209 --> 00:06:56,007 有一位名叫加拿大高级研究院的组织，对在加拿大优势领域进行研究的人们提供了很大帮助。
62 00:06:57,048 --> 00:07:06,245 我认为加拿大作为研究场所的主要特点是没有像美国那么多的资金。
63 00:07:06,665 --> 00:07:09,267 但它使用资金非常明智。
64 00:07:09,367 --> 00:07:34,495 尤其是这种研究的主要资助机构，称为 NSERC，它用钱进行基础的好奇心驱动研究，所有这些先进的神经网络都是从基础的好奇心驱动研究中产生的，而不是从向应用问题砸钱中产生的，而是让科学家们跟随他们的好奇心去尝试理解事物。
65 00:07:35,774 --> 00:07:37,156 说话人 SPEAKER_02：加拿大在这方面做得相当不错。
66 00:07:40,500 --> 00:07:40,860 说话人 SPEAKER_00：谢谢。
67 00:07:40,922 --> 00:07:45,908 说话人 SPEAKER_00：下一个问题来自法新社的伊萨姆·阿赫迈德。
68 00:07:46,550 --> 00:07:48,031 说话人 SPEAKER_00：他们说，恭喜。
69 00:07:48,413 --> 00:07:56,985 说话人 SPEAKER_00：你们和霍普菲尔德博士都警告过不受控制的 AI 和对其工作原理了解不足的危险。
70 00:07:57,545 --> 00:08:00,329 说话人 SPEAKER_00：我们如何避免灾难性场景？
71 00:08:01,524 --> 00:08:04,009 说话人 SPEAKER_02：目前我们还不知道如何避免所有这些。
72 00:08:04,028 --> 00:08:06,050 说话人 SPEAKER_02：这就是我们为什么需要紧急进行更多研究的原因。
73 00:08:07,553 --> 00:08:24,057 说话人 SPEAKER_02：所以我主张我们最优秀的年轻研究人员，或者其中许多人，应该从事人工智能安全研究，政府应该迫使大型公司提供他们所需的计算设施。
74 00:08:26,786 --> 00:08:32,634 说话人 SPEAKER_00：下一个问题来自加拿大通讯社的塔拉·德尚普斯，再次是关于辛顿教授的。
75 00:08:33,296 --> 00:08:40,225 说话人 SPEAKER_00：长期以来，人工智能并没有像今天这样被视为一种性感或流行的技术。
76 00:08:40,264 --> 00:08:47,715 说话人 SPEAKER_00：我想知道您能否分享一下在技术普及之前从事这项技术基础研究时的感受。
研究过程非常有趣，但稍微有点烦恼的是，很多人说，实际上，大多数人工智能领域的人都说神经网络永远不会起作用。
他们对此非常自信，认为这些事情只是浪费时间，我们永远无法用神经网络学习复杂的事情，比如理解自然语言，但他们错了。
79 00:09:16,940 --> 00:09:20,989 说话人 SPEAKER_00：下一个问题来自 CTV 新闻的 Adrian Gobriel。
80 00:09:21,009 --> 00:09:22,152 说话人 SPEAKER_00：这是他的第二个问题。
81 00:09:22,833 --> 00:09:28,004 说话人 SPEAKER_00：他问 Hinton 博士，你能详细谈谈你对 AI 的担忧吗？
82 00:09:28,066 --> 00:09:31,533 说话人 SPEAKER_00：你相信它可能会比人类更聪明吗？
83 00:09:32,195 --> 00:09:35,623 说话人 SPEAKER_00：为什么以及会多快发生这种情况？
84 00:09:36,532 --> 00:09:45,277 说话人 SPEAKER_02：好的，我所认识的顶尖研究人员中，大多数人认为人工智能将比人类更聪明。
85 00:09:46,100 --> 00:09:48,988 说话人 SPEAKER_02：他们对时间尺度有不同的看法。
86 00:09:49,947 --> 00:09:53,052 说话人 SPEAKER_02：其中很多人认为这将在未来 20 年内发生。
87 00:09:53,493 --> 00:09:55,235 说话人 SPEAKER_02：有些人认为这会很快发生。
88 00:09:55,836 --> 00:09:57,519 说话人 SPEAKER_02：有些人认为这需要更长的时间。
89 00:09:58,041 --> 00:10:04,191 说话人 SPEAKER_02：但相当多的优秀研究人员认为，在未来 20 年左右，人工智能的智能将超过我们。
90 00:10:05,192 --> 00:10:07,736 说话人 SPEAKER_02：那时我们需要深思熟虑会发生什么。
91 00:10:13,116 --> 00:10:18,605 说话人 说话人_00：我们不知道下一位提问者的名字，但他们问了一个有趣的问题。
92 00:10:19,066 --> 00:10:22,972 说话人 说话人_00：当你发现你获得了诺贝尔奖时，你首先联系的是谁？
93 00:10:24,674 --> 00:10:25,976 说话人 说话人_02：我在澳大利亚的妹妹。
94 00:10:30,544 --> 00:10:32,488 说话人 说话人_00：关于那个问题的补充，反应如何？
95 00:10:32,508 --> 00:10:37,655 说话人 SPEAKER_02: 我想她说了类似，哦，我的天哪。
96 00:10:41,585 --> 00:10:44,751 说话人 SPEAKER_00: 我们下一个是来自 Tara Deschamps 的后续问题。
97 00:10:44,772 --> 00:10:53,513 说话人 SPEAKER_00: 再次，她来自加拿大通讯社，她问 Hinton 教授，您最初提到在早上得知诺贝尔奖的消息时感到非常惊讶。
98 00:10:54,014 --> 00:10:56,961 说话人 SPEAKER_00: 您能告诉我们自那以后您的一天过得怎么样吗？
99 00:10:58,020 --> 00:11:01,927 说话人 SPEAKER_02: 是的，我几乎没有睡觉。
100 00:11:02,027 --> 00:11:06,953 说话人 SPEAKER_02: 那是凌晨一点，电话响起时我可能已经睡了大约一个小时。
101 00:11:07,875 --> 00:11:13,162 说话人 SPEAKER_02: 我在加利福尼亚，从那时起我可能又多睡了一个小时。
102 00:11:14,244 --> 00:11:16,366 说话人 SPEAKER_02: 所以我现在相当缺觉。
103 00:11:16,346 --> 00:11:29,309 主持人 SPEAKER_02：很多人试图联系我，还有许多多年未见的老朋友的消息，这真是太好了。
104 00:11:33,003 --> 00:11:38,975 主持人 SPEAKER_00：下一个问题来自 BetaKit 的 Isabel Kirkwood，再次向 Hinton 教授提问。
105 00:11:39,596 --> 00:11:52,243 主持人 SPEAKER_00：她问，Hinton 教授，您如何调和接受这种认可与您关于减缓 AI 发展步伐和技术风险的直言不讳？
106 00:11:53,219 --> 00:11:57,926 主持人 SPEAKER_02：我从未建议减缓 AI 的发展，因为我认为这是不可行的。
107 00:11:58,726 --> 00:12:10,363 人工智能有如此多的积极作用，比如在医疗保健领域，但在几乎所有行业中，我认为我们根本无法减缓其发展速度。
108 00:12:15,450 --> 00:12:17,352 您能再说一遍问题的后半部分吗？
109 00:12:17,552 --> 00:12:18,455 当然可以。
110 00:12:18,475 --> 00:12:29,638 她问，如何协调接受这种认可与您关于减缓人工智能发展步伐和技术带来的风险的直言不讳？
111 00:12:30,664 --> 00:12:38,437 说话人 SPEAKER_02：好吧，实际上诺贝尔委员会认识到我在谈论安全性方面的研究与此相关。
112 00:12:39,139 --> 00:12:42,024 说话人 SPEAKER_02：我记不太清楚他们具体说了什么，但提到了这一点。
113 00:12:42,865 --> 00:12:49,837 说话人 SPEAKER_02：我认为我们需要做出严肃的努力来确保其安全性，因为如果我们能保持其安全，那将是非常美好的。
114 00:12:52,619 --> 00:13:12,028 说话人 SPEAKER_00：我们再次收到 Hinton 教授和来自 AFP 的 Issam Ahmed 的跟进，他们问，你认为学生甚至专业人士过度依赖LLMs会导致智力下降，还是会让我们达到更高的层次？
115 00:13:13,105 --> 00:13:15,849 说话人 SPEAKER_02：我认为它不会产生显著的简化效果。
116 00:13:16,169 --> 00:13:22,479 说话人 SPEAKER_02：我想这就像他们第一次有袖珍计算器时，人们说，哦，孩子们不会再学数学了。
117 00:13:22,519 --> 00:13:24,221 说话人 SPEAKER_02：他们不会做乘法。
118 00:13:24,863 --> 00:13:28,388 说话人 SPEAKER_02：如果你有袖珍计算器，你就不需要会做乘法。
119 00:13:29,210 --> 00:13:31,393 说话人 SPEAKER_02：我认为它将与LLMs一样。
120 00:13:31,774 --> 00:13:37,602 说话人 SPEAKER_02：人们可能不会记得那么多事实，你可以问LLM，它会知道。
121 00:13:37,643 --> 00:13:40,547 说话人 SPEAKER_02：但我认为这会使人们变得更聪明，而不是更笨。
122 00:13:42,568 --> 00:13:43,129 说话人 SPEAKER_00：谢谢。
123 00:13:43,149 --> 00:13:47,460 说话人 SPEAKER_00：来自 CTV 新闻的 Adrian Gobriel 的后续提问。
124 00:13:48,160 --> 00:13:51,047 说话人 SPEAKER_00：他问，如果我还能问一个问题的话。
125 00:13:51,086 --> 00:13:55,998 说话人 SPEAKER_00：他说，你提到你得知这个奖项时用了“目瞪口呆”这个词。
126 00:13:56,479 --> 00:13:58,283 说话人 SPEAKER_00：为什么你这么惊讶？
127 00:13:59,443 --> 00:14:02,429 说话人 SPEAKER_02：我绝对不知道自己被提名了。
128 00:14:02,450 --> 00:14:05,174 说话人 SPEAKER_02：我不是物理学家。
129 00:14:05,576 --> 00:14:07,278 说话人 SPEAKER_02：我对物理学非常尊敬。
130 00:14:07,759 --> 00:14:14,854 说话人 SPEAKER_02：我在大学的第一年就退出了物理学，因为我无法处理复杂的数学。
131 00:14:14,833 --> 00:14:19,061 诺贝尔物理学奖对我来说非常意外。
132 00:14:19,402 --> 00:14:29,799 我非常高兴诺贝尔委员会认识到在人工神经网络领域取得了巨大进步。
133 00:14:30,280 --> 00:14:31,543 贺普菲尔德的工作
134 00:14:31,523 --> 00:14:33,125 与物理学密切相关。
135 00:14:33,205 --> 00:14:38,913 说话人 SPEAKER_02：我和 Terry Sanofsky 早期在玻尔兹曼机方面的工作受到了统计物理学的启发。
136 00:14:39,455 --> 00:14:43,480 说话人 SPEAKER_02：但最近，这项工作与物理学的关联越来越少。
137 00:14:43,760 --> 00:14:49,048 说话人 SPEAKER_02：所以我非常惊讶自己获得了物理学奖项。
138 00:14:49,068 --> 00:15:00,905 说话人 SPEAKER_00：下一个问题来自美联社的 Matt O'Brien，他问 Hinton 教授，您能否在电话会议中更详细地谈谈关于 Sam Altman 的评论？
139 00:15:02,219 --> 00:15:07,048 说话人 SPEAKER_02：所以 OpenAI 的建立非常重视安全。
140 00:15:08,571 --> 00:15:13,500 说话人 SPEAKER_02：它的主要目标是开发通用人工智能，并确保其安全性。
141 00:15:15,264 --> 00:15:20,835 说话人 SPEAKER_02：我的一个前学生，伊利亚·苏特科娃，是首席科学家。
142 00:15:20,883 --> 00:15:30,751 说话人 SPEAKER_02：随着时间的推移，发现山姆·奥特曼对安全的关注远不如对利润的关注。
143 00:15:30,772 --> 00:15:33,659 说话人 SPEAKER_02：我认为这很不幸。
144 00:15:37,217 --> 00:15:37,678 说话人 SPEAKER_00：谢谢。
145 00:15:37,719 --> 00:15:41,364 说话人 SPEAKER_00：下一个问题来自 PA Media 的 Jessica Coates。
146 00:15:41,384 --> 00:15:44,427 说话人 SPEAKER_00：这同样是一个针对 Hinton 教授的问题。
147 00:15:45,048 --> 00:15:53,539 说话人 SPEAKER_00: 她问，你提到了围绕人工智能的未知未来以及对其潜在机遇和风险的更深入了解的需求。
148 00:15:54,201 --> 00:15:58,626 说话人 SPEAKER_00: 你认为政府是否会介入，对人工智能实施更严格的监管？
149 00:15:58,647 --> 00:16:02,111 说话人 SPEAKER_00: 政府如何更好地支持人工智能研究？
150 00:16:03,356 --> 00:16:10,205 说话人 SPEAKER_02: 我认为政府可以鼓励大型公司更多地投入资源进行安全研究。
151 00:16:10,544 --> 00:16:22,077 说话人 SPEAKER_02：目前，几乎所有资源都投入到让模型变得更好，以便它们可以拥有闪亮的新模型，现在正在进行一场激烈的竞争，模型也在变得越来越好，这是好事。
152 00:16:22,618 --> 00:16:26,982 说话人 SPEAKER_02：但我们需要在人工智能安全方面付出相当的努力。
153 00:16:27,344 --> 00:16:29,466 说话人 SPEAKER_02：这种努力需要超过 1%。
154 00:16:29,446 --> 00:16:36,297 说话人 SPEAKER_02：可能需要将三分之一的努力投入到人工智能安全，因为如果这些东西变得不安全，那就非常糟糕。
155 00:16:36,357 --> 00:16:43,990 说话人 SPEAKER_00：我们的下一个问题是来自 Tara Deschamps 的。
156 00:16:45,332 --> 00:16:50,981 说话人 SPEAKER_00：她问 Hinton 教授，诺贝尔奖金有什么计划吗？
157 00:16:51,923 --> 00:16:53,666 说话人 SPEAKER_02：没有具体的计划。
158 00:16:53,787 --> 00:17:05,288 说话人 SPEAKER_02：我将把它捐给慈善机构，但我知道有一个慈善机构我会捐一些，它为神经多样性年轻人提供工作机会。
159 00:17:06,170 --> 00:17:09,797 说话人 SPEAKER_02：我会把它捐给一些其他的慈善机构，但我还不知道是哪一个。
160 00:17:12,577 --> 00:17:26,153 说话人 SPEAKER_00：我们接下来向辛顿教授提出的问题，又是来自路透社的瓦隆，他问，您有什么建议可以防止未来出现严重后果吗？
161 00:17:27,035 --> 00:17:31,059 说话人 SPEAKER_00：他们指的是人们应该如何小心对待 AI 及其使用。
162 00:17:31,661 --> 00:17:33,742 说话人 SPEAKER_00：正如您所警告的，它可能很危险。
163 00:17:34,869 --> 00:17:40,759 说话人 SPEAKER_02：我认为个人在使用它时小心谨慎并不能解决这些问题。
164 00:17:41,440 --> 00:17:45,807 说话人 SPEAKER_02：我认为开发 AI 的人需要小心地开发它。
165 00:17:46,728 --> 00:17:50,493 说话人 SPEAKER_02：并且我认为在大公司中需要进行研究，这些公司拥有资源。
166 00:17:51,556 --> 00:17:55,561 说话人 SPEAKER_02：我不认为个人使用它的方式会带来太大的改变。
167 00:18:00,097 --> 00:18:04,006 说话人 SPEAKER_00：我们的下一个问题是伊萨姆·阿赫迈德的后续问题。
168 00:18:04,527 --> 00:18:07,393 说话人 SPEAKER_00：这是在 AFP，再次为辛顿教授提问。
169 00:18:07,833 --> 00:18:12,563 说话人 SPEAKER_00：他们问，我知道你说过预测事情变坏可能意味着什么很难。
170 00:18:13,243 --> 00:18:18,714 说话人 SPEAKER_00：但如果你必须猜测一些可能引起关注的领域，那会是什么？
171 00:18:20,398 --> 00:18:24,066 说话人 SPEAKER_02：所以人工智能有很多不同的风险，它们都有不同的解决方案。
172 00:18:24,748 --> 00:18:28,355 说话人 SPEAKER_02：所以立即的风险包括像伪造视频破坏选举这样的东西。
173 00:18:29,396 --> 00:18:38,474 说话人 SPEAKER_02：我们已经看到政治家们要么指责他人使用伪造视频，要么自己使用伪造视频和伪造图像。
174 00:18:38,454 --> 00:18:39,958 说话人 SPEAKER_02：所以这是一个立即的危险。
175 00:18:40,018 --> 00:18:44,726 说话人 SPEAKER_02：来自网络攻击等事物的非常直接的危险。
176 00:18:44,746 --> 00:18:50,836 说话人 SPEAKER_02：例如，去年网络钓鱼攻击的数量增加了 1200%。
177 00:18:51,336 --> 00:18:56,526 说话人 SPEAKER_02：这是因为这些大型语言模型使得进行网络钓鱼攻击变得非常容易。
178 00:18:56,987 --> 00:19:01,173 说话人 SPEAKER_02：而且你不能再通过拼写错误和语法略有不同来识别它们了。
179 00:19:02,155 --> 00:19:03,298 说话人 SPEAKER_02：他们的英语非常流利。
180 00:19:06,198 --> 00:19:09,064 说话人 SPEAKER_00：下一个问题是来自维多利亚·吉布森的。
181 00:19:09,163 --> 00:19:10,646 说话人 SPEAKER_00：再次，她来自多伦多之星。
182 00:19:11,249 --> 00:19:18,463 说话人 SPEAKER_00：她问希顿教授，您今天已经几次提到了省政府和安大略科学中心。
183 00:19:18,984 --> 00:19:22,231 说话人 SPEAKER_00：为什么在获得这个荣誉时，这件事会浮现在你的脑海中？
184 00:19:23,613 --> 00:19:30,704 说话人 SPEAKER_02：因此，安大略科学中心在激发年轻人好奇心和对科学的兴趣方面起着非常重要的作用。
185 00:19:32,106 --> 00:19:34,631 说话人 SPEAKER_02：它有一些屋顶问题，需要进行翻修。
186 00:19:35,092 --> 00:19:46,329 说话人 SPEAKER_02：翻修的预估费用是 2 亿美元，但政府后来告诉估算费用的人，要将这个数字乘以 1.85。
187 00:19:46,494 --> 00:19:50,766 说话人 SPEAKER_02：为了得到一个更大的数字，以便他们可以据此将其拆除。
188 00:19:50,826 --> 00:19:56,319 说话人 SPEAKER_02：在我看来，拆除它的原因并不是政府给出的那些原因。
189 00:19:56,781 --> 00:20:00,471 说话人 SPEAKER_02：它本可以修复，修复它会更便宜。
190 00:20:03,116 --> 00:20:10,128 说话人 SPEAKER_00：接下来是加拿大通讯社的 Tara Deschamps 向 Hinton 教授提出的问题。
191 00:20:10,609 --> 00:20:19,785 说话人 SPEAKER_00: 她说，当人们谈论加拿大的人工智能和技术格局时，您的名字总是作为加拿大能够取得的成就的例证出现。
192 00:20:20,467 --> 00:20:26,576 说话人 SPEAKER_00: 但人们也说，这个国家必须小心不要浪费您所创造的机会。
193 00:20:26,557 --> 00:20:33,208 说话人 SPEAKER_00: 您认为加拿大能做些什么来保持其在人工智能领域的领先地位？
194 00:20:34,623 --> 00:20:38,728 说话人 SPEAKER_02: 它可以继续资助好奇心驱动的硏究。
195 00:20:38,807 --> 00:20:41,672 说话人 SPEAKER_02：这对留住最优秀的研究人员非常重要。
196 00:20:42,292 --> 00:20:50,903 说话人 SPEAKER_02：但在人工神经网络的时代，我们也需要大量的计算资源来留住大学的研究人员。
197 00:20:51,505 --> 00:20:53,567 说话人 SPEAKER_02：政府正在努力解决这个问题。
198 00:20:53,606 --> 00:20:58,773 说话人 SPEAKER_02：他们为人工智能研究划拨了 20 亿美元用于计算资源。
199 00:20:58,753 --> 00:21:00,981 说话人 SPEAKER_02：所以，我认为他们正在尽他们所能。
200 00:21:01,021 --> 00:21:09,792 说话人 SPEAKER_02：显然，我们比中国或美国这样的国家要小得多，但考虑到他们拥有的资源，我认为加拿大做得相当不错。
201 00:21:13,637 --> 00:21:33,961 说话人 SPEAKER_00：下一个问题来自多伦多大学新闻的拉胡尔·卡尔瓦帕利，他问希顿教授，您在人工神经网络的研究上一直坚持不懈，即使在科学界对该主题兴趣减弱的时期也是如此。
202 00:21:34,563 --> 00:21:42,092 说话人 SPEAKER_00：您对那些可能被认为不受欢迎或徒劳的奋斗的教授和学生有什么话要说吗？
203 00:21:43,337 --> 00:21:44,941 说话人 SPEAKER_02：我的信息是这样的。
204 00:21:45,902 --> 00:21:52,472 说话人 SPEAKER_02：如果你相信某件事，不要放弃，直到你明白为什么那个信念是错误的。
205 00:21:52,873 --> 00:21:57,800 说话人 SPEAKER_02：通常你相信某件事，最终你会弄清楚为什么相信它是错的。
206 00:21:58,442 --> 00:22:08,198 说话人 SPEAKER_02：但只要你还相信某件事，你无法看到为什么它是错的，就像大脑需要某种方式工作一样，所以我们必须弄清楚它是如何学习连接强度来使其工作的。
207 00:22:08,178 --> 00:22:14,688 说话人 SPEAKER_02：只要你们相信这一点，就要继续努力，如果你们看不到为什么它是胡说八道，就不要让别人告诉你们它是胡说八道。
208 00:22:18,073 --> 00:22:18,554 说话人 SPEAKER_00：谢谢。
209 00:22:18,574 --> 00:22:28,388 说话人 SPEAKER_00：下一个问题是来自小林康博的，他来自《读卖新闻》。
210 00:22:28,429 --> 00:22:33,236 说话人 SPEAKER_00：他们问，AI 何时会超越人类的能力？
211 00:22:33,696 --> 00:22:35,619 说话人 SPEAKER_00: 结果会怎样？
212 00:22:36,696 --> 00:22:41,521 说话人 SPEAKER_02: 所以没有人知道具体时间，但我认识的多数优秀研究人员认为这会发生。
213 00:22:42,682 --> 00:22:46,767 说话人 SPEAKER_02: 我的猜测是，这可能在 5 到 20 年之间发生。
214 00:22:47,146 --> 00:22:47,907 说话人 SPEAKER_02: 也可能更长。
215 00:22:48,588 --> 00:22:50,250 说话人 SPEAKER_02：可能性非常小，它可能会更早发生。
216 00:22:50,269 --> 00:22:54,294 说话人 SPEAKER_02：我们也不知道那时会发生什么。
217 00:22:54,974 --> 00:23:06,365 说话人 SPEAKER_02：所以如果你环顾四周，能找到的例子非常少，即更智能的事物被不那么智能的事物所控制，这让你不禁要思考，当 AI 比我们更智能时，它是否会接管控制权。
218 00:23:09,855 --> 00:23:11,678 说话人 SPEAKER_00：谢谢，辛顿教授。
219 00:23:12,318 --> 00:23:19,027 说话人 说话人_00：我们不再看到任何其他问题，但我们很高兴再稍等一会儿。
220 00:23:19,167 --> 00:23:32,784 说话人 说话人_00：如果任何人还有最后一刻的问题，请使用您屏幕底部的问答工具箱，并请提供您的姓名和媒体隶属关系。
221 00:23:33,525 --> 00:23:36,588 说话人 说话人_00：如果你们还有更多问题，我们还有时间。
222 00:23:47,555 --> 00:23:50,221 说话人 说话人_00：这是来自多伦多大学新闻的 Rahul 提出的问题。
223 00:23:50,922 --> 00:24:07,217 说话人 SPEAKER_00：他问格特勒总统，您认为辛顿教授获得诺贝尔奖将如何影响大学，并激发人工智能和其他领域的学术研究？
224 00:24:09,204 --> 00:24:12,568 说话人 SPEAKER_01：我认为这将产生巨大的影响，而且是极其积极的。
225 00:24:13,148 --> 00:24:23,759 说话人 SPEAKER_01：我在多伦多大学当一名年轻的助理教授时，另一位杰出的科学家约翰·波拉尼在 1986 年获得了诺贝尔化学奖。
226 00:24:23,839 --> 00:24:33,069 说话人 SPEAKER_01：我记得当约翰得到这个好消息时，我们知识界的自豪感。
227 00:24:33,049 --> 00:24:40,441 说话人 SPEAKER_01：它一直持续产生如此积极的影响，不仅是在化学领域，而且在多伦多大学。
228 00:24:40,480 --> 00:24:53,519 说话人 SPEAKER_01：我认为 Jeff 今天的胜利将产生类似积极的影响，提振整个大学的士气，同时也有助于我们吸引和留住优秀人才。
229 00:24:53,538 --> 00:24:55,362 说话人 SPEAKER_01：Jeff 已经谈到了这一点。
230 00:24:55,342 --> 00:24:57,384 说话人 SPEAKER_01：针对今天的一些提问。
231 00:24:57,403 --> 00:25:22,529 说话人 SPEAKER_01：我认为无法过分强调这样的胜利对加拿大、多伦多和加拿大安大略大学能够欢迎来自全国乃至世界各地的新锐人才、优秀学生和杰出教师的能力的影响，因为这种认可源于 Jeff 的胜利。
232 00:25:25,648 --> 00:25:26,912 说话人 SPEAKER_00：谢谢，Gertler 校长。
233 00:25:26,971 --> 00:25:35,413 说话人 SPEAKER_00：我们现在回到 Hinton 教授那里，回答 Issam Ahmed 来自 AFP 的另一个问题。
234 00:25:35,935 --> 00:25:40,688 说话人 SPEAKER_00：他们问，您在人工智能领域接下来最感兴趣的领域是什么？
235 00:25:42,625 --> 00:25:48,919 说话人 SPEAKER_02：好吧，我已经 76 岁了，我相信我不会再进行太多前沿研究了。
236 00:25:49,160 --> 00:25:53,730 说话人 SPEAKER_02：我将把我的时间花在倡导人们关注安全上。
237 00:25:54,432 --> 00:25:59,502 说话人 SPEAKER_02：我认为有一些非常激动人心的前沿领域。
238 00:25:59,482 --> 00:26:04,414 说话人 SPEAKER_02：在机器人领域，在让 AI 擅长操作事物方面。
239 00：26：04,915 --> 00：26：12,673 演讲者 SPEAKER_02：目前，我们在这方面比计算机或人工神经网络要好得多，但那里会有很多进步。
240 00：26：13,516 --> 00：26：17,204 议长 SPEAKER_02：不过，这方面可能需要更长的时间。
我也认为这些大型语言模型将会有很大的、很大的推理能力提升。
OpenAI 的最新模型和谷歌的模型，如 Gemini 的最新版本，在推理方面一直在不断进步。
243 00:26:30,596 --> 00:26:33,422 说话人 SPEAKER_02：我认为这将非常令人兴奋。
244 00:26:36,355 --> 00:26:39,118 说话人 SPEAKER_00：下一个问题来自维多利亚·吉布森。
245 00:26:39,199 --> 00:26:42,884 说话人 SPEAKER_00：她来自多伦多星报，这是给辛顿教授的问题。
246 00:26:43,486 --> 00:26:52,000 说话人 SPEAKER_00：她问，您在人工智能可能出错的具体方面提供了一些细节，例如网络攻击、虚假视频等。
247 00:26:52,621 --> 00:26:57,749 Speaker SPEAKER_00: 你能分享一些更具体的例子，说明你认为它如何发挥积极作用吗？
248 00:26:59,211 --> 00:26:59,632 Speaker SPEAKER_02: 哦，是的。
249 00:27:00,031 --> 00:27:02,936 Speaker SPEAKER_02: 所以如果你考虑像医疗保健这样的领域，
250 00:27:03,457 --> 00:27:07,289 Speaker SPEAKER_02: 安大略省预算的大部分都用于医疗保健。
251 00:27:08,401 --> 00:27:10,324 说话人 SPEAKER_02：这可以产生巨大的影响。
252 00:27:11,224 --> 00:27:18,694 说话人 SPEAKER_02：所以我在 2016 年实际上做出过一个预测，到如今，人工智能将能阅读放射科医生通常阅读的所有扫描。
253 00:27:19,256 --> 00:27:20,617 说话人 SPEAKER_02：那个预测是错误的。
254 00:27:20,637 --> 00:27:22,119 说话人 SPEAKER_02：我有些过于乐观了。
255 00:27:22,599 --> 00:27:26,025 说话人 SPEAKER_02：可能还需要再过五年才会发生，但我们显然正在朝着这个方向发展。
256 00:27:27,205 --> 00:27:30,290 说话人 SPEAKER_02：人工智能在诊断方面将会更加出色。
257 00:27:30,510 --> 00:27:37,259 说话人 SPEAKER_02：所以，对于诊断难度较大的病例，医生的正确率只有 40%。
258 00:27:37,240 --> 00:27:47,096 说话人 SPEAKER_02：医生，人工智能系统的正确率为 50%，而医生与人工智能系统的组合正确率可达 60%，这是一个很大的提升。
259 00：27：47,678 --> 00：27：52,026 演讲者 SPEAKER_02：在北美，每年有数十万人死于不良诊断。
260 00：27：52,346 --> 00：27：55,131 演讲者 SPEAKER_02：有了人工智能，诊断会变得更好。
但真正会发生的事情是，你将能够拥有一个 AI 家庭医生，他已经看过一亿名患者，知道大量信息，并且将能够更好地处理你遇到的任何疾病，因为你的 AI 家庭医生已经看过许多类似的病例。
262 00:28:19,292 --> 00:28:21,116 演讲者 演讲者00：谢谢，辛顿教授。
263 00:28:21,838 --> 00:28:24,605 说话人 SPEAKER_00: 我们不再看到任何其他问题。
264 00:28:25,326 --> 00:28:28,433 说话人 SPEAKER_00: 但再次强调，我们还有时间回答一两个问题。
265 00:28:28,453 --> 00:28:38,195 说话人 SPEAKER_00: 所以如果通话中有人想问其他问题，我们再次邀请您在提问时包括您的姓名和您所代表的媒体机构。
266 00:28:38,175 --> 00:28:42,983 说话人 SPEAKER_00: 并在屏幕底部的问答框中打出这些问题。
在等待最后时刻的任何问题到来时，休顿教授，我们很好奇，今天在新闻发布会上，有没有什么我们没有涉及到的您想提到的，或者有没有什么在众多媒体提问中我们可能遗漏了的问题？
268 00：29：03,016 --> 00：29：08,862 演讲者 SPEAKER_02：我们只简单谈到了一件事，那就是好奇心驱动的基础研究的作用。
所以人工神经网络，基础工作都是由大学研究人员完成的，几乎全部是由大学研究人员完成的，只是遵循他们的好奇心。
270 00：29：20,434 --> 00：29：22,978 演讲者 SPEAKER_02：资助这类研究非常重要。
271 00:29:23,478 --> 00:29:32,347 说话人 SPEAKER_02：这并不像其他类型的研究那样昂贵，但它为后来非常昂贵且涉及大量技术的项目奠定了基础。
272 00:29:35,365 --> 00:29:35,826 说话人 SPEAKER_00：谢谢。
273 00:29:35,865 --> 00:29:44,180 说话人 SPEAKER_00：这里还有一条来自多伦多星报的维多利亚·吉布森的消息，可能是对你之前关于医疗保健和 AI 的评论的后续。
274 00:29:44,641 --> 00:29:52,213 说话人 SPEAKER_00：她说，为什么我们认为我们还没有达到你预测的 AI 在医疗保健中扮演更大角色的那个点呢？
275 00:29:52,795 --> 00:29:55,720 Speaker SPEAKER_00: 发生这种情况还有任何障碍吗？
276 00:29:56,762 --> 00:30:01,415 Speaker SPEAKER_02: 一个障碍是医学界非常保守。
277 00:30:01,717 --> 00:30:02,839 Speaker SPEAKER_02: 这有很好的理由。
278 00:30:02,880 --> 00:30:08,375 Speaker SPEAKER_02: 如果你犯错导致人死亡，那当然是一个大问题。
279 00:30:08,490 --> 00:30:13,397 说话人 SPEAKER_02：政策上要保守，但他们相对较慢地采用新技术。
280 00:30:14,199 --> 00:30:21,710 说话人 SPEAKER_02：另一个原因是，我之前对 AI 系统在阅读扫描方面优于放射科医生的速度估计错误。
281 00:30:22,230 --> 00:30:27,157 说话人 SPEAKER_02：现在它们在许多不同类型的扫描中与放射科医生相当，在少数方面甚至更好。
282 00:30:27,137 --> 00:30:30,624 说话人 SPEAKER_02：我认为再过几年，它们肯定会比放射科医生更优秀。
283 00:30:30,923 --> 00:30:39,479 说话人 SPEAKER_02：我们将看到放射科医生和 AI 系统之间的合作，AI 系统读取扫描图像，放射科医生检查 AI 系统是否出错。
284 00:30:40,259 --> 00:30:43,105 说话人 SPEAKER_02：过了一段时间后，AI 系统将几乎完成所有工作。
285 00:30:45,987 --> 00:30:46,667 说话人 SPEAKER_00：好的，太好了。
286 00:30:46,768 --> 00:30:48,190 说话人 SPEAKER_00：谢谢，Hinton 教授。
287 00:30:48,289 --> 00:30:51,874 主持人：今天的问题环节就到这里。
288 00:30:51,894 --> 00:31:00,663 主持人：在聊天框中，您会看到一个可以联系以获取额外问题的电子邮件地址。
289 00:31:01,806 --> 00:31:03,247 主持人：您应该现在就能看到它弹出。
290 00:31:03,307 --> 00:31:08,733 主持人：这个电子邮件地址是 media.relations@utoronto.ca。
291 00:31:08,753 --> 00:31:14,019 说话人 说话人_00：现在，我将邀请 Gertler 总统发表闭幕词。
292 00:31:15,821 --> 00:31:16,722 说话人 说话人_01：嗯，谢谢，Lisa。
293 00:31:17,123 --> 00:31:18,204 说话人 说话人_01：谢谢，Jeff。
294 00:31:18,224 --> 00:31:22,788 说话人 说话人_01：再次祝贺您获得这个了不起的成就，您的诺贝尔奖。
295 00:31:22,848 --> 00:31:38,205 说话人 SPEAKER_01：我确信我代表整个多伦多大学社区，以及加拿大以及世界各地众多朋友、同事和崇拜者，当我们说我们对您今天所获得的成就感到无比自豪时。
296 00:31:38,266 --> 00:31:43,652 说话人 SPEAKER_01：还要感谢今天参加我们这次美好庆祝活动的人。
297 00:31:44,613 --> 00:31:45,053 说话人 SPEAKER_01：干杯。
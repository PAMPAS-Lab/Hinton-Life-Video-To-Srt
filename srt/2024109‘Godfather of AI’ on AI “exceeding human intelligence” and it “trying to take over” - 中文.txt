1 00:00:00,031 --> 00:00:05,157 说话人 SPEAKER_00：我所认识的几乎所有 AI 专家都认为它们将超越人类智能。
2 00:00:05,176 --> 00:00:06,158 说话人 SPEAKER_00：这只是个时间问题。
3 00:00:06,799 --> 00:00:14,246 说话人 SPEAKER_00：在 5 到 20 年之间，大约有一半的概率我们将不得不面对它们试图接管的问题。
4 00:00:16,109 --> 00:00:23,236 说话人 SPEAKER_01：我先问 Geoffrey Inton 是否认为世界正在开始应对这个问题，还是他仍然像以前一样担忧。
5 00:00:24,161 --> 00:00:28,109 说话人 SPEAKER_00：我仍然像以前一样担忧，但我非常高兴世界开始认真对待这个问题。
6 00:00:28,890 --> 00:00:37,567 说话人 SPEAKER_00：特别是，他们开始认真对待这种存在性威胁，即这些事物会变得比我们聪明，我们必须担心它们是否会想要从我们手中夺取控制权。
7 00:00:38,027 --> 00:00:41,515 说话人 SPEAKER_00：这是我们应当认真思考的问题，现在人们开始认真对待这个问题了。
8 00:00:41,554 --> 00:00:43,639 说话人 SPEAKER_00：几年前，他们还认为这只是科幻小说。
9 00:00:43,618 --> 00:00:53,350 说话人 SPEAKER_01：从您的角度来看，您在这个领域的顶端工作，并参与开发了支撑我们看到的 AI 爆炸性发展的理论，这种存在性威胁是真实存在的吗？
10 00:00:54,310 --> 00:00:54,551 说话人 SPEAKER_00：是的。
11 00:00:55,131 --> 00:00:57,715 说话人 SPEAKER_00：所以有些人认为这些事物实际上并不理解。
12 00:00:57,734 --> 00:00:59,057 说话人 SPEAKER_00：他们与我们非常不同。
13 00:00:59,417 --> 00:01:01,198 说话人 SPEAKER_00：他们只是使用了一些统计技巧。
14 00:01:01,859 --> 00:01:02,680 说话人 SPEAKER_00：事实并非如此。
15 00:01:03,942 --> 00:01:06,284 说话人 SPEAKER_00：例如，这些大型语言模型，
16 00:01:06,265 --> 00:01:10,969 说话人 SPEAKER_00：早期的研究是将大脑理解语言的理论。
17 00:01:10,989 --> 00:01:14,914 说话人 SPEAKER_00：这是你们目前拥有的关于大脑理解语言的最佳理论。
18 00:01:15,453 --> 00:01:21,519 说话人 SPEAKER_00：我们也不太清楚它们是如何工作的，或者大脑是如何详细工作的，但我们认为它们可能以相当相似的方式工作。
19 00:01:21,941 --> 00:01:23,561 说话人 SPEAKER_00：是什么引起了你的担忧？
20 00:01:24,043 --> 00:01:25,524 说话人 SPEAKER_00: 这是由两件事结合而成的。
21 00:01:26,325 --> 00:01:32,391 说话人 SPEAKER_00: 所以，玩大型聊天机器人，尤其是谷歌在 GPT-4 之前的那个，还有 GPT-4。
22 00:01:32,370 --> 00:01:34,436 说话人 SPEAKER_00: 它们显然非常能干。
23 00:01:34,456 --> 00:01:35,659 说话人 SPEAKER_00: 它们显然理解很多。
24 00:01:35,759 --> 00:01:38,084 说话人 SPEAKER_00：它们的知识比任何人都多。
25 00:01:38,525 --> 00:01:40,751 说话人 SPEAKER_00：它们就像一个在几乎所有事情上都不太好的专家。
26 00:01:41,914 --> 00:01:43,037 说话人 SPEAKER_00：所以这是第一个担忧。
27 00:01:43,378 --> 00:01:46,245 说话人 SPEAKER_00：第二个是理解
28 00:01:46,799 --> 00:01:59,114 说话者 SPEAKER_00：它们之所以是更高级的智能形式，是因为你可以复制相同的神经网络，每个副本可以查看不同的数据，然后它们可以共享所学到的知识。
29 00:01:59,814 --> 00:02:09,506 说话者 SPEAKER_00：想象一下，如果我们有 10,000 人，他们都可以去攻读一个学位，他们可以高效地分享所学，那么我们每个人都会有 10,000 个学位。
30 00:02:10,426 --> 00:02:11,949 说话者 SPEAKER_00：那时我们就知道很多了。
31 00:02:11,968 --> 00:02:16,754 说话者 SPEAKER_00：我们无法像相同的神经网络的副本那样高效地分享知识。
32 00:02:17,191 --> 00:02:24,745 说话人 SPEAKER_01: 好吧，这里的关键问题是它可能会超过人类智能，甚至超过人类智能的总量。
33 00:02:24,865 --> 00:02:27,911 说话人 SPEAKER_00: 很少有专家对此表示怀疑。
34 00:02:28,372 --> 00:02:33,662 说话人 SPEAKER_00: 我认识的几乎所有 AI 专家都认为它们将超过人类智能。
35 00:02:33,722 --> 00:02:34,764 说话人 SPEAKER_00: 这只是一个时间问题。
36 00:02:35,104 --> 00:02:38,912 说话人 SPEAKER_01：到那时，控制它们真的非常困难。
37 00:02:39,348 --> 00:02:40,188 说话人 SPEAKER_00：嗯，我们不知道。
38 00:02:40,248 --> 00:02:42,432 说话人 SPEAKER_00：我们以前从未遇到过这种情况。
39 00:02:42,772 --> 00:02:46,558 说话人 SPEAKER_00：有一些专家，比如我的朋友 Yann LeCun，他们认为这没问题。
40 00:02:46,658 --> 00:02:47,520 说话人 SPEAKER_00: 我们会给他们目标。
41 00:02:47,560 --> 00:02:48,221 说话人 SPEAKER_00: 没问题。
42 00:02:48,241 --> 00:02:48,961 说话人 SPEAKER_00: 他们会按照我们说的做。
43 00:02:49,383 --> 00:02:50,465 说话人 SPEAKER_00: 他们会服从我们。
44 00:02:52,687 --> 00:02:55,532 说话人 SPEAKER_00：有其他专家认为他们绝对会掌握控制权。
45 00:02:56,233 --> 00:03:00,900 说话人 SPEAKER_00：鉴于这种广泛的意见，我认为谨慎行事是明智的。
46 00:03:00,879 --> 00:03:06,054 说话人 SPEAKER_00：我认为他们有可能掌握控制权，这是一个很大的可能性，不是 1%，而是更多。
47 00:03:06,575 --> 00:03:12,391 说话人 SPEAKER_01：他们不能在某些领域，比如科学研究，而不是，例如，军队中，被控制吗？
48 00:03:13,317 --> 00:03:22,915 说话人 SPEAKER_00：也许吧，但实际上如果你看看所有现行立法，包括欧洲立法，其中都有一个小条款，即所有这些都不适用于军事应用。
49 00:03:23,556 --> 00:03:27,185 说话人 SPEAKER_00：政府不愿意限制自己在国防方面的使用。
50 00:03:28,347 --> 00:03:30,972 说话人 SPEAKER_01：有一些证据
51 00:03:30,951 --> 00:03:36,758 说话人 SPEAKER_01：甚至在当前冲突中，AI 在生成成千上万的目标方面的应用也得到了证实。
52 00:03:37,098 --> 00:03:37,938 说话人 SPEAKER_01: 是的。
53 00:03:37,959 --> 00:03:40,542 说话人 SPEAKER_01: 我的意思是，自从你开始警告 AI 以来，这种情况已经发生了。
54 00:03:40,662 --> 00:03:42,864 说话人 SPEAKER_01: 这是不是你担心的那种途径？
55 00:03:42,883 --> 00:03:44,586 说话人 SPEAKER_00: 我的意思是，这是破晓的边缘。
56 00:03:45,126 --> 00:03:49,591 说话人 SPEAKER_00：我最关心的是这些事物能否自主决定杀人。
57 00:03:50,852 --> 00:03:51,973 说话人 SPEAKER_00：所以是机器人士兵。
58 00:03:52,394 --> 00:03:52,593 说话人 SPEAKER_01：是的。
59 00:03:53,314 --> 00:03:54,915 说话人 SPEAKER_01：这些都很常见。
60 00:03:54,936 --> 00:03:55,456 说话人 SPEAKER_01: 类似于这样的。
61 00:03:55,437 --> 00:04:02,887 说话人 SPEAKER_00: 可能我们可以得到类似日内瓦公约来规范它们，但我认为这将在非常糟糕的事情发生后才会发生。
62 00:04:03,168 --> 00:04:14,743 说话人 SPEAKER_01: 这里有一个类比，与曼哈顿计划以及奥本海默，如果我们自己限制在 G7 和先进民主国家不用于军事，那么中国和俄罗斯正在发生什么？
63 00:04:15,085 --> 00:04:17,408 说话人 SPEAKER_00: 是的，这必须是一个国际协议。
64 00:04:17,427 --> 00:04:18,930 说话人 SPEAKER_00：但是如果你看看化学武器，
65 00:04:19,619 --> 00:04:22,583 说话人 SPEAKER_00：化学武器国际公约运作得相当好。
66 00:04:23,122 --> 00:04:26,947 说话人 SPEAKER_01：你对像俄罗斯这样的地方是否已经摆脱了束缚有什么感觉吗？
67 00:04:27,247 --> 00:04:31,413 说话人 SPEAKER_00：普京几年前说过，谁控制了人工智能，谁就控制了世界。
68 00:04:32,254 --> 00:04:34,197 说话人 SPEAKER_00：我想他们一定非常努力。
69 00:04:34,617 --> 00:04:38,581 说话人 SPEAKER_00：幸运的是，西方在研究上可能比他们领先。
70 00:04:40,403 --> 00:04:44,928 说话人 SPEAKER_00：我们可能仍然略领先于中国，但中国正在投入更多资源。
71 00:04:45,750 --> 00:04:49,574 说话人 SPEAKER_00：至于军事用途的 AI，我认为将会有一场竞赛。
72 00:04:49,622 --> 00:04:57,896 说话人 SPEAKER_01：听起来非常理论化，但如果你顺着这个论点，这个论点线索追踪下去，你真的会非常担心灭绝级事件。
73 00:04:58,697 --> 00:05:00,701 说话人 SPEAKER_00：所以我们应该区分这些不同的风险。
74 00:05:01,262 --> 00:05:06,329 说话人 SPEAKER_00：使用 AI 进行自主致命性武器的风险并不取决于 AI 比我们更聪明。
75 00:05:06,992 --> 00:05:11,738 说话人 SPEAKER_00：这是一个与 AI 本身会变得反叛并试图接管的风险完全不同的风险。
76 00:05:12,440 --> 00:05:13,482 说话人 SPEAKER_00：我对这两件事都感到担忧。
77 00:05:13,843 --> 00:05:16,406 说话人 SPEAKER_00：自主武器显然将会到来。
78 00:05:16,387 --> 00:05:21,875 说话人 SPEAKER_00：至于 AI 是否会失控并试图接管，这是我们可能能够控制也可能无法控制的事情，我们不知道。
79 00:05:21,915 --> 00:05:30,247 说话人 SPEAKER_00：因此，在它变得比我们更聪明之前，我们应该投入大量资源来了解我们是否能够控制它。
80 00:05:30,809 --> 00:05:33,091 说话人 SPEAKER_01：您认为哪种社会正在演变？
81 00:05:33,293 --> 00:05:36,076 说话人 SPEAKER_01：哪些工作还会存在？
82 00:05:36,858 --> 00:05:43,288 说话人 SPEAKER_00：是的，我对 AI 接管大量枯燥的工作非常担忧。
83 00:05:43,387 --> 00:05:45,851 说话人 SPEAKER_00：这应该是个好事。
84 00:05:46,452 --> 00:05:50,201 说话人 SPEAKER_00：这将导致生产力的巨大提升，进而导致财富的巨大增长。
85 00:05:50,682 --> 00:05:53,709 说话人 SPEAKER_00：如果这种财富能够平均分配，那将是极好的。
86 00:05:54,069 --> 00:05:54,992 说话人 SPEAKER_00：但事实并非如此。
87 00:05:55,291 --> 00:06:01,345 说话人 SPEAKER_00：在我们所处的体系中，这种财富将流向富人，而不是那些失去工作的人。
88 00:06:01,459 --> 00:06:03,824 说话人 SPEAKER_00: 我认为这对社会来说将会非常糟糕。
89 00:06:04,185 --> 00:06:12,504 说话人 SPEAKER_00: 因此，它将加剧贫富差距，这增加了右翼民粹主义者当选的机会。
90 00:06:13,144 --> 00:06:21,223 说话人 SPEAKER_01: 所以，为了明确，你认为工作变化的这些社会影响可能会非常深远，
91 00:06:21,709 --> 00:06:28,199 说话人 SPEAKER_01: 以至于我们可能需要重新思考，我知道，福利制度、不平等、全民基本收入的政治？
92 00:06:28,418 --> 00:06:30,482 说话人 SPEAKER_00: 我确实相信普遍基本收入。
93 00:06:30,802 --> 00:06:35,369 说话人 SPEAKER_00: 但是我认为这还不够，因为很多人从他们所做的工作中获得自尊。
94 00:06:36,129 --> 00:06:45,642 说话人 SPEAKER_00: 如果把每个人都放在普遍基本收入上，这解决了他们饿肚子和付不起房租的问题，但并没有解决自尊的问题。
95 00:06:45,963 --> 00:06:48,526 说话人 SPEAKER_00: 那么，你只是尝试
96 00:06:48,507 --> 00:06:55,180 说话人 SPEAKER_01：政府需要介入，我的意思是，在英国，我们通常倾向于退后一步，让经济来决定胜者和败者。
97 00:06:56,242 --> 00:07:02,855 说话人 SPEAKER_00：是的，实际上唐宁街的人咨询过我，我建议他们实行全民基本收入是个好主意。
98 00:07:03,276 --> 00:07:06,684 说话人 SPEAKER_01：我的意思是，你说他们接管的风险是 10%到 20%。
99 00:07:06,803 --> 00:07:07,865 说话人 SPEAKER_01：是的。
100 00:07:08,369 --> 00:07:12,855 说话人 SPEAKER_01：你更确信这将在未来五年内，也许在下届议会中解决吗？
101 00:07:13,197 --> 00:07:21,870 说话人 SPEAKER_00：我的猜测是，从现在起五到二十年内，有大约一半的概率我们将不得不面对他们试图接管的问题。
102 00:07:22,310 --> 00:07:26,697 说话人 SPEAKER_01：你对各国政府迄今为止为试图遏制这一现象所做的努力印象深刻吗？
103 00:07:27,298 --> 00:07:30,764 说话人 SPEAKER_00：我对他们开始认真对待此事的事实印象深刻。
104 00:07:30,745 --> 00:07:34,494 说话人 SPEAKER_00：我对他们中没有人愿意监管军事用途的事实并不感到印象深刻。
105 00:07:35,055 --> 00:07:38,244 说话人 SPEAKER_00：我对大多数规定没有实际效果的事实也不感到印象深刻。
106 00:07:38,264 --> 00:07:48,348 说话人 SPEAKER_01：你认为科技公司是因为需要在人工智能竞赛中成为赢家而放松了对安全的警惕吗？
107 00:07:48,480 --> 00:07:50,403 说话人 SPEAKER_00：我不太清楚一般科技公司的情况。
108 00:07:50,603 --> 00:07:52,706 说话人 SPEAKER_00：我对谷歌很了解，因为我曾经在那里工作过。
109 00:07:53,447 --> 00:07:55,389 说话人 SPEAKER_00：谷歌非常关注这些问题。
110 00:07:55,911 --> 00:07:58,514 说话人 SPEAKER_00：谷歌没有发布大型聊天机器人。
111 00:07:59,115 --> 00:08:01,437 说话人 SPEAKER_00：如果它们说谎，谷歌担心自己的声誉。
112 00:08:03,000 --> 00:08:10,350 说话人 SPEAKER_00：但是一旦 OpenAI 与微软合作，微软将聊天机器人放入必应，谷歌别无选择。
113 00:08:10,850 --> 00:08:17,238 说话人 SPEAKER_00：所以我认为竞争将导致这些事物快速发展，竞争
114 00:08:17,218 --> 00:08:23,483 说话人 SPEAKER_00：意味着他们不会在安全性上投入足够的努力。
115 00:08:24,120 --> 00:08:28,625 说话人 SPEAKER_01：与他们的孩子交谈，给他们提供关于经济未来的建议，他们应该做什么工作，他们应该获得什么学位。
116 00:08:28,687 --> 00:08:32,871 说话人 SPEAKER_01：似乎这个世界正被你描述的这个世界抛向空中。
117 00:08:34,352 --> 00:08:39,820 说话人 SPEAKER_01：你现在会建议别人学习什么，以便能够驾驭这股潮流？
118 00:08:40,421 --> 00:08:46,187 说话人 SPEAKER_00：我不知道，因为很明显，许多中级智力工作将会消失。
119 00:08:46,828 --> 00:08:52,034 说话人 SPEAKER_00：如果你问哪些工作比较安全，我认为最安全的工作是管道工。
120 00:08:52,336 --> 00:08:54,979 说话人 SPEAKER_00: 因为这些事情在物理操作方面还不够好。
121 00:08:55,320 --> 00:08:57,662 说话人 SPEAKER_00: 那可能将是他们最后还能做得很好的事情。
122 00:08:58,243 --> 00:09:00,306 说话人 SPEAKER_00: 所以我认为管道行业在相当长的一段时间内是安全的。
123 00:09:01,105 --> 00:09:03,129 说话人 SPEAKER_00: 驾驶，那不会消失。
124 00:09:03,208 --> 00:09:03,990 说话人 SPEAKER_00: 没有开车，没有。
125 00:09:04,049 --> 00:09:04,330 说话人 SPEAKER_00: 自动驾驶。
126 00:09:04,610 --> 00:09:05,432 说话人 SPEAKER_00: 不，这是没有希望的。
127 00:09:05,971 --> 00:09:07,573 说话人 SPEAKER_00: 我的意思是，这比我们预期的要慢。
128 00:09:07,594 --> 00:09:08,654 说话人 说话人_00：新闻业。
129 00:09:08,936 --> 00:09:13,822 说话人 说话人_00：新闻业可能持续一段时间，但我认为这些事情很快就会成为相当不错的记者。
130 00:09:14,501 --> 00:09:15,984 说话人 说话人_00：而且可能也是相当不错的采访者，对吧。
131 00:09:16,924 --> 00:09:17,466 未知说话人：好的，嗯。
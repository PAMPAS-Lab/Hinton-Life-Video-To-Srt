1 00:00:02,916 --> 00:00:03,258 说话人 SPEAKER_00: 你好。
2 00:00:04,339 --> 00:00:10,288 说话人 SPEAKER_00: 我今天的演讲将围绕一个想法展开，即如何将蒸馏过程应用于大脑。
3 00:00:13,153 --> 00:00:20,164 说话人 SPEAKER_00: 我将关注大脑如何存储形状知识的问题，并考虑三种理论。
4 00:00:21,908 --> 00:00:25,314 说话人 SPEAKER_00: 第一种理论是，我们为所有可能的角度学习一个单独的外观模型。
5 00:00:26,195 --> 00:00:27,277 说话人 SPEAKER_00: 这显然是荒谬的。
6 00:00:28,399 --> 00:00:29,160 说话人 SPEAKER_00: 第二个理论
7 00:00:29,797 --> 00:00:33,344 说话人 SPEAKER_00: 和第一个理论是一样的，但是我们为不同的翻译共享权重。
8 00:00:34,545 --> 00:00:44,142 说话人 SPEAKER_00: 换句话说，我们内置了处理翻译的能力，但我们仍然强迫它学习处理旋转和缩放的能力。
9 00:00:45,463 --> 00:00:55,079 讲者 SPEAKER_00：第三种理论，也就是我将主要讨论的理论，是我们将知识存储在关于整体与部分之间独立于视角的关系的权重中。
10 00:00:56,409 --> 00:01:00,154 讲者 SPEAKER_00：我们使神经活动具有视角等变性。
11 00:01:00,353 --> 00:01:03,917 讲者 SPEAKER_00：也就是说，随着视角的变化，活动也会变化。
12 00:01:03,938 --> 00:01:05,920 讲者 SPEAKER_00：但知识是视角不变的。
13 00:01:06,740 --> 00:01:11,706 讲者 SPEAKER_00: 这灵感来源于计算机图形学，直到最近被深度学习所取代。
14 00:01:16,111 --> 00:01:20,915 讲者 SPEAKER_00: 所以一个问题就是，如果整体与部分关系的知识在权重中，我们如何访问它？
15 00:01:23,528 --> 00:01:31,599 讲者 SPEAKER_00: 我认为的是，我们通过选择一个视角并形成心理图像，将视角无关的知识转化为视角相关的神经活动。
16 00:01:32,620 --> 00:01:43,174 讲者 SPEAKER_00: 所以如果我问你，德国牧羊犬耳朵尖端的距离是比眼睛的距离大还是小，你会形成一个德国牧羊犬头部的心理图像。
17 00:01:43,963 --> 00:01:47,766 说话者 SPEAKER_00: 然后你将从那个心理图像中读出比较差异。
18 00:01:48,768 --> 00:01:52,331 说话者 SPEAKER_00: 构建心理图像并不一定意味着创建一堆像素。
19 00:01:52,731 --> 00:01:59,058 说话者 SPEAKER_00: 它意味着采用一个观点，并从那个观点出发，用关于部分空洞的信息来表达。
20 00:01:59,918 --> 00:02:08,885 说话者 SPEAKER_00: 你可以看到你采用了观点，因为你肯定不会想象德国牧羊犬巨大到占据了你的大部分视野，或者小到只能看到一点点。
21 00:02:09,486 --> 00:02:11,929 说话人 SPEAKER_00: 你没有想象它旋转了 45 度。
22 00:02:12,111 --> 00:02:15,736 说话人 SPEAKER_00: 你所知道的所有这些，所以你选择了一个视角来解决这个问题的。
23 00:02:17,018 --> 00:02:19,861 说话人 SPEAKER_00: 首先，什么是空间关系？
24 00:02:19,882 --> 00:02:28,272 说话人 SPEAKER_00: 嗯，从数学上来说，它是将一个矩形坐标系中的点映射到另一个矩形坐标系中的坐标变换。
25 00:02:29,514 --> 00:02:37,444 说话人 SPEAKER_00：这听起来可能非常抽象和数学化，但我相信人们实际上确实会对孔洞和部件施加矩形坐标系来表示形状。
26 00:02:38,205 --> 00:02:39,687 说话人 SPEAKER_00：我将尽力说服你们这一点。
27 00:02:40,411 --> 00:02:42,354 说话人 SPEAKER_00：这里有一些来自欧文·洛克的心理证据。
28 00:02:43,574 --> 00:02:47,501 说话人 SPEAKER_00：如果你看左边的国家，你不确定这是哪个国家。
29 00:02:48,162 --> 00:02:54,510 说话者 说话者_00: 但如果我现在告诉你它向左倾斜了45度，你就会立刻看出它是非洲。
30 00:02:55,812 --> 00:02:59,296 说话者 说话者_00: 在那之前，你可能认为它像澳大利亚那样是一种反射版本。
31 00:03:01,681 --> 00:03:05,406 说话者 说话者_00: 所以你强加的参照框架决定了你感知到的形状。
32 00:03:06,449 --> 00:03:07,969 说话者 说话者_00: 这同样适用于右边的形状。
33 00:03:08,491 --> 00:03:14,836 讲者：您将其视为直立菱形或倾斜的方形，您所意识到的内容因视角不同而大相径庭。
34 00:03:15,516 --> 00:03:21,402 讲者：如果您将其视为倾斜的方形，您会极其敏感于边缘是否与您的矩形坐标系对齐。
35 00:03:22,204 --> 00:03:27,908 讲者：如果您将其视为直立菱形，边缘不会对齐，您对角度是否为直角完全无意识。
36 00:03:29,229 --> 00:03:32,332 讲者：我们关注的是与矩形坐标系的对齐，而不是直角。
37 00:03:33,533 --> 00:03:34,514 说话人 说话人_00: 这又是更多的证据。
38 00:03:36,031 --> 00:03:46,204 说话人 说话人_00: 如果你拿这两个形状，它们是相同的，但我从不同的角度给你展示，你可以把它们组装成一个四面体。
39 00:03:46,223 --> 00:03:53,092 说话人 说话人_00: 但是如果你让麻省理工学院的教授来做这个，他们需要超过一分钟的时间来把这两个部件组装成一个四面体。
40 00:03:53,813 --> 00:03:59,441 说话人 说话人_00: 现在它是一个两片拼图，你必须把相同形状的面拼在一起，这能有多难呢？
41 00:04:01,143 --> 00:04:04,046 讲者 SPEAKER_00: 我们需要解释为什么这如此困难。
42 00:04:05,191 --> 00:04:12,419 讲者 SPEAKER_00: 事实上，麻省理工学院的教授在麻省理工学院任教的时间越长，解决问题所需的时间就越长。
43 00:04:12,438 --> 00:04:25,012 讲者 SPEAKER_00: 所以这里发生的事情是，如果你看右边的部分，有一个自然坐标系，其中两个轴与桌面平行，第三个轴与重力方向一致。
44 00:04:26,954 --> 00:04:32,480 讲者 SPEAKER_00: 这个坐标系与我们通常使用的矩形坐标系完全无关。
45 00:04:32,747 --> 00:04:34,509 讲者 SPEAKER_00：为了感知四面体。
46 00:04:34,529 --> 00:04:44,841 讲者 SPEAKER_00：更重要的是，这两部分的自然坐标系在组合成四面体时并不对齐。
47 00:04:44,862 --> 00:04:54,454 讲者 SPEAKER_00：所以因为你看到的是相对于矩形坐标系，你根本看不到这些部分是如何组成四面体的，尽管这是显而易见的。
48 00:04:54,494 --> 00:05:01,422 讲者 SPEAKER_00：还有许多其他解决方法，比如，我最好把面拼在一起，使得剩下的面都是三角形。
49 00:05:01,874 --> 00:05:03,315 讲者：那么这很容易解决。
50 00:05:03,336 --> 00:05:05,077 讲者：但不知为什么，人们没有这么做。
51 00:05:05,158 --> 00:05:08,583 讲者：他们试图看它如何进入四面体，但他们看不到。
52 00:05:09,324 --> 00:05:10,966 讲者：这是因为矩形坐标系。
53 00:05:12,026 --> 00:05:14,310 说话人 SPEAKER_00: 在我进一步讲述之前，我应该先声明一下。
54 00:05:15,632 --> 00:05:19,416 说话人 SPEAKER_00: 这个演讲不是关于整个视觉的。
55 00:05:19,737 --> 00:05:21,319 说话人 SPEAKER_00: 视觉基本上是一个采样过程。
56 00:05:22,420 --> 00:05:27,627 说话人 SPEAKER_00: 我们会智能地选择下一步看哪里，这样我们就不必以高分辨率处理整个图像。
57 00:05:29,362 --> 00:05:32,107 演讲者 SPEAKER_00: 但对于每次注视，我们都会重复使用相同的神经网络。
58 00:05:33,007 --> 00:05:35,531 演讲者 SPEAKER_00: 这讨论了第一次注视时发生的事情。
59 00:05:37,334 --> 00:05:41,319 演讲者 SPEAKER_00: 所以这是理论三的一个版本，我称之为身份特定胶囊。
60 00:05:42,000 --> 00:05:44,785 演讲者 SPEAKER_00: 我和几位非常优秀的合作者一起研究了这个课题好几年。
61 00:05:45,565 --> 00:05:52,555 讲者 SPEAKER_00: 想法是将关于特定形状的知识定位到一组特定的神经元中，我称之为胶囊。
62 00:05:53,377 --> 00:05:56,822 讲者 SPEAKER_00: 这个组通过其权重知道所有部分与整体的关系。
63 00:05:58,252 --> 00:06:04,101 讲者 SPEAKER_00: 我们通过注意到多个不同的部分预测整体相同的姿态来识别形状。
64 00:06:05,084 --> 00:06:12,495 讲者 SPEAKER_00: 我所说的整体姿态是指视网膜与整体或部分内在框架之间的坐标变换。
65 00:06:14,017 --> 00:06:16,422 说话者 说话者_00：这就是它是如何工作的。
66 00:06:16,442 --> 00:06:22,692 说话者 说话者_00：假设你识别出你认为是一个嘴巴，你也识别出你认为是一个鼻子。
67 00:06:23,396 --> 00:06:34,312 说话者 说话者_00：在这些椭圆内部，有代表这些部分细节的神经活动，包括它们的姿态，即它们与摄像机的相对位置。
68 00:06:35,153 --> 00:06:38,259 说话者 说话者_00：你还有一个你认为该物体存在的概率。
69 00:06:38,478 --> 00:06:39,461 说话人 说话人_00: 那就是那些 P。
70 00:06:40,562 --> 00:06:50,637 说话人 说话人_00: 每个部分都可以对洞的可能形状、所属的洞、身份的可能以及姿态的可能进行预测。
71 00:06:51,949 --> 00:07:04,810 说话人 说话人_00: 如果你得到几个在身份上达成一致的部分，箭头指向同一个更高层次的胶囊，并且它们在姿态上也达成一致，那么你知道它们处于正确的几何关系中，可以形成洞。
72 00:07:06,733 --> 00:07:15,425 说话人 说话人_00: 这就是 Hough 变换，这是我非常努力追求的一种形状识别理论，我仍然相信其中有很多东西。
73 00:07:16,029 --> 00:07:22,240 讲者 SPEAKER_00：这就是从不同部分预测出的姿态之间的协议，这是整体的真正特征。
74 00:07:23,661 --> 00:07:30,913 讲者 SPEAKER_00：所以，理论 3 的一个新变体是让胶囊不再特定于特定形状。
75 00:07:32,175 --> 00:07:34,338 讲者 SPEAKER_00：所以我称这为通用胶囊。
76 00:07:35,420 --> 00:07:36,762 讲者 SPEAKER_00：它们将会无处不在。
77 00:07:36,802 --> 00:07:38,646 说话人 SPEAKER_00: 我们将它们复制到各个地方。
78 00:07:39,987 --> 00:07:42,391 说话人 SPEAKER_00: 我们将图像划分为许多小区域。
79 00:07:42,827 --> 00:07:48,036 说话人 SPEAKER_00: 在每个区域，我们分配硬件来表示占据该区域的内容。
80 00:07:48,896 --> 00:07:50,178 说话人 SPEAKER_00: 这将是一个视网膜映射图。
81 00:07:51,862 --> 00:07:55,307 说话人 SPEAKER_00: 但我们也在每个位置有不同的表示级别。
82 00:07:57,389 --> 00:08:02,156 说话人 SPEAKER_00: 因此，对于每个表示级别，我们都会有一个通用胶囊。
83 00:08:02,677 --> 00:08:12,091 说话人 SPEAKER_00: 这是一个胶囊，可以表示该位置在该表示级别上的任何事物，但每个位置只有一个。
84 00:08:13,321 --> 00:08:28,747 说话人 SPEAKER_00: 这就是迭代后，即循环神经网络稳定下来时，向其展示静态图像的帧序列后，不同表示级别的嵌入应该看起来像什么。
85 00:08:30,610 --> 00:08:39,244 说话人 SPEAKER_00: 将会有一个最低层的嵌入，这很可能是由卷积神经网络产生的。
86 00:08:39,528 --> 00:08:41,850 说话人 SPEAKER_00: 这些嵌入由箭头表示。
87 00:08:42,270 --> 00:08:44,013 说话人 SPEAKER_00: 当然，箭头只是二维向量。
88 00:08:44,052 --> 00:08:46,735 说话人 SPEAKER_00: 事实上，这些将是具有数千维的向量。
89 00:08:47,096 --> 00:08:48,798 说话人 SPEAKER_00: 然后这里有一个嵌入层的层次结构。
90 00:08:49,119 --> 00:08:57,849 说话人 SPEAKER_00: 在顶部，在这个描绘中还没有稳定下来，我们希望场景的嵌入向量在各个地方都是完全相同的。
91 00:08:57,869 --> 00:09:09,062 说话人 SPEAKER_00: 在下一级，我们希望一个特定物体，比如一个脸，所占据的所有位置都有相同的嵌入向量，完全相同的嵌入向量。
92 00:09:10,307 --> 00:09:15,836 说话人 SPEAKER_00: 在这个层次之下，对于脸的各个部分，我们希望每个部分都有一个相同的嵌入向量。
93 00:09:16,417 --> 00:09:20,503 讲者：也许这些红色箭头代表鼻子，绿色箭头代表嘴巴。
94 00:09:21,465 --> 00:09:28,836 讲者：在鼻子下方，我们可能会有鼻梁和鼻孔等单独的子部分。
95 00:09:29,255 --> 00:09:35,125 讲者：它们将具有不同的嵌入向量，但整个鼻梁将具有相同的嵌入向量，即使它占据了多个位置。
96 00:09:37,067 --> 00:09:39,792 讲者：所以这里发生的事情是我们有岛屿般的共识。
97 00:09:40,514 --> 00:09:48,485 讲者 SPEAKER_00: 这些共识岛屿将代表图像的路径，它如何在多个层面上分割成洞和部分。
98 00:09:50,609 --> 00:09:55,437 讲者 SPEAKER_00: 如果你是物理学家，可以这样想，这是一个多级实值伊辛模型。
99 00:09:56,899 --> 00:09:59,241 讲者 SPEAKER_00: 但它在不同层级之间存在坐标变换。
100 00:10:00,605 --> 00:10:08,817 讲者 SPEAKER_00: 而自旋不再是二进制，而是这些实值向量，这应该有助于局部最小值和避免卡住。
101 00:10:10,331 --> 00:10:19,246 讲者 SPEAKER_00：终于，这是一张单个位置的 Glom 架构图，即前一张图中的一列。
102 00:10:21,289 --> 00:10:31,225 讲者 SPEAKER_00：所以在那个位置，我们将为多个表示级别预留神经元，在这个展示中是 L-1、L 和 L+1 级别。
103 00:10:32,749 --> 00:10:34,652 讲者 SPEAKER_00：它将是一个循环网络。
104 00:10:35,341 --> 00:10:43,732 讲者 SPEAKER_00：它使用相同的权重，在各个时间切片上稳定下来，尽管它可能有像温度这样的超参数，这些参数会随时间变化。
105 00:10:44,813 --> 00:10:45,995 讲者 SPEAKER_00: 基本上，权重是相同的。
106 00:10:48,038 --> 00:10:51,703 讲者 SPEAKER_00: 在一个位置内，你会得到三种类型的交互。
107 00:10:52,764 --> 00:10:56,049 讲者 SPEAKER_00: 如果我们关注右侧的这个级别 L 嵌入，
108 00:10:57,143 --> 00:11:05,054 讲者 SPEAKER_00: 它从这个位置级别 L-1 嵌入接收输入，通过一个具有几个隐藏层的自下而上的神经网络。
109 00:11:06,476 --> 00:11:19,336 讲者 SPEAKER_00: 这种自下而上的神经网络将取一个位于 L-1 层的类似鼻子的结构，并预测应该占据这个位置的鼻子在更高一级的位置。
110 00:11:21,325 --> 00:11:31,938 讲者 SPEAKER_00: 我们还将通过一个具有几个隐藏层的自上而下的神经网络获得自上而下的输入，该神经网络将取 L+1 层的表示，这可能是该位置的一个脸。
111 00:11:32,639 --> 00:11:35,583 讲者 SPEAKER_00: 然后，它将从脸部预测鼻子。
112 00:11:36,205 --> 00:11:38,528 讲者 SPEAKER_00: 因此，在脸和鼻子之间存在一个坐标转换。
113 00:11:39,990 --> 00:11:45,876 讲者 SPEAKER_00: 我们也得到了那些绿色箭头，对于静态图像来说，你只是在做时间积分。
114 00:11:46,357 --> 00:11:50,082 讲者 SPEAKER_00: 对于动态图像，它们会更有趣，因为它们需要编码一些动态变化。
115 00:11:51,464 --> 00:11:57,613 讲者 SPEAKER_00: 还有一种其他类型的交互没有在这个图中显示，因为它是在不同位置之间的交互。
116 00:11:58,232 --> 00:12:00,956 讲者 SPEAKER_00: 这个图完全关于一个位置内的交互。
117 00:12:02,278 --> 00:12:04,842 说话人 SPEAKER_00：您已经看到了蓝色、红色和绿色的交互。
118 00:12:05,583 --> 00:12:10,029 说话人 SPEAKER_00：在位置之间，我们将做一些类似 transformer 的事情，但更简单。
119 00:12:10,465 --> 00:12:18,355 说话人 SPEAKER_00：我们将允许位置通过简单地平均嵌入与附近的地点交互。
120 00:12:19,197 --> 00:12:23,402 说话人 SPEAKER_00：换句话说，每个位置都会预测其邻居的嵌入应该与其位置相同。
121 00:12:24,744 --> 00:12:26,687 讲者 SPEAKER_00: 但这些将是注意力加权平均。
122 00:12:27,388 --> 00:12:32,995 讲者 SPEAKER_00: 也就是说，如果你与另一个位置相似，你会强烈预测它应该与你相同。
123 00:12:34,356 --> 00:12:38,042 讲者 SPEAKER_00: 如果你不同，你只会做出非常弱的预测，认为它应该与你相同。
124 00:12:39,085 --> 00:12:44,091 讲者 SPEAKER_00: 所以这是在 Transformer 中使用的那种注意力，但这里的交互要简单得多。
125 00:12:44,753 --> 00:12:47,035 讲者 SPEAKER_00: 没有关键字、查询和值的问题。
126 00:12:47,616 --> 00:12:50,541 讲者 SPEAKER_00: 在这里，键中的嵌入和值中的查询都是同一件事。
127 00:12:51,543 --> 00:13:02,238 讲者 SPEAKER_00: 因此，一个位于 X 位置的 L 级嵌入与一个位于 Y 位置的 L 级嵌入的交互方式是，我们简单地取这两个嵌入的标量积。
128 00:13:03,139 --> 00:13:05,682 讲者 SPEAKER_00: 因此，LX 是位于 X 位置的 L 级嵌入。
129 00:13:06,676 --> 00:13:12,100 讲者 SPEAKER_00: 然后我们对这些标量积进行指数化并重新归一化，这样所有数值加起来就等于 1。
130 00:13:13,802 --> 00:13:20,688 讲者 SPEAKER_00: 那这样做，指数化的作用就是，如果拟合得很好，那么它将完全主导许多不合适的拟合。
131 00:13:22,691 --> 00:13:30,519 讲者 SPEAKER_00: 现在，这种注意力加权平均的作用是，它将导致层次学习嵌入形成这些岛屿。
132 00:13:31,458 --> 00:13:34,201 讲者 SPEAKER_00: 实际上，我们正在创造回音室。
133 00:13:35,009 --> 00:13:42,740 讲者 SPEAKER_00: 随着它逐渐稳定下来，你会得到这些都相互一致并且越来越强烈地相互一致的嵌入组。
134 00:13:43,461 --> 00:13:48,307 讲者 SPEAKER_00: 回声室在政治中是件坏事，但正是我们想要用于图像分割的。
135 00:13:49,950 --> 00:13:52,032 讲者 SPEAKER_00: 现在我想要谈谈自下而上的神经网络。
136 00:13:52,653 --> 00:13:54,635 讲者 SPEAKER_00: 我没有时间谈论自上而下的神经网络。
137 00:13:54,655 --> 00:13:57,279 讲者 SPEAKER_00：如果您想了解更多，请参阅我的关于档案的论文。
138 00:13:57,659 --> 00:14:03,327 讲者 SPEAKER_00：所以当您试图弄清楚图像中发生的事情时，您会发现一些可能存在歧义的部分。
139 00:14:03,645 --> 00:14:08,753 讲者 SPEAKER_00：您可能已经发现了一些可能是一张嘴，但您不确定，还有可能是一张鼻子，但您不确定。
140 00:14:09,634 --> 00:14:11,436 讲者 SPEAKER_00：您希望它们能够互相消除歧义。
141 00:14:12,538 --> 00:14:17,224 说话人 SPEAKER_00：如果它们处于正确的关系，那么它们就会相互确认。
142 00:14:19,046 --> 00:14:23,110 说话人 SPEAKER_00：实现这一点的办法之一是让这些模糊部分之间有直接交互。
143 00:14:23,871 --> 00:14:25,995 说话人 SPEAKER_00：这就是我所说的变换随机场。
144 00:14:27,157 --> 00:14:32,082 说话人 SPEAKER_00：它之所以称为变换，是因为每次交互都涉及坐标变换。
145 00:14:33,397 --> 00:14:44,289 说话人 SPEAKER_00：如果一个鼻子在寻找可能支持它的嘴巴，那么鼻子需要说的是，嘿，我是一个鼻子，我正在寻找具有以下姿态的嘴巴。
146 00:14:45,010 --> 00:14:56,163 说话人 SPEAKER_00：换句话说，它必须采用鼻子的姿态，即它与摄像机的相对位置，乘以鼻子与嘴巴之间的相对位置，这样它就会寻找这样的嘴巴来确认自己。
147 00:14:56,988 --> 00:15:04,697 说话人 SPEAKER_00：如果存在这样的数学，那么数学就需要发送一条消息，这条消息执行逆坐标变换，以表明，嗯，我并不是你想要的那个。
148 00:15:05,038 --> 00:15:05,840 说话人 SPEAKER_00：我有这个姿态。
149 00:15:06,220 --> 00:15:10,505 说话者 SPEAKER_00: 因此，我确认应该有一个鼻子，这个姿势有点像你想要的，但并不完全一样。
150 00:15:12,589 --> 00:15:13,691 说话者 SPEAKER_00: 这相当复杂。
151 00:15:14,010 --> 00:15:17,215 说话者 SPEAKER_00: 这就是使用集合变换器尝试做这件事时可能发生的事情。
152 00:15:18,456 --> 00:15:20,679 说话者 SPEAKER_00: 但你需要 n 平方阶的交互。
153 00:15:20,740 --> 00:15:22,783 说话人 SPEAKER_00：每一次交互都涉及坐标变换。
154 00:15:23,663 --> 00:15:25,206 说话人 SPEAKER_00：还有另一种做生意的方式。
155 00:15:26,738 --> 00:15:28,541 说话人 SPEAKER_00：那就是使用一种叫做霍夫变换的东西。
156 00:15:29,642 --> 00:15:37,934 说话人 SPEAKER_00：不是允许各个部分直接交互，而是让每个部分对整个身份做出模糊的多模态预测。
157 00:15:39,236 --> 00:15:42,982 说话人 SPEAKER_00: 也就是说，对占据相同位置的整个物体施加的标识。
158 00:15:44,524 --> 00:15:49,011 说话人 SPEAKER_00: 所以你只谈论一件事，那就是占据这个位置的任何物体。
159 00:15:49,873 --> 00:15:51,936 说话人 SPEAKER_00: 由于这个原因，你不需要任何动态路由。
160 00:15:53,197 --> 00:15:56,844 说话人 SPEAKER_00: 现在，如果许多这些多模态预测一致，整体就会存在。
161 00:15:56,903 --> 00:16:06,740 讲者：我们希望所进行的注意力加权平均能够挑选出这种一致性。
162 00:16:08,543 --> 00:16:18,260 讲者：为了让它工作，我们需要在每个局部列中，低层预测一个未归一化的对数概率分布
163 00:16:18,427 --> 00:16:22,591 讲者：在所有可能的对象实例和姿态的空间上。
164 00:16:23,312 --> 00:16:24,734 讲者：这是所有这些的叉积空间。
165 00:16:26,395 --> 00:16:28,197 说话者 SPEAKER_00：这相当复杂，难以表示。
166 00:16:28,859 --> 00:16:35,567 说话者 SPEAKER_00：但如果我们可以做出部分预测，那么简单的平均就能找到一致的预测。
167 00:16:35,647 --> 00:16:36,989 说话者 SPEAKER_00：它会挑选出共同模式。
168 00:16:37,710 --> 00:16:42,414 说话者 SPEAKER_00：但我们必须使用注意力加权平均，这样我们就不尝试用非常不同的事物进行平均。
169 00:16:44,376 --> 00:16:46,559 说话人 SPEAKER_00：要在神经网络中实现这类东西，
170 00:16:47,418 --> 00:17:07,942 说话人 SPEAKER_00：你需要的是在每个对象级别的嵌入向量中的每个神经元代表一个基函数，这个基函数在未归一化的对数概率空间中是一个非常模糊的分布，这个分布是身份映射的乘积，以及可能的变形。
171 00:17:07,961 --> 00:17:15,770 说话人 SPEAKER_00：神经元的活性会缩放这个对数分布和完整的嵌入向量
172 00:17:16,003 --> 00:17:24,517 说话人 SPEAKER_00：表示所有这些缩放分布的总和，这与各个分布的乘积相同，只是它未归一化。
173 00:17:26,759 --> 00:17:34,270 讲者 SPEAKER_00：个体分布可能非常模糊，因为它们只需要代表一件事，那就是占据该位置的对象。
174 00:17:34,711 --> 00:17:42,163 讲者 SPEAKER_00：它们永远不需要同时代表两件事，但它们需要代表关于这一件事可能性的概率分布。
175 00:17:43,290 --> 00:18:01,931 讲者 SPEAKER_00：现在，这个理论的一个大问题，还有许多其他大问题，但这个理论的一个大问题是，自下而上和自上而下的神经网络需要在每个位置都相同，因为现在我们正在使胶囊通用，并且希望通用的胶囊无处不在。
176 00:18:01,951 --> 00:18:03,011 讲者 SPEAKER_00：你希望它们无处不在。
177 00:18:04,513 --> 00:18:06,015 演讲者 演讲者_00：在计算机上这没问题。
178 00:18:06,035 --> 00:18:11,922 演讲者 演讲者_00：在计算机上很棒，因为你可以从内存中一次性获取这些网络之一的权重，然后可以在所有这些不同的位置使用它。
179 00:18:12,261 --> 00:18:13,163 演讲者 演讲者_00：它是卷积的。
180 00:18:14,324 --> 00:18:17,028 演讲者 演讲者_00：但在大脑中这似乎非常浪费。
181 00:18:19,051 --> 00:18:23,921 讲者 SPEAKER_00: 这很浪费，因为你似乎需要分别学习所有这些神经网络。
182 00:18:25,262 --> 00:18:34,459 讲者 SPEAKER_00: 我们知道卷积网络建立在视觉数据分布大致各向同性的基础上。
183 00:18:34,479 --> 00:18:35,760 讲者 SPEAKER_00: 几乎到处都一样。
184 00:18:36,102 --> 00:18:37,884 讲者 SPEAKER_00: 你希望到处都有同样的知识。
185 00:18:39,821 --> 00:18:47,869 讲者：这似乎很疯狂，要在每个位置分别以自下而上和自上而下的神经网络形式学习同样的知识。
186 00:18:48,651 --> 00:18:51,294 讲者：所以问题是，我们如何在大脑中实现权重共享的效果？
187 00:18:52,615 --> 00:18:54,477 讲者：这就是演讲的第二部分将要讨论的内容。
188 00:18:56,019 --> 00:19:04,647 讲者：我将简要介绍由 Rich Caruana 发明的称为蒸馏的技术。
189 00:19:05,184 --> 00:19:10,491 说话人 SPEAKER_00：他称之为压缩，而我大约 10 年后重新发明了它。
190 00:19:11,932 --> 00:19:17,840 说话人 SPEAKER_00：蒸馏是一种从模型中提取知识并将其放入不同结构的模型中的方法。
191 00:19:20,063 --> 00:19:29,454 说话人 SPEAKER_00：它通常被使用，现在相当广泛地使用，将大模型或大模型集的知识转换为更小的模型。
192 00:19:30,717 --> 00:19:39,769 说话人 SPEAKER_00：例如，如果我们有一个在大数据中心运行的语音识别大模型，我们希望将同样的知识放入一个可以在我们的手机上运行的更小的模型中。
193 00:19:41,010 --> 00:19:42,512 演讲者 SPEAKER_00：大型语言模型也是如此。
194 00:19:43,012 --> 00:19:44,795 演讲者 SPEAKER_00：这就是现在蒸馏所用的。
195 00:19:45,435 --> 00:19:46,096 演讲者 SPEAKER_00：它在这一点上做得很好。
196 00:19:46,978 --> 00:19:48,641 演讲者 SPEAKER_00：我将提出对它的另一种用途。
197 00:19:51,003 --> 00:19:54,327 演讲者 SPEAKER_00: 但我想从蒸馏的类比开始。
198 00:19:55,287 --> 00:19:58,112 演讲者 SPEAKER_00: 如果你观察毛毛虫和蝴蝶，它们看起来很不一样。
199 00:19:58,933 --> 00:20:06,767 演讲者 SPEAKER_00: 这是因为毛毛虫是为了从环境中提取营养而优化的，而蝴蝶是为了迁徙和交配而优化的。
200 00:20:08,631 --> 00:20:18,307 演讲者 SPEAKER_00: 现在，在机器学习中，我们通常从训练数据中提取结构，这就是我们学到的模型。
201 00:20:19,317 --> 00:20:30,272 说话人 SPEAKER_00: 但现在我们更了解神经网络后，如果你有一个非常大的模型，参数非常多，或者你有大量的模型集成，提取结构就变得很容易。
202 00:20:31,355 --> 00:20:34,880 说话人 SPEAKER_00: 它们使得提取规律变得容易，但操作起来非常繁琐。
203 00:20:35,840 --> 00:20:37,462 说话人 SPEAKER_00: 它们并没有给我们一个小的生产模型。
204 00:20:38,744 --> 00:20:45,214 说话人 SPEAKER_00: 因此解决方案就是像昆虫那样进行变态。
205 00:20:45,976 --> 00:20:47,518 说话人 SPEAKER_00: 取幼虫阶段，
206 00:20:47,768 --> 00:20:50,671 说话人 SPEAKER_00: 并从那个幼虫阶段，构建出截然不同的事物。
207 00:20:51,372 --> 00:20:53,193 说话人 SPEAKER_00: 而蒸馏在机器学习中正是如此。
208 00:20:54,494 --> 00:20:58,318 说话人 SPEAKER_00: 所以你学到的这个庞大而笨重的模型已经从数据中抽取了结构。
209 00:20:59,800 --> 00:21:01,603 说话者 SPEAKER_00: 但它很大很笨拙。
210 00:21:03,305 --> 00:21:09,211 说话者 SPEAKER_00: 我们倾向于从模型架构和参数值的角度来考虑模型。
211 00:21:09,971 --> 00:21:15,597 说话者 SPEAKER_00: 但还有一种完全不同的思考模型的方式，那就是它是一个从输入到输出的函数。
212 00:21:15,847 --> 00:21:22,939 说话者 SPEAKER_00: 如果我给它很多不同的输入向量，并观察它给出的所有不同的输出向量，这样就能确定函数是什么。
213 00:21:24,180 --> 00:21:28,006 讲者 SPEAKER_00：我们可以忘记大模型的架构和权重，只关注其功能。
214 00:21:29,008 --> 00:21:33,734 讲者 SPEAKER_00：我们可以通过将这个功能转移到小模型来转移大模型中的知识。
215 00:21:35,537 --> 00:21:38,801 讲者 SPEAKER_00：而这个功能可以被转移到具有完全不同架构的模型中。
216 00:21:40,367 --> 00:21:46,594 讲者 SPEAKER_00：所以如果你有一个大模型，假设其输出是 n 个不同类别之间的 n-way softmax。
217 00:21:47,515 --> 00:21:50,137 说话人 SPEAKER_00：目标通常只有一个，后面跟着一大串零。
218 00:21:52,039 --> 00:21:59,426 说话人 SPEAKER_00：这意味着你给它赋予的标签，平均来说，最多只能对函数施加 log n 比特约束。
219 00:21:59,988 --> 00:22:01,048 说话人 SPEAKER_00：你给它一个输入图像。
220 00:22:01,388 --> 00:22:02,369 说话人 SPEAKER_00：这个图像中包含了很多比特。
221 00:22:02,931 --> 00:22:04,051 说话者 说话者_00: 然后你告诉它标签。
222 00:22:04,412 --> 00:22:07,454 说话者 说话者_00: 标签只对函数施加少量约束。
223 00:22:08,801 --> 00:22:28,231 说话者 说话者_00: 但如果我们手里有一个大模型，我们取 logits，即你放入 softmax 中的东西，然后除以大于 1 的温度，以得到软分布，大模型输出的软分布揭示了关于它所相信的函数的更多信息。
224 00:22:29,253 --> 00:22:33,480 说话者 说话者_00: 所以在这里的顶部，可能是单个图像的硬目标。
225 00:22:33,540 --> 00:22:35,063 说话人 SPEAKER_00: 这就是数据库中的内容。
226 00:22:35,380 --> 00:22:41,125 说话人 SPEAKER_00: 如果你查看一个已经训练好的大模型的输出，它会非常自信地认为是一只狗，但可能是一只猫。
227 00:22:42,046 --> 00:22:45,469 说话人 SPEAKER_00: 这并不能提供很多关于功能的信息。
228 00:22:46,210 --> 00:22:58,180 说话人 SPEAKER_00: 但如果我们现在通过使用高温度的 softmax 来软化这些输出，也就是说，我们只是将所有的 logits 除以这个温度，那么我们就会得到一个更加均匀的分布，一个更加柔和的分布。
229 00:22:58,901 --> 00:23:03,345 说话人 说话人_00：如果我们不软化输出，这开始显示出许多隐藏的东西。
230 00:23:04,220 --> 00:23:11,190 说话人 说话人_00：所以，在未软化的输出中，它认为这个物体是牛的概率是10的负6次方，是汽车的概率是10的负9次方。
231 00:23:11,770 --> 00:23:13,153 说话人 说话人_00：基本上，这两个概率都是0。
232 00:23:14,153 --> 00:23:17,940 说话人 说话人_00：如果我们软化它，现在牛和汽车之间的差异就变得显著了。
233 00:23:19,141 --> 00:23:24,288 说话人 SPEAKER_00：现在，这张图片实际上是一只狗，你可能认为牛和车之间的区别无关紧要。
234 00:23:24,308 --> 00:23:30,678 说话人 SPEAKER_00：但实际上，大模型的大部分知识，如果不是全部知识，都不在于正确答案是什么。
235 00:23:31,138 --> 00:23:33,602 说话人 SPEAKER_00：它在于错误答案的相对概率。
236 00:23:34,914 --> 00:23:46,195 说话人 SPEAKER_00：这就是为什么在多选题中，让人们给出第二个答案非常有教育意义，因为这样你可以看到他们是否知道正确答案，而不是他们是否能够判断什么是有可能的，什么是不可能的。
237 00:23:48,681 --> 00:23:54,731 讲者 SPEAKER_00：所以这些软化的输出揭示了我所说的暗知识，因为在大型模型中，它们都将为零，或者说几乎为零。
238 00:23:55,933 --> 00:23:58,137 讲者 SPEAKER_00：这就是你要在小模型上训练的内容。
239 00:23:59,317 --> 00:24:03,461 讲者 SPEAKER_00：所以现在你是在训练小模型，使用的信息要丰富得多。
240 00:24:04,163 --> 00:24:07,886 讲者 SPEAKER_00：当然，当你训练小模型时，你也会在其 softmax 中使用高温。
241 00:24:08,929 --> 00:24:21,544 讲者 SPEAKER_00：所以如果我问你，这是什么物种，然后给你以下答案，这些答案都不正确，那么很明显，雪豹比老虎好得多，老虎比牛好得多，牛比胡萝卜好得多。
242 00:24:23,046 --> 00:24:25,188 讲者 SPEAKER_00：所以你注意到这里有一个类层次结构。
243 00:24:26,108 --> 00:24:29,053 讲者 SPEAKER_00：前三个都是动物，它们都比胡萝卜好得多。
244 00:24:30,028 --> 00:24:36,916 讲者 SPEAKER_00：在大型模型输出的软概率中，你可以看到这个类层次结构，你不需要将其放入其中。
245 00:24:37,251 --> 00:24:44,041 说话人 SPEAKER_00：通过在网络上找到一个某人编造的类层次结构，并将其告诉你的学习系统。
246 00:24:44,442 --> 00:24:47,067 说话人 SPEAKER_00：学习系统将自动识别这个类层次结构。
247 00:24:47,307 --> 00:24:52,935 说话人 SPEAKER_00：它将存在于大模型中，并在错误答案的可信度中体现出来。
248 00:24:53,476 --> 00:25:00,626 说话人 SPEAKER_00：你只需让它在这里给出正确的概率，就可以将所有这些转移到小模型中，当你已经软化输出时。
249 00:25:02,664 --> 00:25:04,868 说话人 SPEAKER_00: 如果您感兴趣的话，这是一个埃及毛。
250 00:25:05,650 --> 00:25:07,271 说话人 SPEAKER_00: 这里有一些 MNIST 的例子。
251 00:25:08,394 --> 00:25:11,519 说话人 SPEAKER_00: 我从 MNIST 中取了一些数字，并使用了一个大型的学习模型。
252 00:25:13,122 --> 00:25:19,751 说话人 SPEAKER_00: 我向您展示了当您软化输出时各种类别的概率。
253 00:25:20,614 --> 00:25:25,922 说话人 SPEAKER_00：如果你看中间那一行，最有可能的是 2，这是正确的。
254 00:25:26,324 --> 00:25:28,567 说话人 SPEAKER_00：但是第二可能的是 0。
255 00:25:28,588 --> 00:25:32,596 说话人 SPEAKER_00：你会注意到那个特定的 2 看起来非常像 0。
256 00:25:33,438 --> 00:25:38,848 说话人 SPEAKER_00：你知道它不是 0，但它比其他任何 2 都更像 0。
257 00:25:38,868 --> 00:25:46,964 说话人 SPEAKER_00: 所以当我告诉你它是一个看起来有点像 0 的 2 时，比仅仅告诉你它是一个 2 时，我给你提供的信息要多得多。
258 00:25:48,565 --> 00:25:53,170 说话人 SPEAKER_00: 事实上，我可以告诉你它是一个看起来有点像 0 但不像 5 的 2。
259 00:25:53,651 --> 00:25:55,492 说话人 SPEAKER_00: 这就是它最不像的东西。
260 00:25:56,173 --> 00:25:58,655 说话人 SPEAKER_00: 这就是那些软化输出告诉你的。
261 00:25:58,675 --> 00:26:03,781 讲者 SPEAKER_00：结果发现，如果你给他们提供这么丰富的输出去学习，神经网络会学得更快。
262 00:26:04,561 --> 00:26:08,707 讲者 SPEAKER_00：所以这里有一个在 MNIST 上的小实验，只是为了展示蒸馏是有效的。
263 00:26:09,728 --> 00:26:14,353 讲者 SPEAKER_00：我用两个 800 个隐藏单元的 vanilla 反向传播网络进行训练。
264 00:26:14,906 --> 00:26:18,271 讲者 SPEAKER_00：并使用 ReLU 作为非线性函数，测试错误为 146 个。
265 00:26:20,454 --> 00:26:31,832 说话人 SPEAKER_00：然后我用更大的网络，更大的隐藏层，加上 dropout，加上权重约束，加上输入抖动，所以各种正则化，错误率降低到 67 个。
266 00:26:31,852 --> 00:26:32,913 说话人 SPEAKER_00：错误率大约减半。
267 00:26:35,017 --> 00:26:38,923 说话人 SPEAKER_00：现在，如果我使用那个大网络的软输出
268 00:26:39,173 --> 00:26:42,637 说话人 SPEAKER_00：并按照之前训练小网络的方式训练它。
269 00:26:42,877 --> 00:26:45,842 讲者 SPEAKER_00：当我在训练小网络时，我不使用 dropout 或权重约束或抖动。
270 00:26:46,363 --> 00:26:48,885 讲者 SPEAKER_00：我只是使用大网络的软输出来训练它。
271 00:26:50,067 --> 00:26:56,036 讲者 SPEAKER_00：所以，声称现在它有了更丰富的数据，因为它可以看到期望输出中事物之间的相似性。
272 00:26:57,198 --> 00:26:58,319 讲者 SPEAKER_00：确实，它有效。
273 00:26:59,500 --> 00:27:03,666 讲者 SPEAKER_00：当我用这些软目标训练小网络时，它出现了 74 个错误。
274 00:27:03,886 --> 00:27:05,890 讲者 SPEAKER_00：这比大网络稍差一些。
275 00:27:06,005 --> 00:27:11,474 讲者 SPEAKER_00：并且它的错误率几乎是小网络直接在数据上训练的一半。
276 00:27:11,494 --> 00:27:15,823 讲者 SPEAKER_00：这是因为软目标展示了小网络应该如何泛化。
他们显示了小网，好的，正确答案是2，但如果你必须猜测第二个答案，那将是0。
278 00：27：23,376 --> 00：27：24,578 议长 SPEAKER_00：这只是一个概括。
当你在训练小网络时，你正在教它以与大网络相同的方式进行泛化。
所以假设大网学习到了一个好的模型，当你训练小网时，你使用的是一个比我们通常使用的更好的目标函数。
281 00:27:38,707 --> 00:27:41,010 说话人 SPEAKER_00: 你不是在训练它得到正确答案。
282 00:27:41,451 --> 00:27:44,355 说话人 SPEAKER_00: 你在训练它以与大网相同的方式进行泛化。
283 00:27:45,017 --> 00:27:47,820 说话人 SPEAKER_00: 我们假设大网已经很好地进行了泛化。
284 00:27:47,840 --> 00:27:49,442 说话人 SPEAKER_00: 所以最终，你正在训练正确的东西。
285 00:27:49,462 --> 00:27:52,268 说话人 SPEAKER_00: 你正在训练它进行泛化，而不是让训练数据拟合。
286 00:27:53,829 --> 00:27:59,838 说话人 SPEAKER_00: 只是为了展示这个效果有多强大，我在所有 10 个数字类别上训练了一个大网络。
287 00:28:01,287 --> 00:28:04,693 说话人 SPEAKER_00: 然后我将大网络的知识迁移到小网络上。
288 00:28:05,655 --> 00:28:09,182 说话人 SPEAKER_00: 在迁移过程中，小网络从未见过数字 3。
289 00:28:09,784 --> 00:28:12,008 说话人 SPEAKER_00：就它而言，3 是一个神话般的数字。
290 00:28:12,648 --> 00:28:23,109 说话人 SPEAKER_00：它有输出类别 3，偶尔，如果比如它得到了一个看起来很像 3 的 2，它会被告知有非常小的概率那是一个 3。
291 00:28:23,781 --> 00:28:28,832 说话人 SPEAKER_00：经过这样的训练，从未见过 3，显然它认为 3 非常罕见。
292 00:28:29,553 --> 00:28:34,102 说话人 SPEAKER_00：所以我们就简单地提高了 3 的偏差，让它不再认为它们那么罕见。
293 00:28:34,422 --> 00:28:35,984 说话人 SPEAKER_00: 它认为它们和其他事物一样普遍。
294 00:28:36,686 --> 00:28:41,655 说话人 SPEAKER_00: 现在，如果你拿那个小浓缩网络，它在 3 秒内的表现非常好。
295 00:28:41,736 --> 00:28:45,282 说话人 SPEAKER_00: 它在 3 秒内的表现儿乎和其他类别一样好。
296 00:28:45,262 --> 00:28:56,217 说话人 SPEAKER_00: 因此，它仅仅通过被告知这个 2 有点像 3，7 有点像 3，这个 5 有点像 3，而 6 根本不像 3，就学会了 3 秒是什么样的。
297 00:28:57,358 --> 00:28:59,181 说话人 SPEAKER_00: 这就足以让你了解 3 是什么样子了。
298 00:29:01,002 --> 00:29:11,777 说话人 SPEAKER_00: 蒸馏的一个有趣之处在于，我们不必先训练一个大网络或一大组网络，然后再将知识蒸馏到另一个网络中，我们可以进行联合蒸馏。
299 00:29:12,482 --> 00:29:15,905 说话人 SPEAKER_00: 也就是说，我们可以同时训练 10 个小网络。
300 00:29:16,967 --> 00:29:21,672 说话人 SPEAKER_00: 如果我们独立训练它们，它们的平均测试误差为 158。
301 00:29:22,772 --> 00:29:24,494 说话人 SPEAKER_00：这些网络比我之前用的要小。
302 00:29:25,175 --> 00:29:28,138 说话人 SPEAKER_00：如果你将它们组合起来，你会得到 143 个错误。
303 00:29:29,480 --> 00:29:36,406 说话人 SPEAKER_00：但你可以做的另一件事是，在你训练它们的时候，你可以让每个网络尝试与其他网络的软答案达成一致。
304 00:29:37,528 --> 00:29:38,409 说话人 SPEAKER_00：这是同伴压力。
305 00:29:38,429 --> 00:29:42,472 说话人 SPEAKER_00: 你试图让它们输出分布与其他网络相似。
306 00:29:43,178 --> 00:29:49,186 说话人 SPEAKER_00: 现在看来，你首先得让它们分别学习一段时间，这样它们就不会立即变得太相似。
307 00:29:50,489 --> 00:29:52,992 说话人 SPEAKER_00: 然后你尝试让它们与其他网络达成一致。
308 00:29:53,993 --> 00:29:58,902 说话人 SPEAKER_00: 结果是，各个网络最终比之前要好得多。
309 00:30:00,564 --> 00:30:03,888 说话人 SPEAKER_00：这个组合的效果并不比单个网络好，但单个网络的效果更好。
310 00:30:04,250 --> 00:30:06,553 说话人 SPEAKER_00：他们在学习过程中共享了知识。
311 00:30:07,022 --> 00:30:12,651 说话人 SPEAKER_00：如果不同的网络看到不同的数据子集，这将特别有效。
312 00:30:13,491 --> 00:30:15,575 说话人 SPEAKER_00：但在这个案例中并没有发生这种情况。
313 00:30:15,635 --> 00:30:33,080 说话人 SPEAKER_00：但是如果你有如此多的数据以至于无法将其全部通过所有网络，如果你将不同的子集通过不同的网络运行并取平均值，那么这就是将神经网络从未见过的数据知识转移到神经网络中的方法，通过尝试与其他已经看到该数据的神经网络达成一致。
314 00:30:33,482 --> 00:30:35,525 说话人 SPEAKER_00：这基本上就是科学界的工作方式。
315 00:30:35,965 --> 00:30:41,571 说话人 SPEAKER_00：我们直接查看数据，但我们也阅读出版物，并试图与其他科学家的观点达成一致。
316 00:30:42,313 --> 00:30:50,622 说话人 SPEAKER_00：因此，共蒸馏提出了在 GLOM 等模型中，模型列如何共享神经网络之间的知识。
317 00:30:52,022 --> 00:30:59,912 说话人 SPEAKER_00：当我们平均一个嵌入与附近网络的嵌入时，我们在做什么
318 00:31:00,077 --> 00:31:05,065 说话人 SPEAKER_00：我们正在创建一个更好的训练信号，用于训练自下而上的神经网络。
319 00:31:06,988 --> 00:31:12,556 说话人 SPEAKER_00：所以我们将得到一个 L 级。我将它称为共识嵌入。
320 00:31:13,477 --> 00:31:27,637 说话人 SPEAKER_00：这是从 L 级该位置的底部向上预测的平均值，从 L+1 级该位置的顶部向下预测的平均值，以及在该级别附近列的注意力加权预测的平均值，
321 00:31:28,731 --> 00:31:37,962 说话者 SPEAKER_00: 将这些信息源的平均值与适当的超参数相结合，你将得到一个共识，即这些信息源之间认为的最佳选择。
322 00:31:39,064 --> 00:31:46,332 说话者 SPEAKER_00: 现在你可以训练自底向上的神经网络和自顶向下的神经网络，以尝试与这个共识达成一致。
323 00:31:46,352 --> 00:31:57,404 说话者 SPEAKER_00: 当然，这个共识包含了神经网络及其附近位置的意见，如果这些神经网络正在查看相同的部分或对象。
324 00:31:58,567 --> 00:32:06,876 说话者 SPEAKER_00: 所以如果我们处于对象层面，并且有一个属于同一面孔的附近位置，它将对面孔向量的样子有一个看法。
325 00:32:07,417 --> 00:32:23,477 讲者 SPEAKER_00：如果那个观点与我们的观点相似，因为它是附近位置上的同一张脸，它将为自下而上的神经网络和自上而下的神经网络提供额外的训练信号，尤其是对于从脸部的一部分预测脸部的自下而上的神经网络。
326 00:32:24,082 --> 00:32:31,309 讲者 SPEAKER_00：而这种训练信号正是允许知识从一个柱状体转移到另一个柱状体的原因。
327 00:32:33,030 --> 00:32:38,016 讲者 SPEAKER_00：所以我认为整个系统正在执行众多局部模型之间的共蒸馏。
328 00:32:39,238 --> 00:32:41,039 讲者 SPEAKER_00：这就是这些视网膜映射中发生的事情。
329 00:32:41,599 --> 00:32:44,663 说话者 SPEAKER_00：这就是为什么它们在统计上并不像你想的那么低效。
330 00:32:45,785 --> 00:32:50,308 说话者 SPEAKER_00：实际上，这种共蒸馏比权重共享有一个很大的优势。
331 00:32:50,769 --> 00:32:53,952 说话者 SPEAKER_00：即使你能做权重共享，也是非常困难的
332 00:32:54,405 --> 00:32:57,971 说话者 SPEAKER_00：要看到如何应用它，如果你有一个非均匀的视网膜。
333 00:32:59,412 --> 00:33:13,029 演讲者 演讲者_00：如果你从黄斑区向外移动时，感受野的大小增加，并且光感受器在网格中分布不均匀，那么你得到的是一个对视觉阵列的不同部分进行不同预处理的视网膜。
334 00:33:15,053 --> 00:33:18,557 演讲者 演讲者_00：在这些自下而上的神经网络中，你想要的不是针对视觉阵列该部分如何进行预处理的特定模型。
335 00:33:19,516 --> 00:33:25,626 演讲者 演讲者_00：你想要的不是一个特定于视觉阵列该部分如何进行预处理的模型。
336 00:33:26,347 --> 00:33:32,058 演讲者 演讲者_00：你希望不同的神经网络实现从视觉阵列到输出的相同功能。
337 00:33:33,181 --> 00:33:35,044 说话人 SPEAKER_00: 与共蒸馏，他们就是这样做的。
338 00:33:35,585 --> 00:33:43,920 说话人 SPEAKER_00: 共蒸馏可以将知识从一个列传递到另一个列，即使这些列对输入进行预处理的方式完全不同。
339 00:33:44,660 --> 00:33:46,863 说话人 SPEAKER_00: 这是你用权重共享无法做到的。
340 00:33:46,883 --> 00:33:48,586 说话人 SPEAKER_00: 所以这就是我要说的全部了。
341 00:33:48,625 --> 00:33:58,299 讲者 SPEAKER_00：本次演讲的主要观点就是建议大家，大脑不能进行权重共享并不意味着它不能获得卷积神经网络的大部分优势。
342 00:33:59,020 --> 00:34:12,000 讲者 SPEAKER_00：因为如果它愿意从附近的区域平均意见，然后让它的局部神经网络尝试与这些平均值达成一致，它就会进行共蒸馏，从而在位置之间传递知识。
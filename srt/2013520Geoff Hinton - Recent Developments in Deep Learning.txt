1
00:00:00,031 --> 00:00:14,067
Speaker SPEAKER_06: He's also inspired academics, professors like me and many others, because of his uncompromising views on research and science and aiming for big questions, real big questions.

2
00:00:15,628 --> 00:00:23,878
Speaker SPEAKER_06: He's also inspired many companies, including Google, to do groundbreaking research and to aim high for long-term big goals.

3
00:00:27,841 --> 00:00:33,911
Speaker SPEAKER_06: that can stand here and one can say unequivocally that he has inspired millions of people.

4
00:00:34,993 --> 00:00:36,837
Speaker SPEAKER_06: And it's not every day you meet someone like that.

5
00:00:37,618 --> 00:00:43,268
Speaker SPEAKER_06: And if you haven't been inspired by him, watch this talk.

6
00:00:43,287 --> 00:00:47,115
Speaker SPEAKER_07: Well, thanks for the introduction and after that it can only be a disappointment.

7
00:00:49,423 --> 00:00:54,350
Speaker SPEAKER_07: This is a talk that I prepared to give to ICASP, which is in Vancouver.

8
00:00:55,091 --> 00:00:56,893
Speaker SPEAKER_07: And they persuaded me to come and give it again here.

9
00:00:57,613 --> 00:01:06,605
Speaker SPEAKER_07: So it contains some fairly sort of basic stuff about deep neural nets that some of you will know very well, but others of you hopefully can benefit from.

10
00:01:06,625 --> 00:01:13,573
Speaker SPEAKER_07: I'm going to talk a little bit about the history of neural network research, particularly nets with lots of layers of features.

11
00:01:14,614 --> 00:01:18,840
Speaker SPEAKER_07: And very briefly about how they're used in speech systems now.

12
00:01:19,748 --> 00:01:30,040
Speaker SPEAKER_07: I'm going to talk about how we then extended that work to make them work for object recognition, and some of the tricks required to get things that were really good at object recognition.

13
00:01:31,683 --> 00:01:34,227
Speaker SPEAKER_07: And I'll talk about a couple of other successes of deep neural nets.

14
00:01:35,227 --> 00:01:42,817
Speaker SPEAKER_07: And then I'll end with some speculation about future speech recognition systems, which I think will be very different from current ones.

15
00:01:46,441 --> 00:01:47,123
Speaker SPEAKER_07: So.

16
00:01:48,688 --> 00:01:50,733
Speaker SPEAKER_07: If you want to recognize patterns, it's obvious how to do it.

17
00:01:50,893 --> 00:01:58,686
Speaker SPEAKER_07: You get some features from the image or whatever the input is, and then you learn what weights to associate with the features to decide which class it is.

18
00:01:59,447 --> 00:02:02,533
Speaker SPEAKER_07: And that's called pattern recognition or statistical pattern recognition.

19
00:02:03,594 --> 00:02:06,359
Speaker SPEAKER_07: And the sort of issues are where do the features come from?

20
00:02:06,379 --> 00:02:08,604
Speaker SPEAKER_07: So one thing you can do is you can hand engineer features.

21
00:02:09,445 --> 00:02:10,747
Speaker SPEAKER_07: People have done a lot of that.

22
00:02:11,622 --> 00:02:23,199
Speaker SPEAKER_07: Another thing you could do is you could say, I'm going to have a way of defining a feature by saying, I'll take certain of the input patterns I see, and I'll have some way of measuring how similar a test case is to those inputs.

23
00:02:24,161 --> 00:02:30,510
Speaker SPEAKER_07: And then I'll have some clever way of selecting which of the input patterns I should use, and a clever way of putting the weights on the ones that I select.

24
00:02:31,332 --> 00:02:32,955
Speaker SPEAKER_07: And that's called a support vector machine.

25
00:02:34,082 --> 00:02:43,859
Speaker SPEAKER_07: And this story would be a lot simpler if support vector machines had really been invented before backpropagation, because that's the natural order of things.

26
00:02:43,919 --> 00:02:46,102
Speaker SPEAKER_07: Unfortunately, we invented backpropagation too soon.

27
00:02:47,463 --> 00:02:49,647
Speaker SPEAKER_07: But let's suppose we invented it after support vector machines.

28
00:02:49,668 --> 00:02:50,489
Speaker SPEAKER_07: The story would be nicer.

29
00:02:51,491 --> 00:02:54,816
Speaker SPEAKER_07: And the last method, of course, is to learn what the features should be.

30
00:02:57,721 --> 00:02:58,622
Speaker SPEAKER_07: So.

31
00:02:58,719 --> 00:03:05,269
Speaker SPEAKER_07: In the mid-80s, actually in the 70s, Paul Wobus came up with the backpropagation algorithm, but didn't manage to convince anybody.

32
00:03:06,030 --> 00:03:12,079
Speaker SPEAKER_07: And then in the mid-80s, several people developed it, including Yann LeCun, and David Rommelhart, and Me and Ron Williams.

33
00:03:13,100 --> 00:03:17,606
Speaker SPEAKER_07: And backpropagation works by saying, we'll have a feedforward neural network.

34
00:03:19,796 --> 00:03:26,926
Speaker SPEAKER_07: And we'll compare what comes out of the network when we give it, say, an image to classify with what we want to get.

35
00:03:27,788 --> 00:03:29,711
Speaker SPEAKER_07: We'll take some measure of the discrepancy.

36
00:03:30,192 --> 00:03:31,993
Speaker SPEAKER_07: We'll send that backwards through the network.

37
00:03:33,056 --> 00:03:41,907
Speaker SPEAKER_07: And we'll use the chain rule so that we can compute for every weight in the network how changing that weight would change the discrepancy with the correct answer.

38
00:03:42,669 --> 00:03:46,935
Speaker SPEAKER_07: And then we'll just update the weights a little bit in the direction that reduces the discrepancy.

39
00:03:47,084 --> 00:03:50,288
Speaker SPEAKER_07: And so that's called stochastic online gradient descent.

40
00:03:51,048 --> 00:03:53,991
Speaker SPEAKER_07: And it turns out it's a very sort of primitive optimization method.

41
00:03:54,491 --> 00:04:00,258
Speaker SPEAKER_07: And for very big data sets, it works better than any of the fancy optimization methods we know how to find with a few tweaks.

42
00:04:01,118 --> 00:04:06,324
Speaker SPEAKER_07: And so we were very excited about backpropagation because we thought we finally solved the problem of where features come from.

43
00:04:06,966 --> 00:04:09,389
Speaker SPEAKER_07: You just have a multi-layer neural net, and you learn all the features.

44
00:04:10,008 --> 00:04:12,953
Speaker SPEAKER_07: And we make these smooth nonlinearities so we have nice derivatives.

45
00:04:13,492 --> 00:04:15,235
Speaker SPEAKER_07: And the problem is solved.

46
00:04:15,467 --> 00:04:17,269
Speaker SPEAKER_07: And in practice, it was a bit disappointing.

47
00:04:20,053 --> 00:04:23,257
Speaker SPEAKER_07: So people developed a story of why backpropagation didn't work.

48
00:04:23,978 --> 00:04:27,144
Speaker SPEAKER_07: And the story, I took this from one of my slides from a while ago.

49
00:04:27,163 --> 00:04:29,346
Speaker SPEAKER_07: So this was the story I was telling.

50
00:04:29,387 --> 00:04:34,733
Speaker SPEAKER_07: It doesn't work because you need too much labeled training data.

51
00:04:35,774 --> 00:04:38,538
Speaker SPEAKER_07: And almost all the data you can get easily is unlabeled.

52
00:04:39,598 --> 00:04:41,822
Speaker SPEAKER_07: Because you have to know what the right answer is to train these nets.

53
00:04:41,862 --> 00:04:45,129
Speaker SPEAKER_07: And you'd be much better off training a generative model that doesn't need to know the right answers.

54
00:04:47,372 --> 00:04:50,740
Speaker SPEAKER_07: The second was the learning time doesn't scale well.

55
00:04:51,000 --> 00:04:52,642
Speaker SPEAKER_07: If you have one hidden layer, it works quite well.

56
00:04:52,944 --> 00:04:55,387
Speaker SPEAKER_07: But as soon as you have multiple hidden layers, it learns much too slowly.

57
00:04:56,509 --> 00:05:02,822
Speaker SPEAKER_07: And then the normal thing, it can get stuck in local optima that aren't as good as they might be.

58
00:05:03,122 --> 00:05:06,208
Speaker SPEAKER_07: It's very hard in a high dimensional space to figure out what's really going on.

59
00:05:06,728 --> 00:05:15,223
Speaker SPEAKER_07: You don't know whether you're stuck in local optima until you develop a much better optimizer and discover that all the things you thought were local optima have downhill directions sloping out of them.

60
00:05:15,242 --> 00:05:16,625
Speaker SPEAKER_07: And so they weren't local optima after all.

61
00:05:18,908 --> 00:05:20,672
Speaker SPEAKER_07: But this story is actually nonsense.

62
00:05:21,733 --> 00:05:23,697
Speaker SPEAKER_07: That's not what's wrong with that propagation.

63
00:05:24,132 --> 00:05:27,975
Speaker SPEAKER_07: What's wrong with backpropagation is we didn't have enough labeled data, but the speech people did.

64
00:05:29,137 --> 00:05:33,141
Speaker SPEAKER_07: We didn't have fast enough computers, and we didn't have a sensible way to initialize the weights.

65
00:05:35,124 --> 00:05:37,646
Speaker SPEAKER_07: And if you fix those three problems, it works really well.

66
00:05:39,228 --> 00:05:40,228
Speaker SPEAKER_07: So it really is the answer.

67
00:05:41,589 --> 00:05:45,694
Speaker SPEAKER_07: At least it's the answer to where do features come from, or is there some plausible way of getting features.

68
00:05:46,214 --> 00:05:49,819
Speaker SPEAKER_07: It's maybe not where features come from in our brains, but it will work on a computer.

69
00:05:50,879 --> 00:05:58,348
Speaker SPEAKER_07: So since the mid-80s, we've got much bigger labeled data sets, first in speech and more recently in vision.

70
00:05:59,310 --> 00:06:02,415
Speaker SPEAKER_07: Computers got a whole lot faster, particularly with graphics processing units.

71
00:06:02,454 --> 00:06:05,819
Speaker SPEAKER_07: We suddenly got a factor of 30, which made a huge difference.

72
00:06:05,839 --> 00:06:09,625
Speaker SPEAKER_07: And we found better ways to initialize the weights without using labeled data.

73
00:06:11,827 --> 00:06:14,831
Speaker SPEAKER_07: And as a result, neural nets got to be much better at some things.

74
00:06:14,932 --> 00:06:18,557
Speaker SPEAKER_07: In particular, they got to be better at acoustic modeling for speech recognition.

75
00:06:18,891 --> 00:06:25,862
Speaker SPEAKER_07: So in speech recognition, you have hidden Markov models for phonemes that say this phoneme has this state, then this state, then this state.

76
00:06:26,384 --> 00:06:31,252
Speaker SPEAKER_07: Then you have to look at the sound wave and say, this bit of the sound wave could maybe be this state of this phoneme.

77
00:06:32,134 --> 00:06:39,285
Speaker SPEAKER_07: And so you need something to associate bits of the sound wave or coefficients you've extracted from the sound wave with individual states of hidden Markov models.

78
00:06:39,665 --> 00:06:40,706
Speaker SPEAKER_07: And of course, you don't know for sure.

79
00:06:40,747 --> 00:06:45,355
Speaker SPEAKER_07: So you need to take some coefficients extracted from the sound wave and say,

80
00:06:45,334 --> 00:06:49,562
Speaker SPEAKER_07: There's a probability of kind of 0.3, it's this bit of this phoneme, and 0.4, it's that bit of that phoneme, and so on.

81
00:06:50,262 --> 00:06:54,029
Speaker SPEAKER_07: And that's what deep neural networks are now routinely used for.

82
00:06:54,050 --> 00:06:57,254
Speaker SPEAKER_07: And they're replacing the previous method, which was Gaussian mixture models.

83
00:06:59,858 --> 00:07:02,584
Speaker SPEAKER_07: So I'm going to talk a little bit about initializing the weights.

84
00:07:02,850 --> 00:07:11,446
Speaker SPEAKER_07: Historically, this was very important in overcoming the belief that these deep neural networks were no good and could never be trained.

85
00:07:11,466 --> 00:07:12,728
Speaker SPEAKER_07: And that was a very strong belief.

86
00:07:13,569 --> 00:07:17,956
Speaker SPEAKER_07: A friend of mine sent a paper to ICML not that long ago.

87
00:07:17,937 --> 00:07:24,824
Speaker SPEAKER_07: And the referee said it shouldn't be accepted by ICML because it was about neural networks, and that wasn't appropriate at ICML.

88
00:07:24,845 --> 00:07:29,571
Speaker SPEAKER_07: In fact, if you look through ICML last year, there were no papers with neural in the title accepted.

89
00:07:29,990 --> 00:07:34,536
Speaker SPEAKER_07: So ICML should not accept papers about neural networks.

90
00:07:34,557 --> 00:07:35,557
Speaker SPEAKER_07: That was only a few years ago.

91
00:07:36,598 --> 00:07:43,286
Speaker SPEAKER_07: One of the IEEE journals actually had an official policy, as far as we can see, of not refereeing neural network papers, just sending them back.

92
00:07:46,101 --> 00:07:51,086
Speaker SPEAKER_07: So it was a strong belief and not entirely without evidence in its favor.

93
00:07:54,689 --> 00:07:59,774
Speaker SPEAKER_07: So here's how you can initialize the weights in one of these networks without needing labels.

94
00:07:59,793 --> 00:08:14,067
Speaker SPEAKER_07: You start by learning a generative model of the structure of the input variables and you do that using, one way to do that is to use a little kind of generative model that has input variables

95
00:08:14,468 --> 00:08:16,574
Speaker SPEAKER_07: And it has one layer of latent variables.

96
00:08:17,658 --> 00:08:19,043
Speaker SPEAKER_07: It has symmetric connections.

97
00:08:19,543 --> 00:08:22,894
Speaker SPEAKER_07: So this is really like a Markov random field that's bipartite.

98
00:08:23,877 --> 00:08:27,550
Speaker SPEAKER_07: It doesn't have any connections among the latent variables, which makes life much easier.

99
00:08:28,237 --> 00:08:32,202
Speaker SPEAKER_07: And it turns out the maximum likelihood training algorithm for this is quite complicated.

100
00:08:33,544 --> 00:08:38,090
Speaker SPEAKER_07: But there's an approximation to it, a very crude approximation, called contrast divergence.

101
00:08:38,110 --> 00:08:38,770
Speaker SPEAKER_07: It's very simple.

102
00:08:39,130 --> 00:08:42,335
Speaker SPEAKER_07: I'm not going to go into that because I want to say how this fits into the bigger story.

103
00:08:42,394 --> 00:08:43,015
Speaker SPEAKER_07: I don't have time.

104
00:08:43,797 --> 00:08:48,643
Speaker SPEAKER_07: But it turns out, given some data, there's a very simple way to train these nets, which are called restricted Boltzmann machines.

105
00:08:49,062 --> 00:08:53,650
Speaker SPEAKER_07: It's not maximum likelihood, it's not optimal, but it's fast and it does a reasonable job.

106
00:08:53,671 --> 00:08:55,315
Speaker SPEAKER_07: And that's good enough for our purposes.

107
00:08:56,096 --> 00:09:03,551
Speaker SPEAKER_07: It means you can show it some pixels here, you can train it quickly, and you get some feature detectors here that are capturing the correlations among pixels.

108
00:09:03,571 --> 00:09:06,496
Speaker SPEAKER_07: So you get things like little edge detectors or corner detectors or things here.

109
00:09:08,501 --> 00:09:17,095
Speaker SPEAKER_07: And then the real trick is, after you've done that, you can then take the vectors of activity of these hidden units and treat those as data and do it again.

110
00:09:17,956 --> 00:09:21,903
Speaker SPEAKER_07: So now we can take this as data and learn a second layer.

111
00:09:21,923 --> 00:09:23,264
Speaker SPEAKER_07: And you can then do it again.

112
00:09:23,304 --> 00:09:24,787
Speaker SPEAKER_07: So you can do it as many times as you like.

113
00:09:25,849 --> 00:09:31,977
Speaker SPEAKER_07: And the crucial property this has is that if you train a whole stack of models like this,

114
00:09:32,937 --> 00:09:35,318
Speaker SPEAKER_07: you can show that you can put them together into one big model.

115
00:09:36,039 --> 00:09:43,868
Speaker SPEAKER_07: And every time you train an extra hidden layer, you'll get a different bound on the log probability of the data.

116
00:09:44,508 --> 00:09:46,831
Speaker SPEAKER_07: And as you make the model deeper, that bound will get better.

117
00:09:48,011 --> 00:09:55,538
Speaker SPEAKER_07: In other words, you definitely win by adding an extra hidden layer in terms of your lower bound on the log probability of the training data.

118
00:09:56,559 --> 00:09:58,282
Speaker SPEAKER_07: So it's the right thing to do in that sense.

119
00:09:58,302 --> 00:09:59,602
Speaker SPEAKER_07: You're making progress.

120
00:10:00,240 --> 00:10:08,450
Speaker SPEAKER_07: Now, the proof, of course, requires various conditions, like you did maximum likelihood learning for each little restricted Boltzmann machine, and that the layers never get narrower.

121
00:10:09,311 --> 00:10:12,953
Speaker SPEAKER_07: And so we just violate the conditions of the proof outrageously all the time.

122
00:10:13,634 --> 00:10:20,883
Speaker SPEAKER_07: But the proof is nevertheless very important, because it shows that under certain circumstances, this business of learning a layer at a time is academically respectable.

123
00:10:21,602 --> 00:10:24,807
Speaker SPEAKER_07: There is some band that's improving, and so it's a reasonable thing to do.

124
00:10:24,866 --> 00:10:26,048
Speaker SPEAKER_07: It's not pure heuristic.

125
00:10:26,589 --> 00:10:29,792
Speaker SPEAKER_07: And that proof gives you enough cover so you can get on and do the research.

126
00:10:30,649 --> 00:10:33,332
Speaker SPEAKER_07: and not getting papers routinely rejected.

127
00:10:36,356 --> 00:10:39,600
Speaker SPEAKER_07: So here's how we used it in speech recognizers.

128
00:10:42,104 --> 00:10:47,289
Speaker SPEAKER_07: Each sort of green column here is a frame of coefficients you've extracted from the sound wave.

129
00:10:48,150 --> 00:10:51,054
Speaker SPEAKER_07: And in fact, the best thing to use is just filter bank outputs.

130
00:10:51,827 --> 00:10:56,692
Speaker SPEAKER_07: In speech, they developed more complicated things called Melchizedek coefficients.

131
00:10:56,711 --> 00:11:07,221
Speaker SPEAKER_07: But they would develop mainly to decorrelate things and to reduce dimensionality so that a dumb method called Gaussian mixture models, which is just basically smooth table lookup, could model this data.

132
00:11:08,363 --> 00:11:16,269
Speaker SPEAKER_07: And once you use a restricted Boltzmann machine here, then you can afford to have a less processed version of the input.

133
00:11:16,309 --> 00:11:19,253
Speaker SPEAKER_07: In other words, it can decide what features it wants to extract.

134
00:11:19,451 --> 00:11:22,177
Speaker SPEAKER_07: And you can look at more windows of coefficients.

135
00:11:22,197 --> 00:11:24,000
Speaker SPEAKER_07: You can look at a bigger time period.

136
00:11:25,524 --> 00:11:27,386
Speaker SPEAKER_07: For the inputs, these are going to be real values.

137
00:11:27,827 --> 00:11:29,831
Speaker SPEAKER_07: Later on, we're just going to use binary values units.

138
00:11:30,493 --> 00:11:37,866
Speaker SPEAKER_07: And so you use a special kind of Boltzmann machine for the inputs, which is called a Gaussian restricted Boltzmann machine, where these are real and these are binary stochastic.

139
00:11:38,908 --> 00:11:42,134
Speaker SPEAKER_07: Then once you've learned this model on unlabeled data,

140
00:11:42,773 --> 00:11:47,821
Speaker SPEAKER_07: you can think of copying these vectors of activity over there, treating that as training data, and learning another model.

141
00:11:48,402 --> 00:11:50,104
Speaker SPEAKER_07: Then you copy it over, learn another model.

142
00:11:50,884 --> 00:11:57,816
Speaker SPEAKER_07: Once you've learned all these models, you now want to combine them into one big model that's going to be a generative model of the data.

143
00:11:58,157 --> 00:12:00,860
Speaker SPEAKER_07: In other words, it's still just a model of what the inputs look like.

144
00:12:01,402 --> 00:12:06,489
Speaker SPEAKER_07: What I mean by that is, with this generative model, if you were to generate from the model, you would get things that look like the data.

145
00:12:07,791 --> 00:12:11,397
Speaker SPEAKER_07: And the way you would generate from this big model, which is very surprising,

146
00:12:11,798 --> 00:12:14,970
Speaker SPEAKER_07: is you would take the top two layers and go backwards and forwards forever.

147
00:12:16,636 --> 00:12:22,821
Speaker SPEAKER_07: And then you would, after going backwards and forwards forever here, you would just go chunk, chunk.

148
00:12:24,251 --> 00:12:28,258
Speaker SPEAKER_07: In other words, these are directed top-down connections, but these are symmetric connections.

149
00:12:28,677 --> 00:12:36,409
Speaker SPEAKER_07: And that's a huge surprise, because you'd think that if you put a bunch of symmetric things together like this, then you would get a great big Boltzmann machine.

150
00:12:36,429 --> 00:12:36,951
Speaker SPEAKER_07: But you don't.

151
00:12:37,490 --> 00:12:39,234
Speaker SPEAKER_07: You can go and read the papers about why you don't.

152
00:12:39,695 --> 00:12:40,916
Speaker SPEAKER_07: I'm not going to try and explain it now.

153
00:12:41,197 --> 00:12:45,383
Speaker SPEAKER_07: I've explained this many, many times, and I've never convinced anybody.

154
00:12:46,307 --> 00:12:47,710
Speaker SPEAKER_07: So, read the papers.

155
00:12:48,230 --> 00:12:49,072
Speaker SPEAKER_07: That is what you get.

156
00:12:49,092 --> 00:13:07,624
Speaker SPEAKER_07: And so what we know how to do now is we know how to take data, learn features one layer at a time without knowing any labels, then put it all together into a great big generative model that has multiple layers of features and is a better generative model than just the lowest level ones alone.

157
00:13:08,261 --> 00:13:12,730
Speaker SPEAKER_07: And once we've done all that, we can then say, now let's throw all the theory away.

158
00:13:13,731 --> 00:13:19,162
Speaker SPEAKER_07: And let's just, instead of using the weight matrices this way, let's use them in the reverse direction.

159
00:13:19,182 --> 00:13:20,825
Speaker SPEAKER_07: And let's treat it as a great big neural net.

160
00:13:21,725 --> 00:13:23,830
Speaker SPEAKER_07: And we'll just put some class labels on top.

161
00:13:25,011 --> 00:13:28,177
Speaker SPEAKER_07: We'll use what we learned here to initialize the weights.

162
00:13:28,764 --> 00:13:32,230
Speaker SPEAKER_07: And then we'll use the backpropagation algorithm to train that thing.

163
00:13:32,250 --> 00:13:39,124
Speaker SPEAKER_07: And the big difference from previous uses of backpropagation is that we used unlabeled data to initialize all our feature detectors.

164
00:13:40,086 --> 00:13:43,892
Speaker SPEAKER_07: So the problem of feature discovery has been solved on unlabeled data.

165
00:13:44,293 --> 00:13:46,636
Speaker SPEAKER_07: You find good feature detectors and multiple layers of them.

166
00:13:47,097 --> 00:13:52,528
Speaker SPEAKER_07: And so in the high layers, you've got quite complicated feature detectors, all found without using any labeled data.

167
00:13:52,508 --> 00:13:58,356
Speaker SPEAKER_07: If you've only got a small amount of labeled data, you now turn backpropagation loose in that net and train it up.

168
00:13:58,878 --> 00:14:04,164
Speaker SPEAKER_07: And it will work much better than if you, with that small amount of labeled data, tried to train the whole net.

169
00:14:04,785 --> 00:14:08,392
Speaker SPEAKER_07: Because the labeled data is just slightly changing the features you found unsupervised.

170
00:14:08,952 --> 00:14:10,475
Speaker SPEAKER_07: It's not having to design the features.

171
00:14:12,357 --> 00:14:17,905
Speaker SPEAKER_07: However, the unfortunate thing about the story is, if you've got a lot of labeled data, you don't need all this stuff.

172
00:14:18,647 --> 00:14:21,350
Speaker SPEAKER_07: You just need to initialize the scales of the weights correctly.

173
00:14:21,888 --> 00:14:26,913
Speaker SPEAKER_07: But we didn't discover that until we did all this stuff and discovered you could actually learn these deep nets.

174
00:14:26,933 --> 00:14:32,298
Speaker SPEAKER_07: Until that point, people believed you just couldn't learn them, because they would initialize the weights wrong, and then it just wouldn't learn.

175
00:14:33,520 --> 00:14:42,289
Speaker SPEAKER_07: After we'd done all this, just before we sent the paper off, I got the student to check, maybe if we just keep exploring all possible scales for the weights, it will learn.

176
00:14:42,309 --> 00:14:45,374
Speaker SPEAKER_07: Because we had this wonderful graph where our system just learns beautifully.

177
00:14:45,394 --> 00:14:49,177
Speaker SPEAKER_07: And if you initialize the weights at random, it doesn't learn anything at all.

178
00:14:49,158 --> 00:14:58,845
Speaker SPEAKER_07: But then we explored all the scales for the weights, and we discovered this horrific thing, which is if you get the scales just right, you can actually learn these deep nets quite well.

179
00:14:58,865 --> 00:15:01,071
Speaker SPEAKER_07: Not as well as with the pre-training, but almost as well.

180
00:15:01,091 --> 00:15:05,142
Speaker SPEAKER_07: And with enough labeled data, that can be good enough.

181
00:15:06,893 --> 00:15:12,759
Speaker SPEAKER_07: The reason why this generative pre-training makes sense is that you can train all these feature detectors.

182
00:15:12,779 --> 00:15:15,163
Speaker SPEAKER_07: You can design your feature detectors on unlabeled data.

183
00:15:16,404 --> 00:15:19,288
Speaker SPEAKER_07: After you put all these models together, you've got very simple inference.

184
00:15:19,347 --> 00:15:20,489
Speaker SPEAKER_07: It's just a feed-forward net.

185
00:15:21,029 --> 00:15:27,518
Speaker SPEAKER_07: It's doing approximate inference, but it's accurate approximate inference.

186
00:15:31,125 --> 00:15:43,600
Speaker SPEAKER_07: That means that you can take data, you can infer all your feature detectors much, much faster than you can if you use one of the normal directed graphical models, where you have a phenomenon called explaining away, which makes inference very complicated.

187
00:15:48,524 --> 00:16:05,865
Speaker SPEAKER_07: So unfortunately, even though the pre-training makes it easy to optimize the net and reduces overfitting, now that we know how to initialize the weights, if you have a lot of labeled data, you can just initialize the weights to appropriate scales, and then just train the backpropagation on the labeled data, and it works just fine.

188
00:16:05,885 --> 00:16:09,049
Speaker SPEAKER_07: It will work a little bit better if you pre-train, but only a little bit.

189
00:16:09,811 --> 00:16:14,397
Speaker SPEAKER_07: If, however, you don't have much labeled data, all this pre-training layer at a time is still very worth doing.

190
00:16:16,250 --> 00:16:21,259
Speaker SPEAKER_07: So the first place this was used when, really successfully, was in acoustic models and speech recognizers.

191
00:16:21,940 --> 00:16:28,129
Speaker SPEAKER_07: And it was two students of mine, while I wasn't there, explored this technique for acoustic models.

192
00:16:28,831 --> 00:16:35,863
Speaker SPEAKER_07: One was a very good speech student called Abdulrahman Mohamed, and the other was a very good neural net and learning student called George Dahl.

193
00:16:36,565 --> 00:16:39,809
Speaker SPEAKER_07: And they got it to work really well with a very deep net with eight hidden layers.

194
00:16:40,549 --> 00:16:46,879
Speaker SPEAKER_07: They then went off and did internships at IBM and Microsoft, and another student went to Google.

195
00:16:48,020 --> 00:16:54,230
Speaker SPEAKER_07: And at those companies, they applied it to very big data sets and large vocabulary, and it worked really well.

196
00:16:55,130 --> 00:16:57,254
Speaker SPEAKER_07: And now it's just replaced all that.

197
00:16:57,495 --> 00:17:01,039
Speaker SPEAKER_07: At Google and Microsoft and IBM, they don't use their Gaussian mixture models anymore.

198
00:17:01,059 --> 00:17:03,283
Speaker SPEAKER_07: They use these big deep nets because they work much better.

199
00:17:03,753 --> 00:17:08,019
Speaker SPEAKER_07: And the reduction in word error rate is fairly dramatic.

200
00:17:08,059 --> 00:17:10,403
Speaker SPEAKER_07: It's about a 30% reduction in the word error rate.

201
00:17:12,346 --> 00:17:14,028
Speaker SPEAKER_07: That is 30% of the errors disappear.

202
00:17:15,711 --> 00:17:19,597
Speaker SPEAKER_07: There's a nice review paper on that written by all four groups that were doing it.

203
00:17:21,480 --> 00:17:28,191
Speaker SPEAKER_07: And what I want to talk about mainly today is what you have to do to take that success and also make it work for object recognition.

204
00:17:29,233 --> 00:17:32,317
Speaker SPEAKER_07: And recognizing objects and images appears to be much tougher.

205
00:17:32,636 --> 00:17:38,603
Speaker SPEAKER_07: It's a much bigger problem because in the acoustic model for speech, the acoustic model doesn't have to know that much.

206
00:17:38,682 --> 00:17:46,671
Speaker SPEAKER_07: It's plausible that a few million connections could know enough about how to make bets about bits of phonemes from what's in the sound wave.

207
00:17:47,191 --> 00:17:49,353
Speaker SPEAKER_07: There's not a huge amount of knowledge there.

208
00:17:49,373 --> 00:17:58,261
Speaker SPEAKER_07: Whereas if you want something that'll recognize objects and images in a sort of generic way, it needs to have a huge amount of knowledge about what objects look like.

209
00:17:59,001 --> 00:18:01,489
Speaker SPEAKER_07: In the speech systems, that's in what's called the language model.

210
00:18:02,250 --> 00:18:08,167
Speaker SPEAKER_07: And that we didn't touch, we just replaced the front end.

211
00:18:08,721 --> 00:18:18,401
Speaker SPEAKER_07: So, the computer vision community was pretty convinced that these deep neural nets would not be able to compete with their methods, even though their methods became progressively crummier over the years.

212
00:18:18,902 --> 00:18:29,643
Speaker SPEAKER_07: They started off with very good methods, like what David Lowe did for his thesis, which kind of respected geometry and tried to recognize objects by finding a whole bunch of features that were consistently related to be part of that object.

213
00:18:29,663 --> 00:18:30,965
Speaker SPEAKER_07: Sort of sensible things.

214
00:18:30,945 --> 00:18:32,929
Speaker SPEAKER_07: that took 3D geometry into account.

215
00:18:32,949 --> 00:18:47,049
Speaker SPEAKER_07: And they ended up, because that didn't work as well as they hoped, they ended up falling back on the dumbest kind of machine learning, where you treat an image as just a bag of visual words and forget about their spatial relations, except that some of the words are over here and some of the words are over there.

216
00:18:48,010 --> 00:18:50,114
Speaker SPEAKER_07: And it turned out that worked better.

217
00:18:50,153 --> 00:18:56,143
Speaker SPEAKER_07: And instead of saying, well, I don't care that it works better, it's wrong, they said, OK, let's all switch to that.

218
00:18:56,123 --> 00:18:58,827
Speaker SPEAKER_07: That was a big mistake, I believe.

219
00:18:59,909 --> 00:19:00,711
Speaker SPEAKER_07: They all switched to that.

220
00:19:00,891 --> 00:19:01,992
Speaker SPEAKER_07: And they said, that's the way to do it.

221
00:19:02,032 --> 00:19:04,195
Speaker SPEAKER_07: And your deep neural nets are no good, because we can do it better this way.

222
00:19:05,459 --> 00:19:08,824
Speaker SPEAKER_07: And it turned out they could do it better, because they had very small training sets.

223
00:19:09,305 --> 00:19:13,050
Speaker SPEAKER_07: And all they had to do was take their hand-engineered features.

224
00:19:13,271 --> 00:19:20,763
Speaker SPEAKER_07: Actually, take the features that David Lowe had hand-engineered, change them slightly so they had different names,

225
00:19:21,232 --> 00:19:22,817
Speaker SPEAKER_07: then tune a few parameters.

226
00:19:23,638 --> 00:19:26,746
Speaker SPEAKER_07: And to do that, all you needed to do was have a small data set.

227
00:19:27,488 --> 00:19:29,251
Speaker SPEAKER_07: But eventually, they produced big data sets.

228
00:19:30,654 --> 00:19:34,845
Speaker SPEAKER_07: And so they produced a data set with 1.2 million high-resolution training images.

229
00:19:35,666 --> 00:19:39,174
Speaker SPEAKER_07: And the task is to identify 1,000 different object classes.

230
00:19:39,761 --> 00:19:41,844
Speaker SPEAKER_07: Each image has been labeled.

231
00:19:42,905 --> 00:19:44,547
Speaker SPEAKER_07: Now, the images contain several things.

232
00:19:44,807 --> 00:19:47,069
Speaker SPEAKER_07: So you can't necessarily tell what label it's going to have.

233
00:19:47,931 --> 00:19:50,493
Speaker SPEAKER_07: So the rules of the game are you get to give five bets.

234
00:19:51,035 --> 00:19:54,458
Speaker SPEAKER_07: And if the label that a person associated with the image is in your top five bets, you win.

235
00:19:54,478 --> 00:19:55,078
Speaker SPEAKER_07: Otherwise, you lose.

236
00:19:56,160 --> 00:20:09,414
Speaker SPEAKER_07: And a bunch of computer vision teams competed at this, including some very good teams, like Andrew Zissman's group at Oxford, and INRIA, which is the French National Research Labs, in collaboration with Zero2Europe.

237
00:20:09,461 --> 00:20:21,436
Speaker SPEAKER_07: And if you look what results they got, then all the computer vision systems, these are good computer vision systems, were getting 26% or 27% or a little bit worse.

238
00:20:22,898 --> 00:20:25,080
Speaker SPEAKER_07: And we were getting 16% on the same data.

239
00:20:25,121 --> 00:20:26,643
Speaker SPEAKER_07: And if we used more data, we got 15%.

240
00:20:26,884 --> 00:20:28,865
Speaker SPEAKER_07: And these guys couldn't use more data.

241
00:20:29,446 --> 00:20:30,969
Speaker SPEAKER_07: But I've just done it on the same data set.

242
00:20:30,989 --> 00:20:31,509
Speaker SPEAKER_07: We got 16%.

243
00:20:31,670 --> 00:20:34,272
Speaker SPEAKER_07: So that's like a 10% gap, which is huge.

244
00:20:34,333 --> 00:20:39,199
Speaker SPEAKER_07: It's a much bigger gap than we got initially with the speech stuff.

245
00:20:41,743 --> 00:20:43,667
Speaker SPEAKER_07: So then we sold it to Google, and that's the end of the talk.

246
00:20:48,557 --> 00:20:50,863
Speaker SPEAKER_07: Oh, I should carry on, should I?

247
00:20:50,883 --> 00:20:51,063
Speaker SPEAKER_07: OK.

248
00:20:52,185 --> 00:20:56,294
Speaker SPEAKER_07: So here's some examples of things from the test set.

249
00:20:57,236 --> 00:21:03,127
Speaker SPEAKER_07: And so if you look at this laser pointer, does anybody have a laser pointer that works?

250
00:21:04,255 --> 00:21:05,916
Speaker SPEAKER_07: That scared it.

251
00:21:06,458 --> 00:21:10,544
Speaker SPEAKER_07: If you look there, you can see there's a motor scooter, but it's not the only thing in the image.

252
00:21:10,564 --> 00:21:11,605
Speaker SPEAKER_07: There's lots of other stuff in the image.

253
00:21:12,247 --> 00:21:15,270
Speaker SPEAKER_07: Now, the labels don't include labels for people.

254
00:21:15,310 --> 00:21:19,237
Speaker SPEAKER_07: So you would normally attend to the people, but they're not labeled in these images.

255
00:21:20,538 --> 00:21:22,662
Speaker SPEAKER_07: But it still, it gets the motor scooter, right?

256
00:21:22,701 --> 00:21:25,806
Speaker SPEAKER_07: You can't argue that this stuff is unable to deal with segmentation.

257
00:21:25,826 --> 00:21:27,729
Speaker SPEAKER_07: I mean, it finds the motor scooter in there.

258
00:21:27,749 --> 00:21:32,675
Speaker SPEAKER_07: Here, I would have said the right answer was sort of big red convertible.

259
00:21:33,667 --> 00:21:36,210
Speaker SPEAKER_07: The right answer is actually grill, and it gets that as its second answer.

260
00:21:36,730 --> 00:21:38,392
Speaker SPEAKER_07: And the other answers it gets are pretty plausible.

261
00:21:39,453 --> 00:21:45,058
Speaker SPEAKER_07: If you look at the last one, the stupid network says Dalmatian, and obviously the answer is cherry.

262
00:21:46,759 --> 00:21:52,185
Speaker SPEAKER_07: The network does have a few other bets, like Staffordshire bull terrier, or grape, or elderberry, or currant.

263
00:21:52,566 --> 00:21:55,087
Speaker SPEAKER_07: And if you think red currant, those look quite like red currants.

264
00:21:55,689 --> 00:21:57,590
Speaker SPEAKER_07: But that's an example of the network getting something wrong.

265
00:21:59,553 --> 00:22:02,055
Speaker SPEAKER_07: A judiciously chosen example of it getting something wrong.

266
00:22:05,848 --> 00:22:11,556
Speaker SPEAKER_07: So the question is, how long will it be before this type of object recognition is actually in practical applications?

267
00:22:12,436 --> 00:22:15,621
Speaker SPEAKER_07: And the answer is, it went live last week.

268
00:22:16,361 --> 00:22:19,545
Speaker SPEAKER_07: So if you go to Google+, you can upload your own photos.

269
00:22:20,846 --> 00:22:24,790
Speaker SPEAKER_07: And you can search for terms.

270
00:22:26,012 --> 00:22:27,413
Speaker SPEAKER_07: There's quite a few terms you can search for.

271
00:22:28,556 --> 00:22:33,882
Speaker SPEAKER_07: And it will find in your own untagged photos examples of those.

272
00:22:35,650 --> 00:22:39,354
Speaker SPEAKER_07: I got these things from the web, so I couldn't leak anything.

273
00:22:39,374 --> 00:22:40,315
Speaker SPEAKER_07: These are already on the web.

274
00:22:42,877 --> 00:22:45,601
Speaker SPEAKER_07: So this is a search in somebody's photo collection for jewelry.

275
00:22:46,501 --> 00:22:53,088
Speaker SPEAKER_07: And everything there is actually jewelry, including the motorbike, which is obviously a motorbike made of little jewels.

276
00:22:53,789 --> 00:22:57,614
Speaker SPEAKER_07: And the one on the right is for snake, and they're all snakes.

277
00:22:58,474 --> 00:22:59,596
Speaker SPEAKER_07: And so it does work pretty well.

278
00:23:00,857 --> 00:23:01,478
Speaker SPEAKER_07: And it's good enough.

279
00:23:02,378 --> 00:23:05,623
Speaker SPEAKER_07: As Alan Mackler said, it's consumer level.

280
00:23:05,788 --> 00:23:08,012
Speaker SPEAKER_07: I think that was meant to mean it was good.

281
00:23:10,036 --> 00:23:12,140
Speaker SPEAKER_07: Now, there were a bunch of tricks required to get it to work.

282
00:23:13,701 --> 00:23:20,133
Speaker SPEAKER_07: The first trick was to get Yann LeCun to work for 20 years to figure out how to get it to work.

283
00:23:20,693 --> 00:23:21,635
Speaker SPEAKER_07: That was a very good trick.

284
00:23:22,837 --> 00:23:28,708
Speaker SPEAKER_07: He figured out lots of things about how to make convolutional nets work better and better, and we used all of them.

285
00:23:29,801 --> 00:23:43,384
Speaker SPEAKER_07: Another trick which is standard was to take the images and take a large patch that was most of the image and then move this patch around so that we got to deal with translations by just translating the data.

286
00:23:44,065 --> 00:23:46,669
Speaker SPEAKER_07: Now that's a really dumb thing to do and it has to be wrong.

287
00:23:46,709 --> 00:23:49,093
Speaker SPEAKER_07: It's almost as stupid as bags of visual words.

288
00:23:50,086 --> 00:23:58,336
Speaker SPEAKER_07: You really want to deal with the effects of changes of viewpoint by understanding how viewpoint affects things, like they do in computer graphics.

289
00:23:58,715 --> 00:24:02,601
Speaker SPEAKER_07: It's really dumb to think you can deal with viewpoint just by seeing things from all possible viewpoints.

290
00:24:03,541 --> 00:24:04,923
Speaker SPEAKER_07: But that's what these nets do at present.

291
00:24:05,484 --> 00:24:06,045
Speaker SPEAKER_07: And it works.

292
00:24:06,445 --> 00:24:10,009
Speaker SPEAKER_07: And that just shows we're going to be able to make this work much better.

293
00:24:11,892 --> 00:24:12,512
Speaker SPEAKER_07: OK.

294
00:24:12,532 --> 00:24:17,358
Speaker SPEAKER_07: Another trick was not to use the standard logistic neurons.

295
00:24:19,364 --> 00:24:23,048
Speaker SPEAKER_07: Neural nets use neurons with a kind of logistic input-output non-linearity.

296
00:24:23,650 --> 00:24:26,753
Speaker SPEAKER_07: And most people just sort of learn that that's what neural networks use.

297
00:24:27,314 --> 00:24:28,916
Speaker SPEAKER_07: I remember when we made that up.

298
00:24:29,436 --> 00:24:31,159
Speaker SPEAKER_07: And so I know there was no good reason for it.

299
00:24:34,443 --> 00:24:35,404
Speaker SPEAKER_07: So we tried other things.

300
00:24:36,125 --> 00:24:38,528
Speaker SPEAKER_07: And Jan LeCun tried other things, too.

301
00:24:39,230 --> 00:24:43,836
Speaker SPEAKER_07: And in fact, one thing you can try is, what if we had a whole bunch of logistic units?

302
00:24:44,643 --> 00:24:48,586
Speaker SPEAKER_07: that all have the same incoming and outgoing weights, but at slightly different thresholds.

303
00:24:49,367 --> 00:24:50,189
Speaker SPEAKER_07: So I'll come to that in a minute.

304
00:24:55,354 --> 00:24:56,414
Speaker SPEAKER_07: So I'll explain that in a minute.

305
00:24:56,714 --> 00:25:04,442
Speaker SPEAKER_07: And also, we came up with a much better regularizer that allows you to use big hidden layers with far too many parameters and not overfit.

306
00:25:05,584 --> 00:25:07,826
Speaker SPEAKER_07: So I'm going to explain rectified linear units and dropout.

307
00:25:09,508 --> 00:25:14,413
Speaker SPEAKER_07: But first, I just want to go over what convolutional nets are, because they're the most important thing here.

308
00:25:14,848 --> 00:25:20,838
Speaker SPEAKER_07: The idea is that you have feature detectors that you're going to learn.

309
00:25:20,878 --> 00:25:22,662
Speaker SPEAKER_07: They look at the input image.

310
00:25:23,564 --> 00:25:27,790
Speaker SPEAKER_07: But for any given feature detector, you replicate it across the input image.

311
00:25:27,810 --> 00:25:31,175
Speaker SPEAKER_07: So it has a kernel, a set of weights, and you apply those set of weights everywhere.

312
00:25:31,195 --> 00:25:34,561
Speaker SPEAKER_07: So you're doing a convolution followed by a non-linearity.

313
00:25:35,048 --> 00:25:39,193
Speaker SPEAKER_07: So in the picture there, you see three copies of the same feature detector.

314
00:25:39,614 --> 00:25:43,700
Speaker SPEAKER_07: So all the red weights have the same value, and all the green weights have the same value, and all the blue weights have the same value.

315
00:25:44,540 --> 00:25:48,046
Speaker SPEAKER_07: And you actually have a bunch of different maps, each with a different kernel.

316
00:25:48,886 --> 00:25:50,388
Speaker SPEAKER_07: And you can do that for several layers.

317
00:25:52,010 --> 00:25:54,755
Speaker SPEAKER_07: There's other ingredients to convolutional neural nets, but that's the main one.

318
00:25:55,134 --> 00:26:00,442
Speaker SPEAKER_07: I'm not going to describe the pooling, because I don't believe that's the right thing to do, but it works.

319
00:26:02,365 --> 00:26:03,807
Speaker SPEAKER_07: So that's convolutional nets.

320
00:26:05,643 --> 00:26:09,208
Speaker SPEAKER_07: You can actually apply convolutional nets to speech.

321
00:26:09,228 --> 00:26:14,935
Speaker SPEAKER_07: And I actually did that in the 80s, where we had little filters that looked over time.

322
00:26:14,996 --> 00:26:16,598
Speaker SPEAKER_07: And you replicated the filters over time.

323
00:26:17,400 --> 00:26:19,863
Speaker SPEAKER_07: Actually, it was just one filter that stood there and let the data go by.

324
00:26:19,903 --> 00:26:22,227
Speaker SPEAKER_07: And that was called time delay neural nets.

325
00:26:22,988 --> 00:26:25,672
Speaker SPEAKER_07: And that's actually where Jan got the idea of convolutional nets from.

326
00:26:25,771 --> 00:26:27,954
Speaker SPEAKER_07: He generalized that to 2D.

327
00:26:27,934 --> 00:26:30,298
Speaker SPEAKER_07: But it turned out that what I was doing was stupid.

328
00:26:30,317 --> 00:26:33,101
Speaker SPEAKER_07: I was using filters in time.

329
00:26:33,161 --> 00:26:36,285
Speaker SPEAKER_07: And in speech, it's much better off to use them in frequency.

330
00:26:37,065 --> 00:26:40,128
Speaker SPEAKER_07: Because the time is taken care of by the hidden Markov models.

331
00:26:40,528 --> 00:26:43,992
Speaker SPEAKER_07: They can deal with different onset times and different rates.

332
00:26:45,134 --> 00:26:51,182
Speaker SPEAKER_07: But in frequency, you get the variation in where the formants are due to the variation in vocal track length.

333
00:26:52,282 --> 00:26:55,967
Speaker SPEAKER_07: And doing convolution locally in frequency

334
00:26:56,115 --> 00:26:58,558
Speaker SPEAKER_07: gives you a significant improvement in speech recognition.

335
00:26:59,359 --> 00:27:02,344
Speaker SPEAKER_07: And some students in Canada figured that out.

336
00:27:03,204 --> 00:27:06,028
Speaker SPEAKER_07: And IBM has shown that it works very well.

337
00:27:08,732 --> 00:27:10,454
Speaker SPEAKER_07: So here's the rectified linear units.

338
00:27:11,056 --> 00:27:15,741
Speaker SPEAKER_07: We take a logistic unit, and we take a whole bunch of them that have different biases.

339
00:27:16,163 --> 00:27:24,874
Speaker SPEAKER_07: So you have a learned bias B, but you then have a bias of a bit less than that, B minus 0.5, B minus 1.5, B minus 2.5.

340
00:27:25,816 --> 00:27:29,221
Speaker SPEAKER_07: And so you have a whole bunch of feature detectors with progressively higher thresholds.

341
00:27:30,541 --> 00:27:32,826
Speaker SPEAKER_07: And they've all got the same incoming and outgoing weights.

342
00:27:32,865 --> 00:27:35,730
Speaker SPEAKER_07: So the only question is, how many of them will turn on when you give them some input?

343
00:27:37,070 --> 00:27:46,805
Speaker SPEAKER_07: And if you look at how many of them turn on, then you're interested in the sum of the logistic of all these things with step biases.

344
00:27:47,546 --> 00:27:54,355
Speaker SPEAKER_07: And that sum turns out to be almost identical to log of 1 plus e to the x, which is shown in the black curve there.

345
00:27:55,076 --> 00:27:59,761
Speaker SPEAKER_07: The logistics are in the exponential family and log of 1 plus e to the x isn't.

346
00:28:01,423 --> 00:28:20,865
Speaker SPEAKER_07: But it turns out that if you take the limit of that sum and make it an integral, it is exactly log of 1 plus e to the x. So we know that log of 1 plus e to the x is behaving like a whole bunch of logistics, but it's more powerful than one logistic because it doesn't saturate at the top, and so it's got much bigger dynamic range.

347
00:28:21,435 --> 00:28:26,544
Speaker SPEAKER_07: And having seen that you say well, do I really need to have the log and the X?

348
00:28:26,663 --> 00:28:31,311
Speaker SPEAKER_07: Maybe I can just do max of the input and 0 and that works fine.

349
00:28:34,435 --> 00:28:35,698
Speaker SPEAKER_07: It's not quite so justified.

350
00:28:35,759 --> 00:28:40,526
Speaker SPEAKER_07: I mean, I like the derivation from logistic units, but at the end throw the derivation away and just get on with it.

351
00:28:41,166 --> 00:28:44,471
Speaker SPEAKER_07: And that learns much faster than the logistic units.

352
00:28:45,192 --> 00:28:46,836
Speaker SPEAKER_07: It has a bit more capacity.

353
00:28:47,103 --> 00:28:54,612
Speaker SPEAKER_07: And so most people doing these deep neural nets have switched to using these rectified linear units now.

354
00:28:54,711 --> 00:28:56,694
Speaker SPEAKER_07: You might ask what kinds of units are coming next.

355
00:28:57,796 --> 00:29:04,183
Speaker SPEAKER_07: People in Montreal have tried, the rectified linear unit is a linear unit or zero, whichever is larger.

356
00:29:05,365 --> 00:29:12,874
Speaker SPEAKER_07: So you can generalize that to say, why don't we have a whole bunch of different linear units in a pool and just take whichever gives the biggest output?

357
00:29:13,900 --> 00:29:15,470
Speaker SPEAKER_07: And that's an interesting non-linearity.

358
00:29:15,791 --> 00:29:18,830
Speaker SPEAKER_07: And they've shown that in some circumstances, that works slightly better.

359
00:29:19,199 --> 00:29:23,663
Speaker SPEAKER_07: I still don't know whether it's generally better, or just sometimes better, but it's a sensible thing to do.

360
00:29:24,744 --> 00:29:44,147
Speaker SPEAKER_07: I think that the fact that we used logistic units for many years, and there's something simple that works much better, or significantly better, and the fact there's many variants of that that work even better, what it strongly suggests to me is that we've been far too conservative in designing these nets, and we should think about all sorts of complicated functions, but sensible ones.

361
00:29:44,587 --> 00:29:46,911
Speaker SPEAKER_07: And maybe we could even get some inspiration from the brain,

362
00:29:47,869 --> 00:29:51,253
Speaker SPEAKER_07: So what we call layers in neural nets are actually cortical areas.

363
00:29:52,336 --> 00:29:54,377
Speaker SPEAKER_07: And in each cortical area, you have mini-columns.

364
00:29:54,878 --> 00:29:59,724
Speaker SPEAKER_07: And these mini-columns do quite complicated little computations before they send it on to the next area.

365
00:30:00,526 --> 00:30:05,813
Speaker SPEAKER_07: And I think we might be much better off having neural nets where you go to the next level.

366
00:30:05,893 --> 00:30:08,696
Speaker SPEAKER_07: At the next level, you do some quite complicated little computations.

367
00:30:08,737 --> 00:30:11,800
Speaker SPEAKER_07: It involves some recurrence and maybe takes a few iterations.

368
00:30:12,501 --> 00:30:14,163
Speaker SPEAKER_07: And then you send that to the next level.

369
00:30:15,005 --> 00:30:17,688
Speaker SPEAKER_07: you can back propagate through recurrent nets.

370
00:30:17,708 --> 00:30:19,289
Speaker SPEAKER_07: So you can back propagate through all this stuff.

371
00:30:19,589 --> 00:30:27,598
Speaker SPEAKER_07: All back propagation requires is that you have a module and you know how if you send stuff in, stuff will later come out.

372
00:30:28,359 --> 00:30:29,320
Speaker SPEAKER_07: So you have the forward function.

373
00:30:29,622 --> 00:30:36,809
Speaker SPEAKER_07: You also know how if you send derivatives of the error function with respect to what comes out in the top, you can get derivatives

374
00:30:37,009 --> 00:30:39,193
Speaker SPEAKER_07: of the error function with respect to what goes in at the bottom.

375
00:30:39,674 --> 00:30:41,878
Speaker SPEAKER_07: That's all that's required for backpropriation.

376
00:30:41,898 --> 00:30:45,265
Speaker SPEAKER_07: And we should be more ambitious in the kinds of modules we explore.

377
00:30:45,285 --> 00:30:46,488
Speaker SPEAKER_07: Jan's been saying this for many years.

378
00:30:48,832 --> 00:30:51,175
Speaker SPEAKER_07: So I think maybe we can get closer to cortical columns.

379
00:30:52,038 --> 00:30:57,027
Speaker SPEAKER_07: And the kind of neural inspiration I like is when

380
00:30:57,849 --> 00:31:00,253
Speaker SPEAKER_07: Making it more like the brain makes it work better.

381
00:31:00,753 --> 00:31:02,896
Speaker SPEAKER_07: There's lots of people who say, you know, you ought to make it like the brain.

382
00:31:02,916 --> 00:31:04,259
Speaker SPEAKER_07: Like Henry Markram, for example.

383
00:31:04,920 --> 00:31:07,544
Speaker SPEAKER_07: He says, give me a billion dollars and I'll make something like the brain.

384
00:31:07,584 --> 00:31:09,707
Speaker SPEAKER_07: But he doesn't actually know how to make it work.

385
00:31:09,727 --> 00:31:12,510
Speaker SPEAKER_07: He just knows how to make it more and more like the brain without actually working.

386
00:31:13,333 --> 00:31:16,096
Speaker SPEAKER_07: And that seems to me not the right approach.

387
00:31:16,176 --> 00:31:19,761
Speaker SPEAKER_07: What we should do is we should stick with things that actually work and try and make them more like the brain.

388
00:31:19,981 --> 00:31:23,126
Speaker SPEAKER_07: And in particular, notice when making it more like the brain is helpful.

389
00:31:23,607 --> 00:31:26,592
Speaker SPEAKER_07: There's not much point making it more like the brain if it just makes it work worse.

390
00:31:30,384 --> 00:31:34,548
Speaker SPEAKER_07: So here's one thing we couldn't do until recently with neural nets.

391
00:31:35,329 --> 00:31:42,857
Speaker SPEAKER_07: Anybody who enters one of these machine learning competitions knows that if you want to win the competition, you have to train lots of different models and average them.

392
00:31:42,877 --> 00:31:43,878
Speaker SPEAKER_07: And the more different, the better.

393
00:31:45,401 --> 00:31:49,044
Speaker SPEAKER_07: But this averaging of different models is a very effective way to avoid overfitting.

394
00:31:50,526 --> 00:31:51,787
Speaker SPEAKER_07: It's basically a committee.

395
00:31:52,788 --> 00:31:56,291
Speaker SPEAKER_07: In universities, it can avoid getting anything done at all.

396
00:31:57,300 --> 00:32:00,247
Speaker SPEAKER_07: But in fact, economists know this now.

397
00:32:00,287 --> 00:32:08,462
Speaker SPEAKER_07: When a sensible economist wants to predict what will happen next in the economy, you get 15 experts, and you average what they say.

398
00:32:09,384 --> 00:32:11,328
Speaker SPEAKER_07: And that's better than picking one expert at random.

399
00:32:12,391 --> 00:32:16,338
Speaker SPEAKER_07: It's probably worse than picking the best expert, but you don't know who the best expert is.

400
00:32:17,585 --> 00:32:21,387
Speaker SPEAKER_07: So until recently, the way to average many models was to use decision trees.

401
00:32:21,929 --> 00:32:24,070
Speaker SPEAKER_07: This was a technology that came from Leo Breiman.

402
00:32:24,832 --> 00:32:31,396
Speaker SPEAKER_07: A decision tree is very fast to fit to data, and it's very fast to use at test time.

403
00:32:31,416 --> 00:32:38,263
Speaker SPEAKER_07: So we can afford to fit lots of them, and we can afford to run many of them at test time.

404
00:32:38,284 --> 00:32:39,944
Speaker SPEAKER_07: A decision tree is a fairly weak model.

405
00:32:40,045 --> 00:32:42,307
Speaker SPEAKER_07: It's not nearly as powerful as a neural network.

406
00:32:42,326 --> 00:32:44,788
Speaker SPEAKER_07: But you can fit a lot quickly, and you can run them at test time.

407
00:32:44,990 --> 00:32:47,592
Speaker SPEAKER_07: So that's called random forests, and that works pretty well.

408
00:32:47,842 --> 00:32:51,371
Speaker SPEAKER_07: And what we'd like to do is do the equivalent of random forests with neural nets.

409
00:32:51,912 --> 00:32:56,162
Speaker SPEAKER_07: We'd like to fit a whole bunch of neural nets, and we'd like to average them together at test time.

410
00:32:56,663 --> 00:32:58,627
Speaker SPEAKER_07: And the problem is it's too much computation.

411
00:32:59,569 --> 00:33:03,095
Speaker SPEAKER_07: So if you take one of our nets, it takes a long time to train.

412
00:33:03,115 --> 00:33:05,298
Speaker SPEAKER_07: It takes like a week on a GPU to train one of these nets.

413
00:33:06,239 --> 00:33:07,882
Speaker SPEAKER_07: It's a big one, the object recognition one.

414
00:33:08,782 --> 00:33:12,788
Speaker SPEAKER_07: And at test time, you don't want to be running lots of these big nets.

415
00:33:13,088 --> 00:33:14,069
Speaker SPEAKER_07: It's going to be very expensive.

416
00:33:14,730 --> 00:33:17,796
Speaker SPEAKER_07: So it looks like you can't do this kind of model averaging with neural nets.

417
00:33:18,517 --> 00:33:19,617
Speaker SPEAKER_07: But it turns out that's wrong.

418
00:33:19,637 --> 00:33:21,180
Speaker SPEAKER_07: There's a very simple way to do something.

419
00:33:21,560 --> 00:33:22,863
Speaker SPEAKER_07: There's a form of model averaging.

420
00:33:25,105 --> 00:33:28,851
Speaker SPEAKER_07: Before I show you how to do it, I want to distinguish two kinds of model averaging.

421
00:33:28,982 --> 00:33:30,144
Speaker SPEAKER_07: They both work just fine.

422
00:33:30,684 --> 00:33:33,887
Speaker SPEAKER_07: They both have nice guarantees on callback-Libla divergences.

423
00:33:34,327 --> 00:33:38,192
Speaker SPEAKER_07: That is, they show that the average of the models does better than picking one model at random.

424
00:33:40,192 --> 00:33:51,124
Speaker SPEAKER_07: The first is the most obvious way, which is simply to say, if I've got two models, A and B, and I've got, say, three classes, I just average the probability distributions predicted by each model.

425
00:33:52,484 --> 00:33:55,867
Speaker SPEAKER_07: So when we combine the models, we just average their output probabilities.

426
00:33:57,400 --> 00:34:00,503
Speaker SPEAKER_07: The second is to take the geometric mean of the output probabilities.

427
00:34:00,544 --> 00:34:01,645
Speaker SPEAKER_07: You just multiply them together.

428
00:34:01,685 --> 00:34:04,789
Speaker SPEAKER_07: Now, when you do that, you get some numbers that don't add up to 1 anymore.

429
00:34:05,351 --> 00:34:09,916
Speaker SPEAKER_07: So you have to renormalize them by dividing by the sum.

430
00:34:09,936 --> 00:34:13,402
Speaker SPEAKER_07: There's some important differences between these two ways of averaging models.

431
00:34:14,384 --> 00:34:19,751
Speaker SPEAKER_07: In the first method, for example, the average model is always vaguer than the individual models.

432
00:34:21,813 --> 00:34:26,260
Speaker SPEAKER_07: The average model can never be more precise than the most precise of the individual models.

433
00:34:26,476 --> 00:34:29,079
Speaker SPEAKER_07: You have to get higher entropy this way.

434
00:34:29,099 --> 00:34:32,302
Speaker SPEAKER_07: When you take a geometric mean, you can get either higher entropy or lower entropy.

435
00:34:34,887 --> 00:34:37,068
Speaker SPEAKER_07: So in that sense, you might think it was more powerful.

436
00:34:37,088 --> 00:34:38,690
Speaker SPEAKER_07: But anyway, this is just two ways of averaging.

437
00:34:41,335 --> 00:34:44,898
Speaker SPEAKER_07: Now, I'm going to show you how to do this geometric averaging with neural nets very efficiently.

438
00:34:46,101 --> 00:34:48,423
Speaker SPEAKER_07: What we do, let's take a neural net that has one hidden layer.

439
00:34:49,545 --> 00:34:54,170
Speaker SPEAKER_07: Each time we showed a training example, we leave out half of the hidden units.

440
00:34:56,141 --> 00:34:58,864
Speaker SPEAKER_07: So what we're doing is we're sampling from different model architectures.

441
00:35:01,349 --> 00:35:07,356
Speaker SPEAKER_07: But the units we keep always have the same weights whenever we keep them.

442
00:35:07,376 --> 00:35:15,525
Speaker SPEAKER_07: So you can think of it as if we've got H hidden units, we've got 2 to the H possible architectures from which we're sampling at random.

443
00:35:15,726 --> 00:35:16,907
Speaker SPEAKER_07: So most of them we never sample.

444
00:35:17,409 --> 00:35:20,192
Speaker SPEAKER_07: But these architectures are doing massive amount of weight sharing.

445
00:35:26,364 --> 00:35:28,967
Speaker SPEAKER_07: So only a few of these architectures get trained.

446
00:35:29,708 --> 00:35:30,751
Speaker SPEAKER_07: One per training example.

447
00:35:31,931 --> 00:35:36,277
Speaker SPEAKER_07: But this massive weight sharing is much better regularized than things like L2 and L1.

448
00:35:36,918 --> 00:35:41,985
Speaker SPEAKER_07: Because the weights are being pulled not towards zero, but towards what other models would like them to be.

449
00:35:42,264 --> 00:35:43,847
Speaker SPEAKER_07: They're being pulled towards sensible values.

450
00:35:44,507 --> 00:35:45,528
Speaker SPEAKER_07: So it's not just shrinkage.

451
00:35:46,971 --> 00:35:54,280
Speaker SPEAKER_07: And so having this huge set of, exponentially large set of models, with all this weight sharing, turns out to work extremely well.

452
00:35:56,369 --> 00:35:58,192
Speaker SPEAKER_07: One question is, what do we do at test time?

453
00:35:58,211 --> 00:35:59,833
Speaker SPEAKER_07: Do we have to run exponentially many models?

454
00:36:00,355 --> 00:36:01,195
Speaker SPEAKER_07: And here's a neat trick.

455
00:36:02,518 --> 00:36:09,608
Speaker SPEAKER_07: At test time, what you do, instead of leaving out the units with a probability of a half, you just half the outgoing weights.

456
00:36:11,271 --> 00:36:17,300
Speaker SPEAKER_07: So you get the same expected value output with no variance at test time.

457
00:36:17,320 --> 00:36:20,844
Speaker SPEAKER_07: And so you're using all the units, but you use only half as strong.

458
00:36:21,565 --> 00:36:24,891
Speaker SPEAKER_07: And it turns out that does exactly compute the mean,

459
00:36:24,989 --> 00:36:26,871
Speaker SPEAKER_07: of all 2 to the H models.

460
00:36:26,891 --> 00:36:29,532
Speaker SPEAKER_07: It computes the geometric mean.

461
00:36:29,552 --> 00:36:31,494
Speaker SPEAKER_07: And so you sort of get it for free.

462
00:36:32,576 --> 00:36:35,358
Speaker SPEAKER_07: So we just run one net of test time.

463
00:36:35,378 --> 00:36:38,762
Speaker SPEAKER_07: It's twice as big as any of the individual nets we ran during training.

464
00:36:39,302 --> 00:36:43,266
Speaker SPEAKER_07: And now we've computed the geometric mean.

465
00:36:43,286 --> 00:36:49,092
Speaker SPEAKER_07: So this makes it possible to train a huge number of different models of training time efficiently, and to combine them at test time efficiently.

466
00:36:50,773 --> 00:36:53,697
Speaker SPEAKER_07: If you have more hidden layers, you just keep doing dropout.

467
00:36:53,980 --> 00:37:05,213
Speaker SPEAKER_07: You leave out 50% of the units in every layer.

468
00:37:05,233 --> 00:37:16,005
Speaker SPEAKER_07: You leave out 50% of the units in every layer.

469
00:37:16,507 --> 00:37:19,652
Speaker SPEAKER_07: then at test time you have all the aerodynamics.

470
00:37:19,672 --> 00:37:29,889
Speaker SPEAKER_07: It's not exactly the same as computing the geometric mean of all the models anymore, but it's a close approximation and it works just fine.

471
00:37:29,909 --> 00:37:32,833
Speaker SPEAKER_07: You can also leave out things in the input, and that helps too.

472
00:37:33,574 --> 00:37:40,626
Speaker SPEAKER_07: That technique was already invented by Pascal Vincent and the University of Montreal and its collaborators, and they got to it via a different route.

473
00:37:40,646 --> 00:37:44,371
Speaker SPEAKER_07: But leaving out input, random inputs, also is a very good way to do it.

474
00:37:45,297 --> 00:37:50,083
Speaker SPEAKER_07: People who know about machine learning are very familiar with that technique.

475
00:37:50,103 --> 00:37:57,251
Speaker SPEAKER_07: If you compare Naive Bayes with the logistic regression, in logistic regression you use all of the inputs to get the output.

476
00:37:57,271 --> 00:37:59,153
Speaker SPEAKER_07: In Naive Bayes, you leave out all but one of them.

477
00:38:01,817 --> 00:38:11,288
Speaker SPEAKER_07: And it turns out if you don't have much data, it's better to train by leaving out all but one of them and train a whole bunch of different models and then average them.

478
00:38:11,307 --> 00:38:15,253
Speaker SPEAKER_07: So in Naive Bayes, it's a very simple form of dropout on the inputs.

479
00:38:18,170 --> 00:38:19,411
Speaker SPEAKER_07: So dropout works very well.

480
00:38:23,735 --> 00:38:30,581
Speaker SPEAKER_07: And it'll reduce the errors by a whole lot on most neural nets if they're overfitting.

481
00:38:30,822 --> 00:38:33,865
Speaker SPEAKER_07: If they're not overfitting, you should just be using a bigger neural net.

482
00:38:34,967 --> 00:38:41,652
Speaker SPEAKER_07: However much data you have, I keep explaining this to Google, however much data you have, you can always use a neural net such that it will overfit.

483
00:38:42,373 --> 00:38:43,315
Speaker SPEAKER_07: And that's what you should do.

484
00:38:43,675 --> 00:38:44,735
Speaker SPEAKER_07: And then you should regularize it.

485
00:38:45,416 --> 00:38:47,298
Speaker SPEAKER_07: And if you look at the regime people are in,

486
00:38:48,393 --> 00:38:53,543
Speaker SPEAKER_07: Statisticians live in a regime where your primary regularizer is the fact that you don't have enough parameters.

487
00:38:54,364 --> 00:38:59,394
Speaker SPEAKER_07: So the sort of default for a statistician is to have 1,000 training examples and 50 parameters.

488
00:39:00,217 --> 00:39:01,038
Speaker SPEAKER_07: Then they're sort of happy.

489
00:39:01,077 --> 00:39:01,780
Speaker SPEAKER_07: That's familiar.

490
00:39:02,380 --> 00:39:05,447
Speaker SPEAKER_07: If you say, what about having 1,000 training examples and a million parameters?

491
00:39:05,987 --> 00:39:08,393
Speaker SPEAKER_07: They say, don't be stupid.

492
00:39:09,672 --> 00:39:10,893
Speaker SPEAKER_07: But with Dropout, you can do that.

493
00:39:11,554 --> 00:39:13,416
Speaker SPEAKER_07: And if you think, what do people do?

494
00:39:14,777 --> 00:39:17,521
Speaker SPEAKER_07: Well, people, you live for 10 to the 9 seconds.

495
00:39:18,442 --> 00:39:20,744
Speaker SPEAKER_07: So let's suppose you had 10 training examples per second.

496
00:39:21,364 --> 00:39:23,608
Speaker SPEAKER_07: That's not exactly a good model of video, but it'll do for now.

497
00:39:24,268 --> 00:39:26,710
Speaker SPEAKER_07: You get 10 to the 10 training examples.

498
00:39:29,353 --> 00:39:31,876
Speaker SPEAKER_07: But you have 10 to the 14 or 10 to the 15 parameters.

499
00:39:32,757 --> 00:39:37,282
Speaker SPEAKER_07: So you've got 10,000 or 100,000 parameters per training example.

500
00:39:37,853 --> 00:39:42,338
Speaker SPEAKER_07: So we're in the regime of massively more parameters than training samples.

501
00:39:42,818 --> 00:39:46,023
Speaker SPEAKER_07: And that's because synapses are massively cheaper than experiences.

502
00:39:46,664 --> 00:39:52,230
Speaker SPEAKER_07: A second of experience costs a lot of energy, whereas a synapse for a lifetime costs almost nothing compared with that.

503
00:39:53,371 --> 00:39:55,673
Speaker SPEAKER_07: So we're in a totally different regime from statisticians.

504
00:40:01,460 --> 00:40:05,346
Speaker SPEAKER_07: Another way to think about dropout is that it's like sex.

505
00:40:05,764 --> 00:40:15,418
Speaker SPEAKER_07: There's this puzzle in sexual reproduction, which is, you get all these nicely co-adapted genes, you then mate with somebody, and you produce an offspring, and half your nicely co-adapted genes have disappeared.

506
00:40:15,458 --> 00:40:18,023
Speaker SPEAKER_07: And that seems like a bad thing to do.

507
00:40:18,563 --> 00:40:20,306
Speaker SPEAKER_07: It seems like you'd be better just cloning yourself.

508
00:40:21,407 --> 00:40:27,177
Speaker SPEAKER_07: And indeed, if the environment didn't change at all, I think you'd be better just cloning yourself.

509
00:40:27,197 --> 00:40:30,641
Speaker SPEAKER_07: But suppose that the environment might suddenly change.

510
00:40:31,972 --> 00:40:35,456
Speaker SPEAKER_07: then having big set of co-adaptive genes is a very bad idea.

511
00:40:36,277 --> 00:40:40,844
Speaker SPEAKER_07: Because if the environment changes, they won't all work so well together.

512
00:40:41,264 --> 00:40:44,208
Speaker SPEAKER_07: Whereas having lots of little sets of co-adaptive genes is a much better idea.

513
00:40:45,009 --> 00:40:52,038
Speaker SPEAKER_07: So for example, if you work for al-Qaeda, it's much better to have lots of little conspiracies than one big conspiracy.

514
00:40:52,599 --> 00:40:55,822
Speaker SPEAKER_07: Because at test time, conditions are likely to be different from at training time.

515
00:40:56,403 --> 00:40:58,907
Speaker SPEAKER_07: And if you have lots of little conspiracies, some of them will still work.

516
00:41:01,164 --> 00:41:04,469
Speaker SPEAKER_07: I am in Canada, right?

517
00:41:08,293 --> 00:41:08,632
Speaker SPEAKER_07: Okay.

518
00:41:11,076 --> 00:41:12,958
Speaker SPEAKER_07: So let me tell you a couple of other cases.

519
00:41:13,097 --> 00:41:13,958
Speaker SPEAKER_07: How am I doing for time?

520
00:41:18,784 --> 00:41:19,844
Speaker SPEAKER_07: Okay.

521
00:41:19,864 --> 00:41:24,050
Speaker SPEAKER_07: Let me tell you a couple of other cases where we now have a recipe.

522
00:41:24,550 --> 00:41:27,172
Speaker SPEAKER_07: And the recipe is have a deep net

523
00:41:28,688 --> 00:41:30,849
Speaker SPEAKER_07: Pre-train it if you don't have much labeled training data.

524
00:41:30,869 --> 00:41:33,213
Speaker SPEAKER_07: But if you do have lots of labeled training data, don't even bother.

525
00:41:33,793 --> 00:41:39,099
Speaker SPEAKER_07: Initialize the weights to sensible values so that when you back propagate the derivatives, don't either die out or explode.

526
00:41:40,079 --> 00:41:41,661
Speaker SPEAKER_07: Use rectified linear units.

527
00:41:42,702 --> 00:41:47,047
Speaker SPEAKER_07: Use dropout to regularize it so that you can use many more parameters than you have training examples.

528
00:41:47,967 --> 00:41:51,992
Speaker SPEAKER_07: And if it's got spatial structure in the data, use convolution at the front end.

529
00:41:52,012 --> 00:41:52,833
Speaker SPEAKER_07: So that's our recipe.

530
00:41:53,414 --> 00:41:57,057
Speaker SPEAKER_07: And that recipe will now solve all problems.

531
00:41:58,152 --> 00:42:03,101
Speaker SPEAKER_07: So if you go to Cargill, that's a place where they have machine learning competitions.

532
00:42:03,541 --> 00:42:09,032
Speaker SPEAKER_07: And until recently, all the competitions were being won by amateurs, which should have been very embarrassing for machine learning professionals.

533
00:42:09,914 --> 00:42:16,987
Speaker SPEAKER_07: The big machine learning groups didn't get involved in the competitions, even though there were serious financial prizes.

534
00:42:18,824 --> 00:42:21,788
Speaker SPEAKER_07: And that's one reason was that random forests kept winning.

535
00:42:22,650 --> 00:42:24,853
Speaker SPEAKER_07: Random forests do quite well in those competitions.

536
00:42:25,353 --> 00:42:30,802
Speaker SPEAKER_07: But more recently we've shown that our recipe beats random forests and it does it fairly consistently.

537
00:42:33,266 --> 00:42:39,135
Speaker SPEAKER_07: So one nice example is predicting the activity of a molecule from descriptors of the molecule.

538
00:42:39,155 --> 00:42:45,945
Speaker SPEAKER_07: And so Merck set up a competition where you have about 11,000 descriptors of a molecule.

539
00:42:47,916 --> 00:42:55,347
Speaker SPEAKER_07: And each particular task you have to do uses from 3,000 to 9,000 of those descriptors.

540
00:42:55,487 --> 00:42:57,329
Speaker SPEAKER_07: But it's all drawn from the same pool of descriptors.

541
00:42:58,731 --> 00:43:04,259
Speaker SPEAKER_07: And then you have to predict for the molecule whether it will bind well to something.

542
00:43:05,260 --> 00:43:07,143
Speaker SPEAKER_07: And they've measured that for a whole bunch of molecules.

543
00:43:08,123 --> 00:43:10,166
Speaker SPEAKER_07: But typically, they've only measured it for a few thousand.

544
00:43:10,788 --> 00:43:15,273
Speaker SPEAKER_07: So you've got a problem where the dimensionality of the input is about the same as the number of training examples.

545
00:43:16,586 --> 00:43:19,349
Speaker SPEAKER_07: And George tried training standard backprop nets on this.

546
00:43:20,550 --> 00:43:27,318
Speaker SPEAKER_07: And the problem is, once you use more than 30 hidden units, the thing overfits terribly, even if you use strong L2 regularization.

547
00:43:28,059 --> 00:43:32,503
Speaker SPEAKER_07: Because you've now got like 50 times more parameters than you have data points.

548
00:43:33,764 --> 00:43:38,208
Speaker SPEAKER_07: But if you use dropout on this, you can use like 500 hidden units and multiple layers of them.

549
00:43:38,730 --> 00:43:42,132
Speaker SPEAKER_07: So you can use hundreds of times more parameters than there are training examples.

550
00:43:42,813 --> 00:43:46,036
Speaker SPEAKER_07: And it works really well.

551
00:43:46,456 --> 00:43:53,045
Speaker SPEAKER_07: The other thing that you can do is you can take many different tasks and train one net to solve all these problems.

552
00:43:53,445 --> 00:43:55,949
Speaker SPEAKER_07: So the milk competition, there's 15 targets.

553
00:43:56,369 --> 00:43:59,574
Speaker SPEAKER_07: And you give them one set of molecules and ask, how well do they bind to this target?

554
00:43:59,634 --> 00:44:01,615
Speaker SPEAKER_07: And another set and ask, how well do they bind to that target?

555
00:44:02,217 --> 00:44:03,858
Speaker SPEAKER_07: And you just train up one big net to do it all.

556
00:44:06,623 --> 00:44:09,947
Speaker SPEAKER_07: For the missing descriptors, you just treat those as dropout on the inputs.

557
00:44:11,228 --> 00:44:11,989
Speaker SPEAKER_07: And it's really done.

558
00:44:12,010 --> 00:44:14,673
Speaker SPEAKER_07: You just train the thing on a GPU.

559
00:44:15,092 --> 00:44:21,025
Speaker SPEAKER_07: And if George trains it, it does really well because he's very good at training these things.

560
00:44:21,847 --> 00:44:26,458
Speaker SPEAKER_07: And just that neural net would have won the competition.

561
00:44:26,577 --> 00:44:35,396
Speaker SPEAKER_07: He actually averaged it with other things, but afterwards he went back and after he won, discovered that the neural net by itself would have won.

562
00:44:38,565 --> 00:44:40,268
Speaker SPEAKER_07: Here's another one of those competitions.

563
00:44:40,467 --> 00:44:43,710
Speaker SPEAKER_07: You're given the title of a job advertisement in England.

564
00:44:44,311 --> 00:44:46,735
Speaker SPEAKER_07: You're given the body of the description.

565
00:44:47,355 --> 00:44:48,757
Speaker SPEAKER_07: And you're given the location.

566
00:44:48,836 --> 00:44:51,539
Speaker SPEAKER_07: And you have to predict the salary.

567
00:44:52,260 --> 00:44:59,248
Speaker SPEAKER_07: And another of my students, Vlad Ni, used exactly the same technology for that and won.

568
00:44:59,347 --> 00:45:00,188
Speaker SPEAKER_07: Won it quite comfortably.

569
00:45:00,548 --> 00:45:02,811
Speaker SPEAKER_07: And the person who came second was also using a neural net.

570
00:45:03,773 --> 00:45:07,797
Speaker SPEAKER_07: So we now have a technology that at least can win these dumb competitions.

571
00:45:08,586 --> 00:45:15,733
Speaker SPEAKER_07: Presumably, something else better is gonna come along soon, but for now, it's good.

572
00:45:17,434 --> 00:45:20,237
Speaker SPEAKER_07: I wanna finish by talking a bit about speech recognition.

573
00:45:21,838 --> 00:45:25,900
Speaker SPEAKER_07: So current speech recognition systems have a lot of different components.

574
00:45:25,942 --> 00:45:29,525
Speaker SPEAKER_07: They have a language model, which tries to predict the next word.

575
00:45:30,766 --> 00:45:33,728
Speaker SPEAKER_07: And so, if you can't hear it very well, the predictions will help.

576
00:45:35,750 --> 00:45:38,492
Speaker SPEAKER_07: They have hidden Markov models to model the phonemes.

577
00:45:38,978 --> 00:45:45,728
Speaker SPEAKER_07: They have an acoustic model to map from the coefficients you extract from the sound wave to probabilities of bits of the Markov model.

578
00:45:46,869 --> 00:45:49,853
Speaker SPEAKER_07: All of these are initially trained sort of separately, and then they're all put together.

579
00:45:50,054 --> 00:45:55,362
Speaker SPEAKER_07: And if you want really good performance, you now try and train the whole thing together discriminatively at the end.

580
00:45:55,382 --> 00:45:58,425
Speaker SPEAKER_07: But it's a big hodgepodge of different methods.

581
00:45:58,447 --> 00:46:03,653
Speaker SPEAKER_07: And now that we've had some success, our imperialist inclinations are becoming clear.

582
00:46:04,434 --> 00:46:08,360
Speaker SPEAKER_07: We would like to just do a whole speech system that's just one big recurrent neural net.

583
00:46:09,202 --> 00:46:17,074
Speaker SPEAKER_07: That is, we would like to say the task is to get from a sound wave to a transcription.

584
00:46:17,235 --> 00:46:18,096
Speaker SPEAKER_07: What's a transcription?

585
00:46:18,376 --> 00:46:22,222
Speaker SPEAKER_07: It's just a sequence of characters with spaces included.

586
00:46:22,822 --> 00:46:32,137
Speaker SPEAKER_07: And so what we want to do is take sound waves in here, characters out there, and the hell with all your technology, we're just going to train a great big neural net to do it.

587
00:46:32,556 --> 00:46:35,079
Speaker SPEAKER_07: Now there's all sorts of reasons why you might believe that was hopeless.

588
00:46:35,179 --> 00:46:42,909
Speaker SPEAKER_07: And one very good reason is the language model has to know a lot about words and it has to be able to predict what words coming next.

589
00:46:43,550 --> 00:46:46,574
Speaker SPEAKER_07: And we're proposing to do that with something that just works at the character level.

590
00:46:48,135 --> 00:46:50,137
Speaker SPEAKER_07: And that seems unreasonable.

591
00:46:51,860 --> 00:46:53,882
Speaker SPEAKER_07: We're not going to have a separate pronunciation dictionary.

592
00:46:53,922 --> 00:46:55,324
Speaker SPEAKER_07: We're going to learn all that.

593
00:46:55,911 --> 00:47:02,219
Speaker SPEAKER_07: And we're not going to train it with forced alignment of the acoustic targets, which is what normal neural nets have done.

594
00:47:02,280 --> 00:47:11,311
Speaker SPEAKER_07: So normally when you train these deep nets, you first train a dumb old-fashioned model that aligns the HMM with the input data.

595
00:47:11,610 --> 00:47:13,773
Speaker SPEAKER_07: And then you use that to bootstrap your fancy neural net.

596
00:47:14,213 --> 00:47:16,157
Speaker SPEAKER_07: It's like using an atom bomb to get an H-bomb going.

597
00:47:16,597 --> 00:47:18,099
Speaker SPEAKER_07: You can't do an H-bomb without an atom bomb.

598
00:47:18,880 --> 00:47:20,501
Speaker SPEAKER_07: I think.

599
00:47:22,864 --> 00:47:24,686
Speaker SPEAKER_07: I hope Homeland Security is listening.

600
00:47:25,612 --> 00:47:28,396
Speaker SPEAKER_07: Okay.

601
00:47:29,496 --> 00:47:31,840
Speaker SPEAKER_07: So Alex Graves spent a lot of time working on this.

602
00:47:32,780 --> 00:47:36,204
Speaker SPEAKER_07: And he already had the system that was best at reading online handwriting.

603
00:47:37,626 --> 00:47:39,510
Speaker SPEAKER_07: And it's a recurrent net.

604
00:47:40,250 --> 00:47:43,855
Speaker SPEAKER_07: And he developed it further by having a recurrent net that also has multiple hidden layers.

605
00:47:44,356 --> 00:47:49,442
Speaker SPEAKER_07: So it's deep in time because it's recurrent, but it's also deep in space because it's got these multiple hidden layers.

606
00:47:50,197 --> 00:47:53,981
Speaker SPEAKER_07: And on the TIMIT database, the best previous result was something like 18.9%.

607
00:47:54,222 --> 00:47:56,786
Speaker SPEAKER_07: And they've now got it down to 18.7%.

608
00:47:56,905 --> 00:47:59,548
Speaker SPEAKER_07: And that was after lots and lots of work.

609
00:48:00,650 --> 00:48:01,610
Speaker SPEAKER_07: He got a lot better.

610
00:48:01,630 --> 00:48:04,434
Speaker SPEAKER_07: 17.7% is a huge improvement, a very unexpected.

611
00:48:07,057 --> 00:48:09,621
Speaker SPEAKER_07: You can't go to his talk to learn the details, because that was at ICASP.

612
00:48:11,623 --> 00:48:13,204
Speaker SPEAKER_07: But you could look it up on the web.

613
00:48:13,425 --> 00:48:15,867
Speaker SPEAKER_07: So he has a paper at ICASP.

614
00:48:17,333 --> 00:48:20,960
Speaker SPEAKER_07: Now on Timit, it's a small vocabulary, it's not big vocabulary.

615
00:48:20,980 --> 00:48:25,528
Speaker SPEAKER_07: So you're avoiding the problem of having the recurrent net, learn a language model.

616
00:48:26,170 --> 00:48:27,893
Speaker SPEAKER_07: See a language model's a bit like object recognition.

617
00:48:27,914 --> 00:48:32,282
Speaker SPEAKER_07: A language model has to, to be a really good language model, it has to know everything you know.

618
00:48:32,943 --> 00:48:38,514
Speaker SPEAKER_07: And that's a bit ambitious, but to be a fairly good language model, it still has to know a lot.

619
00:48:39,068 --> 00:48:41,791
Speaker SPEAKER_07: It has to know about people's names.

620
00:48:41,871 --> 00:48:43,034
Speaker SPEAKER_07: It has to know about places.

621
00:48:43,094 --> 00:48:45,378
Speaker SPEAKER_07: It has to know about all sorts of funny foreign words.

622
00:48:45,958 --> 00:48:53,990
Speaker SPEAKER_07: It seems implausible that you get a little recurrent net, or even a big recurrent net, working at the character level to model language.

623
00:48:54,512 --> 00:48:58,659
Speaker SPEAKER_07: So I'm going to finish by showing you something that will make you think that's a bit less implausible.

624
00:48:59,860 --> 00:49:05,329
Speaker SPEAKER_07: Ilya Sotskiva trained

625
00:49:05,815 --> 00:49:12,423
Speaker SPEAKER_07: a recurrent neural net that had about 8 million parameters in it, which isn't that many, to predict the next character in some text.

626
00:49:12,563 --> 00:49:20,833
Speaker SPEAKER_07: And the text he took from Wikipedia, he got rid of all the really weird characters, like we got rid of all the French accents, which is a bit naughty.

627
00:49:21,434 --> 00:49:31,467
Speaker SPEAKER_07: And he got it down to 86 characters, so digits and letters, and capital letters and lowercase letters are different, and so on.

628
00:49:31,920 --> 00:49:37,228
Speaker SPEAKER_07: And then he broke it up into half a billion characters from Wikipedia into 100 strings, 100 long.

629
00:49:37,730 --> 00:49:38,952
Speaker SPEAKER_07: There's still 5 million of those.

630
00:49:39,893 --> 00:49:43,619
Speaker SPEAKER_07: And then the recurrent would see the first 11 characters, and it would have to start predicting them.

631
00:49:45,121 --> 00:49:48,186
Speaker SPEAKER_07: And he trains it just to predict the next character.

632
00:49:48,206 --> 00:49:51,172
Speaker SPEAKER_07: There's some extra little tricks in how you do that that I'm not going to go into.

633
00:49:51,592 --> 00:49:53,695
Speaker SPEAKER_07: But this is basically a recurrent with 8 million parameters.

634
00:49:53,755 --> 00:49:55,177
Speaker SPEAKER_07: It's trained to predict the next character.

635
00:49:56,304 --> 00:49:57,847
Speaker SPEAKER_07: And the question is, is this insane?

636
00:49:58,007 --> 00:50:01,650
Speaker SPEAKER_07: Because we know language is naturally made of words, or at least morphemes.

637
00:50:02,052 --> 00:50:03,793
Speaker SPEAKER_07: And going down to the character level seems crazy.

638
00:50:04,414 --> 00:50:09,920
Speaker SPEAKER_07: Although, of course, it's very good for agglutinative languages like Finnish, where words are made of 15 morphemes.

639
00:50:10,461 --> 00:50:12,123
Speaker SPEAKER_07: You don't want to try and model words there.

640
00:50:13,606 --> 00:50:15,248
Speaker SPEAKER_07: The amazing thing is it works very well.

641
00:50:16,588 --> 00:50:22,936
Speaker SPEAKER_07: His recurrent net was about equal to the state of the art for character recognition, slightly worse.

642
00:50:23,557 --> 00:50:27,903
Speaker SPEAKER_07: But it could do things like it could balance brackets 30 characters later.

643
00:50:28,405 --> 00:50:31,588
Speaker SPEAKER_07: Because the recurrent net would remember I've got an opening bracket and I better close it.

644
00:50:32,289 --> 00:50:36,675
Speaker SPEAKER_07: In fact if you gave it two opening brackets it would get slightly worried and it would close one fairly quickly.

645
00:50:38,418 --> 00:50:40,942
Speaker SPEAKER_07: The recurrent net we determined could count brackets.

646
00:50:41,001 --> 00:50:43,144
Speaker SPEAKER_07: It could count none, one, two or many.

647
00:50:43,704 --> 00:50:46,068
Speaker SPEAKER_07: If you gave it three or more it would close one very quickly.

648
00:50:48,831 --> 00:50:50,815
Speaker SPEAKER_07: And it wouldn't necessarily close the right number.

649
00:50:51,976 --> 00:50:54,398
Speaker SPEAKER_07: OK, so he trained this on a GPU for a long time.

650
00:50:54,418 --> 00:50:55,239
Speaker SPEAKER_07: And how well did it work?

651
00:50:56,081 --> 00:50:58,385
Speaker SPEAKER_07: Well, the first question is, he's not telling it about words.

652
00:50:58,625 --> 00:50:59,786
Speaker SPEAKER_07: So will it learn English words?

653
00:50:59,806 --> 00:51:01,989
Speaker SPEAKER_07: Or will it produce gobbledygook?

654
00:51:02,010 --> 00:51:03,210
Speaker SPEAKER_07: So here's some text produced.

655
00:51:04,052 --> 00:51:09,539
Speaker SPEAKER_07: And the way you produce text from the model is you give it some characters to get it going.

656
00:51:09,780 --> 00:51:12,784
Speaker SPEAKER_07: You then say, give me the distribution for the next character.

657
00:51:12,804 --> 00:51:20,193
Speaker SPEAKER_07: And if it says there's a probability of 1 in 100 that it's an x, then with a probability of 1 in 100, you sample an x.

658
00:51:20,257 --> 00:51:24,789
Speaker SPEAKER_07: Produce that as your output and you tell the language model, the next character was an X, what do you predict next?

659
00:51:25,652 --> 00:51:27,677
Speaker SPEAKER_07: And so it can progressively give you predictions.

660
00:51:28,760 --> 00:51:32,849
Speaker SPEAKER_07: And so here's some text it produced, one character at a time.

661
00:51:33,826 --> 00:51:36,548
Speaker SPEAKER_07: And the interesting thing is, there are no non-words here.

662
00:51:37,070 --> 00:51:45,340
Speaker SPEAKER_07: And what that means is, as soon as it's produced enough of an English word, so there's only one completion, it never hypothesizes any of the inconsistent characters.

663
00:51:46,282 --> 00:51:47,043
Speaker SPEAKER_07: Almost never.

664
00:51:47,063 --> 00:51:52,369
Speaker SPEAKER_07: It does produce non-words, and the non-words are things like ephemerable, or continged.

665
00:51:53,030 --> 00:51:54,831
Speaker SPEAKER_07: So continged is not a word of English.

666
00:51:55,492 --> 00:51:58,436
Speaker SPEAKER_07: But continged on knowing the network did it, it could be.

667
00:51:58,456 --> 00:52:00,880
Speaker SPEAKER_07: OK.

668
00:52:02,277 --> 00:52:07,664
Speaker SPEAKER_07: So you'll notice this, it's got a sense of humor too, it likes the Irish intelligence agencies in the Mediterranean region.

669
00:52:10,027 --> 00:52:16,036
Speaker SPEAKER_07: You'll notice it's got no coherence across sentences, but it's got a lot of syntax, it's got a lot of semantics.

670
00:52:16,818 --> 00:52:24,909
Speaker SPEAKER_07: A lot of it's fairly shallow semantics, but it didn't have any problems learning what words were, and it didn't have any problems learning short range syntax.

671
00:52:24,929 --> 00:52:29,735
Speaker SPEAKER_07: So it really could, this recurrent net really could learn quite a good language model.

672
00:52:30,155 --> 00:52:36,862
Speaker SPEAKER_07: Not quite as good as the whole-word language models yet, but good enough to make you suspect that maybe we could put everything into one recurrent net.

673
00:52:37,943 --> 00:52:51,981
Speaker SPEAKER_07: And so I'll conclude just by showing you that if you read half a billion characters from Wikipedia with no prior knowledge, that is, the prior knowledge consists of it's a neural net with eight million connections and it does this, right?

674
00:52:52,481 --> 00:52:53,181
Speaker SPEAKER_07: That's the prior.

675
00:52:53,822 --> 00:52:55,985
Speaker SPEAKER_07: Oh, and you're going to train it by gradient descent.

676
00:52:56,965 --> 00:52:59,148
Speaker SPEAKER_07: And so here's a philosophical question for you.

677
00:52:59,585 --> 00:53:06,152
Speaker SPEAKER_07: If that was your prior, and you saw a string of half a billion characters from Wikipedia, would you end up knowing the meaning of life?

678
00:53:08,634 --> 00:53:12,478
Speaker SPEAKER_07: Now, it'd be pretty boring if you thought the meaning of life was 42, because that's in Wikipedia, right?

679
00:53:13,278 --> 00:53:19,445
Speaker SPEAKER_07: It'd be much more interesting if you thought the meaning of life was something that you generated for yourself, having read Wikipedia.

680
00:53:20,266 --> 00:53:20,806
Speaker SPEAKER_07: So here we go.

681
00:53:21,288 --> 00:53:22,068
Speaker SPEAKER_07: Let's hope this works.

682
00:53:22,789 --> 00:53:23,070
Speaker SPEAKER_07: OK.

683
00:53:23,309 --> 00:53:27,855
Speaker SPEAKER_07: What you do is you say the meaning of life is, and then you get it to carry on producing characters.

684
00:53:28,014 --> 00:53:29,135
Speaker SPEAKER_07: It's a fun game.

685
00:53:29,655 --> 00:53:32,460
Speaker SPEAKER_07: We were hoping it would say 42, and then we thought, that's stupid hope, right?

686
00:53:32,480 --> 00:53:33,422
Speaker SPEAKER_07: Because that's trivial.

687
00:53:35,063 --> 00:53:43,657
Speaker SPEAKER_07: So on the first version of the model we trained, which wasn't very good, on about the third attempt, it said, the meaning of life is literally recognition.

688
00:53:45,079 --> 00:53:48,184
Speaker SPEAKER_07: I thought, boy, we've got it.

689
00:53:49,666 --> 00:53:53,172
Speaker SPEAKER_07: Ilya then trained for another month, and we got a much better model.

690
00:53:53,952 --> 00:53:55,695
Speaker SPEAKER_07: And he ran the model 10 times.

691
00:53:56,257 --> 00:53:58,380
Speaker SPEAKER_07: And you run the model until you hit a full stop.

692
00:53:59,456 --> 00:54:02,400
Speaker SPEAKER_07: And then we pick the best of the 10.

693
00:54:03,141 --> 00:54:08,088
Speaker SPEAKER_07: So there's some selection, but there's only a few bits of selection here.

694
00:54:09,228 --> 00:54:26,409
Speaker SPEAKER_07: And so here's how it completed the meaning of life is.

695
00:54:40,750 --> 00:54:41,990
Speaker SPEAKER_07: So it's almost there.

696
00:54:42,010 --> 00:54:43,472
Speaker SPEAKER_07: It's almost there.

697
00:54:45,074 --> 00:54:46,715
Speaker SPEAKER_07: Okay.

698
00:54:46,735 --> 00:54:47,117
Speaker SPEAKER_07: I'm done.

699
00:55:02,452 --> 00:55:04,275
Speaker SPEAKER_06: Alright, so we have time for a few questions.

700
00:55:07,838 --> 00:55:07,938
Unknown Speaker: Yeah.

701
00:55:09,438 --> 00:55:17,010
Speaker SPEAKER_01: Is there a parallel between the initial unsupervised bit that you do and unsupervised dimensionality reduction?

702
00:55:17,550 --> 00:55:18,210
Speaker SPEAKER_01: Unsupervised?

703
00:55:18,271 --> 00:55:20,414
Speaker SPEAKER_01: Dimensionality reduction, like PCA.

704
00:55:20,936 --> 00:55:22,257
Speaker SPEAKER_01: Like, is there some kind of parallel there?

705
00:55:24,722 --> 00:55:28,306
Speaker SPEAKER_07: Not much.

706
00:55:29,536 --> 00:55:37,045
Speaker SPEAKER_07: In many systems, you're going to try and reduce the dimensionality of the input, because you don't want too many parameters, or you don't want to do too much computation.

707
00:55:37,646 --> 00:55:39,889
Speaker SPEAKER_07: And so PCA is throwing away information.

708
00:55:40,630 --> 00:55:44,257
Speaker SPEAKER_07: Often, if you're lucky, that information is noise in the minor components.

709
00:55:46,318 --> 00:55:50,224
Speaker SPEAKER_07: But typically, these nets work better if you expand the input.

710
00:55:51,326 --> 00:55:55,391
Speaker SPEAKER_07: So for images, for example, the first hidden layer is going to be bigger than the number of pixels.

711
00:55:56,434 --> 00:56:02,393
Speaker SPEAKER_07: And so it's not like we're going to try and get regularization by reducing dimensionality.

712
00:56:04,521 --> 00:56:09,617
Speaker SPEAKER_07: You can use the same methods for reducing dimensionality, but that's not really the primary way it's working.

713
00:56:13,545 --> 00:56:13,885
Speaker SPEAKER_00: Yeah.

714
00:56:14,567 --> 00:56:17,711
Speaker SPEAKER_00: So I know you don't know the answer to this, but I just wanted your best guess.

715
00:56:17,771 --> 00:56:25,282
Speaker SPEAKER_00: Are these models similar to, let's say, the method used for cortical processing in the human brain, do you think?

716
00:56:26,103 --> 00:56:28,927
Speaker SPEAKER_00: Or is there like a vast difference?

717
00:56:28,947 --> 00:56:32,771
Speaker SPEAKER_00: And will we know the answer within the next 20 years?

718
00:56:32,791 --> 00:56:34,474
Speaker SPEAKER_07: I think there's about this much difference.

719
00:56:40,630 --> 00:56:43,773
Speaker SPEAKER_07: I think we may well know the answer within the next 20 years.

720
00:56:44,594 --> 00:56:55,891
Speaker SPEAKER_07: I think, now I've thought this consistently for a long time, but I think in the next 5 to 10 years we're going to understand a lot more about how the brain is doing these kinds of computations.

721
00:56:57,693 --> 00:57:00,376
Speaker SPEAKER_07: It better be in the next 10 years.

722
00:57:01,690 --> 00:57:03,692
Speaker SPEAKER_07: Eventually, we're going to understand how the brain does it.

723
00:57:04,413 --> 00:57:05,675
Speaker SPEAKER_07: And that's going to be very exciting.

724
00:57:05,855 --> 00:57:07,036
Speaker SPEAKER_07: It's going to help the technology.

725
00:57:07,076 --> 00:57:08,677
Speaker SPEAKER_07: It may even help things like education.

726
00:57:09,679 --> 00:57:10,940
Speaker SPEAKER_07: But it has to be doing it somehow.

727
00:57:12,001 --> 00:57:13,844
Speaker SPEAKER_07: And it's pretty effective what the brain's doing.

728
00:57:14,445 --> 00:57:19,170
Speaker SPEAKER_07: And as these techniques get better and better, it looks like a better and better bet for trying to understand what the brain's up to.

729
00:57:19,831 --> 00:57:27,900
Speaker SPEAKER_07: And what I'd like to see is that we stay with the set of things that work and make them work better and better by making them more and more like the brain.

730
00:57:28,048 --> 00:57:32,574
Speaker SPEAKER_07: And if we can do that, I think we will get to what the brain's actually up to.

731
00:57:32,594 --> 00:57:36,159
Speaker SPEAKER_07: And I think we'll get there about 1,000 years quicker than Henry Markham.

732
00:57:40,204 --> 00:57:43,228
Speaker SPEAKER_04: Yeah.

733
00:57:43,248 --> 00:57:51,280
Speaker SPEAKER_04: So I'm interested in the potential of generative models built up using unsupervised learning.

734
00:57:51,300 --> 00:57:52,842
Speaker SPEAKER_07: That's what I really believe in, by the way.

735
00:57:53,202 --> 00:57:57,128
Speaker SPEAKER_07: All this discriminative trained stuff, it's just embarrassing that it works.

736
00:57:58,626 --> 00:58:05,784
Speaker SPEAKER_04: I really wish I knew if you were serious.

737
00:58:06,237 --> 00:58:17,489
Speaker SPEAKER_04: Fairly far back in the presentation, you showed the initialization using unsupervised data, and you reached the point where we have a deep-leaf net that's now running in the generative direction.

738
00:58:17,829 --> 00:58:29,721
Speaker SPEAKER_04: I was wondering, do you think that that is a good model for accomplishing, say, novel yet realistic generation, and what could you tell us about current research trends in that direction?

739
00:58:29,880 --> 00:58:30,942
Speaker SPEAKER_07: Yeah, I do.

740
00:58:31,682 --> 00:58:33,644
Speaker SPEAKER_07: I think that's ultimately the way to go.

741
00:58:34,536 --> 00:58:38,603
Speaker SPEAKER_07: But you know, you let the results direct you.

742
00:58:39,443 --> 00:58:43,610
Speaker SPEAKER_07: And right now, we can make a lot of mileage out of the fact that discriminative methods are working very well.

743
00:58:45,632 --> 00:58:47,615
Speaker SPEAKER_07: But for people, we don't get big data sets.

744
00:58:48,076 --> 00:58:50,500
Speaker SPEAKER_07: What we do get is we get big correlations in the inputs.

745
00:58:50,519 --> 00:58:55,666
Speaker SPEAKER_07: You get words and objects, but they're just correlations in the input.

746
00:58:56,659 --> 00:59:02,335
Speaker SPEAKER_07: So ultimately, I'm completely convinced that generative methods will be better, and that to do vision, you need to do inverse computer graphics.

747
00:59:02,815 --> 00:59:04,400
Speaker SPEAKER_07: You need to have a proper graphics model in there.

748
00:59:04,641 --> 00:59:07,288
Speaker SPEAKER_07: And as soon as you do that, you'll be able to deal with viewpoint and their problems.

749
00:59:08,831 --> 00:59:10,094
Speaker SPEAKER_07: That's the direction to go.

750
00:59:15,070 --> 00:59:16,775
Speaker SPEAKER_03: My question relates a little bit to that.

751
00:59:16,855 --> 00:59:24,998
Speaker SPEAKER_03: So the models you present learn some form of abstraction or structure of the domain, however they do it.

752
00:59:25,097 --> 00:59:30,092
Speaker SPEAKER_03: And of course, it's difficult to tease that structurally out of the network.

753
00:59:30,072 --> 00:59:43,893
Speaker SPEAKER_03: But so ultimately, if it really is learning, you know, the correct abstractions, you know, nouns and verbs, then you should be able to do things like one-shot learning.

754
00:59:43,954 --> 00:59:54,612
Speaker SPEAKER_03: So if I say this is a floopy, then I shouldn't have to be like Google and have, you know, a lot of text that includes the word floopy in order to learn how to use that.

755
00:59:54,672 --> 00:59:56,534
Speaker SPEAKER_03: And so you have any thoughts on that?

756
00:59:56,514 --> 01:00:00,021
Speaker SPEAKER_03: on how to do one-child learning with this?

757
01:00:00,782 --> 01:00:02,206
Speaker SPEAKER_07: So it relates to the last question.

758
01:00:02,266 --> 01:00:10,902
Speaker SPEAKER_07: If you can build good generative models, so you get internal representations that are, in a sense, that are what a computer graphics person would describe as correct.

759
01:00:11,923 --> 01:00:18,556
Speaker SPEAKER_07: They're the interesting parts of the object, and you have the spatial relation between those parts and other parts, or between those parts and the whole object.

760
01:00:18,739 --> 01:00:25,271
Speaker SPEAKER_07: then from one shot, you're going to be able to see, oh, this is an object which has these familiar parts arranged in this new way.

761
01:00:26,192 --> 01:00:30,619
Speaker SPEAKER_07: And oh, there's another one in a completely different viewpoint, but it's the same parts arranged in the same way.

762
01:00:31,760 --> 01:00:38,030
Speaker SPEAKER_07: I think that's the only hope for doing this one-shot generalization across big changes in viewpoint or lighting.

763
01:00:38,010 --> 01:00:45,621
Speaker SPEAKER_03: The way to do it is... Do you see that being learned or that being... first you inject it in a hand-engineered way and then... No, I see that being learned.

764
01:00:46,083 --> 01:00:56,677
Speaker SPEAKER_07: But I think what you can inject in a hand-engineered way is, for example, the knowledge that coordinate transforms are linear operations.

765
01:00:57,333 --> 01:01:03,547
Speaker SPEAKER_07: And so you can put in a big prior in this network saying, learn to handle this stuff with a sequence of linear operations.

766
01:01:04,510 --> 01:01:08,699
Speaker SPEAKER_07: That's very, very strong constraint, because most things aren't linear operations.

767
01:01:09,141 --> 01:01:14,092
Speaker SPEAKER_07: And it's completely insane that we're using these multiple layers of nonlinearities

768
01:01:14,072 --> 01:01:20,706
Speaker SPEAKER_07: to deal with stuff that can be modeled, if you go to the right computer graphics representation, as linear operations.

769
01:01:20,887 --> 01:01:21,608
Speaker SPEAKER_07: That's kind of insane.

770
01:01:21,949 --> 01:01:25,336
Speaker SPEAKER_07: Because it's only linear things that you can extrapolate along the way.

771
01:01:25,356 --> 01:01:31,050
Speaker SPEAKER_07: You're never going to extrapolate to totally different viewpoints unless you can find the linear manifold that everything lies on.

772
01:01:32,413 --> 01:01:34,376
Speaker SPEAKER_07: And that's what we have to do.

773
01:01:38,304 --> 01:01:51,840
Speaker SPEAKER_05: So I guess it's fair, but on the other hand, it's part of that is the point that you can maybe do experiments on the model and see disabling a certain part here or there.

774
01:01:52,601 --> 01:01:55,905
Speaker SPEAKER_05: And also, some parts are important.

775
01:01:55,925 --> 01:02:02,472
Speaker SPEAKER_05: So I'm kind of wondering if you have a network with millions of nodes,

776
01:02:02,639 --> 01:02:11,269
Speaker SPEAKER_05: or trillions of nodes, is it feasible to look at what those weights actually represent?

777
01:02:11,349 --> 01:02:12,371
Speaker SPEAKER_07: To some extent.

778
01:02:12,590 --> 01:02:23,684
Speaker SPEAKER_07: I mean, the work done by Jeff Dean and Andrew Ng and Kwok Lee at Google on unsupervised learning from frames of YouTube videos,

779
01:02:24,239 --> 01:02:32,375
Speaker SPEAKER_07: They showed that several layers into the network using unsupervised learning, you get neurons that really did represent cats.

780
01:02:33,036 --> 01:02:42,875
Speaker SPEAKER_07: I mean, if you multiply together the weights all the way down as if it were a linear system, which it isn't, then you can see the receptive field as cat-like.

781
01:02:42,894 --> 01:02:46,382
Speaker SPEAKER_07: For better, what you can do is you can take a neuron deep within the system,

782
01:02:46,480 --> 01:02:54,034
Speaker SPEAKER_07: And you can take an input and you can say, now let's keep changing the input in the direction, make this neuron happier and happier.

783
01:02:54,757 --> 01:02:58,304
Speaker SPEAKER_07: And so you can optimize the input image for making this neuron fire.

784
01:02:59,385 --> 01:03:03,675
Speaker SPEAKER_07: And then you can see what it likes most, at least what it likes most from where you started.

785
01:03:04,597 --> 01:03:06,159
Speaker SPEAKER_07: And it's a cat.

786
01:03:06,646 --> 01:03:07,989
Speaker SPEAKER_07: And it's a very convincing cat.

787
01:03:08,009 --> 01:03:08,951
Speaker SPEAKER_07: And other ones are people.

788
01:03:09,813 --> 01:03:13,141
Speaker SPEAKER_07: So you can do some work and find out what some of these internal things are doing.

789
01:03:13,561 --> 01:03:21,320
Speaker SPEAKER_07: And it's very clear, their big unsupervised net that had like a billion connections was learning to recognize familiar object classes.

790
01:03:22,342 --> 01:03:24,047
Speaker SPEAKER_07: At least some of the neurons were doing that.

791
01:03:27,907 --> 01:03:32,054
Speaker SPEAKER_04: One last question.

792
01:03:32,074 --> 01:03:43,813
Speaker SPEAKER_04: You've talked about nets that are both recurrent and non-recurrent, and I'm wondering, for the recurrent nets, were they also deep in the stacked direction, and is that important?

793
01:03:44,353 --> 01:03:45,375
Speaker SPEAKER_07: Okay, good question.

794
01:03:45,956 --> 01:03:47,958
Speaker SPEAKER_07: So until fairly recently,

795
01:03:48,293 --> 01:03:55,346
Speaker SPEAKER_07: We sort of said, well, if you ask what it means for a current net to be deep in the stack direction, that's just some missing connections, right?

796
01:03:55,425 --> 01:04:06,025
Speaker SPEAKER_07: If you have a hidden state and a full matrix connectivity to the next hidden state, then if you start leaving out blocks, then you get a layered system.

797
01:04:07,827 --> 01:04:09,411
Speaker SPEAKER_07: So we thought, well, why not just go for the full one?

798
01:04:10,132 --> 01:04:11,313
Speaker SPEAKER_07: It turns out,

799
01:04:12,407 --> 01:04:17,735
Speaker SPEAKER_07: that there's two reasons why not just go for the full one when you're doing speech.

800
01:04:18,735 --> 01:04:26,806
Speaker SPEAKER_07: One is that in order to get the speech recognition to work well in the recurrent net, Alex had to use nets in both directions.

801
01:04:26,865 --> 01:04:27,867
Speaker SPEAKER_07: So it was offline.

802
01:04:28,027 --> 01:04:31,811
Speaker SPEAKER_07: He would go forwards and he would also go backwards.

803
01:04:33,434 --> 01:04:35,956
Speaker SPEAKER_07: And he does that at each layer.

804
01:04:36,717 --> 01:04:40,001
Speaker SPEAKER_07: So he goes forwards and backwards four different times in a four-layer network.

805
01:04:40,657 --> 01:04:44,101
Speaker SPEAKER_07: And that's spreading information around from the future and the past, and that's a big help.

806
01:04:44,802 --> 01:04:55,516
Speaker SPEAKER_07: So actually layering it and doing a full forward backward at each layer, not forward backward in the HMM sense, but running a recurrent net in both directions, was a big help.

807
01:04:55,576 --> 01:04:58,059
Speaker SPEAKER_07: That's what got him down from like 24% to 17.7%.

808
01:04:58,280 --> 01:05:04,588
Speaker SPEAKER_07: So it turns out, and that was one of the innovations, layering these things.


1 00:00:24,989 --> 00:00:31,278 主持人 SPEAKER_02：我非常荣幸，能够介绍今天的演讲者。
2 00:00:32,301 --> 00:00:35,165 主持人 SPEAKER_02：杰夫·辛顿来自多伦多大学。
3 00:00:36,106 --> 00:00:45,883 主持人 SPEAKER_02：他于 1970 年毕业于剑桥大学，获得实验心理学学士学位，并于 1978 年毕业于爱丁堡大学，获得人工智能博士学位。
4 00:00:46,353 --> 00:00:51,887 主持人 SPEAKER_02：我们这些位于加拿大边境以南的野蛮人并不太重视血统。
5 00:00:52,750 --> 00:01:00,168 说话人 SPEAKER_02：但作为一名计算机科学家，我不得不注意到，Jeff 是逻辑学家乔治·布尔（George Boole）的曾曾孙。
6 00:01:01,633 --> 00:01:02,454 说话人 SPEAKER_02：非常酷。
7 00:01:02,789 --> 00:01:18,031 说话人 SPEAKER_02：在他的众多成就中，Jeff 与 Terry Sosnowski 共同发明了玻尔兹曼机，并与 Paul Wehrbusch、David Rumelhart 和 Ron Williams 一起引入了反向传播算法来训练多层神经网络。
8 00:01:18,852 --> 00:01:30,028 说话人 SPEAKER_02：他是伦敦大学学院 Gatsby 计算神经科学单元的创始主任，这是世界上研究计算神经科学的一流场所之一。
9 00:01:30,430 --> 00:01:35,719 讲者 SPEAKER_02：他的学生名单上都是机器学习和计算机视觉领域的杰出计算机科学家。
10 00:01:36,480 --> 00:01:48,102 讲者 SPEAKER_02：他们包括里克·塞尔斯基、理查德·泽梅尔、拉达福·尼尔、卡尔·拉斯穆森、布伦丹·弗雷和 EYT，这些研究科学家在我们许多人中都是众所周知的。
11 00:01:48,891 --> 00:01:57,069 讲者 SPEAKER_02：他在 1998 年当选为皇家学会会员，2005 年获得了 IJCAI 研究卓越终身成就奖。
12 00:01:58,513 --> 00:02:08,335 讲者 SPEAKER_02：尽管他获得了众多奖项，并且作为受邀演讲者和论文导师广受欢迎，但他仍然保持着惊人的生产力和创新精神。
13 00:02:09,395 --> 00:02:25,530 讲者 SPEAKER_02：如果你还没有听说过深度信念网络、对比散度、专家乘积、唤醒-睡眠算法、语义哈希，或者其他杰夫的众多发明，那么你可能也不知道 ICML 或 NIPS 是什么。
14 00:02:26,472 --> 00:02:37,403 讲者 SPEAKER_02：回到一月份，Yann LeCun，众多受杰夫影响的计算机科学家之一，发起了一个与机器学习相关的梗，写下了“杰夫·辛顿的事实”。
15 00:02:38,025 --> 00:02:45,752 讲者 SPEAKER_02：这是“杰夫·诺里斯的事实”的类比，我在得知这个之前并不知道。
16 00:02:47,155 --> 00:02:48,977 讲者 SPEAKER_02：所以，这里有一些。
17 00:02:49,076 --> 00:02:51,399 说话人 SPEAKER_02：Jeff Hinton 不需要创建隐藏单元。
18 00:02:51,639 --> 00:02:53,981 说话人 SPEAKER_02：当他靠近时，它们会自行隐藏。
19 00:02:55,008 --> 00:02:58,793 说话人 SPEAKER_02：Jeff Hinton 曾经构建了一个神经网络，在 MNIST 上打败了 Chuck Norris。
20 00:03:01,497 --> 00:03:05,001 说话人 SPEAKER_02：与 Jeff Hinton 的遭遇后，支持向量变得不稳定。
21 00:03:07,645 --> 00:03:11,330 说话人 SPEAKER_02：今天的介绍中，我制作了一个由 Jan 的两个事实组成的混搭。
22 00:03:11,629 --> 00:03:14,493 说话人 SPEAKER_02：它说，永远不要打断 Jeff Hinton 的演讲。
23 00:03:14,794 --> 00:03:16,796 说话人 SPEAKER_02：他会让你后悔不已。
24 00:03:18,463 --> 00:03:22,110 说话人 SPEAKER_02：这真的很不公平，因为 Jeff 是我认识的最甜的人之一。
25 00:03:22,129 --> 00:03:24,394 说话人 SPEAKER_02：他体内没有平均场。
26 00:03:25,395 --> 00:03:31,926 说话人 SPEAKER_02：你们将有充足的机会来测试这一点，因为他将在我们身边度过整个夏天的大部分时间。
27 00:03:32,868 --> 00:03:34,532 说话人 SPEAKER_02：那么，无需多言，杰夫。
28 00:03:41,330 --> 00:03:52,730 说话人 SPEAKER_00：所以今天，我将讨论机器学习中的一个新想法，它也对生物学中的几个主要问题有影响。
29 00:03:53,853 --> 00:04:04,131 说话人 说话人_00：所以，本次演讲的主题将围绕机器学习中的这个新想法展开，但它在一定程度上受到了进化论理论的一些工作的启发。
30 00:04:04,110 --> 00:04:08,375 说话人 说话人_00：我以前研究过这个，但后来放弃了，因为我没有做好实验。
31 00:04:08,935 --> 00:04:11,317 说话人 说话人_00：然后，Dimitri 爸爸来到了多伦多并做了一场演讲。
32 00:04:11,758 --> 00:04:13,019 说话人 说话人_00：我想，那是一个相同的概念。
33 00:04:13,378 --> 00:04:14,800 说话人 说话人_00: 因此我回去做了正确的实验。
34 00:04:14,820 --> 00:04:15,640 说话人 说话人_00: 它确实工作得很好。
35 00:04:18,903 --> 00:04:23,288 说话人 说话人_00: 为了解释进化论中的某个理论，它也解释了神经元如何通信。
36 00:04:24,990 --> 00:04:28,312 说话人 说话人_00: 在进化论中，有关于有性繁殖的问题。
37 00:04:28,872 --> 00:04:33,257 说话人 说话人_00: 问题在于，要既好又合适，你的基因需要协同工作得很好。
38 00:04:33,641 --> 00:04:36,728 说话人 说话人_00: 所以你找到了一组很好的基因，它们相互适应得很好。
39 00:04:37,750 --> 00:04:38,471 说话人 说话人_00: 然后你进行交配。
40 00:04:39,012 --> 00:04:40,156 说话人 说话人_00: 然后一半的基因消失了。
41 00:04:40,355 --> 00:04:41,920 说话人 说话人_00: 这是一个简单的模型，描述了发生了什么。
42 00:04:42,259 --> 00:04:43,201 说话人 说话人_00: 基因中有一半消失了。
43 00:04:43,262 --> 00:04:46,228 说话人 说话人_00: 因此，你不再拥有那一大组相互适应的基因了。
44 00:04:46,249 --> 00:04:47,050 说话人 说话人_00: 所以，你的效果不是很好。
45 00:04:47,932 --> 00:04:49,956 说话人 说话人_00: 这真是个谜。
46 00:04:50,036 --> 00:04:51,920 说话人 说话人_00: 那为什么不是坏事呢？
47 00:04:52,627 --> 00:05:03,319 说话人 说话人_00: 所以，Livnat、Pepper、Dimitra 和 Feldman 在 2008 年的论文声称，有性生殖的全部目的就是为了打破这些共适应。
48 00:05:04,360 --> 00:05:06,682 说话人 说话人_00: 之所以这可能是个好主意，有两个原因。
49 00:05:07,204 --> 00:05:08,906 说话人 SPEAKER_00: 一个是为了长期优化。
50 00:05:09,326 --> 00:05:15,432 说话人 SPEAKER_00: 所以短期内可能不好，但它能让你摆脱必须同时改变很多东西才能取得进展的情况。
51 00:05:15,954 --> 00:05:17,514 说话人 SPEAKER_00: 所以这可能有助于长期优化。
52 00:05:18,235 --> 00:05:21,519 说话人 SPEAKER_00: 从机器学习的角度来看，我们还没有太多证据。
53 00:05:21,870 --> 00:05:25,694 说话人 说话人_00：另一件事是，这可能会使你更能抵御环境变化。
54 00:05:26,615 --> 00:05:37,545 说话人 说话人_00：所以如果环境发生变化，那么如果你有一个庞大而复杂的共适应，通过依赖许多不同的基因产生你想要的效果，那么可能会出现某些混乱。
55 00:05:38,466 --> 00:05:49,836 说话人 说话人_00：但如果你有大量较小的共适应，通过使用少量基因以多种方式实现功能，那么当环境发生变化时，其中一些方式更有可能存活下来。
56 00:05:50,355 --> 00:05:51,999 说话人 说话人_00：结果证明这是一个很大的影响。
57 00:05:55,225 --> 00:05:58,771 说话人 SPEAKER_00：另一个生物学问题是神经元如何进行通讯。
58 00:05:59,372 --> 00:06:02,177 说话人 SPEAKER_00：因此，皮层神经元进行大量的信号处理，并且在这方面非常擅长。
59 00:06:02,997 --> 00:06:06,124 说话人 SPEAKER_00：然而，它们并不向彼此发送真实数字。
60 00:06:06,725 --> 00:06:11,012 说话人 SPEAKER_00：据我们所知，它们发送这些活动尖峰，要么是一，要么是零。
61 00:06:11,533 --> 00:06:13,336 说话人 说话人_00：尖峰的时间是随机的。
62 00:06:13,956 --> 00:06:15,519 说话人 说话人_00：现在，这完全疯狂。
63 00:06:15,735 --> 00:06:21,622 说话人 说话人_00：因为对于相同数量的能量，它们可以发送一个与某些振荡相关的精确时间尖峰。
64 00:06:22,043 --> 00:06:26,428 说话人 说话人_00：因此，它们可以通过尖峰的时间来传递一个模拟值。
65 00:06:26,449 --> 00:06:28,031 说话人 SPEAKER_00: 那么，问题来了，他们为什么不这么做呢？
66 00:06:31,475 --> 00:06:31,536 说话人 SPEAKER_00: 好的。
67 00:06:31,555 --> 00:06:35,600 说话人 SPEAKER_00: 当我向你介绍这个机器学习中的想法时，我将尝试回答这两个问题。
68 00:06:37,235 --> 00:06:47,747 说话人 SPEAKER_00: 我们需要理解神经元如何通过单个尖峰进行通信的原因之一，是因为我们希望制作出由许多核心实现的真正大的神经网络。
69 00:06:48,428 --> 00:06:56,238 说话人 SPEAKER_00: 因此，神经元的状况，一些神经元会在一个核心中，而一些神经元在另一个核心中，它们需要在不同核心之间传递神经元的状况。
70 00:06:56,639 --> 00:06:58,062 说话人 SPEAKER_00: 因此，您希望这具有低带宽。
71 00:06:58,442 --> 00:07:04,709 说话人 SPEAKER_00: 如果它们只需要一个比特来传递它们的状况，那么这比目前人们所做的方法提高了 32 倍。
72 00:07:07,795 --> 00:07:13,062 说话人 SPEAKER_00: 表面上看，似乎很难相信一个比特比发送模拟值更好。
73 00:07:13,923 --> 00:07:21,394 说话人 SPEAKER_00: 我认为这是因为作为工程师，我们习惯于进行信号处理。
74 00:07:21,834 --> 00:07:29,485 说话人 SPEAKER_00: 你会做一些事情，比如假设存在一个线性动力学系统，然后拟合一个线性动力学模型。
75 00:07:30,266 --> 00:07:31,887 说话人 SPEAKER_00: 要做到这一点，你需要使用实数。
76 00:07:34,310 --> 00:07:36,334 说话人 SPEAKER_00: 这不是大脑解决的问题类型。
77 00:07:37,040 --> 00:07:42,550 发言人 SPEAKER_00：工程师从他们想要拟合的模型有一个很好的想法，现在他们想要识别参数。
78 00:07:43,492 --> 00:07:49,244 发言人 SPEAKER_00：大脑带着大量输入数据进入这个完全无法理解的世界。
79 00:07:50,064 --> 00:07:52,509 发言人 SPEAKER_00：这是一个真正的混乱。
80 00:07:53,391 --> 00:07:57,579 发言人 SPEAKER_00：在这种情况下，正确的方法是拟合无数不同的模型。
81 00:07:58,201 --> 00:08:00,303 说话人 SPEAKER_00: 你不想真的让一个特定模型非常合适。
82 00:08:00,324 --> 00:08:03,910 说话人 SPEAKER_00: 你想要拟合无数个不同的模型，然后利用大众的智慧。
83 00:08:04,411 --> 00:08:10,420 说话人 SPEAKER_00: 也就是说，当有人问你问题时，你让所有无数个不同的模型给出答案，然后取一个共识。
84 00:08:10,440 --> 00:08:16,029 说话人 SPEAKER_00: 这与仔细拟合一个精确模型非常不同的计算方式。
85 00:08:18,935 --> 00:08:20,458 说话人 说话人_00：所以我将回到那个问题。
86 00:08:21,158 --> 00:08:23,161 说话人 说话人_00：现在我要跳回去了。
87 00:08:23,766 --> 00:08:24,348 说话人 说话人_00：很长时间。
88 00:08:25,968 --> 00:08:31,714 说话人 说话人_00：在20世纪80年代中期，人们开发了具有多层特征检测器的神经网络。
89 00:08:32,294 --> 00:08:37,200 说话人 SPEAKER_00：我们有一个名为反向传播的学习算法，它在学习多层特征检测器方面是合理的。
90 00:08:37,879 --> 00:08:46,328 说话人 SPEAKER_00：所以你输入一些数据，让它通过网络，比较结果与预期，然后利用链式法则反向遍历网络，以确定如何改变每个神经元的输入连接权重。
91 00:08:47,068 --> 00:08:49,991 说话人 SPEAKER_00：一开始这非常令人兴奋。
92 00:08:50,753 --> 00:08:53,115 说话人 SPEAKER_00：但它对深层网络从未工作得很好。
93 00:08:53,416 --> 00:08:56,780 说话人 SPEAKER_00：除了 Yann LeCun 使用的卷积深度网络外。
94 00:08:57,522 --> 00:09:00,988 说话人 SPEAKER_00：但其他的都不太奏效。
95 00:09:01,008 --> 00:09:05,595 说话人 SPEAKER_00：其中一个问题是，当时，获取大量标记数据非常困难。
96 00:09:06,355 --> 00:09:11,024 说话人 SPEAKER_00：你可以获取图像，或者获取声波，但获取它们的准确标签真的很困难。
97 00:09:11,684 --> 00:09:12,947 说话人 说话人_00：所以我们无法获取足够多的标注数据。
98 00:09:15,190 --> 00:09:17,614 说话人 说话人_00：当然，一个解决方案就是非常努力地获取标注数据。
99 00:09:17,653 --> 00:09:18,775 说话人 说话人_00：语音领域的人就是这样做的。
100 00:09:18,796 --> 00:09:20,317 说话人 说话人_00：他们获得了大量的标注数据集。
101 00:09:20,686 --> 00:09:36,792 说话人 SPEAKER_00: 另一种解决方案是看看我们是否可以通过尝试建模输入数据，尝试构建一个感官输入的生成模型，而不是试图决定它的正确标签，来学习大多数特征检测器的层，也许除了最后一层。
102 00:09:37,773 --> 00:09:39,495 说话人 SPEAKER_00: 结果这种方法效果相当不错。
103 00:09:40,878 --> 00:09:43,361 说话人 SPEAKER_00: 所以自 1985 年以来，发生了很多事情。
104 00:09:43,842 --> 00:09:45,004 说话人 SPEAKER_00: 计算机变得更快了。
105 00:09:45,024 --> 00:09:47,688 说话人 SPEAKER_00: 这是发生的主要事情。
106 00:09:48,225 --> 00:09:49,707 说话人 SPEAKER_00: 标注数据集变得更大了。
107 00:09:50,349 --> 00:09:52,311 说话人 SPEAKER_00: 因此现在可以训练大的反向传播网络。
108 00:09:53,952 --> 00:09:59,640 说话人 SPEAKER_00: 我们还找到了使用无监督学习更好地初始化这些多层网络权重的更好方法。
109 00:10:00,240 --> 00:10:04,826 说话人 SPEAKER_00: 所有三个因素的结合意味着我们现在可以合理地初始化网络。
110 00:10:05,488 --> 00:10:07,931 说话人 SPEAKER_00: 然后，我们可以将它们释放到大数据集上。
111 00:10:08,312 --> 00:10:09,633 说话人 SPEAKER_00: 我们使用未标记的数据进行初始化。
112 00:10:09,653 --> 00:10:16,562 说话人 SPEAKER_00: 将它们释放到大数据集上，并用大型快速计算机或一大群大型快速计算机对它们进行彻底训练。
113 00:10:17,065 --> 00:10:18,606 说话人 说话人_00: 他们表现得非常好。
114 00:10:20,650 --> 00:10:21,873 说话人 说话人_00: 那么，这里有一些宣传。
115 00:10:24,657 --> 00:10:27,761 说话人 说话人_00: 到目前为止，最令人印象深刻的可能是语音识别的应用。
116 00:10:29,083 --> 00:10:41,283 说话人 说话人_00: 以前，我和我的部分学生，但主要是我的学生，展示了一个相对较小的数据集，即三小时的语音数据，有一个标准基准叫做
117 00:10:41,686 --> 00:10:58,075 说话者 SPEAKER_00：我们可以通过使用深度神经网络将声学帧窗口映射到对中间帧所代表的音素片段的预测，从而超越现有的无监督基准。
118 00:10:58,730 --> 00:11:02,053 说话者 SPEAKER_00：在语音处理中，你使用隐马尔可夫模型来处理时间对齐。
119 00:11:02,514 --> 00:11:06,138 说话者 SPEAKER_00：对于每个音素，你有一个具有几个状态（通常是三个）的隐马尔可夫模型。
120 00:11:06,999 --> 00:11:12,764 说话者 SPEAKER_00：有时对于某个音素，你可能会有许多不同的隐马尔可夫模型，这取决于你认为与之相邻的音素是什么。
121 00:11:14,025 --> 00:11:21,474 说话人 SPEAKER_00: 但是问题在于从声学输入预测出这个音素片段的可能性。
122 00:11:21,494 --> 00:11:23,876 说话人 SPEAKER_00: 而这正是深度神经网络做得更好的地方。
123 00:11:24,447 --> 00:11:34,541 说话人 SPEAKER_00: 如果你将这些预测输入到一个解码器，它使用各种关于语言的知识，那么它给出的单词字符串比使用之前的标准声学模型更准确。
124 00:11:36,702 --> 00:11:48,178 说话人 SPEAKER_00: 这里是一些最近综述论文的结果，这篇论文值得关注，因为论文的作者来自 MSR 研究、IBM、谷歌以及多伦多大学。
125 00:11:48,759 --> 00:11:50,522 说话人 SPEAKER_00：这是在更大的数据集上。
126 00:11:51,379 --> 00:11:55,566 说话人 SPEAKER_00：所以 MSR 使用了相当大的 309 小时的数据集。
127 00:11:55,605 --> 00:12:07,683 说话人 SPEAKER_00：他们使用标准方法在这之前将这些数据集的错误率从 27%降低到了 18%。
128 00:12:07,904 --> 00:12:08,905 说话人 SPEAKER_00：这是一个巨大的改进。
129 00:12:09,768 --> 00:12:12,851 说话人 说话人_00: 然后在另一个测试集上，他们将其从23%降低到16%。
130 00:12:14,033 --> 00:12:19,361 说话人 说话人_00: IBM，可能拥有最好的语音识别器，最精心调校的一个，
131 00:12:19,779 --> 00:12:26,327 说话人 说话人_00: 在一个相对较小的数据集上，将他们精心调校的系统错误率从18.8%降低到17.5%。
132 00:12:27,629 --> 00:12:30,533 说话人 说话人_00: 这是一个精心调校的系统。
133 00:12:30,552 --> 00:12:33,416 说话人 SPEAKER_00：在必应语音搜索中，他们降低了很多。
134 00:12:35,118 --> 00:12:37,321 说话人 SPEAKER_00：谷歌使用了谷歌认为的小数据集。
135 00:12:38,721 --> 00:12:41,846 说话人 SPEAKER_00：他们将错误率降低到了 12.3。
136 00:12:44,068 --> 00:12:47,592 说话人 SPEAKER_00：他们将错误率从 12.3 降低到了。
137 00:12:49,057 --> 00:12:52,201 说话人 SPEAKER_00: 训练在所谓的大数据集上的 16%。
138 00:12:53,283 --> 00:12:54,966 说话人 SPEAKER_00: 我不知道有多大，但肯定很大。
139 00:12:55,746 --> 00:12:57,308 说话人 SPEAKER_00: 所以你给它更多的数据，你得到 16%。
140 00:12:57,590 --> 00:13:00,734 说话人 SPEAKER_00: 你给它更少的数据，并且给它这个更好的，这个深度神经网络，它得到 12%。
141 00:13:01,014 --> 00:13:02,376 说话人 SPEAKER_00: 现在已经下降到 12%以下了。
142 00:13:02,456 --> 00:13:11,188 说话人 SPEAKER_00: 大部分进展都是由一个暑期实习生完成的，当然也得到了语音组的很大帮助。
143 00:13:12,530 --> 00:13:16,416 说话人 SPEAKER_00: 这就是深度神经网络做有用事情的最好例子之一。
144 00:13:17,392 --> 00:13:23,782 说话人 SPEAKER_00: MSR 已经宣布他们将要将其系统，即深度神经网络系统，投入实际使用。
145 00:13:28,008 --> 00:13:28,207 说话人 SPEAKER_00: 好吧。
146 00:13:28,227 --> 00:13:31,133 说话人 SPEAKER_00: 那么，问题是我们能用大深度神经网络做什么呢？
147 00:13:32,014 --> 00:13:33,076 说话人 SPEAKER_00: 为什么它们不是一切问题的答案呢？
148 00:13:33,096 --> 00:13:36,279 说话人 SPEAKER_00: 这就是我们第一次做反向传播时想到的，我现在又开始这么想了。
149 00:13:37,461 --> 00:13:39,225 说话人 SPEAKER_00：嗯，这里有一件比较棘手的事情要做。
150 00:13:39,544 --> 00:13:41,408 说话人 SPEAKER_00：要训练一个大型的深度神经网络，
151 00:13:42,147 --> 00:13:47,534 说话人 SPEAKER_00：你需要很多核心，然后分配到各个核心上，长时间运行。
152 00:13:48,495 --> 00:13:55,547 说话人 SPEAKER_00：经过几周使用数千个核心，你用大约 17 亿参数训练出了你那庞大的网络。
153 00:13:56,769 --> 00:14:03,479 说话人 SPEAKER_00: 然后，有人问，为什么不训练 500 个这样的模型然后取平均值呢？
154 00:14:03,499 --> 00:14:06,562 说话人 SPEAKER_00: 嗯，这听起来像是件辛苦的工作。
155 00:14:08,086 --> 00:14:11,630 说话人 SPEAKER_00: 但我们知道，平均很多模型
156 00:14:12,015 --> 00:14:13,138 说话人 SPEAKER_00: 总是能带来很大的收益。
157 00:14:13,778 --> 00:14:27,938 说话人 SPEAKER_00：如果你想赢得像 Netflix 这样的机器学习竞赛，你应该做的是尝试很多不同的模型，丢弃那些没有用的，保留那些表现相对较好的，尤其是那些彼此不同的，然后对它们进行平均。
158 00:14:28,399 --> 00:14:30,743 说话人 SPEAKER_00：获胜者平均使用了超过 100 个模型。
159 00:14:31,604 --> 00:14:34,008 说话人 SPEAKER_00：我们非常希望用大型深度神经网络来做这件事。
160 00:14:34,447 --> 00:14:35,809 说话人 SPEAKER_00：这可能会使它们的工作效果更好。
161 00:14:36,331 --> 00:14:41,077 说话人 说话人_00：为此提供更多证据的是，你可以使用一种相当软弱的机器学习方法，即决策树
162 00:14:41,513 --> 00:14:45,384 说话人 说话人_00：单独一个决策树效果并不好。
163 00:14:46,046 --> 00:14:48,793 说话人 说话人_00：但是如果你拿很多这样的树，那么它们效果非常好。
164 00:14:48,974 --> 00:14:50,057 说话人 说话人_00：这被称为随机森林。
165 00:14:50,658 --> 00:14:54,831 说话人 SPEAKER_00: 我认为连接，那个让你了解你如何跳舞的东西，
166 00:14:55,115 --> 00:14:58,259 说话人 SPEAKER_00: 从 3D 数据中，使用的是随机森林。
167 00:14:58,980 --> 00:15:04,486 说话人 SPEAKER_00: 决策树的特点是训练速度快，测试时也非常快。
168 00:15:05,067 --> 00:15:07,250 说话人 SPEAKER_00: 所以在测试时，你可以将很多结果平均起来。
169 00:15:07,792 --> 00:15:09,293 说话人 SPEAKER_00: 此外，你也可以负担起训练一大堆。
170 00:15:09,813 --> 00:15:13,259 说话人 SPEAKER_00: 我们真的很想用这些大型深度神经网络来做同样的事情。
171 00:15:13,278 --> 00:15:14,441 说话人 SPEAKER_00: 表面上看，这可能会很棘手。
172 00:15:17,725 --> 00:15:22,370 说话人 SPEAKER_00: 在我说明我们将如何做到这一点之前，让我先告诉你两种平均模型的方法。
173 00:15:22,890 --> 00:15:32,381 说话人 SPEAKER_00：最标准的做法是，如果每个模型在类别上产生一个概率分布，这里有三个类别，你所做的就是简单地平均这些概率。
174 00:15:34,082 --> 00:15:49,000 说话人 SPEAKER_00：我们将给出一个比模型预测的分布更柔和的分布，并且保证使用这个平均分布比随机选择一个模型更有可能。
175 00:15:49,368 --> 00:15:51,191 说话人 SPEAKER_00：当然，最好的模型可能比平均模型要好。
176 00:15:51,530 --> 00:15:53,313 说话人 SPEAKER_00：但是当你有很多模型时，通常不会是这样的。
177 00:15:54,556 --> 00:15:58,682 说话人 SPEAKER_00：我们可以通过取几何平均数而不是算术平均数来组合分布。
178 00:15:59,423 --> 00:16:03,750 说话人 SPEAKER_00：我们只需将模型对不同类别的概率相乘。
179 00:16:04,791 --> 00:16:07,294 说话人 SPEAKER_00：然后我们取第 n 次方根，如果我们有 n 个模型。
180 00:16:08,557 --> 00:16:11,201 说话人 SPEAKER_00：然后我们得到一些数字，它们的和不为 1，所以我们必须重新归一化。
181 00:16:13,124 --> 00:16:17,990 说话人 SPEAKER_00：但这也有一个很好的特性，如果你使用这个几何平均数，
182 00:16:18,460 --> 00:16:24,445 说话人 SPEAKER_00：这将比随机选择模型给出更好的预测。
183 00:16:28,849 --> 00:16:28,908 说话人 SPEAKER_00：好的。
184 00:16:28,928 --> 00:16:30,410 说话人 SPEAKER_00：那么这就是本次演讲的主要内容。
185 00:16:31,652 --> 00:16:34,333 说话人 SPEAKER_00：我们将要做的，是使用一个略微过大的神经网络。
186 00:16:34,514 --> 00:16:36,655 说话人 SPEAKER_00：我将从一个只有一个隐藏层的神经网络开始。
187 00:16:37,697 --> 00:16:44,342 说话人 SPEAKER_00：每次我将一些训练数据呈现给它时，我将以 0.5 的概率随机激活每个隐藏单元。
188 00:16:44,823 --> 00:16:46,105 说话人 SPEAKER_00：所以你假装它们不存在。
189 00:16:47,923 --> 00:16:52,428 说话人 SPEAKER_00：现在我们有一个包含 2 的 n 次方可能架构的空间，如果有任何隐藏单元的话。
190 00:16:52,769 --> 00:16:54,772 说话人 SPEAKER_00：所以 h 到 h 架构是 h 个隐藏单元。
191 00:16:55,633 --> 00:16:57,775 说话人 SPEAKER_00：我们从那个空间中采样架构。
192 00:16:59,197 --> 00:17:01,179 说话人 SPEAKER_00：我们将采样大量不同的架构。
193 00:17:02,400 --> 00:17:05,183 说话人 SPEAKER_00：每种架构只能看到一个训练示例。
194 00:17:05,545 --> 00:17:10,450 说话人 SPEAKER_00：它只会看到一次，因为两次采样相同内容的概率可以忽略不计。
195 00:17:10,470 --> 00:17:12,432 说话人 SPEAKER_00：几乎所有架构都不会被采样。
196 00:17:13,253 --> 00:17:16,917 说话人 SPEAKER_00：但所有这些架构都在共享相同的权重，这极大地规范了它们。
197 00:17:17,843 --> 00:17:20,948 说话人 SPEAKER_00: 那么，问题是，这种效果会好吗？
198 00:17:21,769 --> 00:17:25,013 说话人 SPEAKER_00: 另一个问题，好的，你在训练时这么做。
199 00:17:25,314 --> 00:17:34,848 说话人 SPEAKER_00: 所以在训练时，对于每个训练示例，你采样一个架构，然后稍微更新你实际使用的隐藏单元的权重，然后转到下一个示例。
200 00:17:35,108 --> 00:17:36,230 说话人 SPEAKER_00: 但是测试时你怎么办？
201 00:17:36,250 --> 00:17:37,973 说话人 SPEAKER_00：因为你在测试时如何平均所有这些事物呢？
202 00:17:38,755 --> 00:17:44,644 说话人 SPEAKER_00：结果如果你愿意取几何平均数，那就非常简单了。
203 00:17:48,251 --> 00:17:51,914 说话人 SPEAKER_00：我们可以考虑拥有一个非常大的可能模型数量。
204 00:17:52,855 --> 00:18:00,321 说话人 SPEAKER_00：我们只训练了其中的一小部分，但仍然是一个很大的数字，和训练样本数量的展示次数一样多。
205 00:18:02,805 --> 00:18:11,771 说话人 SPEAKER_00：所以你可以把它看作是非常极端的袋装，即通过提供不同的训练数据来使你的模型不同，并对其进行强烈的正则化。
206 00:18:12,413 --> 00:18:17,297 说话人 SPEAKER_00：然后在测试时，我们可以这样做
207 00:18:17,867 --> 00:18:26,338 说话人 SPEAKER_00：因为在训练过程中每个隐藏单元以一半的概率存在，所以在测试时我们让它始终存在，但将其输出权重减半。
208 00:18:27,220 --> 00:18:29,502 说话人 SPEAKER_00：因此，从它那里得到的预期贡献是相同的。
209 00:18:30,825 --> 00:18:36,551 说话人 SPEAKER_00：现在，如果你这样做，那么你将有一个 softmax 输出组，它在类别之间计算概率分布。
210 00:18:37,192 --> 00:18:44,382 说话人 SPEAKER_00：你可以证明这正好计算了所有这些 2 的 H 次方个网络的预测的几何平均值。
211 00:18:45,694 --> 00:18:47,636 说话人 SPEAKER_00：这是一个很好的属性。
212 00:18:47,676 --> 00:18:54,463 说话人 SPEAKER_00：这意味着我们只运行了一个比我们在训练中运行的任何网络都大一倍的平均网络。
213 00:18:55,105 --> 00:19:00,450 说话人 SPEAKER_00：因此，对于这个两倍的因素，我们可以将这些网络全部平均起来，包括我们未训练的所有网络。
214 00:19:02,211 --> 00:19:02,372 未知说话人：嗯？
215 00:19:03,153 --> 00:19:04,134 说话人 SPEAKER_02: 观众成员 2 这是因为年龄问题。
216 00:19:04,153 --> 00:19:04,835 未知说话人：他们中的大多数从未接受过训练。
217 00:19:05,154 --> 00:19:05,915 说话人 SPEAKER_00：我们从未接受过训练，对吧。
218 00:19:06,415 --> 00:19:08,278 说话人 SPEAKER_00：另一方面，它们与接受过训练的模型共享权重。
219 00:19:09,859 --> 00:19:13,743 说话人 SPEAKER_00：所以即使是没有经过训练的，也因为这种大规模权重共享而会做出合理的事情。
220 00:19:14,517 --> 00:19:17,842 说话人 SPEAKER_00：这是一种有趣的模型平均方法，我们将模型平均与大规模权重共享结合起来。
221 00:19:21,929 --> 00:19:25,394 说话人 SPEAKER_00：如果我们有更多的隐藏层，我们就在所有的隐藏层中使用 dropout。
222 00:19:26,215 --> 00:19:31,143 说话人 SPEAKER_00：我们总是使用，几乎总是使用 50%的 dropout，因为这不再是自由参数了。
223 00:19:32,546 --> 00:19:35,810 说话人 SPEAKER_00: 我的意思是，一半其实并不完全是调整。
224 00:19:35,830 --> 00:19:37,773 说话人 SPEAKER_00: 我们只是用一半。
225 00:19:38,632 --> 00:19:40,994 说话人 SPEAKER_00: 当然，我们尝试过使用不同的数字。
226 00:19:41,575 --> 00:19:44,659 说话人 SPEAKER_00: 任何介于 0.3 和 0.7 之间的数字都有非常相似的行为。
227 00:19:45,398 --> 00:19:52,707 说话人 SPEAKER_00: 很可能的情况是，如果你有一个非常大的网络，如果你负担得起，使用更多的 dropout 更好。
228 00:19:54,348 --> 00:19:56,771 说话人 SPEAKER_00: 也就是说，每个案例保留更小的比例。
229 00:19:58,753 --> 00:20:05,922 说话人 SPEAKER_00: 如果你有多层隐藏层并且每层使用 50%的 dropout，那么在测试时使用均值网络，
230 00:20:06,391 --> 00:20:11,919 说话人 SPEAKER_00: 这并不等同于运行所有可能的网络并通过平均它们来取几何平均。
231 00:20:12,338 --> 00:20:14,541 说话人 SPEAKER_00: 但这很接近那个。
232 00:20:14,561 --> 00:20:18,367 说话人 SPEAKER_00: 所以你可以做实验，多次随机运行并取平均值。
233 00:20:18,708 --> 00:20:22,053 说话人 SPEAKER_00: 你会发现这和运行一次平均场网络非常相似。
234 00:20:25,959 --> 00:20:27,540 说话人 SPEAKER_00: 你也可以在输入层做同样的事情。
235 00:20:28,382 --> 00:20:29,884 说话人 SPEAKER_00: 这也很有帮助。
236 00:20:30,251 --> 00:20:36,298 说话人 SPEAKER_00: 这已经被 Yoshua Bengio 的团队和其他地方的人使用，你只需省略一些输入。
237 00:20:36,318 --> 00:20:39,021 说话人 SPEAKER_00: 所以如果是图像，你只需将一些像素设置为 0。
238 00:20:39,823 --> 00:20:44,508 说话人 SPEAKER_00: 这充当了一个很好的正则化器，当然，在训练期间会对你造成伤害，但会使你更好地泛化。
239 00:20:44,887 --> 00:20:45,969 说话人 SPEAKER_00：这是一种很好的添加噪音的方式。
240 00:20:46,730 --> 00:20:49,053 说话人 SPEAKER_00：这只是将这种推广到所有层。
241 00:20:50,173 --> 00:20:52,696 说话人 SPEAKER_00：在输入层，你可能不想遗漏一半的像素。
242 00:20:53,297 --> 00:20:54,858 说话人 SPEAKER_00：你希望留下 20%左右或类似的像素。
243 00:20:58,180 --> 00:21:03,826 说话人 SPEAKER_00：实际上有一个关于 dropout 的熟悉例子，或者说是对了解逻辑回归的人来说，一种特定的 dropout 例子。
244 00:21:04,507 --> 00:21:15,098 说话人 SPEAKER_00：如果你在做逻辑回归，并且没有足够的数据来很好地拟合你的模型，那么你很容易过拟合，你可以做的是只保留一个输入，其余的都丢弃。
245 00:21:16,279 --> 00:21:20,505 说话人 SPEAKER_00：所以现在你将对这个模型进行逻辑回归，并学习这个权重。
246 00:21:22,086 --> 00:21:24,068 说话人 SPEAKER_00：你将在大量数据上这样做。
247 00:21:24,469 --> 00:21:25,431 说话人 说话人_00: 然后你就退出了。
248 00:21:25,550 --> 00:21:27,613 说话人 说话人_00: 然后你专注于另一个，学习另一个权重。
249 00:21:28,201 --> 00:21:32,184 说话人 说话人_00: 如果你那样做逻辑回归，那叫朴素贝叶斯。
250 00:21:33,066 --> 00:21:35,528 说话人 说话人_00: 测试时，你将这些全部一起使用。
251 00:21:36,609 --> 00:21:40,153 说话人 SPEAKER_00：如果你想要一个分布，你真的应该取他们所有人预测的几何平均数。
252 00:21:40,535 --> 00:21:44,019 说话人 SPEAKER_00：但是如果你只对最可能的类别感兴趣，你不需要这样做。
253 00:21:45,019 --> 00:21:49,785 说话人 SPEAKER_00：因此，朴素贝叶斯实际上是一个使用 dropout 来避免过拟合的例子。
254 00:21:50,806 --> 00:21:54,089 说话人 SPEAKER_00：当然，这立刻就暗示了这一点。
255 00:21:54,694 --> 00:21:58,920 说话人 SPEAKER_00：为什么只考虑使用所有或只使用其中之一的可能性呢？
256 00:21:59,701 --> 00:22:09,334 说话人 SPEAKER_00：为什么不考虑为这些使用验证集来学习 dropout 率，从而进行子集选择，但又是概率性的子集选择呢？
257 00:22:09,795 --> 00:22:16,784 说话人 SPEAKER_00：你不会说，我要选择一个特定的特征子集，并试图找到最好的子集，这正是统计学家大部分时间在做的事情。
258 00:22:17,444 --> 00:22:23,992 说话人 SPEAKER_00：你说，我将通过给每个子集一个不再是 0.5 的概率来产生数以亿计的子集。
259 00:22:24,157 --> 00:22:27,163 说话人 SPEAKER_00：这种无用的功能被包含的概率很低。
260 00:22:27,644 --> 00:22:29,708 说话人 SPEAKER_00：有用的功能则概率要高得多。
261 00:22:30,549 --> 00:22:34,837 说话人 SPEAKER_00：现在你可以将这些模型平均一下，模型平均会带来很大的收益。
262 00:22:34,857 --> 00:22:38,704 说话人 SPEAKER_00：据我所知，这种想法几乎还没有被研究过。
263 00:22:42,833 --> 00:22:44,817 说话人 SPEAKER_00: 所以，在深度神经网络中。
264 00:22:45,099 --> 00:22:58,455 说话人 SPEAKER_00: 我们的经验是，如果你拿任何一个表现出过拟合的深度神经网络，例如在语音领域，任何一个因为不提前停止而会过拟合的神经网络，你可以通过使用 Dropout 来使其工作得更好。
265 00:22:58,997 --> 00:23:03,122 说话人 SPEAKER_00: 它比提前停止效果更好，并且可以大幅减少错误数量。
266 00:23:04,644 --> 00:23:07,527 说话人 SPEAKER_00: 谷歌的人说，是的，但是我没有过拟合，因为我有这么多数据。
267 00:23:07,928 --> 00:23:14,095 说话人 SPEAKER_00: 那个问题的答案是，嗯，你应该有过拟合，因为你可能有很多数据，但这仅仅意味着你应该使用一个更大的神经网络。
268 00:23:17,247 --> 00:23:20,912 说话人 SPEAKER_00: 我的说法是，你可以取无限数据的极限，但我可以取无限计算的极限。
269 00:23:24,958 --> 00:23:31,205 说话人 SPEAKER_00: 好吧，这里有一些关于一个无聊的老任务 MNIST 的初始实验，它是机器学习中的果蝇。
270 00:23:32,468 --> 00:23:35,172 说话人 SPEAKER_00: 人们说你不应该使用 MNIST，因为它很无聊。
271 00:23:35,592 --> 00:23:37,714 说话人 说话人_00: 就像告诉生物学家，你不应该使用果蝇。
272 00:23:37,795 --> 00:23:39,317 说话人 说话人_00: 有那么多生物学家都使用果蝇。
273 00:23:39,356 --> 00:23:40,900 说话人 说话人_00: 你为什么使用果蝇？
274 00:23:40,920 --> 00:23:43,462 说话人 说话人_00: 嗯，你使用果蝇是因为那么多其他生物学家都用果蝇。
275 00:23:44,003 --> 00:23:45,025 说话人 SPEAKER_00: 你可以比较事物。
276 00:23:45,786 --> 00:23:47,067 说话人 SPEAKER_00: 关于它，了解得很多。
277 00:23:48,279 --> 00:23:50,722 说话人 SPEAKER_00: 因此它正在识别手写数字。
278 00:23:50,742 --> 00:23:59,392 说话人 SPEAKER_00: 如果你查看 Yann LeCun 的网站，对于一种纯神经网络的最好报告结果是测试集上有 160 个错误。
279 00:24:00,333 --> 00:24:03,457 说话人 SPEAKER_00：你通过加入一些知识可以做得更好。
280 00:24:03,596 --> 00:24:13,528 说话人 SPEAKER_00：你可以通过使网络卷积或利用如果移动某物，它仍然属于同一类的事实来转换数据来加入知识。
281 00:24:13,980 --> 00:24:16,042 说话人 SPEAKER_00：所以你通过篡改数据来加入知识。
282 00:24:16,623 --> 00:24:19,806 说话人 SPEAKER_00：通过做这些技巧，你现在可以将错误率降低到大约 25 个左右。
283 00:24:20,247 --> 00:24:33,077 说话人 SPEAKER_00: 但如果你不这样做，如果你进行一种即使像素随机排列也能工作的学习形式，那么最佳报告的结果是 160 个错误。
284 00:24:36,060 --> 00:24:41,546 说话人 SPEAKER_00: 就在这里这一行。
285 00:24:41,605 --> 00:24:42,807 说话人 SPEAKER_00: 这是 160 个错误。
286 00:24:44,086 --> 00:24:52,621 说话人 SPEAKER_00: 如果你尝试各种不同的神经网络、深度网络，这里有一个网络，蓝色的，每个层有 800 个单元。
287 00:24:53,182 --> 00:24:56,106 说话人 SPEAKER_00：这比您通常用于 MNIST 的要多，并且有两个隐藏层。
288 00:24:57,450 --> 00:25:03,299 说话人 SPEAKER_00：您用大的学习率来训练它，但同时对权重进行约束，以防止其爆炸。
289 00:25:04,261 --> 00:25:06,786 说话人 SPEAKER_00：顺便说一句，这已经可以帮助您做得比 160 小时更好。
290 00:25:06,984 --> 00:25:09,308 说话人 SPEAKER_00：然后，这里有一大堆不同的网络。
291 00:25:09,670 --> 00:25:14,398 说话人 SPEAKER_00: 现在看看用这个大的学习率训练它们，然后衰减学习率会发生什么。
292 00:25:15,381 --> 00:25:16,723 说话人 SPEAKER_00: 所以你看，它们并没有过拟合。
293 00:25:17,726 --> 00:25:19,548 说话人 SPEAKER_00: 它们的表现都差不多。
294 00:25:20,611 --> 00:25:25,180 说话人 SPEAKER_00: 如果你让网络更大，给它们更多层，
295 00:25:26,307 --> 00:25:28,832 说话人 SPEAKER_00: 这个有每层 1,200 个单位，共三层。
296 00:25:29,211 --> 00:25:31,355 说话人 SPEAKER_00: 这对于正常的训练来说太大了。
297 00:25:31,656 --> 00:25:32,798 说话人 SPEAKER_00: 这样会过度拟合得非常严重。
298 00:25:34,342 --> 00:25:35,403 说话人 SPEAKER_00: 那个实际上做得很好。
299 00:25:35,503 --> 00:25:42,897 说话人 SPEAKER_00：这使错误数降至 100 多，对于没有额外信息的 MNIST 方法来说，这是一个非常好的性能。
300 00:25:43,839 --> 00:25:46,644 说话人 SPEAKER_00：所以支持向量机大约有 140 个错误。
301 00:25:46,924 --> 00:25:48,145 说话人 SPEAKER_00：它们又回到了 MNIST 这里。
302 00:25:48,586 --> 00:25:54,256 说话人 SPEAKER_00：而最初人们之所以转向支持向量机，正是因为这种从反向传播到支持向量机之间的差距。
303 00:25:55,116 --> 00:25:56,499 说话人 SPEAKER_00: 但现在我们把它反过来。
304 00:25:58,821 --> 00:25:59,123 说话人 SPEAKER_00: 好的。
305 00:25:59,143 --> 00:26:03,549 说话人 SPEAKER_00: 但 MNIST 是一个有点无聊的任务。
306 00:26:03,569 --> 00:26:05,553 说话人 SPEAKER_00: 为了使这个工作，我们确实使用了权重约束。
307 00:26:06,253 --> 00:26:10,539 说话人 SPEAKER_00: 如果您使用 L2 惩罚来保持权重小或 L1 惩罚来保持
308 00:26:11,093 --> 00:26:20,324 说话人 SPEAKER_00: 通常更好的做法是，对每个隐藏单元，使用验证集来确定输入权重向量的最大长度。
309 00:26:21,265 --> 00:26:26,412 说话人 SPEAKER_00: 然后在学习过程中，如果权重向量小于这个值，就让它保持不变。
310 00:26:26,892 --> 00:26:33,440 说话人 SPEAKER_00: 一旦达到这个水平，当它超过这个水平时，就通过除法缩小它，使其回到最大长度。
311 00:26:35,762 --> 00:26:38,605 说话人 SPEAKER_00：这比我们在尝试中使用 L2 衰减要好得多。
312 00:26:38,807 --> 00:26:39,867 说话人 SPEAKER_00：这就是我们在这里使用的。
313 00:26:48,134 --> 00:26:54,583 说话人 SPEAKER_00：这是一个硬约束，但我可以给你一种思考方式。
314 00:26:55,364 --> 00:27:06,240 说话人 SPEAKER_00：当你进行这个除法以将长度归一化时，如果你在优化时拥有恰到好处的拉格朗日乘数，你将保持这个长度。
315 00:27:06,921 --> 00:27:11,346 说话人 SPEAKER_00: 这些拉格朗日乘数就像权重成本，对权重的惩罚。
316 00:27:12,067 --> 00:27:13,509 说话人 SPEAKER_00: 但是这些惩罚是会变化的。
317 00:27:13,549 --> 00:27:16,233 说话人 SPEAKER_00: 所以当权重增长时，
318 00:27:16,567 --> 00:27:18,210 说话人 SPEAKER_00: 当它们很小时，没有惩罚。
一旦它们达到约束，如果某些权重想要增长很多，而其他权重的增长则不那么多，你必须输入大的拉格朗日乘数，这实际上会将想要增长的权重推回到零。
因此，每个连接上的 L2 惩罚会根据其他连接想要改变的程度而适应。
321 00:27:38,922 --> 00:27:43,428 说话者 说话者_00：是的。
322 00:27:43,448 --> 00:27:43,828 说话者 说话者_00：是的。
323 00:27:43,848 --> 00:27:43,909 说话人 SPEAKER_00: 好的。
324 00:27:45,913 --> 00:27:51,480 说话人 SPEAKER_00: 所以这里是对 Timmy 的实验，这更有趣。
325 00:27:51,520 --> 00:27:59,710 说话人 SPEAKER_00: 你在声学帧的窗口上训练神经网络。
326 00:28:00,471 --> 00:28:04,455 说话人 SPEAKER_00: 你试图预测中心帧对应哪些 HMM 的状态。
327 00:28:06,397 --> 00:28:07,720 说话者 SPEAKER_00：使用标准微调，
328 00：28：08,324 --> 00：28：15,671 演讲者 SPEAKER_00：在进行这种无监督的预训练后，你会得到 22.7%。
329 00:28:16,031 --> 00:28:17,313 说话者 说话者_00：这已经是一个非常不错的结果了。
330 00：28：18,894 --> 00：28：19,955 议长 SPEAKER_00：我们可以做得更好。
331 00:28:20,056 --> 00:28:24,339 说话人 SPEAKER_00: 我们采取了一种不同的数据预处理方式，因为我们已经过度拟合了很多。
332 00:28:25,060 --> 00:28:27,002 说话人 SPEAKER_00: 我们使用丹·波维的 Kaldi 系统来做这件事。
333 00:28:27,483 --> 00:28:28,984 说话人 SPEAKER_00: 这是我们第一次使用它。
334 00:28:29,605 --> 00:28:36,353 说话人 SPEAKER_00: 然后当你对这个网络应用 dropout 时，你可以降低到 19.7%，这是说话人无关方法的记录。
335 00:28:36,712 --> 00:28:37,773 说话人 SPEAKER_00: 这是一个很大的下降。
336 00:28:42,157 --> 00:28:45,703 说话人 SPEAKER_00: 所以这里，这是电话错误率。
337 00:28:46,224 --> 00:28:54,417 说话人 SPEAKER_00: 在训练过程中，你也可以看看你如何对单个帧进行分类，这被称为分类率。
338 00:28:55,259 --> 00:29:05,836 说话人 SPEAKER_00: 如果只是进行正常的微调，不使用 dropout，你会看到错误率下降，然后过拟合。
339 00:29:06,323 --> 00:29:11,150 说话人 SPEAKER_00: 因此，如果能在恰当的点停止，就能获得这些不同网络中的一个点。
340 00:29:12,211 --> 00:29:16,277 说话人 SPEAKER_00: 使用 dropout 后，它会下降并持续下降。
341 00:29:16,297 --> 00:29:19,301 说话人 SPEAKER_00: 如果你看这里的交叉熵误差，分类率不会上升。
342 00:29:19,362 --> 00:29:20,983 说话人 SPEAKER_00: 交叉熵误差略有上升。
343 00:29:21,965 --> 00:29:23,548 说话人 SPEAKER_00: 但基本上，几乎没有过拟合。
344 00:29:23,827 --> 00:29:28,976 说话人 SPEAKER_00: 所有这些不同的架构都非常相似，并且都比使用早期停止得到的结果要好得多。
345 00:29:30,196 --> 00:29:31,960 说话人 SPEAKER_00: 所以如果你在使用早期停止，就停止吧。
346 00:29:35,280 --> 00:29:38,929 说话人 SPEAKER_00: 为了展示其通用性，我们认为我们可以进行文档分类。
347 00:29:39,711 --> 00:29:41,454 说话人 SPEAKER_00：对于那个来说效果不太好，但是还是可以的。
348 00:29:42,076 --> 00:29:53,019 说话人 SPEAKER_00：所以我们选取了一个相当大的文档集，我们认为大约有一百万份文档，这些文档由 2,000 个最频繁出现的单词的计数来表示。
349 00:29:53,961 --> 00:29:56,445 说话人 SPEAKER_00：这里有一种小型的类别层次结构。
350 00:29:57,066 --> 00:30:00,751 说话人 SPEAKER_00：我们通过选取 50 个非重叠类别来简化了它。
351 00:30:01,092 --> 00:30:02,594 说话人 SPEAKER_00：这就是层次结构的第二层。
352 00:30:03,075 --> 00:30:08,923 说话人 SPEAKER_00：我们移除了一类占数据约三分之一的类别，还移除了一些几乎没有代表者的类别。
353 00:30:09,384 --> 00:30:14,109 说话人 SPEAKER_00：然后我们有了 50 个类别的数据，可以看到 dropout 如何帮助。
354 00:30:16,794 --> 00:30:20,880 说话人 SPEAKER_00：如果你用那个 dropout 进行训练，
355 00:30:21,265 --> 00:30:23,929 说话人 SPEAKER_00: 你会得到一种经典的早期停止偏好。
356 00:30:24,348 --> 00:30:27,133 说话人 SPEAKER_00: 而使用 Dropout，你可能会得到大约一两个百分点的提升。
357 00:30:27,773 --> 00:30:28,674 说话人 SPEAKER_00: 你不会过拟合。
358 00:30:31,317 --> 00:30:32,740 说话人 SPEAKER_00: 所以对于文档分类来说，它是有效的。
359 00:30:32,799 --> 00:30:34,261 说话人 SPEAKER_00：与其他事物相比，这不是一个很大的胜利。
360 00:30:34,903 --> 00:30:38,607 说话人 SPEAKER_00：但总的来说，我们的经验是随机选择一个任务。
361 00:30:38,627 --> 00:30:43,795 说话人 SPEAKER_00：这将通过使用稍微大一点的网络和使用 Dropout 来减少 5%到 10%的错误。
362 00:30:45,817 --> 00:30:46,878 说话人 SPEAKER_00：这是一个更有趣的任务。
363 00:30:46,939 --> 00:30:50,022 说话人 SPEAKER_00: 这是在对彩色图像进行分类。
364 00:30:50,289 --> 00:30:56,881 说话人 SPEAKER_00: 这些是小彩色图像，所以我设计了这个任务，使其尽可能类似于 MNIST，但仍然使用来自网络的真实图像。
365 00:30:57,563 --> 00:30:59,486 说话人 SPEAKER_00: 因此，它们是 32x32 的彩色图像。
366 00:31:01,930 --> 00:31:03,011 说话人 SPEAKER_00: 有 10 个不同的类别。
367 00:31:03,712 --> 00:31:05,916 说话人 SPEAKER_00: 这些是测试集中鸟类的示例。
368 00:31:07,038 --> 00:31:10,644 说话人 SPEAKER_00: 您可以看到，图像已经找到，图像是这样产生的。
369 00:31:10,979 --> 00:31:19,448 说话人 SPEAKER_00: 首先在网络上搜索鸟类及其相关类别。
370 00:31:20,808 --> 00:31:28,857 说话人 SPEAKER_00: 然后将所有这些内容交给多伦多的大学生，并说，指示大致如下。
图像中是否有一个占主导地位的事物，并且它可能被合理地标记为“鸟”？
您发现过了一段时间后，您在支付这些本科生，他们实际上在笔记本电脑上互相玩游戏，而不是标注图像，因为标注图像非常无聊。
只有我发现喜欢给图片打标签的人是 Radford Neal 的女儿，她会看一百张图片，然后说，猫，猫，猫。
374 00：32：02,511 --> 00：32：04,134 议长 SPEAKER_00：我想她当时大约两岁。
375 00:32:05,055 --> 00:32:06,855 说话人 SPEAKER_00: 因此图像标签员应该有两岁。
376 00:32:06,875 --> 00:32:07,757 说话人 SPEAKER_00: 他们真的很投入。
377 00:32:07,777 --> 00:32:09,218 说话人 SPEAKER_00: 他们可能愿意付钱让你做这件事。
378 00:32:10,987 --> 00:32:15,673 说话人 SPEAKER_00: 所以亚历克斯·克雷泽夫斯基训练了一个非常大的卷积神经网络。
379 00:32:16,034 --> 00:32:16,835 说话人 SPEAKER_00: 他尝试了很多事情。
380 00:32:17,296 --> 00:32:21,362 说话人 SPEAKER_00: 他最好的网络误差率大约是 18%，这是在这个领域的记录。
381 00:32:22,103 --> 00:32:26,067 说话人 SPEAKER_00: 然后他把误差率降低到了 16%。
382 00:32:26,548 --> 00:32:31,256 说话人 SPEAKER_00: 通过使用图像的翻译等手段，他现在降低了这么多。
383 00:32:32,518 --> 00:32:39,086 说话人 SPEAKER_00: 但如果你不作弊，不使用添加额外知识的转换数据，那将是一个很大的胜利。
384 00:32:41,007 --> 00:32:43,028 说话人 SPEAKER_00: 这些数字总是在变化，因为它一直在改进事物。
385 00:32:45,931 --> 00:32:50,894 说话人 SPEAKER_00: 这是一个更严肃的对象识别测试。
386 00:32:51,655 --> 00:32:58,362 说话人 SPEAKER_00: 所以我厌倦了计算机视觉的人说，你们神经网络的人只是在尝试像 Caltech 101 这样的简单集合。
387 00:32:59,162 --> 00:33:09,471 说话人 SPEAKER_00：所以我联系了 Malik，他一直在说深度神经网络在物体识别方面并没有真正证明自己，因为它们总是使用过于简单的东西。
388 00:33:09,490 --> 00:33:10,592 说话人 SPEAKER_00：所以我跟 Jitendra 谈了。
389 00:33:11,045 --> 00:33:16,737 说话人 SPEAKER_00：Jitendra 同意，如果我们能在 ImageNet 上工作，那将是一个真正的成果。
390 00:33:16,757 --> 00:33:19,221 说话人 SPEAKER_00：如果我们能在 ImageNet 上实现它，那么 ImageNet 将会非常令人印象深刻。
391 00:33:21,185 --> 00:33:28,902 说话人 SPEAKER_00: 在 2010 年有一场竞赛，他们公布了测试集，所以你可以自己尝试。
392 00:33:29,336 --> 00:33:33,584 说话人 SPEAKER_00: 因此有 130 万张训练图像，1000 个不同的类别。
393 00:33:34,444 --> 00:33:36,048 说话人 SPEAKER_00: 你需要进行 1000 种分类。
394 00:33:36,628 --> 00:33:38,372 说话人 SPEAKER_00: 每个类别大约有 1000 个示例。
395 00:33:38,652 --> 00:33:40,955 说话人 SPEAKER_00：所以分享特征将非常重要。
396 00:33:42,939 --> 00:33:48,929 说话人 SPEAKER_00：2010 年比赛的冠军在他们的第一个选择上获得了 47%的错误率。
397 00:33:49,931 --> 00:33:56,121 说话人 SPEAKER_00：如果你认为可以进入你的前五个选择，那么他们的错误率是 25%。
398 00:33:57,079 --> 00:33:58,701 说话人 SPEAKER_00：这就是 2010 年的技术水平。
399 00:33:58,741 --> 00:34:01,224 说话人 SPEAKER_00：现在的最先进技术稍微好一点了。
400 00:34:01,265 --> 00:34:01,845 说话人 SPEAKER_00：他们下降到了 45%。
401 00:34:02,967 --> 00:34:03,867 说话人 SPEAKER_00：所以又有一场竞赛。
402 00:34:03,887 --> 00:34:07,632 说话人 SPEAKER_00：那场竞赛的获胜者回到这场竞赛中，并展示了他们在这次竞赛中获得了 45%。
403 00:34:08,132 --> 00:34:11,715 说话人 说话人_00：据我们所知，目前最先进的技术达到了45%。
404 00:34:11,735 --> 00:34:14,659 说话人 说话人_00：Alex 尝试了大量的神经网络。
405 00:34:18,583 --> 00:34:22,327 说话人 说话人_00：他最终得到了一个包含七个隐藏层（不包括最大池化层）的神经网络。
406 00:34:22,780 --> 00:34:25,862 说话人 说话人_00：早期层是卷积层，所以参数并不多。
407 00:34:26,583 --> 00:34:29,266 说话人 SPEAKER_00：后面的层是完全连接的，所以它们有很多参数。
408 00:34:29,867 --> 00:34:33,831 说话人 SPEAKER_00：它们有大约 4,000 个完全连接的单元，所以这里有 1600 万个参数。
409 00:34:34,713 --> 00:34:35,994 说话人 SPEAKER_00：而且有两个这样的层。
410 00:34:38,097 --> 00:34:42,420 说话人 SPEAKER_00：他使用了这种转换训练数据的技巧。
411 00:34:42,460 --> 00:34:47,606 说话人 SPEAKER_00: 因此他将图像缩小到 256x256。
412 00:34:48,092 --> 00:34:49,635 说话人 SPEAKER_00: 但后来并没有使用所有这些图像。
413 00:34:49,715 --> 00:34:51,317 说话人 SPEAKER_00: 他使用了稍微小一点的块。
414 00:34:51,336 --> 00:34:52,677 说话人 SPEAKER_00: 因此他得到了很多不同的块。
415 00:34:53,219 --> 00:34:54,800 说话人 SPEAKER_00：所以他正在多个不同阶段处理事情。
416 00:34:55,181 --> 00:34:56,902 说话人 SPEAKER_00：他从未两次使用相同的训练示例。
417 00:34:58,465 --> 00:35:01,827 说话人 SPEAKER_00：他还使用了其他一些我没有时间详细说明的技巧。
418 00:35:02,809 --> 00:35:07,855 说话人 SPEAKER_00：这些并不与 dropout 的效果相关。
419 00:35:11,219 --> 00:35:16,664 说话人 SPEAKER_00: 他的错误率下降到了大约 48%。
420 00:35:17,117 --> 00:35:19,740 说话人 SPEAKER_00: 关于使用所有这些方法的最新技术状态。
421 00:35:22,043 --> 00:35:36,001 说话人 SPEAKER_00: 然后，他在全局连接层应用了 dropout，现在最好的一个错误率下降到了 39%，前五名的错误率下降到了 19%。
422 00:35:36,961 --> 00:35:38,202 说话人 SPEAKER_00: 这是一项巨大的改进。
423 00:35:38,403 --> 00:35:42,027 说话人 SPEAKER_00: 我的意思是，从事计算机视觉的人会告诉你，这是一个很大的进步。
424 00:35:46,193 --> 00:35:48,016 说话人 SPEAKER_00: 这里有一些来自 Alex 网络的样本。
425 00:35:49,960 --> 00:35:53,106 说话人 SPEAKER_00: 这是在他获得最佳结果之前。
426 00:35:53,146 --> 00:35:54,869 说话人 SPEAKER_00: 所以这是他网络的一个略差版本。
427 00:35:55,110 --> 00:35:56,652 说话人 SPEAKER_00: 只是为了向您展示这项任务是什么样的。
428 00:35:57,293 --> 00:35:59,978 说话人 SPEAKER_00: 你会得到这样一张图片，你必须说出它是千种事物中的哪一种。
429 00:36:01,641 --> 00:36:04,668 说话人 SPEAKER_00: 而 Alex 的神经网络说它是一只海獭。
430 00:36:05,550 --> 00:36:08,235 说话人 SPEAKER_00: 现在，我实际上是一位深度神经网络的辩护者。
431 00:36:08,958 --> 00:36:16,369 说话人 SPEAKER_00: 我认为说海獭是个很好的东西，因为注意，你没有鸟嘴尖，这里有你海獭的湿毛。
432 00:36:17,572 --> 00:36:20,096 说话人 SPEAKER_00: 我认为识别海獭的湿毛非常聪明。
433 00:36:20,737 --> 00:36:27,186 说话人 SPEAKER_00: 它确实能分辨出鹌鹑，而且它还能分辨出我无法区分鹌鹑的其他东西，比如之后的松鸡和鹌鹑。
434 00:36:29,190 --> 00:36:32,215 说话人 SPEAKER_00: 这个很明显是一辆铲雪车。
435 00:36:33,764 --> 00:36:35,045 说话人 说话人_00: 然后你看它的错误。
436 00:36:35,065 --> 00:36:37,387 说话人 说话人_00: 错误总是更有信息量。
437 00:36:37,867 --> 00:36:40,050 说话人 说话人_00: 好像钻井平台和垃圾车看起来都还好。
438 00:36:40,530 --> 00:36:41,771 说话人 说话人_00: 救生艇，你认为它为什么是救生艇？
439 00:36:41,791 --> 00:36:43,652 说话人 SPEAKER_00: 嗯，你知道，这可能不是雪。
440 00:36:43,672 --> 00:36:45,875 说话人 SPEAKER_00: 可能是有点泡沫的水。
441 00:36:45,916 --> 00:36:50,300 说话人 SPEAKER_00: 如果你再看一遍，这里就是救生艇前面的旗帜。
442 00:36:50,739 --> 00:36:52,221 说话人 SPEAKER_00: 这里是救生艇后面的旗帜。
443 00:36:52,242 --> 00:36:53,583 说话人 SPEAKER_00: 这是救生艇的船桥。
444 00:36:53,963 --> 00:36:55,704 说话人 SPEAKER_00: 它确实非常像一艘救生艇。
445 00:36:57,907 --> 00:36:59,588 说话人 SPEAKER_00: 就像我说的，我是神经网络的支持者。
446 00:37:00,489 --> 00:37:01,971 说话人 SPEAKER_00: 这个，它完全错了，好吧？
447 00:37:02,150 --> 00:37:03,331 说话人 SPEAKER_00: 它没有进入前五名。
448 00:37:03,751 --> 00:37:06,394 说话人 SPEAKER_00: 正确答案是剑鞘。
449 00:37:09,880 --> 00:37:11,382 说话人 SPEAKER_00: 这很奇怪，它说蚯蚓。
450 00:37:11,561 --> 00:37:15,288 说话人 SPEAKER_00: 斧头，你可以看到为什么说斧头，两个大的垂直部分。
451 00:37:15,588 --> 00:37:17,952 说话人 SPEAKER_00: 这可能和颜色和森林有关。
452 00:37:19,293 --> 00:37:20,735 说话人 SPEAKER_00: 扫帚，你能看到为什么说扫帚。
453 00:37:21,056 --> 00:37:22,739 说话人 SPEAKER_00: 但是蚯蚓，为什么说蚯蚓？
454 00:37:22,759 --> 00:37:25,523 说话人 SPEAKER_00: 但是如果你仔细看，你能看到这里有几条蚯蚓。
455 00:37:26,092 --> 00:37:41,918 说话人 SPEAKER_00：现在你可能认为这只是幻想，但 Alex 能做的是，他可以从类别标签反向传播，告诉我哪些像素对你在该类别标签上的信心影响最大。
456 00:37:41,938 --> 00:37:44,422 说话人 SPEAKER_00：所以你可以看到标签对像素的敏感性。
457 00:37:44,976 --> 00:37:49,621 说话人 SPEAKER_00：他还没有用这个来做，但有一个例子，其中的对象是螨虫，在图像中非常小。
458 00:37:49,661 --> 00:37:52,224 说话人 SPEAKER_00：这是一片叶子，里面有个螨虫，在图像的一侧。
459 00:37:52,784 --> 00:37:56,989 说话人 SPEAKER_00：如果你从那里反向传播，导致它说“螨虫”的是螨虫的像素。
460 00:37:58,590 --> 00:38:00,532 说话人 SPEAKER_00：这并不是完全的幻想。
461 00:38:03,375 --> 00:38:04,976 说话人 SPEAKER_00：我喜欢展示这些，让我们再看一些。
462 00:38:06,838 --> 00:38:11,702 说话人 SPEAKER_00：ImageNet 有各种各样的物品类别，你可以看到为什么它们允许你进入前五名。
463 00:38:11,742 --> 00:38:13,143 说话人 SPEAKER_00: 那么正确的答案是什么？
464 00:38:13,681 --> 00:38:18,407 说话人 SPEAKER_00: 我可能会说是微波炉，如果不是微波炉的话，那可能是洗碗机，如果那是指洗碗机的话。
465 00:38:20,771 --> 00:38:21,751 说话人 SPEAKER_00: 就是这样说的。
466 00:38:22,914 --> 00:38:24,376 说话人 SPEAKER_00: 正确答案是电炉。
467 00:38:24,456 --> 00:38:26,199 说话人 SPEAKER_00: 我甚至不知道电炉在哪里。
468 00:38:26,239 --> 00:38:27,340 说话人 SPEAKER_00: 我想它可能就在这里。
469 00:38:27,360 --> 00:38:30,204 说话人 SPEAKER_00: 或者它可能就是，你知道的，那里肯定有一个。
470 00:38:30,224 --> 00:38:32,527 说话人 SPEAKER_00: 因为有洗碗机和微波炉的地方，肯定有电炉。
471 00:38:33,309 --> 00:38:34,250 说话人 SPEAKER_00: 这里有一个洗手盆。
472 00:38:34,710 --> 00:38:36,853 说话人 SPEAKER_00: 所以那里的正确答案非常可疑。
473 00:38:37,114 --> 00:38:38,456 说话人 SPEAKER_00: 实际上它给出了更好的答案。
474 00:38:40,057 --> 00:38:42,641 说话人 SPEAKER_00: 顺便说一下，如果测试集
475 00:38:43,027 --> 00:38:45,311 说话人 SPEAKER_00: 有一些答案并不是最好的答案。
476 00:38:45,793 --> 00:38:48,617 说话人 SPEAKER_00: 如果你全部答对了，那就是零错误。
477 00:38:48,918 --> 00:38:51,844 说话人 SPEAKER_00: 但如果你在测试集中的答案更好，那就是负错误。
478 00:38:51,864 --> 00:38:54,210 说话人 SPEAKER_00: 因此，我的目标是让这个系统达到负错误率。
479 00:38:56,434 --> 00:38:59,159 说话人 SPEAKER_00: 它可以识别像自动检票机这样的分布式设备。
480 00:38:59,621 --> 00:39:04,090 说话人 SPEAKER_00: 它有来自目录中的物品，比如防弹衣。
481 00:39:05,452 --> 00:39:06,534 说话人 SPEAKER_00: 这是我的最爱错误。
482 00:39:07,434 --> 00:39:13,541 说话人 SPEAKER_00: 你向它展示一些 iPhone 耳机，它会说那是螺旋夹，口红，或者螺丝。
483 00:39:14,161 --> 00:39:15,523 说话人 说话人_00: 然后它是一只蚂蚁。
484 00:39:16,945 --> 00:39:19,027 说话人 说话人_00: 它究竟为什么会认为它们是蚂蚁呢？
485 00:39:19,487 --> 00:39:22,811 说话人 说话人_00: 但是如果你看它，这就是你不想看到的蚂蚁的样子。
486 00:39:23,211 --> 00:39:25,974 说话人 说话人_00: 它是一只巨大的蚂蚁，它正要咬你。
487 00:39:26,014 --> 00:39:26,956 说话者 SPEAKER_00: 它正在俯视你。
488 00:39:27,036 --> 00:39:27,797 说话者 SPEAKER_00: 这是触角的部位。
489 00:39:28,998 --> 00:39:30,820 说话者 SPEAKER_00: 这是蚜虫眼中的蚂蚁。
490 00:39:31,929 --> 00:39:36,795 说话者 SPEAKER_00: 现在，我们无法证明这就是它说“但是”的原因，但除此之外，它为什么说“但是”呢？
491 00:39:41,201 --> 00:39:42,822 说话人 SPEAKER_00：还有另一种思考 dropout 的方法。
492 00:39:43,123 --> 00:39:45,025 说话人 SPEAKER_00：所以，我已经向你展示了 dropout 是如何工作的。
493 00:39:45,045 --> 00:39:54,697 说话人 SPEAKER_00：另一种思考方式，类似于进化理论家的观点，就是不要从模型平均的角度去想，而是从隐藏单元需要做什么去想。
494 00:39:55,318 --> 00:39:58,782 说话人 SPEAKER_00：所以，网络中的一个隐藏单元希望做一些有用的事情。
495 00:39:59,268 --> 00:40:06,114 说话者 SPEAKER_00：如果他确切地知道他将与谁合作，那么他可以调整自己的行为以适应他们可能的行为。
496 00:40:07,094 --> 00:40:08,735 说话者 SPEAKER_00：然后你会得到这些复杂的协同适应。
497 00:40:09,277 --> 00:40:14,001 说话者 SPEAKER_00：但如果你不知道还有谁会参加，如果你试图调整以适应他们，那你就没戏了。
498 00:40:14,300 --> 00:40:17,344 说话者 SPEAKER_00：你最好做一些对个人有用的东西。
499 00:40:17,364 --> 00:40:19,585 说话人 SPEAKER_00：将隐藏单元变成坚韧的个体主义者。
500 00:40:20,226 --> 00:40:25,771 说话人 SPEAKER_00：除了这一点，他们是坚韧的个体主义者，想要做些有用的事情，考虑到其他人通常在做什么。
501 00:40:26,110 --> 00:40:28,092 说话人 SPEAKER_00：所以，考虑到所有这些组合性的许多
502 00:40:28,630 --> 00:40:32,474 说话人 SPEAKER_00：其他人你可能需要与之合作的集合，做一些通常有帮助的事情。
503 00:40:32,755 --> 00:40:37,639 说话者 SPEAKER_00: 这会使你做的与他们的不同，但不是依赖于具体是哪些。
504 00:40:38,961 --> 00:40:44,527 说话者 SPEAKER_00: 这使你更能抵御他们变化，也能抵御环境变化。
505 00:40:44,849 --> 00:40:48,853 说话者 SPEAKER_00: 能够抵御环境变化正是防止过拟合的关键。
506 00:40:49,092 --> 00:40:53,077 说话者 SPEAKER_00: 在机器学习中，当从训练集切换到测试集时，环境发生变化。
507 00:40:53,097 --> 00:40:55,179 说话人 SPEAKER_00: 你真正需要稳健应对的是这些变化。
508 00:40:56,902 --> 00:40:58,384 说话人 SPEAKER_00: 这就是一个例子。
509 00:40:59,579 --> 00:41:00,760 说话人 SPEAKER_00: 这里有两个训练案例。
510 00:41:02,442 --> 00:41:04,585 说话人 SPEAKER_00: 这些是输入，这些是期望的输出。
511 00:41:05,485 --> 00:41:07,347 说话人 SPEAKER_00: 这里有一组表现完美的权重。
512 00:41:09,610 --> 00:41:11,632 说话人 SPEAKER_00: 你看 5 加 11 等于 6。
513 00:41:11,853 --> 00:41:13,215 说话人 SPEAKER_00: 如果你把这些加进去，你得到 4。
514 00:41:16,278 --> 00:41:18,001 说话人 SPEAKER_00: 但是 5 和 11 是共适应的。
515 00:41:19,001 --> 00:41:23,206 说话人 SPEAKER_00：这个减 5 似乎走错了方向。
516 00:41:23,811 --> 00:41:27,036 说话人 SPEAKER_00：并且它依赖于这个加 11 的存在，否则它说的东西真的很愚蠢。
517 00:41:27,335 --> 00:41:31,882 说话人 SPEAKER_00：如果你使用 dropout，而这个加 11 有时不存在，你会发现这个减 5 是个糟糕的主意。
518 00:41:32,282 --> 00:41:34,005 说话人 SPEAKER_00：如果你使用 dropout，你会得到更接近这样的权重。
519 00:41:35,806 --> 00:41:40,373 说话人 SPEAKER_00：实际上，你将得到两倍的权重，但当你将它们放入平均网络中时，你将它们减半，然后你会得到这样的权重。
520 00:41:41,134 --> 00:41:49,784 说话人 SPEAKER_00：然后这些权重将起作用。
521 00:41:51,030 --> 00:41:53,715 说话人 SPEAKER_00：在机器学习中，有几种正则化的方法。
522 00:41:53,815 --> 00:41:57,221 说话人 SPEAKER_00：一种是将模型平均化，另一种是添加噪声。
523 00:41:57,380 --> 00:42:04,192 说话人 SPEAKER_00：人们已经证明，在线性系统的权重中添加高斯噪声与 L2 惩罚是等价的。
524 00:42:05,233 --> 00:42:11,503 说话人 SPEAKER_00：实际上，向输入添加高斯噪声与 L2 惩罚是等价的，抱歉。
525 00:42:11,764 --> 00:42:15,789 说话人 SPEAKER_00：是的，向输入添加高斯噪声，就像有 L2 惩罚一样。
526 00:42:16,090 --> 00:42:18,474 说话人 SPEAKER_00：随着网络变得更加复杂，这会变得稍微复杂一些。
527 00:42:18,914 --> 00:42:22,760 说话人 SPEAKER_00: 人们通常认为添加噪声与进行模型平均化是不同的。
528 00:42:22,780 --> 00:42:29,068 说话人 SPEAKER_00: Dropout 展示的是一个既可以看作是添加噪声也可以看作是模型平均化的案例。
529 00:42:29,309 --> 00:42:30,811 说话人 SPEAKER_00: 这两种方法实际上并没有真正的区别。
530 00:42:35,378 --> 00:42:41,827 说话人 SPEAKER_00: 我想再谈谈使用 Dropout 的一个优势，这可能与进化生物学有关，我想。
531 00：42：42,195 --> 00：42：47,101 演讲者 SPEAKER_00：当你训练一个带有 dropout 的网络时，神经元对他们的同事变化非常强大。
他们不知道会是谁在那里，所以他们最好做一些能够应对这些变化的事情。
533 00：42：53,210 --> 00：42：56,353 演讲者 SPEAKER_00：这意味着这些网络在遗传算法方面应该非常好。
534 00：42：57,896 --> 00：43：01,920 演讲者 SPEAKER_00：你应该能够拿两个不同的网，拿一半。
535 00:43:02,001 --> 00:43:03,643 说话人 SPEAKER_00：假设你只有一个隐藏层。
536 00:43:03,663 --> 00:43:06,827 说话人 SPEAKER_00：从一个网络中取一半的隐藏单元，从另一个网络中也取一半的隐藏单元。
537 00:43:07,728 --> 00:43:10,532 说话人 SPEAKER_00：这些隐藏单元已经对同事的变化具有鲁棒性。
538 00:43:10,916 --> 00:43:19,005 说话人 SPEAKER_00：所以当你从每个网络中各取一半时，你得到的东西还不能正常工作，但它已经相当好了。
539 00:43:19,385 --> 00:43:20,585 说话人 SPEAKER_00：我知道 Skiva 已经测试过了。
540 00:43:20,626 --> 00:43:21,327 说话人 SPEAKER_00：它工作得相当不错。
541 00:43:21,646 --> 00:43:25,911 说话人 SPEAKER_00：然后进行一点训练，它的工作效果几乎和父母一样好。
542 00:43:25,931 --> 00:43:38,443 说话人 SPEAKER_00：所以现在你可以做的是，你可以拿一个大集群，或者大农场，你可以进行大量的计算，几乎不需要通信。
543 00:43:39,047 --> 00:43:44,574 说话人 SPEAKER_00: 所以你在少量本地核心上运行了很多网络。
544 00:43:45,534 --> 00:43:49,278 说话人 SPEAKER_00: 有时候，这些网络中的一个会为伴侣做广告。
545 00:43:51,061 --> 00:43:53,563 说话人 SPEAKER_00: 她考虑了可能性，选择了其中一个。
546 00:43:54,543 --> 00:43:59,630 说话人 SPEAKER_00: 然后你从母亲和父亲那里各得到一半的隐藏单元。
547 00:44:01,271 --> 00:44:03,012 说话人 SPEAKER_00: 母网络随后被暂停。
548 00:44:03,574 --> 00:44:06,556 说话人 SPEAKER_00: 妈妈们都知道这件事。
549 00:44:06,925 --> 00:44:09,949 说话人 SPEAKER_00: 那个核心用于运行子网络。
550 00:44:09,969 --> 00:44:11,452 说话人 SPEAKER_00: 子网络随后进行更多训练。
551 00:44:12,432 --> 00:44:16,079 说话人 SPEAKER_00: 然后你对谁最终会做得更好做出预测，是母亲还是这个孩子。
552 00:44:16,139 --> 00:44:18,762 说话人 SPEAKER_00: 这个孩子可能还没有母亲做得好，因为它还没有完成训练。
553 00:44:19,284 --> 00:44:20,324 说话人 SPEAKER_00: 但你必须做出一些预测。
554 00:44:20,364 --> 00:44:22,347 说话人 SPEAKER_00: 这个孩子有可能比母亲做得更好吗？
555 00:44:22,809 --> 00:44:23,650 说话人 SPEAKER_00: 如果是这样，你就杀掉母亲。
556 00:44:24,190 --> 00:44:25,373 说话人 SPEAKER_00: 如果不是，你就杀掉孩子。
557 00:44:25,876 --> 00:44:28,800 说话人 SPEAKER_00: 你可能会问父亲怎么办。
558 00:44:30,021 --> 00:44:33,766 说话人 SPEAKER_00: 在这个算法中，快速改变性别非常重要。
559 00:44:34,407 --> 00:44:37,313 说话人 SPEAKER_00：所以一个网络，有时候是父亲，有时候是母亲。
560 00:44:37,873 --> 00:44:39,255 说话人 SPEAKER_00：如果你是父亲，你永远不会被杀。
561 00:44:41,057 --> 00:44:44,282 说话人 SPEAKER_00：所以要摆脱他们，你必须进行性别转换。
562 00:44:45,023 --> 00:44:45,184 说话人 SPEAKER_00：好的。
563 00:44:49,070 --> 00:44:54,277 说话人 SPEAKER_02: 嗯。
564 00:44:54,297 --> 00:44:54,396 说话人 SPEAKER_00: 嗯。
565 00:44:54,416 --> 00:44:55,579 说话人 SPEAKER_00: 当然。
566 00:44:56,251 --> 00:44:59,775 说话人 SPEAKER_00: 只是一种标准的 dropout，我得到了。
567 00:44:59,875 --> 00:45:00,715 说话人 SPEAKER_00: 这样做不太合适。
568 00:45:02,418 --> 00:45:07,583 说话人 SPEAKER_00: 很可能在生物学中，有两个是因为组织群交太复杂了。
569 00:45:10,367 --> 00:45:11,969 说话人 SPEAKER_00: 但我不知道。
570 00:45:12,489 --> 00:45:13,391 说话人 SPEAKER_00: 我们还没有做过实验。
571 00:45:13,411 --> 00:45:20,699 说话人 SPEAKER_00：我们甚至还没有，我们只进行了一个非常初步的实验，以表明如果你做两件这样的事情，孩子会很快学会做得好。
572 00:45:21,561 --> 00:45:23,422 说话人 SPEAKER_00：所以从 dropout 中获得的鲁棒性非常有帮助。
573 00:45:23,992 --> 00:45:32,003 说话人 SPEAKER_00：至于遗传算法，你可以使用你的一百万个空闲核心，并且可以充分利用它们，而不需要任何通信。
574 00:45:33,365 --> 00:45:33,626 说话人 SPEAKER_00：好的。
575 00:45:34,407 --> 00:45:36,471 说话人 SPEAKER_00：现在来说点不太一样的事情。
576 00:45:39,514 --> 00:45:48,668 说话人 SPEAKER_00：如果你思考 Dropout 所做的工作，它是在取一个神经元，在我的网络中，这个神经元是逻辑神经元，所以它计算一个概率 P。
577 00:45:49,576 --> 00:45:52,659 说话人 SPEAKER_00：然后它从逻辑神经元给出一个数字，P。
578 00:45:53,059 --> 00:45:55,563 说话人 SPEAKER_00：这就是逻辑神经元的输出，我们称之为神经活动。
579 00:45:56,523 --> 00:46:03,030 说话者 SPEAKER_00: 然后以 0.5 的概率将这个数字，这个真实值，发送到上一层。
580 00:46:03,650 --> 00:46:06,134 说话者 SPEAKER_00: 这就是 dropout 所做的事情。
581 00:46:06,153 --> 00:46:16,005 说话人 说话人_00：嗯，这正好与发送一个概率为 P 的 0.5 的期望值相同。好吗？
582 00:46:17,047 --> 00:46:18,289 说话人 说话人_00：方差略有不同。
583 00:46:19,130 --> 00:46:20,170 说话人 说话人_00：那么为什么不试试看呢？
584 00:46:22,634 --> 00:46:25,416 说话人 说话人_00：所以考虑一个神经网络。
585 00:46:25,956 --> 00:46:29,121 讲者 SPEAKER_00：在我们预训练这些神经网络时，我们无论如何都使用随机二进制神经元。
586 00:46:30,001 --> 00:46:39,751 讲者 SPEAKER_00：那么，为什么不用随机选择一个比特位来代替发送这些实数值进行反向传播呢？
587 00:46:39,771 --> 00:46:44,777 讲者 SPEAKER_00：所以你计算 P，然后以概率 P 发送一个 1。
588 00:46:48,233 --> 00:46:55,942 讲者 SPEAKER_00：这有点奇怪，因为我们一直认为反向传播的突破在于意识到需要发送那些实数。
589 00:46:57,023 --> 00:46:58,985 说话人 SPEAKER_00：实际上，如果你这么做，效果真的很好。
590 00:47:01,528 --> 00:47:06,916 说话人 SPEAKER_00：神经网络显然学习速度较慢，你需要将其做得更大才能学习到相同数量的东西。
591 00:47:07,235 --> 00:47:08,577 说话人 SPEAKER_00：但它泛化能力更好。
592 00:47:09,338 --> 00:47:17,007 说话人 SPEAKER_00：并且从我所做的初步实验来看，你在性能上获得的提升与使用 dropout 获得的提升相似。
593 00:47:17,478 --> 00:47:28,913 说话人 SPEAKER_00: 这句话的意思是，如果你有一个很大的深度神经网络，并且你依赖正则化它，那么它很大，对吧？
594 00:47:28,932 --> 00:47:30,235 说话人 SPEAKER_00: 它没有足够的训练数据。
595 00:47:31,237 --> 00:47:34,860 说话人 SPEAKER_00: 你希望使用像 dropout 或噪声这样的方法来正则化它。
596 00:47:35,922 --> 00:47:42,692 说话人 SPEAKER_00: 实际上，神经元发送一个比特比发送一个模拟值要好。
597 00:47:45,541 --> 00:47:56,956 讲者 SPEAKER_00：在 dropout 中，如果你的方差，如果你发送一半，如果你以一半的概率发送 p，你的平均值是半 p。因此，你的值将偏离那个值四分之一 p。
598 00:47:57,016 --> 00:47:59,541 讲者 SPEAKER_00：所以当你，无论什么。
599 00:48:00,601 --> 00:48:01,623 讲者 SPEAKER_00：这就是你得到的方差。
600 00:48:01,724 --> 00:48:06,871 讲者 SPEAKER_00：有一个随机位，但你发送的是 0.5，这是方差。
601 00:48:07,391 --> 00:48:10,054 说话人 SPEAKER_00: 你可以看到，当 p 等于一半时，它们是相同的。
602 00:48:10,114 --> 00:48:11,396 说话人 SPEAKER_00: 这很好，因为它们必须是这样。
603 00:48:11,831 --> 00:48:16,356 说话人 SPEAKER_00: 当 P 很大时，这个家伙的方差比那个家伙低。
604 00:48:18,878 --> 00:48:22,501 说话人 SPEAKER_00: 但当 P 很小时，这个家伙的方差更大。
605 00:48:23,541 --> 00:48:25,724 说话人 SPEAKER_00：当 P 值较小时，这个值与 P 的平方成正比。
606 00:48:25,764 --> 00:48:32,971 说话人 SPEAKER_00：这个值与 P 成正比。因此，当 P 值较小时，使用 SCASI 位，你得到的方差甚至比使用 dropout 还要大。
607 00:48:33,951 --> 00:48:36,514 说话人 SPEAKER_00：这就是当 P 值较小时的高斯极限。
608 00:48:36,534 --> 00:48:37,393 说话人 SPEAKER_00：你忽略这个项。
609 00:48:38,275 --> 00:48:40,336 说话人 SPEAKER_00: 方差现在与 P 成正比。
610 00:48:40,942 --> 00:48:43,766 说话人 SPEAKER_00: 大概就是这样，神经元的样子。
611 00:48:43,806 --> 00:48:50,695 说话人 SPEAKER_00: 许多神经科学家会告诉你，神经元的作用是计算泊松率，然后根据泊松分布发出尖峰。
612 00:48:52,416 --> 00:48:56,623 说话人 SPEAKER_00: 你必须考虑到它不能立即发出另一个尖峰，这使事情变得更加复杂。
613 00:48:56,943 --> 00:49:00,306 说话人 SPEAKER_00: 忽略这一点，泊松模型是神经元的一个相当好的模型。
614 00:49:00,887 --> 00:49:04,552 说话人 SPEAKER_00: 这始终是个谜，他们为什么不使用 spikes 的时间？
615 00:49:04,802 --> 00:49:06,666 说话人 SPEAKER_00: 他们为什么要做所有这些随机性？
616 00:49:07,047 --> 00:49:08,731 说话人 SPEAKER_00: 答案是因为他们想这么做。
617 00:49:08,751 --> 00:49:11,759 说话者 SPEAKER_00：如果你想让模型拟合数据，这是正确的事情。
618 00:49:12,519 --> 00:49:14,284 说话者 SPEAKER_00：如果你发送了实数，你会做得更差。
619 00:49:17,431 --> 00:49:19,817 说话者 SPEAKER_00：所以这是一段有趣的历史，我之前已经暗示过了。
620 00:49:20,878 --> 00:49:21,681 说话者 SPEAKER_00：当我们
621 00:49:22,166 --> 00:49:27,235 说话人 SPEAKER_00：训练，首次发现可以无监督地训练大量特征层。
622 00:49:27,255 --> 00:49:30,139 说话人 SPEAKER_00：我们使用被称为玻尔兹曼机的随机二进制单元。
623 00:49:31,541 --> 00:49:38,010 说话人 SPEAKER_00：Terry Sanofsky 在 1980 年左右坚持使用随机二进制单元，那是我第一次见到他，因为他认为这就是神经元的样子。
624 00:49:38,632 --> 00:49:43,398 说话人 SPEAKER_00：因此我们尝试开发使用它们的方法，因为这就是神经元所做的事情。
625 00:49:44,019 --> 00:49:46,282 说话人 SPEAKER_00：我们发现可以这样训练每个检测器。
626 00:49:46,403 --> 00:49:49,829 说话人 SPEAKER_00：但是做了之后，我们知道反向传播对微调很有用。
627 00:49:50,429 --> 00:49:58,081 说话人 SPEAKER_00：所以我们假装它们是确定性的神经元，这是一种糟糕的智力妥协，我们使用了反向传播。
628 00:49:58,101 --> 00:50:07,233 说话人 SPEAKER_00：结果发现，如果你不这样做，如果你在正向传播中继续将它们作为随机二进制神经元运行，但在反向传播中使用标准的反向传播。
在反向传播过程中，你使用那些 P 和 1 减 P，但它们永远不会需要被传递到神经元外部。
然后，它实际上训练得更慢，但泛化能力更好。
所以，这样做是更好的选择。
因此，我想以一些解释来结束，解释为什么皮层神经元不发送模拟值，因为这始终是个谜。
633 00:50:35,856 --> 00:50:39,541 说话人 SPEAKER_00: 很多人说，他们根本无法高效地做到这一点。
634 00:50:40,534 --> 00:50:45,121 说话人 SPEAKER_00: 但实际上，发送一个尖峰需要一些糖，因为你要发送这股去极化波。
635 00:50:46,623 --> 00:50:50,307 说话人 SPEAKER_00: 发送一个精确时间尖峰所需的糖量是相同的。
636 00:50:51,409 --> 00:50:53,711 说话人 SPEAKER_00: 这并不是不使用时间的理由。
637 00:50:54,072 --> 00:50:57,797 说话人 SPEAKER_00：更重要的是，我们知道有一些神经元会使用非常精确的时间。
638 00:50:58,277 --> 00:51:06,168 说话人 SPEAKER_00：所以当我定位声音时，我依赖于声音到达我两只耳朵的时间差。
639 00:51:06,929 --> 00:51:08,652 说话人 SPEAKER_00：现在，这是声音的 1 毫秒。
640 00:51:09,255 --> 00:51:12,519 说话人 SPEAKER_00：而到达我两只耳朵的时间差更像是这样。
641 00:51:12,621 --> 00:51:13,621 说话人 SPEAKER_00: 这是距离的差异。
642 00:51:14,242 --> 00:51:16,284 说话人 SPEAKER_00: 我们谈论的是毫秒级别的一小部分。
643 00:51:17,467 --> 00:51:23,034 说话人 SPEAKER_00: 您的神经元通过这种方式发送一个脉冲，那种方式发送一个脉冲，并观察它们在哪里重叠。
644 00:51:23,414 --> 00:51:25,217 说话人 SPEAKER_00: 因此，它们非常精确地使用脉冲时间。
645 00:51:26,338 --> 00:51:28,221 说话人 SPEAKER_00：我们都知道，如果进化愿意的话，它可以做到这一点。
646 00:51:28,862 --> 00:51:36,833 说话人 SPEAKER_00：在你的海马体中，如果你是一只老鼠，你就有，实际上你就是。
647 00:51:37,554 --> 00:51:40,400 说话人 SPEAKER_00：你会有一些细胞在你处于某个特定地点时活跃。
648 00:51:41,822 --> 00:51:45,447 说话人 SPEAKER_00：因此，细胞活跃的事实会告诉你你就在那里。
649 00:51:46,268 --> 00:51:51,438 说话者 SPEAKER_00：但是当它相对于振荡发生时，就能告诉你你在该位置场内的哪个位置。
650 00:51:52,559 --> 00:51:54,884 说话者 SPEAKER_00：关于这一点仍有争议，但已经相当明确了。
651 00:51:55,204 --> 00:52:03,478 说话者 SPEAKER_00：所以你使用尖峰的精确时间来告诉你你在哪里，以及它发生的事实告诉你你大概在这里。
652 00:52:04,251 --> 00:52:06,255 说话者 SPEAKER_00：所以我们知道它们可以使用时间来尖峰。
653 00:52:06,315 --> 00:52:09,099 说话人 说话人_00：我认为这根本不是一种合理的解释。
654 00:52:09,360 --> 00:52:10,061 说话人 说话人_00：他们可以做到。
655 00:52:10,561 --> 00:52:11,603 说话人 说话人_00：有做这件事的高效方法。
656 00:52:11,884 --> 00:52:12,445 说话人 说话人_00：但他们做不到。
657 00:52:14,086 --> 00:52:15,949 说话人 SPEAKER_00：另一个论点是进化从未想过这一点。
658 00:52:16,010 --> 00:52:17,793 说话人 SPEAKER_00：我的意思是，它从未偶然遇到这个想法。
659 00:52:18,454 --> 00:52:19,876 说话人 SPEAKER_00：我觉得这非常不可能。
660 00:52:20,396 --> 00:52:23,402 说话人 SPEAKER_00：进化可以将相同的细胞变成牙齿和眼睛。
661 00:52:24,003 --> 00:52:32,436 说话人 SPEAKER_00：如果它能做到那样，那么，你知道，用尖峰的时间似乎很显然。
662 00:52:33,175 --> 00:52:34,195 说话人 SPEAKER_00：我们甚至可以想到这一点。
663 00:52:35,918 --> 00:52:36,657 说话人 SPEAKER_00：所以我不相信这一点。
664 00:52:38,360 --> 00:52:43,965 说话人 SPEAKER_00：我目前的解释是神经元不发送模拟值，因为它们不想这么做。
665 00:52:44,025 --> 00:52:46,047 说话者 SPEAKER_00：他们最好发送随机脉冲。
666 00:52:46,527 --> 00:52:48,369 说话者 SPEAKER_00：这是因为它是一个非常优秀的正则化器。
667 00:52:49,170 --> 00:53:00,201 说话者 SPEAKER_00：神经元真正关心的是将数以亿计的模型拟合到它们所面对的奇怪数据中，平均这些模型的说法，发送随机脉冲是做这件事的一个非常好的方法。
668 00:53:01,081 --> 00:53:01,782 说话者 SPEAKER_00：好的，我完成了。
669 00:53:14,958 --> 00:53:16,920 说话人 SPEAKER_03：我们还有时间问几个问题。
670 00:53:16,940 --> 00:53:20,284 说话人 SPEAKER_03：但如果你想提问，你必须对着麦克风说。
671 00:53:28,235 --> 00:53:28,554 说话人 SPEAKER_03：没有人吗？
672 00:53:28,695 --> 00:53:29,255 说话人 SPEAKER_03：哦，好的。
673 00:53:29,436 --> 00:53:34,623 说话人 SPEAKER_03：将会有一个人发言。
674 00:53:34,663 --> 00:53:42,972 说话人 SPEAKER_06：皮质神经元不发送模拟值的一个另类理由是，随机发送更节能。
675 00:53:44,336 --> 00:53:45,577 说话人 SPEAKER_00：为什么更节能？
676 00:53:45,597 --> 00:53:50,885 说话人 SPEAKER_00：因为能量在于将去极化波传送到轴突。
677 00:53:51,846 --> 00:53:55,409 说话者 SPEAKER_00：你必须，你的钠泵必须将离子再次泵回。
678 00:53:55,789 --> 00:54:03,398 说话者 SPEAKER_06：不，不，不是发送尖峰，就像做与对齐尖峰和查看它在波中的位置相关的工作。
679 00:54:05,561 --> 00:54:07,003 说话者 SPEAKER_00：这并不难。
680 00:54:08,063 --> 00:54:15,250 说话者 SPEAKER_00：真正让我信服这一点的是，我想出了一个计划，这样我就可以仔细地计时尖峰。
681 00:54:15,789 --> 00:54:24,978 说话者 SPEAKER_00：例如，你可以做标量积，这是一个突触强度向量与 spikes 时间向量的标量积。
682 00:54:25,920 --> 00:54:31,144 说话者 SPEAKER_00：你可以做这个标量积，然后很容易将其转换回下一层的 spike 时间。
683 00:54:31,704 --> 00:54:33,907 说话者 SPEAKER_00：所以想出做这件事的机制并不难。
684 00:54:34,668 --> 00:54:36,670 说话者 SPEAKER_00：只是大脑皮层似乎没有使用它。
685 00:54:37,594 --> 00:54:40,467 说话人 SPEAKER_00：我认为有一种节能的方式来做到这一点。
686 00:54:41,797 --> 00:54:43,018 说话人 SPEAKER_00：是的。
687 00:54:43,039 --> 00:54:49,628 说话人 SPEAKER_03：所以在 dropout 的情况下，你会在训练时将其随机性作为正则化项打开。
688 00:54:49,969 --> 00:54:55,516 说话人 SPEAKER_03：但是在测试时，为了恢复这些模型的力量，你会对它们进行平均，对吧？
689 00:54:55,777 --> 00:55:01,485 说话人 SPEAKER_03：但如果这是你对大脑工作方式的猜想，并且随机性始终存在，那就好像它始终在训练。
690 00:55:01,585 --> 00:55:02,847 说话人 SPEAKER_03：它何时可以兑现？
691 00:55:03,027 --> 00:55:08,335 说话人 SPEAKER_03：它如何兑现并实际上平均这些模型，以便在测试时间，比如现在，表现良好？
692 00:55:08,356 --> 00:55:10,639 说话人 SPEAKER_00：所以为了平均这些模型，它需要做什么
693 00:55:10,619 --> 00:55:13,443 说话者 SPEAKER_00: 运行一段时间并使用尖峰率。
694 00:55:13,804 --> 00:55:18,431 说话者 SPEAKER_00: 基本上，你需要对时间进行积分，并且是随机的。
695 00:55:19,391 --> 00:55:24,079 说话者 SPEAKER_00：如果你观察人们在做决策，他们会随着时间的推移变得越来越准确。
696 00:55:26,061 --> 00:55:35,675 说话者 SPEAKER_00：所以大脑使用随机脉冲的方式，如果这就是原因，它必须以正确的方式运行模型平均，即多次运行。
697 00:55:36,356 --> 00:55:40,103 说话者 SPEAKER_00：因此，如果你花更多的时间通过时间平均结果，你会得到更准确的结果。
698 00:55:41,246 --> 00:55:44,030 说话者 SPEAKER_00：心理学家有很多证据表明，当你试图做决策时，你会这样做。
699 00:55:44,070 --> 00:55:45,833 Speaker SPEAKER_00: 这就是为什么如果你给他们更多时间，你就能得到更准确的结果。
700 00:55:54,708 --> 00:55:54,927 Speaker SPEAKER_07: 嗨。
701 00:55:54,947 --> 00:56:00,257 Speaker SPEAKER_07: 那么这种方法是否已经应用于更一般的 Boltzmann 机器，更一般的模型呢？
702 00:56:02,300 --> 00:56:04,463 Speaker SPEAKER_00: 嗯，Boltzmann 机器本身就在做这件事，对吧？
703 00:56:04,483 --> 00:56:07,507 说话人 SPEAKER_00：玻尔兹曼机全是随机的尖峰。
704 00:56:08,467 --> 00:56:12,536 说话人 SPEAKER_04：这是真的。
705 00:56:30,639 --> 00:56:37,688 说话人 SPEAKER_05：我不知道尖峰，尖峰的强度，我的意思是，在你的情况下，它是 0.20 的常数。
706 00:56:38,367 --> 00:56:42,532 说话人 SPEAKER_05：但在观察中，它可能非常多样，对吧？
707 00:56:43,134 --> 00:56:43,373 说话人 SPEAKER_00: 不。
708 00:56:43,914 --> 00:56:46,317 说话人 SPEAKER_00: 针尖的大小几乎都差不多。
709 00:56:47,097 --> 00:56:50,601 说话人 SPEAKER_00: 所以通过在轴突中发送去极化波来传播尖峰。
710 00:56:51,643 --> 00:56:54,204 说话人 SPEAKER_00: 这是一个数字事件。
711 00:56:54,226 --> 00:56:59,331 说话人 SPEAKER_00: 也就是说，每个阶段都有数字清理，这样如果...
712 00:57:00,391 --> 00:57:05,820 说话人 SPEAKER_00: 对不起，在一边去极化，让波开始传播。
713 00:57:06,280 --> 00:57:09,025 说话人 SPEAKER_00: 从另一端出来的完全是可预测的。
714 00:57:10,228 --> 00:57:14,916 说话人 SPEAKER_00: 即使动作有分支等，从另一端出来的也将保持不变。
715 00:57:15,898 --> 00:57:17,940 说话人 SPEAKER_00: 所以，这是一种数字系统。
716 00:57:18,742 --> 00:57:21,086 说话人 SPEAKER_00: 你在这头放一个一，另一头出来也是一个一。
717 00:57:22,048 --> 00:57:23,471 说话人 SPEAKER_00: 它们的大小基本上总是相同的。
718 00:57:25,134 --> 00:57:25,233 未知说话人: 好的。
719 00:57:25,822 --> 00:57:30,909 说话人 SPEAKER_00：现在，有一些模型，比如人们制作了一些计算神经科学模型，其中存在不同高度的尖峰。
720 00:57:30,949 --> 00:57:32,050 说话人 SPEAKER_00：Mike Lewinke 有一个这样的模型。
721 00:57:32,271 --> 00:57:34,894 说话人 SPEAKER_00：但真实的尖峰并不是这样的。
722 00:57:34,914 --> 00:57:38,481 说话人 SPEAKER_03：观众成员 1 另一个模型。
723 00:57:41,764 --> 00:57:41,945 未知说话者：再来一次。
724 00:57:41,965 --> 00:57:43,248 说话者 SPEAKER_01：观众成员 2，谢谢。
725 00:57:43,307 --> 00:57:45,931 说话者 SPEAKER_01：支持向量机可以看作是一个神经元。
726 00:57:46,371 --> 00:57:48,916 说话者 SPEAKER_01：因此，可以应用 dropout 到支持向量机。
727 00:57:48,956 --> 00:57:49,737 说话人 SPEAKER_01: 你试过这个吗？
728 00:57:50,746 --> 00:57:52,309 说话人 SPEAKER_01: 没有，我没试过。
729 00:57:52,369 --> 00:57:56,199 说话人 SPEAKER_00: 我不确定我是否理解你，但我喜欢这样的观点：支持向量机就是一个神经元。
730 00:58:03,637 --> 00:58:06,222 说话人 SPEAKER_00: 你可以事后给我解释一下。
731 00:58:14,708 --> 00:58:20,179 说话人 SPEAKER_04：所以，如果你将 Dropout 视为正则化器，它会在任何给定时间关闭一半的节点。
732 00:58:20,699 --> 00:58:25,309 说话人 SPEAKER_04：另一个稻草人方案就是简单地让一半的节点始终处于开启状态。
733 00:58:26,210 --> 00:58:27,673 说话人 SPEAKER_04：你尝试过那样做，结果... 哦，是的。
734 00:58:27,733 --> 00:58:28,695 说话人 SPEAKER_00：Dropout 要好得多。
735 00:58:30,460 --> 00:58:30,559 说话人 SPEAKER_00: 好的。
736 00:58:30,579 --> 00:58:33,105 说话人 SPEAKER_00: 所以，如果你看 Jan 的网页上的那个 160，
737 00:58:33,980 --> 00:58:37,784 说话人 SPEAKER_00: 有很多对调整反向传播很了解的人。
738 00:58:38,846 --> 00:58:40,188 说话人 SPEAKER_00: 约翰·普拉特尝试了很多方法。
739 00:58:40,748 --> 00:58:47,396 说话人 SPEAKER_00：约翰·普拉特通过运行一个大型的隐藏层，仅一个隐藏层，并且非常缓慢地训练，得到了 160 分。
740 00:58:47,878 --> 00:58:49,480 说话人 SPEAKER_00：他曾经得到过 160 分。
741 00:58:52,322 --> 00:58:54,485 说话人 SPEAKER_00：很多人尝试过，但做得更差。
742 00:58:54,954 --> 00:58:57,418 说话人 SPEAKER_00：如果你不进行数据转换或加入知识。
743 00:58:58,320 --> 00:59:01,003 说话者 说话者_00：这一点已经被非常仔细地研究了。
744 00:59:02,445 --> 00:59:05,168 说话者 说话者_00：在 Jan 的网页上有一个结果是 153。
745 00:59:05,248 --> 00:59:09,134 说话者 说话者_00：这是一个未发表的结果，而且是我做的。
746 00:59:11,237 --> 00:59:13,621 说话者 说话者_00：这个结果是通过使用这些权重约束得到的。
747 00:59:13,641 --> 00:59:15,682 说话人 SPEAKER_00：实际上，你可以击败 160 的重量限制。
748 00:59:16,224 --> 00:59:20,530 说话人 SPEAKER_00：你只需使用重量限制，就可以降低到大约 150。
749 00:59:20,550 --> 00:59:24,135 说话人 SPEAKER_00：但你不会接近 110 或 120。
750 00:59:25,414 --> 00:59:29,679 说话人 SPEAKER_04：还有一点是带有 dropout 的学习速度较慢。
751 00:59:29,981 --> 00:59:30,161 说话人 SPEAKER_00: 是的。
752 00:59:30,762 --> 00:59:36,309 说话人 SPEAKER_04: 你有没有想过，你知道的，一开始不用 dropout，然后，你知道的。
753 00:59:36,329 --> 00:59:37,710 说话人 SPEAKER_00: 所以，这些都是比较新的内容。
754 00:59:38,650 --> 00:59:41,956 说话人 SPEAKER_00: 我们只写了一篇关于这个的论文，但刚刚被拒绝了。
755 00:59:42,655 --> 00:59:49,264 说话人 SPEAKER_00：所以，几乎所有要尝试的事情，我们还没有尝试过。
756 00:59:50,045 --> 00:59:50,146 说话人 SPEAKER_00：好的。
757 00:59:50,166 --> 00:59:52,789 说话人 SPEAKER_00：我们刚刚得到了初步结果，表明它工作得非常好。
758 00:59:58,135 --> 00:59:59,860 说话人 SPEAKER_03：好了，我们的时间快到了。
759 00:59:59,981 --> 01:00:12,077 说话人 SPEAKER_03：那么让我们感谢 Jeff。
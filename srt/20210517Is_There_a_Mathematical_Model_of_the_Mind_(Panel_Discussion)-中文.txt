1 00:00:16,821 --> 00:00:17,463 演讲者 SPEAKER_07: 大家好。
2 00:00:17,542 --> 00:00:19,144 演讲者 SPEAKER_07: 欢迎参加本次讨论小组。
3 00:00:19,204 --> 00:00:23,169 演讲者 SPEAKER_07: 讨论的主题是，是否存在心智的数学模型？
4 00:00:24,109 --> 00:00:27,675 演讲者 SPEAKER_07: 前半部分是讨论小组，随后是观众提问环节。
5 00:00:29,036 --> 00:00:36,283 演讲者 SPEAKER_07: 请使用 Google Meet 聊天或 Dory 链接或 Twitter 话题标签提出您的问题。
6 00:00:36,744 --> 00:00:38,426 演讲者 SPEAKER_07: 您可以在思考问题时提出问题。
7 00:00:38,447 --> 00:00:43,673 演讲者 SPEAKER_07: 您不必等到问答环节开始时再提问。
8 00:00:44,834 --> 00:00:50,222 演讲者 SPEAKER_07: 有幸与来自各自领域的杰出分析师和 luminaries 共同出席，我感到非常荣幸。
9 00:00:50,942 --> 00:01:01,279 讲者 SPEAKER_07：我们有来自卡内基梅隆大学和加州大学伯克利分校的 Lenore Blum，她从事计算理论的研究，并获得了科学、数学和工程领域的总统奖。
10 00:01:01,378 --> 00:01:11,313 讲者 SPEAKER_07：来自加州大学伯克利分校的 Jack Galland，他是一位认知神经科学家，获得了《时代》杂志发明家奖，并经常出现在广播谈话节目和播客中。
11 00:01:11,783 --> 00:01:17,069 讲者 SPEAKER_07：来自谷歌和多伦多大学的 Jeffrey Hinton，他从事深度学习的基础研究，并获得了图灵奖。
12 00:01:17,590 --> 00:01:24,519 讲者 SPEAKER_07：来自斯坦福大学的 Percy Liang，他从事自然语言处理研究，并获得了总统早期职业奖。
13 00:01:25,180 --> 00:01:28,686 讲者 SPEAKER_07：加州大学伯克利分校的袁宇，从事统计学和机器学习研究。
14 00:01:29,126 --> 00:01:34,433 讲者 SPEAKER_07：她是美国国家科学院院士，也是 COPS EL Scott 奖获得者。
15 00:01:34,993 --> 00:01:36,576 讲者 SPEAKER_07：我是谷歌的 Reena Maligray。
16 00:01:36,617 --> 00:01:37,418 讲者 SPEAKER_07：我将担任主持人。
17 00:01:37,438 --> 00:01:40,542 说话人 SPEAKER_07：我从事机器学习算法和理论的研究。
18 00:01:41,146 --> 00:01:42,608 说话人 SPEAKER_07：那么，让我们从第一个问题开始。
19 00:01:43,370 --> 00:01:45,293 说话人 SPEAKER_07：是否存在心智的数学模型？
20 00:01:46,093 --> 00:01:51,001 说话人 SPEAKER_07：在这里，当我提到心智时，我们实际上是在指它的算法能力。
21 00:01:51,022 --> 00:01:57,953 说话人 SPEAKER_07：这不必是人的大脑，但可能是一个具有相同能力或更有能力的智能系统。
22 00:01:58,974 --> 00:02:05,945 说话人 SPEAKER_07：我们能否找到这样的算法，并自信地用数学证明它具有这些能力？
23 00:02:06,525 --> 00:02:08,789 说话人 SPEAKER_07：或者这项研究主要是一门实证科学？
24 00:02:11,807 --> 00:02:15,032 说话人 SPEAKER_07：谁愿意来回答这个问题？
25 00:02:15,052 --> 00:02:16,414 说话人 SPEAKER_01：我能对此发表评论吗？
26 00:02:16,435 --> 00:02:17,997 说话人 SPEAKER_01：当然可以，请说。
27 00:02:18,897 --> 00:02:23,444 说话人 SPEAKER_01：我认为我们做的很多事都无法被算法很好地捕捉。
28 00:02:23,906 --> 00:02:27,150 说话人 SPEAKER_01：所以，我们做的很多事都是直接的直觉推理或感知。
29 00:02:27,591 --> 00:02:34,600 讲者 SPEAKER_01：当然，存在学习算法，也就是说，你有一套改变突触权重的规则。
30 00:02:35,181 --> 00:02:40,650 讲者 SPEAKER_01：但改变突触权重的这些规则的结果是一个包含数十亿权重的庞大系统。
31 00:02:41,103 --> 00:02:57,137 讲者 SPEAKER_01：我认为，对于诸如直觉推理这类事物，你永远无法找到一个比说“他有这 1000 亿个权重”更简单的解释来说明为什么有人相信某事。
32 00:02:57,116 --> 00:03:01,722 讲者 SPEAKER_01：而且它们是由这些经验创造的，如果你想解释它们是如何产生的。
33 00:03:01,762 --> 00:03:10,069 讲者 SPEAKER_01：结果是，他拥有成千上万的小规则，使某些事情变得可信，而其他事情则不可信。
34 00:03:10,370 --> 00:03:15,876 讲者 SPEAKER_01：在这个特定的情况下，比如有 20 万条规则说这样，有 10 万条规则说那样。
35 00:03:16,556 --> 00:03:18,098 讲者 SPEAKER_01：所以他这样说。
36 00:03:18,818 --> 00:03:24,424 讲者 SPEAKER_01：通过说出所有个别规则来表示这一点，并不会对你有很大帮助。
37 00:03:26,530 --> 00:03:27,911 演讲者 演讲者_00: 好的，我想说几句话。
38 00:03:28,711 --> 00:03:31,936 演讲者 演讲者_07: Percy 举手了，那我来叫 Percy。
39 00:03:31,955 --> 00:03:32,856 演讲者 演讲者_07: 然后是 Lenore。
40 00:03:33,598 --> 00:03:36,961 演讲者 演讲者_00: 是的，谢谢 Armino 邀请我参加这个讨论小组。
41 00:03:37,001 --> 00:03:52,176 演讲者 演讲者_00：您所提出的背景，即它不必是为了人类心智，与我们设计数学意识模型时 Manuel Blum、Alvin Blum 和我所采取的视角非常接近。
42 00:03:52,237 --> 00:03:53,778 演讲者 演讲者_00：所以我们正在研究意识。
43 00:03:54,280 --> 00:03:56,401 演讲者 演讲者_00：所以我将简要介绍我们的模型。
44 00:03:56,382 --> 00:04:09,037 演讲者 演讲者_00：在设计我们的模型时，我们并不是在寻找一个复杂的脑或认知模型，而是在寻找一个非常简单的模型来理解意识。
45 00:04:09,717 --> 00:04:20,050 讲者：我们的数学理论处于一个非常、非常高的水平，一个远高于神经元和神经元活动的抽象水平，因此它非常简单。
46 00:04:20,069 --> 00:04:23,875 讲者：我们称我们的模型为意识 AI，
47 00:04:26,403 --> 00:04:27,764 讲者：或者图灵机。
48 00:04:27,845 --> 00:04:33,690 讲者：这某种程度上是对图灵简单而强大的计算模型的致敬。
49 00:04:34,071 --> 00:04:37,675 讲者 SPEAKER_00: 就像 Jeff 说的，我们的模型实际上并不是图灵机。
50 00:04:38,355 --> 00:04:53,372 讲者 SPEAKER_00: 由于它不是输入，至少对于意识来说，不是输入输出决定了它的功能意识，而是一系列的东西，比如它的架构，在我们的案例中，我们使用的是全局工作空间架构。
51 00:04:53,351 --> 00:05:04,557 讲者 SPEAKER_00: 它是一种预测动态，预测、反馈、错误纠正学习和某些关键的特殊过程。
52 00:05:04,637 --> 00:05:08,728 讲者 SPEAKER_00: 在我们的案例中，是关于世界和其他言语的模型。
53 00:05:08,708 --> 00:05:15,398 说话人 SPEAKER_00：我认为，从我们的角度来看，这并不是真正的输入输出映射。
54 00:05:15,577 --> 00:05:21,226 说话人 SPEAKER_00：而是结构赋予了它意识的定性特征。
55 00:05:21,346 --> 00:05:27,836 说话人 SPEAKER_00：我想，在认知方面也是如此。
56 00:05:28,517 --> 00:05:32,343 说话人 SPEAKER_00：所以我认为输入输出映射可能过于局限。
57 00:05:35,668 --> 00:05:36,589 讲者 SPEAKER_07：珀西，你想举手吗？
58 00:05:36,790 --> 00:05:37,651 讲者 SPEAKER_07：你想下一个发言吗？
59 00:05:37,884 --> 00:05:39,826 讲者 SPEAKER_05：是的，所以我可以讲几点。
60 00:05:40,208 --> 00:05:44,833 讲者 SPEAKER_05：我对这个问题有点困扰，因为我不习惯从心灵的角度思考问题。
61 00:05:45,553 --> 00:05:53,463 讲者 SPEAKER_05：为了让大家有一个参考，我是一名计算机科学家，我想构建对世界有用的系统。
62 00:05:54,142 --> 00:06:02,812 讲者 SPEAKER_05：我个人认为，我的目标是思考一种理想化的智能。
63 00:06:02,793 --> 00:06:19,557 讲者 SPEAKER_05：这种智能与人类智能是分开的，因为我们知道，尽管人类有很多值得称赞的伟大之处，但人类容易受到各种缺陷的影响，比如偏见，以及思考方式不够理性或逻辑。
64 00:06:20,036 --> 00:06:31,213 讲者 SPEAKER_05：我认为，长期以来，人工智能之所以取得了巨大的进步，主要是因为它本质上把人类放在了至高无上的位置，思考我们如何模拟或模仿。
65 00:06:31,192 --> 00:06:32,915 讲者 SPEAKER_05：人类智能的一些方面。
66 00:06:33,776 --> 00:06:39,625 讲者 SPEAKER_05：但我认为人工智能正变得越来越明显，你知道的，是一种非常不同、外星型的智能。
67 00:06:39,646 --> 00:06:43,672 讲者 SPEAKER_05：它具有巨大的能力，远远超过人类。
68 00:06:43,732 --> 00:06:47,536 讲者 SPEAKER_05：但它也存在一些基本弱点，比如对抗样本。
69 00:06:47,978 --> 00:06:53,286 讲者 SPEAKER_05：我认为这个类比，我想，可能只能带我们走到这一步
70 00:06:53,485 --> 00:07:00,971 讲者 SPEAKER_05：显然，大家可能都熟悉这个过度使用的类比，即建造鸟类与飞机。
71 00:07:01,153 --> 00:07:07,319 讲者 SPEAKER_05：我认为我们仍然处于某种意义上建造振翅飞行的飞机的模式中。
72 00:07:07,478 --> 00:07:11,463 讲者 SPEAKER_05：它为我们能建造什么提供了一个非常有说服力的存在证明。
73 00:07:11,822 --> 00:07:21,653 说话人 SPEAKER_05：但我认为，我希望我们能找到一些方法，使我们的系统、我们构建的人工智能系统在未来更加
74 00:07:21,632 --> 00:07:22,853 说话人 SPEAKER_05：并且更加可靠。
75 00:07:23,555 --> 00:07:38,612 说话人 SPEAKER_05：所以关于更贴近当前问题的数学模型问题，我抓住了“模型”这个词，因为，正如统计学家乔治·博克斯所说，你知道，所有的模型都是错误的，但有些是有用的。
76 00:07:39,273 --> 00:07:45,379 说话人 SPEAKER_05：我认为这就是我想看待任何模型的方式。
77 00:07:45,399 --> 00:07:51,065 讲者 SPEAKER_05：我认为现实世界的复杂性并不会被任何简单的对象所捕捉，但这并不意味着我们不能拥有整洁的抽象和模型来帮助我们思考如何构建事物，需要某种类型的支架，即使是现有的深度学习，也有关于，你知道的，
78 00:07:51,045 --> 00:08:12,480 讲者 SPEAKER_05：并不是任何简单的对象所能捕捉的，但这并不意味着我们不能拥有整洁的抽象和模型来帮助我们思考如何构建事物，需要某种支架，即使是现有的深度学习，也有关于，你知道的，
79 00:08:12,459 --> 00:08:19,408 讲者 SPEAKER_05：有一个清晰的损失函数，你可以取梯度，这些都是，它是事物工作原理的数学模型。
80 00:08:19,428 --> 00:08:30,322 讲者 SPEAKER_05：这已经被证明是非常有成效的，尽管心智或智能的许多方面并没有被真正捕捉。
81 00:08:34,748 --> 00:08:38,011 演讲者 SPEAKER_07: 杰克，本，你们还没有发言，请回应。
82 00:08:40,912 --> 00:08:42,313 演讲者 SPEAKER_07: 但是你的麦克风被静音了，本。
83 00:08:42,793 --> 00:08:43,115 演讲者 SPEAKER_03: 我明白了。
84 00:08:44,235 --> 00:08:44,976 演讲者 SPEAKER_03: 好的。
85 00:08:44,996 --> 00:08:46,619 演讲者 SPEAKER_03：杰克，你想先说吗？
86 00:08:46,639 --> 00:08:47,200 演讲者 SPEAKER_07：不，不，本。
87 00:08:47,220 --> 00:08:47,480 演讲者 SPEAKER_04：请说。
88 00:08:48,861 --> 00:08:49,663 演讲者 SPEAKER_03：好的。
89 00:08:50,124 --> 00:08:55,870 说话人 SPEAKER_03：为了回应珀西的观点，我认为人工智能具有巨大的潜力。
90 00:08:56,392 --> 00:09:00,336 说话人 SPEAKER_03：但我认为人类大脑仍然非常高效。
91 00:09:00,977 --> 00:09:09,028 说话人 SPEAKER_03：所以，如果我们从能耗的角度来看，我认为这种智能与人工智能是不同类型的。
92 00:09:09,008 --> 00:09:13,855 说话人 SPEAKER_03：我真的不认为我们所知的这种人工智能能胜过它。
93 00:09:14,057 --> 00:09:17,522 演讲者 SPEAKER_03：也许您有更好的方法来提供能量。
94 00:09:18,182 --> 00:09:30,563 演讲者 SPEAKER_03：对我来说，当我看到 Raina 提出的问题时，它实际上是我们的数学对心智的描述与通过神经科学测量对心智的理解的协同进化。
95 00:09:31,245 --> 00:09:33,970 演讲者 SPEAKER_03：现在提到这个重要的问题
96 00:09:34,101 --> 00:09:46,380 演讲者 SPEAKER_03：对我来说，你也可以把之前的一些演讲者计算成将某种对心智的定性理解融入到机器学习人工智能系统中，并执行机器学习人工智能任务。
97 00:09:46,400 --> 00:09:47,543 讲者 SPEAKER_03：这就是一种方法。
98 00:09:48,144 --> 00:09:56,897 讲者 SPEAKER_03：另一种方法是您实际上使用数学模型来帮助我们生成关于心灵的可验证假设，我们可以收集数据来验证。
99 00:09:57,097 --> 00:10:01,323 讲者 SPEAKER_03：我的意思是，我和 Jack 一起工作，所以您可以看到我想要他先发言。
100 00:10:02,467 --> 00:10:06,533 讲者 SPEAKER_03：但是对我来说，重要的是实证证据，对吧？
101 00:10:06,552 --> 00:10:22,211 讲者 SPEAKER_03：当你比较两种数学模型和人类大脑时，如果你观察神经元在我们大脑中的结构，实际上它们并没有物理连接，树突之间有间隙。
102 00:10:23,173 --> 00:10:27,898 讲者 SPEAKER_03：而且还有很多其他因素在帮忙，对吧？
103 00:10:27,918 --> 00:10:29,320 讲者 SPEAKER_03：有一种叫做
104 00:10:29,653 --> 00:10:31,557 讲者 SPEAKER_03：神经突触抑制，对吧？
105 00:10:31,576 --> 00:10:37,404 讲者 SPEAKER_03：你把轴突冷却下来，那么通信速度就会快得多。
106 00:10:37,445 --> 00:10:41,890 讲者 SPEAKER_03：有两种通信方式，一种是化学的，另一种是电的。
107 00:10:42,852 --> 00:10:47,158 讲者 SPEAKER_03：所以所有这些，可以说也许可以简化，我们不需要这些，这是可能的。
108 00:10:47,558 --> 00:10:54,068 讲者 SPEAKER_03：但这也让我们学到，也许神经元的设计不应该全部连接在一起。
109 00:10:54,109 --> 00:10:56,471 说话人 SPEAKER_03：我们将进行删除，我们可以
110 00:10:56,451 --> 00:11:00,976 说话人 SPEAKER_03：探讨儿童的学习过程以及神经元如何构建。
111 00:11:00,998 --> 00:11:03,500 说话人 SPEAKER_03：我感到我们有很多可以从进化中学习的地方。
112 00:11:04,381 --> 00:11:09,268 说话人 SPEAKER_03：关键在于我们如何知道我们所说的模型是好的，对吧？
113 00:11:09,288 --> 00:11:13,712 讲者 SPEAKER_03：即便是作为法官 Boxer，我们所说的数学模型也是有用的。
114 00:11:14,113 --> 00:11:16,416 讲者 SPEAKER_03：我实际上只是教了信息论课程。
115 00:11:16,777 --> 00:11:19,539 讲者 SPEAKER_03：所以这在我的脑海中非常关于香农信息论。
116 00:11:19,519 --> 00:11:23,047 讲者 SPEAKER_03：这是最优雅的通信数学理论之一。
117 00:11:23,508 --> 00:11:33,826 说话人 SPEAKER_03：有一点是，熵-互信息的关键概念也具有物理意义，并为压缩或信道传输提供了物理容量限制。
118 00:11:34,668 --> 00:11:37,072 说话人 SPEAKER_03：我们能否 somehow
119 00:11:38,115 --> 00:11:43,400 说话人 SPEAKER_03：也给我们一些标准，比如说，这个数学模型必须满足哪些性质。
120 00:11:43,421 --> 00:11:50,288 说话人 SPEAKER_03：为了说现在足够好，需要收集哪些经验测量数据？
121 00:11:50,567 --> 00:11:51,708 说话人 SPEAKER_03: 然后我们进入下一个层次。
122 00:11:53,750 --> 00:11:54,011 说话人 SPEAKER_03: 杰克？
123 00:11:56,894 --> 00:11:59,797 说话人 SPEAKER_04: 好的，这些都是非常好的观点。
124 00:12:00,118 --> 00:12:03,380 说话人 SPEAKER_04: 我同意大家说的大部分内容。
125 00:12:03,765 --> 00:12:08,350 说话人 SPEAKER_04：我觉得这是一个很难回答的问题，因为我们还没有定义我们的术语。
126 00:12:08,951 --> 00:12:11,215 说话人 SPEAKER_04：这里的问题是，什么是心智的数学模型？
127 00:12:11,274 --> 00:12:12,775 说话人 SPEAKER_04：实际上我不知道什么是心智。
128 00:12:13,998 --> 00:12:20,325 说话人 SPEAKER_04：我假设你们，大多数是机器学习人工智能领域的人，认为心智是运行在硬件上的软件。
129 00:12:20,586 --> 00:12:21,886 讲者 SPEAKER_04：我认为这完全没问题。
130 00:12:21,927 --> 00:12:28,794 讲者 SPEAKER_04：我认为神经科学家可能也是这样想的。
131 00:12:29,432 --> 00:12:32,538 讲者 SPEAKER_04：你知道，科学的一般规则是事事都有数学。
132 00:12:32,557 --> 00:12:34,081 讲者 SPEAKER_04：你可能不知道数学是什么。
133 00:12:34,201 --> 00:12:35,984 讲者 SPEAKER_04：我想我们都可以回答这个问题。
134 00:12:36,004 --> 00:12:38,908 讲者 SPEAKER_04：当然，在某种程度上，存在关于心智的数学理论。
135 00:12:38,947 --> 00:12:44,417 讲者 SPEAKER_04：我们只是不知道那是什么，因为我们对正在运行的软件了解不多。
136 00:12:44,456 --> 00:12:53,030 讲者 SPEAKER_04：而且可能我们也没有合适的数学来描述它，因为大脑是一个庞大、复杂、非线性的动力学系统。
137 00:12:53,631 --> 00:12:56,537 说话人 SPEAKER_04：我们甚至没有好的数学来描述这样的系统。
138 00:12:56,972 --> 00:13:00,225 说话人 SPEAKER_04：但我认为真正的问题不是，是否存在大脑的数学模型？
139 00:13:00,246 --> 00:13:02,215 说话人 SPEAKER_04：而是大脑的数学模型是什么？
140 00:13:02,335 --> 00:13:06,392 说话人 SPEAKER_04：我发现这个讨论很有趣，它主要
141 00:13:06,524 --> 00:13:10,087 讲者 SPEAKER_04：似乎归结为我们可以抽象出什么？
142 00:13:10,748 --> 00:13:20,498 讲者 SPEAKER_04：没有人愿意将心智建模为轴突内微管的运动。
143 00:13:20,518 --> 00:13:21,399 讲者 SPEAKER_04：这简直疯狂。
144 00:13:21,438 --> 00:13:24,361 讲者 SPEAKER_04：所以我们必须抽象出一些东西。
145 00:13:24,881 --> 00:13:27,384 讲者 SPEAKER_04：人们对这个问题有不同的看法。
146 00:13:30,488 --> 00:13:35,653 讲者 SPEAKER_04：尽管我是一名神经科学家，并且多年来一直担任神经生理学家，但我的个人观点是神经元可能并不重要。
147 00:13:35,971 --> 00:13:40,197 讲者 SPEAKER_04：大脑中可能存在一些小精灵，每次产生尖峰时就会用魔杖互相击打。
148 00:13:40,236 --> 00:13:46,486 讲者 SPEAKER_04：只要这些小精灵具有相同的非线性动力学特性，你可能会得到相同的结果。
149 00:13:47,168 --> 00:13:51,533 说话者 SPEAKER_04：我想这是个挺奇怪的问题。
150 00:13:51,695 --> 00:14:03,952 说话者 SPEAKER_04：但我觉得，我不知道，我觉得我们可能应该关注一下这个问题，因为这似乎是大家都在隐含地处理的问题，尽管这并不是我们最初的问题。
151 00:14:05,013 --> 00:14:08,057 说话者 SPEAKER_07：那么，在我们进入下一个问题之前，有什么快速的意见吗？
152 00:14:11,801 --> 00:14:15,225 说话者 SPEAKER_00：所以 Jeff 想发言，我也想。
153 00:14:15,326 --> 00:14:26,700 讲者 SPEAKER_00：我想快速回应 Jack 和 Bin 刚才所说的话，因为我认为这里有一个更根本的问题，那就是为什么要有心智的数学理论？
154 00:14:26,679 --> 00:14:36,532 讲者 SPEAKER_00：我认为 Jack 指出，当图灵在 30 年代谈论他的图灵机时，我们并没有精确的算法概念。
155 00:14:36,952 --> 00:14:41,158 讲者 SPEAKER_00：我们并没有精确的程序或编程语言的概念。
156 00:14:41,217 --> 00:14:43,360 讲者 SPEAKER_00：然后它演变成了复杂性。
157 00:14:43,760 --> 00:14:49,467 讲者：所有这些概念都变成了数学上的形式化，并且得到了澄清。
158 00:14:49,807 --> 00:14:55,455 讲者：即使不是每个人都回到图灵的算法概念，我们都有一种想法
159 00:14:55,434 --> 00:15:00,145 讲者：有一个基本的、清晰的概念，我们可以共同依赖。
160 00:15:00,687 --> 00:15:10,229 讲者：因此，我认为能够拥有某种基本理论，帮助我们形式化和澄清思想，这一点非常重要。
161 00:15:10,971 --> 00:15:11,773 说话者 SPEAKER_00: 并且
162 00:15:11,753 --> 00:15:13,917 说话者 SPEAKER_00: 神经科学真的很复杂。
163 00:15:14,477 --> 00:15:22,495 说话者 SPEAKER_00: 如果我们能够对一些更复杂的事物有一个大致的了解，我认为这将是非常基础的。
164 00:15:22,514 --> 00:15:25,561 说话者 SPEAKER_00: 而香农的信息论也做了类似的事情。
165 00:15:26,903 --> 00:15:31,111 演讲者 SPEAKER_00：那么，为什么要有心智的数学理论，我认为这是一个问题。
166 00:15:31,530 --> 00:15:34,394 演讲者 SPEAKER_07：杰弗里，在我们进入下一个问题之前，你能快速评论一下吗？
167 00:15:34,414 --> 00:15:42,645 演讲者 SPEAKER_01：是的，我想做一个类比，微观事物和宏观事物之间可以有两种不同的关系。
168 00:15:43,126 --> 00:15:46,451 演讲者 SPEAKER_01：所以在热力学中，你有一个很好的宏观理论。
169 00:15:47,072 --> 00:15:50,417 说话人 SPEAKER_01：这是分子的运动结果
170 00:15:50,683 --> 00:15:55,611 说话人 SPEAKER_01：但在某种意义上，分子都是一样的，你可以抽象出来得到热力学，这很棒。
171 00:15:55,692 --> 00:15:56,192 说话人 SPEAKER_01：太棒了。
172 00:15:56,232 --> 00:15:59,258 说话人 SPEAKER_01：这就是科学之王的其中一种。
173 00:15:59,298 --> 00:16:08,052 讲者 SPEAKER_01：那么，如果你在湍流状态下考虑纳维-斯托克斯方程，你可以观察进入船闸的水流。
174 00:16:08,673 --> 00:16:12,759 讲者 SPEAKER_01：这里有一些简单的底层事物，就像神经网络的学习算法。
175 00:16:13,330 --> 00:16:20,080 讲者 SPEAKER_01：但莱昂纳多所绘制的涡流，例如，你永远也不会有一个美好的算法理论。
176 00:16:20,780 --> 00:16:24,365 讲者 SPEAKER_01：你可以有一种现象论的理论，来解释你得到什么样的涡流。
177 00:16:24,626 --> 00:16:27,711 说话人 SPEAKER_01: 你可以理解关于纳维-斯托克斯方程之间的一些关系。
178 00:16:28,371 --> 00:16:34,561 说话人 SPEAKER_01: 如果纳维-斯托克斯方程学会产生漂亮的涡流，那就更有趣了，这更像是神经网络。
179 00:16:35,722 --> 00:16:39,248 说话人 SPEAKER_01: 但这似乎是两种非常不同的关系。
180 00:16:39,288 --> 00:16:42,072 说话人 SPEAKER_01: 当有人说，这是否是一种算法理论，
181 00:16:42,894 --> 00:16:46,639 主持人 SPEAKER_01：我感觉像是在追求第一个，当现实更像第二个时。
182 00:16:48,301 --> 00:16:48,561 主持人 SPEAKER_07：谢谢。
183 00:16:50,222 --> 00:16:51,224 主持人 SPEAKER_07：让我们继续下一个问题。
184 00:16:51,845 --> 00:16:56,048 主持人 SPEAKER_07：那么，当您审视今天的深度学习框架时，您认为主要缺少哪些元素？
185 00:16:57,169 --> 00:17:03,017 演讲者 SPEAKER_07：人脑中是否有什么智力或成分是我们应该添加到深度学习中的？
186 00:17:06,019 --> 00:17:06,701 演讲者 SPEAKER_01：我可以先说吗？
187 00:17:07,761 --> 00:17:10,484 演讲者 SPEAKER_01：好的，请说。
188 00:17:10,505 --> 00:17:12,767 演讲者 SPEAKER_01：是的，还有很多缺失。
189 00:17:13,944 --> 00:17:15,165 讲者 SPEAKER_01：但这主要不是逻辑。
190 00:17:16,488 --> 00:17:28,500 讲者 SPEAKER_01：所以如果你看看大多数神经网络，直到出现变换器之前，它们都是通过将活动向量与权重向量进行标量积来激活神经元的。
191 00:17:28,519 --> 00:17:32,824 讲者 SPEAKER_01：然后变换器出现了，开始将活动向量与活动向量进行标量积。
192 00:17:33,404 --> 00:17:35,165 说话人 SPEAKER_01：这对你来说有很多好处。
193 00:17:35,227 --> 00:17:38,670 说话人 SPEAKER_01：它对相关协方差更加敏感。
194 00:17:38,953 --> 00:17:40,615 说话人 SPEAKER_01：因此你获得了各种新的能力。
195 00:17:41,196 --> 00:17:44,019 说话人 SPEAKER_01：这就是他们之前所缺少的，而现在他们通过变压器得到了这一点。
196 00:17:44,480 --> 00:17:56,894 讲者 SPEAKER_01：他们遗漏了其他的东西，我坚信这在大脑中无处不在，自从 Terry Sanofsky 在我年轻时告诉我这件事以来就是这样，那就是我们大多数神经网络中只有两个时间尺度。
197 00:17:57,336 --> 00:18:04,944 我们有随着每个新输入而变化的活动的时标，以及随着学习而非常缓慢变化的权重的时标。
198 00:18:06,005 --> 00:18:13,173 这意味着当你询问诸如短期记忆之类的事情时，你会倾向于将这些归因于神经网络活动。
199 00:18:14,413 --> 00:18:19,499 当然，你不能复制一大群神经元，以便在用神经元做其他事情的同时存储它们的状态。
200 00:18:20,259 --> 00:18:31,352 说话人 SPEAKER_01：在我看来，必须存在快速权重，即快速适应和快速衰减的权重，它们是现有权重的叠加。
201 00:18:31,951 --> 00:18:33,513 说话人 SPEAKER_01：并且使用快速权重，
202 00:18:33,982 --> 00:18:37,748 说话人 SPEAKER_01：你可以做到真正的递归，你可以做真正的递归。
203 00:18:38,148 --> 00:18:39,289 说话人 SPEAKER_01：而我们目前缺少这一点。
204 00:18:39,309 --> 00:18:45,478 讲者 SPEAKER_01：所以在真正的递归中，你是在重复使用相同的神经元和相同的连接进行递归调用。
205 00:18:46,219 --> 00:18:49,022 讲者 SPEAKER_01：而且只有当你能记住这些神经元的态时，你才能这样做。
206 00:18:49,042 --> 00:18:50,184 讲者 SPEAKER_01：所以你需要像栈这样的东西。
207 00:18:50,664 --> 00:18:53,449 讲者 SPEAKER_01：而且你可以把这个栈放入由快速权重组成的关联记忆中。
208 00:18:54,450 --> 00:18:55,771 说话人 SPEAKER_01：我认为
209 00:18:56,089 --> 00:18:57,957 说话人 SPEAKER_01：我们几乎没有足够的时间尺度。
210 00:18:57,977 --> 00:19:04,040 说话人 SPEAKER_01：如果你看看生物学，看看眼睛的适应，有大约六个不同的时间尺度，或者更多。
211 00:19:05,354 --> 00:19:06,516 说话人 SPEAKER_01：我们需要更多的时间尺度。
212 00:19:06,536 --> 00:19:13,467 说话人 SPEAKER_01：特别是，我们至少还需要一个非常重要的时间尺度，它能够赋予你短期记忆的权重，而这个权重具有很高的容量。
213 00:19:14,189 --> 00:19:17,173 说话人 SPEAKER_01：但是由于它的容量非常高，你不需要让它那么高效。
214 00:19:17,193 --> 00:19:19,017 说话人 SPEAKER_01：所以你可以使用 Azure 产品学习。
215 00:19:19,037 --> 00:19:21,039 说话人 SPEAKER_01：你不需要使用错误纠正学习。
216 00:19:21,380 --> 00:19:25,366 说话人 SPEAKER_01：您可以用 Azure 产品学习在短时间内获取大量信息，快速记忆。
217 00:19:25,386 --> 00:19:26,970 说话人 SPEAKER_01：我认为这是一个巨大的缺失元素。
218 00:19:27,450 --> 00:19:29,032 说话人 SPEAKER_01：我来告诉你为什么我们没有它。
219 00:19:29,501 --> 00:19:37,717 说话人 SPEAKER_01：我们没有它的原因是我们的硬件太糟糕了，从内存中获取一个权重需要 200 个周期，所以你最好多次重用这个权重。
220 00:19:38,538 --> 00:19:42,865 讲者 SPEAKER_01：现在，如果你有一个快速权重，它对每个训练案例都是不同的，所以你不能重复使用它。
221 00:19:43,507 --> 00:19:48,455 讲者 SPEAKER_01：所以你使用的实际硬件强烈反对使用快速权重。
222 00:19:49,127 --> 00:19:56,036 讲者 SPEAKER_01：我在 2010 年让伊利亚实现了快速权重，当时我们在做早期的语言模型，基于字符的语言模型，它们是有效的。
223 00:19:56,636 --> 00:20:03,224 讲者 SPEAKER_01：只是它们并不高效，因为你不能分摊从内存中获取权重的成本。
224 00:20:03,244 --> 00:20:12,394 讲者 SPEAKER_01：而且令人疯狂的是，我们的脑理论强烈地受到记忆检索成本的影响，因为大脑并不这样做。
225 00:20:14,958 --> 00:20:16,299 演讲者 SPEAKER_07: 好的，Lenore，你举手了。
226 00:20:16,903 --> 00:20:17,625 演讲者 SPEAKER_00: 是的。
227 00:20:18,105 --> 00:20:29,175 演讲者 SPEAKER_00: 所以，你知道，Yashua Bengio、Rufus von Rollen 和 Ryota Kanai 正在进行一些当前的工作，旨在将全局工作空间架构纳入深度学习。
228 00:20:29,616 --> 00:20:43,671 讲者 SPEAKER_00：我不确定这有多重要，有多好，但我知道我们在模型中也使用了认知神经科学家伯恩哈德·巴尔斯的全局工作空间架构，这非常强大。
229 00:20:43,651 --> 00:21:01,223 讲者 SPEAKER_00：实际上，他们认为全球工作空间并不令人惊讶，因为在 20 世纪 60 年代，卡内基梅隆大学的艾伦·纽厄尔、赫布·西蒙、拉吉·雷迪等人就已经提出了认知的原始模型，即他们的黑板模型。
230 00:21:01,625 --> 00:21:06,974 讲者 SPEAKER_00：所以我认为将理论初期的观点
231 00:21:06,954 --> 00:21:11,981 讲者 SPEAKER_00：融入深度学习可能实际上非常有趣。
232 00:21:12,542 --> 00:21:22,096 讲者 SPEAKER_00：那么，这个具有全局广播和竞争上台机会的全局工作空间模型是一个非常强大的模型。
233 00:21:22,897 --> 00:21:25,381 讲者 SPEAKER_00：这只是个评论。
234 00:21:25,481 --> 00:21:28,766 讲者 SPEAKER_04：杰克？
235 00:21:29,247 --> 00:21:33,373 讲者 SPEAKER_04：我想跟进杰夫所说的，可能是一个稍微不同的观点。
236 00:21:33,814 --> 00:21:36,237 演讲者 SPEAKER_04：如果你研究大脑，你会学到一件事。
237 00:21:36,502 --> 00:21:39,306 演讲者 SPEAKER_04：研究大脑你会发现它们极其动态。
238 00:21:39,425 --> 00:21:41,788 演讲者 SPEAKER_04：没有任何东西是稳定的，包括表征。
239 00:21:42,449 --> 00:21:46,693 演讲者 SPEAKER_04：所以表征在所有你能记录的时间尺度上都在变化。
240 00:21:47,875 --> 00:21:50,278 讲者 SPEAKER_04：特别是，注意力实际上会改变表征。
241 00:21:50,357 --> 00:21:59,788 讲者 SPEAKER_04：所以在大脑的任何地方，注意力都会改变信息的方式，而不仅仅是像音量控制那样改变信息表征的方式。
242 00:22:00,689 --> 00:22:03,372 讲者 SPEAKER_04：并且大脑区域表征变化的大小与该区域的注意力效应成正比。
243 00:22:04,009 --> 00:22:09,695 讲者 SPEAKER_04：大脑区域表征可以变化的大小与该区域的注意力效应成正比。
在大脑初级视觉皮层中，注意力对神经元放电频率的影响大约为2%。
245 00:22:14,902 --> 00:22:17,104 说话者 SPEAKER_04：如果你看的话，它真的很，真的很简单。
246 00：22：17,845 --> 00：22：24,672 演讲者 SPEAKER_04：如果你看看注意力如何改变表现，那是一个微小的影响，你无法衡量。
如果你进入视觉系统的中级，注意力可能产生大约15%或20%的影响，并且对整体尖峰率
248 00:22:34,097 --> 00:22:36,942 讲者 SPEAKER_04：然后它也改变了表示方式，增加了 15%或 20%。
249 00:22:37,182 --> 00:22:46,776 讲者 SPEAKER_04：如果你进入前额叶皮层，注意力可以改变从零到全速的尖峰率，注意力也可以完全改变信息的表示方式。
250 00:22:47,196 --> 00:22:50,942 讲者 SPEAKER_04：这一点已经在神经生理学和 MRI 中得到证实。
251 00:22:51,103 --> 00:22:54,548 讲者 SPEAKER_04：事实上，这是我们神经生理学和 MRI 之间最接近的对应关系之一。
252 00:22:55,849 --> 00:23:03,800 讲者 SPEAKER_04: 这件事，这个不断发生的动态过程，在在线表示中一直存在
253 00:23:04,152 --> 00:23:25,047 讲者 SPEAKER_04: 就我所知，目前基本上在人工智能中没有任何反映，因为整个领域基本上是基于这种训练测试的范式，你知道，你学习，你的网络学习，然后你将其释放到野外，之后只更新了一小部分。
254 00:23:25,211 --> 00:23:29,659 讲者 SPEAKER_04: 所以我认为这可能与 Jeff 所说的非常接近。
255 00:23:29,719 --> 00:23:40,154 讲者 SPEAKER_04: 大脑中存在多个时间尺度的活动，注意力和学习之间的区别几乎完全是时间尺度的差异。
256 00:23:40,895 --> 00:23:51,692 讲者 SPEAKER_04: 我认为如果你经常反复关注某件事，那么这些短期塑性变化就会变成长期塑性变化，这就是我们所说的学习。
257 00:23:52,178 --> 00:23:56,994 讲者 SPEAKER_04: 所以我认为这是我们目前人工智能技术中的一大缺失功能。
258 00:23:58,439 --> 00:23:58,539 讲者 SPEAKER_07: 好的。
259 00:23:58,559 --> 00:24:00,365 讲者 SPEAKER_07: 宾，让我们把回答控制在 1 分钟以内。
260 00:24:01,932 --> 00:24:04,595 讲者 SPEAKER_03: 是的，我也认为动态性非常重要。
261 00:24:04,734 --> 00:24:11,323 讲者 SPEAKER_03: 而且真实大脑的架构也在不断变化，这取决于记忆学习，有些东西被删除，有些东西被添加。
262 00:24:11,343 --> 00:24:14,586 讲者 SPEAKER_03: 这取决于连接的线条是如何的。
263 00:24:15,107 --> 00:24:17,111 讲者 SPEAKER_03: 所以这实际上并不在其中。
264 00:24:17,451 --> 00:24:21,996 说话人 SPEAKER_03：可能是个硬件问题，但我还想指出这个环境，对吧？
265 00:24:22,057 --> 00:24:27,383 说话人 SPEAKER_03：在早些时候的演讲中，我们知道我们用深度学习学到的所有数据集都是不完整的。
266 00:24:27,363 --> 00:24:29,486 说话人 SPEAKER_03：这不是一个真实的环境。
267 00:24:29,605 --> 00:24:37,335 说话人 SPEAKER_03：所以你是在要求这个 AI 在一个非常人为的、合成的、并且常常是误导性的环境中学习。
268 00:24:37,355 --> 00:24:44,304 讲者 SPEAKER_03：你实际上并没有提供，即使你有最好的架构，你也可以为架构提供这种类型的数据来学习。
269 00:24:44,364 --> 00:24:47,728 讲者 SPEAKER_03：这不会只是像真实生活经验那样的学习。
270 00:24:47,748 --> 00:24:51,272 讲者 SPEAKER_03：所以我想要引入的是你提供给这个结构的内容。
271 00:24:51,854 --> 00:24:55,377 讲者 SPEAKER_03：而且你不仅结构有缺陷，还有
272 00:24:55,779 --> 00:24:57,942 说话人 SPEAKER_03：数据同样也起着巨大的作用。
273 00:24:59,244 --> 00:24:59,545 说话人 SPEAKER_05：珀西？
274 00:25:01,007 --> 00:25:04,332 说话人 SPEAKER_05：是的，这个有点儿，我想说点儿有点儿离谱的东西。
275 00:25:04,352 --> 00:25:15,106 说话人 SPEAKER_05：再次，超越我们通常对单个智能体心智的构想，这种构想通常是以一种集中的方式来思考的。
276 00:25:15,768 --> 00:25:21,957 演讲者 SPEAKER_05：我认为，你知道，因为我们正在处理人工
277 00:25:21,936 --> 00:25:29,586 讲者 SPEAKER_05：神经网络和，你知道，系统，我认为事物的发展可能非常、非常不同。
278 00:25:29,906 --> 00:25:36,595 讲者 SPEAKER_05：我认为经常会有不同的利益相关者创建多个模型。
279 00:25:37,895 --> 00:25:44,784 讲者 SPEAKER_05：这些，我认为，关于，你知道，如何构建一种群体或这些事物的集合，它们可以相互交互？
280 00:25:45,025 --> 00:25:51,893 讲者 SPEAKER_05：我的意思是，对于人类来说，我们有语言和其他方式，但并不能说我们不能
281 00:25:51,873 --> 00:25:57,816 讲者 SPEAKER_05：你知道，进行反向传播或者发送更多种类的连续信号。
282 00:25:57,998 --> 00:26:01,723 讲者 SPEAKER_05：你知道，以不同的方式构建智能系统。
283 00:26:02,484 --> 00:26:06,069 讲者 SPEAKER_05：还有一件很有趣的事情，就是这种移植。
284 00:26:06,089 --> 00:26:15,944 讲者 SPEAKER_05：如果你有这些大型预训练模型，比如 BERT 和 NLP，它们被用于很多不同的地方。
285 00:26:16,886 --> 00:26:25,660 说话者 SPEAKER_05：所以有一种感觉，你知道，这其实就是这些系统建立起来的现实情况。
286 00:26:25,875 --> 00:26:34,573 说话者 SPEAKER_05：在实践中，一个单一来源如何影响众多事物，这有什么影响。
287 00:26:34,613 --> 00:26:41,926 说话者 SPEAKER_05：所以我认为，你知道，我们的人类类比，比如进化和学习，可能表现出两种适应类型。
288 00:26:41,906 --> 00:26:46,593 说话者 SPEAKER_05：可能表现出两种适应类型。
289 00:26:46,653 --> 00:26:56,204 演讲者 SPEAKER_05：但我认为这是一种更系统的方式，可能还有其他方式来划分这个空间。
290 00:26:57,886 --> 00:26:58,507 演讲者 SPEAKER_07：下一个问题。
291 00:26:59,228 --> 00:27:00,569 演讲者 SPEAKER_07：我们如何记住事情？
292 00:27:01,371 --> 00:27:07,117 演讲者 SPEAKER_07：如果我们遇到某人，我们以后可以回忆起我们遇到了谁，我们谈论了什么，相关的事件和人。
293 00:27:08,359 --> 00:27:10,362 演讲者 SPEAKER_07：那么这是一种知识图谱吗？
294 00:27:10,612 --> 00:27:14,338 演讲者 SPEAKER_07：这是我们大脑中的物体知识图谱，它是如何在脑中实现的？
295 00:27:14,358 --> 00:27:17,884 演讲者 SPEAKER_07：是否存在一个类似于 C++风格的查找表，其中包含指针对象？
296 00:27:18,967 --> 00:27:26,019 演讲者 SPEAKER_07：那么如何在深度学习系统中自动产生这样的知识图谱呢？
297 00:27:26,599 --> 00:27:32,550 说话人 SPEAKER_07：让我们把回答简短一些。
298 00:27:32,570 --> 00:27:33,092 说话人 SPEAKER_05：谁想去？
299 00:27:34,877 --> 00:27:35,839 说话人 SPEAKER_05：我可以发表一些看法。
300 00:27:36,821 --> 00:27:56,261 说话人 SPEAKER_05：关于知识图谱，有一点可能需要澄清的是，今天我们认为知识图谱基本上是事实的三元组存储，我认为这实际上并不是，也许你可以对这些基础事实进行各种类型的查询，但我认为
301 00:27:56,241 --> 00:28:11,719 讲者 SPEAKER_05：你知道，知识表示，我认为，是一个非常有趣的一般性话题，至少在逻辑传统中，如果你想要存储的不仅仅是事实，比如，你知道，珀西正在谷歌会议中，或者房间里有五只猫，或者像否定或命题态度或表示信念这样的东西，那么就会复杂得多。
302 00:28:11,699 --> 00:28:22,061 讲者 SPEAKER_05：我现在正在开会，但是有五只猫在房间里，或者类似的东西，或者否定，或者命题态度，或者表示信念。
303 00:28:22,182 --> 00:28:30,500 讲者 SPEAKER_05：我知道杰夫说过这样的话，而雷娜认为这些类型的知识非常具有挑战性。
304 00:28:30,480 --> 00:28:34,205 讲者 SPEAKER_05：这些类型的知识表示非常具有挑战性。
305 00:28:34,286 --> 00:28:47,964 演讲者 SPEAKER_05：我认为现代的解决方案就是假装问题不存在，然后学习各种类型的关联，让你能够假装拥有某些能力。
306 00:28:48,025 --> 00:28:55,355 演讲者 SPEAKER_05：但我认为关于知识表示的根本问题仍然没有解决。
307 00:28:57,198 --> 00:28:57,298 未知演讲者：下一个。
308 00:29:00,231 --> 00:29:00,531 演讲者 SPEAKER_07：杰克？
309 00:29:01,835 --> 00:29:07,844 讲者 SPEAKER_04：我感到有必要谈谈大脑，但我受限于您要求简短回答。
310 00:29:08,444 --> 00:29:13,133 讲者 SPEAKER_04：我认为将人脑中的两种记忆系统分开思考是非常有用的。
311 00:29:13,252 --> 00:29:16,198 讲者 SPEAKER_04：其中一个是长期存储，可能
312 00:29:16,567 --> 00:29:32,810 讲者 SPEAKER_04：可以将其视为一种捷径，最好将其想象成某种霍普菲尔德网络，它以一种我们不太理解的方式埋藏在突触中，涉及到海马体以及，你知道的，边缘叶皮层和大脑深处的其他部分。
313 00:29:33,211 --> 00:29:35,654 讲者 SPEAKER_04: 然后你会有这种
314 00:29:36,292 --> 00:29:49,611 讲者 SPEAKER_04: 我会说这是一种更现代的，可以说是皮层工作记忆存储，它与语言紧密相连，广泛分布在人类大脑皮层的各个部分，当你有传入的感觉信息时，它实际上会被分配。
315 00:29:50,182 --> 00:29:57,211 讲者 SPEAKER_04: 可能与你在整个皮层语义网络中已有的先验知识相互作用。
316 00:29:57,231 --> 00:30:08,588 讲者 SPEAKER_04: 然后以某种方式，在多个点上，感觉信息被转换成那种工作记忆语义知识。
317 00:30:08,608 --> 00:30:16,701 讲者 SPEAKER_04：在多个点上，长期记忆似乎也被探索、挑逗并与它们互动。
318 00:30:16,780 --> 00:30:18,383 讲者 SPEAKER_04：最终你得到的是一个基本上很大的“汤”。
319 00:30:18,801 --> 00:30:23,586 这是由长期先验、短期先验和传入的感觉信息组成的，它们都在相互作用。
320 00:30:24,567 --> 00:30:26,891 讲者 SPEAKER_04：它们无处不在地相互作用。
321 00:30:27,290 --> 00:30:30,054 讲者 SPEAKER_04：对此没有简单的答案。
322 00:30:30,994 --> 00:30:35,240 讲者 SPEAKER_04：我应该提一下，神经科学家对长期记忆是如何存储的毫无头绪。
323 00:30:35,980 --> 00:30:41,006 讲者 SPEAKER_04：绝对是这样，我的意思是，他们有一些想法，有理论，但没有任何关于任何事情的真正可靠数据。
324 00:30:42,847 --> 00:30:43,729 讲者 SPEAKER_03：本？
325 00:30:44,771 --> 00:30:58,451 讲者 SPEAKER_03：嗯，似乎有一种信念，如果你接受一个概念，那么就像一把锤子，它似乎既被存储为工具，也被存储为功能和视觉。
326 00:30:58,952 --> 00:31:05,261 讲者 SPEAKER_03：所以我认为每个概念也有多个表示，在不同的部分，它们是相互关联的。
327 00:31:05,241 --> 00:31:16,413 讲者 SPEAKER_03：我不知道短期记忆和长期记忆是如何连接的，我相信长期记忆肯定可能为同一经验有多个存储位置，然后将它们连接起来。
328 00:31:16,733 --> 00:31:31,689 讲者 SPEAKER_03：至于短期记忆，我不知道它会如何工作，它们是否以某种方式通过同一事物的多个视角相互关联，或者它们是独立存储的，然后在后来连接起来？
329 00:31:32,088 --> 00:31:35,011 讲者 SPEAKER_03：我可以，也许杰克可以提供一些线索。
330 00:31:34,991 --> 00:31:37,397 讲者 SPEAKER_03：与短期、长期记忆一起工作的思维。
331 00:31:37,999 --> 00:31:39,963 讲者 SPEAKER_04：是的，我能快速补充一点吗？
332 00:31:40,244 --> 00:31:40,644 讲者 SPEAKER_04：请说，是的。
333 00:31:40,746 --> 00:31:43,913 讲者 SPEAKER_04：因为本的论点很重要，我忽略了它。
334 00:31:44,314 --> 00:31:53,035 讲者 SPEAKER_04：如果你只是思考“狗”这个概念，比如你向某人展示狗的电影或你说“狗”这个词，你就能得到这个，你可以将大脑活动映射到与狗相关的 10 个不同区域，我们拥有的数据显示，你知道狗的外观存储在视觉系统附近，狗的声音存储在听觉系统附近，前额叶皮层包含关于比如你小时候被狗咬过，你不喜欢狗的先验信息，当然我们的
335 00:31:53,385 --> 00:32:14,465 讲者 SPEAKER_04：与狗相关的信息分布在 10 个不同的相关区域，我们拥有的数据显示，你知道狗的外观存储在视觉系统附近，狗的声音存储在听觉系统附近，前额叶皮层包含关于比如你小时候被狗咬过，你不喜欢狗的先验信息，当然我们的
336 00:32:15,204 --> 00:32:23,336 讲者 SPEAKER_04：我们最终拥有关于狗的统一概念，但在硬件上，它以这种非常分散的分布式代码表示。
337 00:32:23,837 --> 00:32:25,339 演讲者 SPEAKER_04: 嗯，这个已经被提到了。
338 00:32:27,022 --> 00:32:32,912 演讲者 SPEAKER_01: 关于这一点的一个评论是，如果你问什么是单词，实际上单词是一个声音和意义的关联。
339 00:32:33,633 --> 00:32:34,314 演讲者 SPEAKER_01: 这就是单词的定义。
340 00:32:37,679 --> 00:32:39,321 演讲者 SPEAKER_07: 我们转到下一个之前还有其他快速评论吗？
341 00:32:39,721 --> 00:32:40,182 演讲者 SPEAKER_07：下一个问题？
342 00:32:43,183 --> 00:32:49,796 演讲者 SPEAKER_07：好的，那么下一个问题是，作为人类，我们能否学习技能和功能，这些技能和功能可以层层叠加，随着时间的推移而增长。
343 00:32:50,317 --> 00:32:54,143 演讲者 SPEAKER_07：那么，人脑中是否存在模块库？
344 00:32:54,243 --> 00:32:55,365 演讲者 SPEAKER_07：它们在物理上是分开的吗？
345 00:32:56,146 --> 00:33:01,256 演讲者 SPEAKER_07：有没有一个叫做栈的程序，可以追踪正在被调用的函数以及传递的参数？
346 00:33:02,057 --> 00:33:05,242 演讲者 SPEAKER_07：我们是否需要在深度网络中这样的栈和模块？
347 00:33:06,438 --> 00:33:12,564 演讲者 SPEAKER_01：所以我在之前的回答中已经回答了这个问题，我认为我们确实有栈，并且它们是通过快速权重实现的。
348 00:33:13,045 --> 00:33:15,186 演讲者 SPEAKER_01：这也是神经网络能够进行真实递归的原因。
349 00:33:17,388 --> 00:33:17,788 演讲者 SPEAKER_07：有趣。
350 00:33:19,590 --> 00:33:19,990 演讲者 SPEAKER_07：还有其他人吗？
351 00:33:25,297 --> 00:33:31,702 演讲者 SPEAKER_07：杰克，对模块的命名有什么猜测吗？
352 00:33:33,234 --> 00:33:37,680 演讲者 SPEAKER_07：所以在编程语言中，当你调用模块时，作为函数，你必须传递参数。
353 00:33:38,441 --> 00:33:39,642 讲者 SPEAKER_07：有什么猜测吗？
354 00:33:40,303 --> 00:33:42,945 讲者 SPEAKER_07：我知道这非常困难……是的，我不这么认为。
355 00:33:43,026 --> 00:34:01,605 讲者 SPEAKER_04：我的意思是，正如 Menar 之前提到的，有来自 20 世纪 50 年代非常古老的想法，这些想法是经典的冯·诺依曼架构启发下的关于大脑工作原理的想法，但它们如何映射到神经网络，我认为你们在人工神经网络领域还没有解决这个问题。
356 00：34：02,007 --> 00：34：03,208 议长 SPEAKER_04：我们肯定不会解决这个问题
357 00:34:03,593 --> 00:34:15,166 讲者 SPEAKER_04：直到你们在这两个 AI 领域的主权之间建立更正式的关系，你们才会在大脑中进行。
358 00:34:15,186 --> 00:34:17,407 讲者 SPEAKER_07：那么这如何在机器学习系统中发生呢？
359 00:34:17,788 --> 00:34:20,030 讲者 SPEAKER_07：你们会如何拥有可以相互调用的不同模块呢？
360 00:34:25,155 --> 00:34:25,416 讲者 SPEAKER_07：有人吗？
361 00:34:28,059 --> 00:34:30,322 说话人 SPEAKER_02：我看不出机器学习系统有什么问题。
362 00:34:30,362 --> 00:34:32,284 说话人 SPEAKER_02：你只是有不同的模块可以相互调用。
363 00:34:33,192 --> 00:34:35,275 说话人 SPEAKER_07：它们是否必须是物理上分离的模块？
364 00:34:35,735 --> 00:34:42,487 说话人 SPEAKER_07：或者可能是一个网络，它可以执行多个功能。
365 00:34:43,128 --> 00:34:45,853 说话人 SPEAKER_07：但函数真正分开并不必要。
366 00:34:45,873 --> 00:34:48,317 说话人 SPEAKER_07：如果它们都分开，那么它们将无法共享子程序。
367 00:34:49,338 --> 00:34:52,483 说话人 SPEAKER_01：显然，我认为我们有分布式表示。
368 00:34:52,543 --> 00:34:54,327 说话人 SPEAKER_01：所以我们做了大量的共享。
369 00:34:55,989 --> 00:34:59,976 讲者 SPEAKER_01：但你问的问题是在机器学习系统中，而不是在神经网络中。
370 00:35:00,445 --> 00:35:02,929 讲者 SPEAKER_07：是的，对不起，我是指深度学习。
371 00:35:03,009 --> 00:35:07,336 讲者 SPEAKER_03：我可以提出一个观点吗？
372 00:35:09,139 --> 00:35:17,695 讲者 SPEAKER_03：是的，我想提出的是，你知道，这个概念最初是作为图或者神经元集合开始的。
373 00:35:18,333 --> 00:35:20,320 说话人 SPEAKER_03：与其他神经元集合进行通信。
374 00:35:21,202 --> 00:35:31,077 说话人 SPEAKER_03：我认为对于人脑来说，我相信在经过多年的进化后，其中有很多固有的组织结构。
375 00:35:31,393 --> 00:35:32,936 说话人 SPEAKER_03：这确实是一种优势。
376 00:35:32,976 --> 00:35:36,000 说话人 SPEAKER_03：我相信这种联系已经在未来被开发时形成了。
377 00:35:36,019 --> 00:35:38,322 讲者 SPEAKER_03：不仅仅是有了骨头突然就变成了人。
378 00:35:39,184 --> 00:35:43,710 讲者 SPEAKER_03: 所以这就像，这是深度学习所不具备的。
379 00:35:44,231 --> 00:35:50,699 讲者 SPEAKER_03: 我们如何将其构建进去，以便从 ImageNet 开始，对吧？
380 00:35:50,719 --> 00:35:54,003 讲者 SPEAKER_03: 如果问我，那是一个非常，非常贫瘠的环境。
381 00:35:54,083 --> 00:35:55,865 讲者 SPEAKER_03: 而且没有好的
382 00:35:55,846 --> 00:35:57,847 讲者 SPEAKER_03：就像先验一样，你已经在里面了。
383 00:35:58,389 --> 00:35:59,489 讲者 SPEAKER_01：实际上并不完全是这样。
384 00:35:59,630 --> 00:36:12,563 讲者 SPEAKER_01：所以如果你愿意像谷歌那样使用大量的计算，你可以拥有这些用于设计架构的进化系统，并且可以证明它们设计的架构可以略微提高性能。
385 00:36:13,603 --> 00:36:22,072 讲者 SPEAKER_01：并且那里的进化阶段非常，这将会对应于给你一个适合这个任务的架构的进化。
386 00:36:23,099 --> 00:36:29,166 讲者 SPEAKER_03：是的，我之前不知道你们在做这件事，很好，你们会与学术界分享吗？
387 00:36:29,728 --> 00:36:34,974 讲者 SPEAKER_01：哦，是的，这篇文章已经发表了，你可以通过 Kwok Lee 查找 Kwok Lee 的论文。
388 00:36:36,396 --> 00:36:39,420 讲者 SPEAKER_03：架构和权重都共享吗？
389 00:36:41,503 --> 00:36:44,887 讲者 SPEAKER_01: 你正在同时学习，你有一个内部循环
390 00:36:45,273 --> 00:36:52,159 讲者 SPEAKER_01: 这就像发展和学习，也就是说，对于这个架构，我在这个任务上能做得有多好？
391 00:36:52,500 --> 00:36:54,942 讲者 SPEAKER_01: 然后你还有一个外部循环，那就是对架构进行尝试。
392 00:36:56,182 --> 00:36:59,646 讲者 SPEAKER_03: 所以你认为，用数据思考，你不需要任何先验信息来输入。
393 00:37:00,106 --> 00:37:03,228 说话人 SPEAKER_03：我正在争论基因让我们处于优势。
394 00:37:03,449 --> 00:37:05,690 说话人 SPEAKER_03：而你认为我们实际上不需要这一步。
395 00:37:06,072 --> 00:37:09,875 说话人 SPEAKER_03：我们可以直接，我的意思是，这里的成本是巨大的计算量。
396 00:37:10,576 --> 00:37:13,018 说话人 SPEAKER_01：我的意思是，如果你愿意
397 00:37:14,094 --> 00:37:31,159 讲者 SPEAKER_01：与我们的物种在整个进化历史中所做的所有计算竞争，因为你是谷歌，所以你可以承担多次学习同一任务并逐渐调整架构，从而在任务学习上越来越好。
398 00:37:31,882 --> 00:37:35,710 讲者 SPEAKER_03：但是，如何拥有不同天才大脑中的不同创造力呢？
399 00:37:35,851 --> 00:37:44,693 讲者 SPEAKER_03：谷歌会以这种方式创造出无处不在的天才，还是会创造出这种创造型大脑的不同版本？
400 00:37:47,608 --> 00:37:54,706 讲者 SPEAKER_01：我不知道答案，但显然目前你无法与我们整个进化历史中所做的所有计算竞争。
401 00:37:55,146 --> 00:37:58,213 讲者 SPEAKER_01：所以，进化部分必须有所限制。
402 00:37:58,815 --> 00:38:05,230 你不能让它搜索所有可能性，而是让它搜索一些可能性集合，并在这些可能性中
403 00:38:05,210 --> 00:38:16,943 就像你说的，我将有一个基本模块，它将包含几层复制操作，然后在旁边加入一些卷积操作，然后我将它们组合起来，并从中制作出一些东西。
404 00:38:17,264 --> 00:38:20,588 现在，我该如何设计这个模块，以便在训练时让一切工作得更好？
405 00:38:21,210 --> 00:38:22,391 说话人 SPEAKER_01: 您可以进行该搜索。
406 00:38:22,911 --> 00:38:25,614 说话人 SPEAKER_07: 那么，Jeffrey，你如何知道需要创建一个新的模块呢？
407 00:38:27,467 --> 00:38:32,556 说话人 SPEAKER_01: 嗯，您有这个模块的超参数，您正在探索模块的超参数，对吧？
408 00:38:32,577 --> 00:38:35,882 说话人 SPEAKER_07: 那您怎么知道现在需要为新概念分配一个新的模块呢？
409 00:38:38,547 --> 00:38:40,088 讲者 SPEAKER_01：这些并不完全是概念模块。
410 00:38:40,128 --> 00:38:45,177 讲者 SPEAKER_01：这些是从中构建感知系统模块的，例如，多层感知系统。
411 00:38:46,440 --> 00:38:50,606 讲者 SPEAKER_07：那么，新的概念是如何在我们的大脑或深度学习系统中形成的呢？
412 00:38:51,952 --> 00:38:53,835 讲者 SPEAKER_01：哦，我想这就是了。
413 00:38:54,697 --> 00:38:57,583 讲者 SPEAKER_01：让我们将这一点与先天的进化论观点区分开来。
414 00:38:58,724 --> 00:39:00,829 讲者 SPEAKER_01：我认为概念是吸引子。
415 00:39:01,731 --> 00:39:03,092 讲者 SPEAKER_01：它们都是分布式表示。
416 00:39:03,574 --> 00:39:08,923 讲者 SPEAKER_01：所以帕帕·德米特里乌斯说过，当他去集会时，集会真是个糟糕的想法。
417 00:39:08,963 --> 00:39:11,228 讲者 SPEAKER_01：它们只是一群祖母细胞。
418 00:39:11,208 --> 00:39:15,536 讲者 SPEAKER_01：如果你要有一群细胞，你希望它们做不同的事情，而不是都做同样的事情。
419 00:39:16,157 --> 00:39:19,264 讲者 SPEAKER_01：集合相当于编码理论家所说的副本码。
420 00:39:19,603 --> 00:39:22,009 讲者 SPEAKER_01：我们知道副本码是一个非常糟糕的想法。
421 00:39:23,050 --> 00:39:26,958 讲者 SPEAKER_01：如果你问，随机梯度下降是否会学会复制代码？
422 00:39:27,038 --> 00:39:27,559 不，它不会。
423 00:39:27,619 --> 00:39:28,922 它会始终让它们做不同的事情。
424 00:39:29,202 --> 00:39:33,030 它不会复制很多相同的神经元。
425 00:39:33,500 --> 00:39:41,253 讲者 SPEAKER_01：所以我认为我们得到了吸引子，这是许多相互作用的神经元的分布式表示，以及它们可以稳定到的快乐状态的数量。
426 00:39:41,273 --> 00:39:42,215 Speaker SPEAKER_01: 这是一个新概念。
427 00:39:43,356 --> 00:39:44,659 Speaker SPEAKER_07: 好的，让我们继续下一个问题。
428 00:39:44,679 --> 00:39:51,190 Speaker SPEAKER_07: 所以作为人类，当我们做决定或识别一把椅子时，我们可以解释为什么它是椅子。
429 00:39:51,610 --> 00:39:53,994 说话人 SPEAKER_07：为什么这对我们的深度学习系统来说这么难做？
430 00:39:54,695 --> 00:39:55,677 说话人 SPEAKER_07：请快速回答。
431 00:39:56,668 --> 00:40:03,958 说话人 SPEAKER_01：我有一个经典的例子，就是我们说，当我们识别出 2 时，我们可以解释为什么它是 2，而我们的解释总是完全错误的。
432 00:40:04,380 --> 00:40:08,965 说话人 SPEAKER_01：我可以，你给我一个解释为什么某物是 2，我会给你一些不符合你模式的 2。
433 00:40:09,427 --> 00:40:13,952 Speaker SPEAKER_01: 我们认为我们可以这样做，但我不确定我们能否解释为什么我们认为某物是椅子。
434 00:40:15,315 --> 00:40:15,755 讲者 SPEAKER_07：还有其他人吗？
435 00:40:15,856 --> 00:40:16,516 讲者 SPEAKER_07：快速回应？
436 00:40:17,137 --> 00:40:18,019 讲者 SPEAKER_05：我同意这一点。
437 00:40:18,260 --> 00:40:21,465 讲者 SPEAKER_05：我认为人类更具可解释性的观念是一种错觉。
438 00:40:22,126 --> 00:40:29,862 说话者 SPEAKER_05：我认为，如果有什么的话，我们对制造更可解释的人工系统更有希望，因为我们可以在它们内部进行观察，这是我们不能对人类做到的。
439 00:40:30,822 --> 00:40:32,106 说话者 SPEAKER_01：同样适用于无偏见。
440 00:40:32,166 --> 00:40:37,155 说话者 SPEAKER_01：我们可以观察它们，并以我们无法对人类做到的方式使它们无偏见。
441 00:40:37,175 --> 00:40:37,675 说话者 SPEAKER_07：还有其他人吗？
442 00:40:37,695 --> 00:40:38,117 演讲者 SPEAKER_07: 快吗？
443 00:40:38,266 --> 00:40:44,072 演讲者 SPEAKER_03: 我认为，决策是以可解释的方式做出，还是我们先做出决策，然后再解释它。
444 00:40:44,612 --> 00:40:49,858 演讲者 SPEAKER_03: 我觉得自己更像，我做出决策，我不知道如何做出决策，然后合理化它并使其可解释。
445 00:40:50,398 --> 00:40:52,820 演讲者 SPEAKER_03: 实际上，这非常不清楚，是的。
446 00:40:52,981 --> 00:40:56,885 说话人 SPEAKER_03：所以我同意我们想要解释，因为我们需要沟通，我们需要表达自己。
447 00:40:58,487 --> 00:41:00,610 说话人 SPEAKER_03：但这并不意味着决策是以可解释的方式做出的。
448 00:41:03,371 --> 00:41:03,512 说话人 SPEAKER_00：是的。
449 00:41:03,532 --> 00:41:05,775 说话人 SPEAKER_00：完全同意大家所说的。
450 00:41:06,346 --> 00:41:07,007 演讲者 SPEAKER_00: 到目前为止。
451 00:41:08,949 --> 00:41:10,632 演讲者 SPEAKER_00: 是的。
452 00:41:10,652 --> 00:41:12,235 演讲者 SPEAKER_07: 是的，我有一个问题要问你。
453 00:41:12,275 --> 00:41:15,338 演讲者 SPEAKER_07: 所以你写过关于意识调谐机器的文章。
454 00:41:16,059 --> 00:41:20,387 说话人 SPEAKER_07：那么你如何将意识与程序联系起来？
455 00:41:21,047 --> 00:41:24,552 说话人 SPEAKER_07：当你提到一个有意识的调谐机器时，这个程序实际上是有意识的吗？
456 00:41:25,094 --> 00:41:25,974 说话人 SPEAKER_07：它有意识的感觉吗？
457 00:41:26,094 --> 00:41:28,759 说话人 SPEAKER_07：还是它只是在模拟感觉及其影响？
458 00:41:30,157 --> 00:41:32,460 说话人 SPEAKER_00: 好吧，这是一个有争议的问题。
459 00:41:33,923 --> 00:41:35,507 说话人 SPEAKER_00: 所以以几种方式来回答它。
460 00:41:35,547 --> 00:41:43,481 说话人 SPEAKER_00: 首先，关于意识，从古至今，哲学家们对意识有着许多不同的看法，对吧？
461 00:41:43,521 --> 00:41:48,351 说话人 SPEAKER_00: 而直到 20 世纪 80 年代，它才
462 00:41:48,331 --> 00:41:53,099 讲者 SPEAKER_00: 不可接受，这始终是一个科学研究的禁忌领域。
463 00:41:53,659 --> 00:42:08,403 讲者 SPEAKER_00: 20 世纪 80 年代，随着伯纳德·巴尔的全球工作空间理论和 fMRI 的出现，你可以无创地观察大脑，人们开始更科学地研究意识。
464 00:42:08,744 --> 00:42:12,972 讲者 SPEAKER_00: 现在关于意识有两种主要理论。
465 00:42:12,952 --> 00:42:16,561 讲者 SPEAKER_00: 嗯，有很多理论，但有两种是相对竞争的。
466 00:42:17,081 --> 00:42:28,210 讲者 SPEAKER_00：其中一个是全局神经元工作空间理论，这里我们关注的是全局工作空间，观察大脑，像 Dehane、Shanzhou、Meshour 等人
467 00:42:28,190 --> 00:42:36,067 讲者 SPEAKER_00：在脑内观察事物，大致勾勒出可能是短期记忆、长期记忆、意识等。
468 00:42:36,527 --> 00:42:40,436 讲者 SPEAKER_00：然后还有一种理论，看起来更偏向数学。
469 00:42:40,456 --> 00:42:44,545 讲者 SPEAKER_00：所以我认为我们的理论非常接近全局神经元
470 00:42:44,525 --> 00:42:45,346 说话人 SPEAKER_00: 工作空间。
471 00:42:45,427 --> 00:42:48,932 说话人 SPEAKER_00: 我们把它看作是我们抽象出来的东西。
472 00:42:48,972 --> 00:42:55,344 说话人 SPEAKER_00: 然后还有一个与之竞争的理论，称为集成信息理论。
473 00:42:55,403 --> 00:43:06,322 说话人 SPEAKER_00: 这似乎是一个更数学化的理论，它基于某种反馈系统来衡量意识。
474 00:43:06,862 --> 00:43:09,688 说话者 SPEAKER_00: 好的，所以再次
475 00:43:09,668 --> 00:43:12,331 说话者 SPEAKER_00: 我们也喜欢这个理论，因为它可以给出一个度量。
476 00:43:12,472 --> 00:43:17,358 说话者 SPEAKER_00: 但是，正如人们所指出的，在这个理论中，恒温器也有意识的度量。
477 00:43:17,898 --> 00:43:23,565 说话者 SPEAKER_00: 所以 Giulio Tononi 是这个理论的发明者，也是与 Koch 一起。
478 00:43:24,306 --> 00:43:30,875 讲者 SPEAKER_00：我认为他们今天会这么说，我们所有的计算机都有一定程度的意识。
479 00:43:31,315 --> 00:43:38,224 讲者 SPEAKER_00：因为如果恒温器有意识，当然计算机也会有。
480 00:43:38,304 --> 00:43:44,996 讲者 SPEAKER_00：但我们现在不相信计算机或系统有太多意识。
481 00:43:45,498 --> 00:43:51,248 讲者 SPEAKER_00：但正如我之前说的，关键不是算法或输入输出映射，我们感觉这一点非常关键。
482 00:43:51,668 --> 00:43:54,753 说话人 SPEAKER_00：这实际上是架构、动态以及它在做什么。
483 00:43:55,153 --> 00:44:00,043 说话人 SPEAKER_00：所以如果你解开某个系统，如果某个系统有意识，你解开它，
484 00:44:01,036 --> 00:44:06,070 说话人 SPEAKER_00：并且它可能有逻辑上等价输入输出，它可能不再有意识。
485 00:44:06,150 --> 00:44:08,757 说话人 SPEAKER_00：这也符合 IIT（集成信息理论）。
486 00:44:09,217 --> 00:44:13,510 演讲者 演讲者_00：我们认为动态和架构对于赋予这种意识至关重要。
487 00:44:13,871 --> 00:44:16,277 演讲者 演讲者_00：所以，好吧。
488 00:44:16,257 --> 00:44:23,889 演讲者 演讲者_00：现在，关于动物是否是我们人类，唯一有意识的实体，存在争议。
489 00:44:24,028 --> 00:44:38,172 演讲者 演讲者_00：我们再次认为这种意识非常普遍，有许多研究，如何知道某物是否有意识，需要像图灵测试这样的东西来测试系统是否有意识，我们正在研究这个问题。
490 00:44:38,291 --> 00:44:41,016 演讲者 演讲者_00：有一种叫做镜像测试的东西，
491 00:44:41,181 --> 00:44:47,289 演讲者 演讲者_07：让我们简短地回答，然后继续。
492 00:44:47,309 --> 00:44:47,489 演讲者 演讲者_00：好的。
493 00:44:48,311 --> 00:44:58,003 演讲者 演讲者_00：所以我们正在研究一种广义测试，一种广义的镜像测试，它可以用来测试系统是否具有意识。
494 00:44:58,965 --> 00:45:04,393 说话人 SPEAKER_00: 但简短的回答是，目前机器可能没有意识。
495 00:45:05,134 --> 00:45:05,373 说话人 SPEAKER_07: 谢谢。
496 00:45:05,775 --> 00:45:09,719 说话人 SPEAKER_07: 为了公平起见，让我们开始回答观众的问题。
497 00:45:12,467 --> 00:45:21,438 说话人 SPEAKER_06: 是的，我们有一个来自 Twitter 的问题。
498 00:45:23,240 --> 00:45:25,322 演讲者 SPEAKER_06：这与之前的一些讨论有关。
499 00:45:25,422 --> 00:45:29,027 演讲者 SPEAKER_06：所以问题是来自 Adriano 的，问题如下。
500 00:45:29,788 --> 00:45:32,931 演讲者 SPEAKER_06：人类是如何记住存储记忆的时间顺序的？
501 00:45:33,913 --> 00:45:37,918 演讲者 SPEAKER_06：也就是说，我知道我今天做了什么，以及它们的顺序。
502 00:45:38,219 --> 00:45:40,983 讲者 SPEAKER_06: 这可能依赖于，引用来说，位置细胞。
503 00:45:42,164 --> 00:45:43,588 讲者 SPEAKER_06: 深度学习中有没有等效的东西？
504 00:45:47,173 --> 00:45:53,143 讲者 SPEAKER_01: 我想如果你有任何可以学习序列的东西，那就没问题，对吧？
505 00:45:53,583 --> 00:45:57,590 讲者 SPEAKER_01: 任何循环网络都会记住事物的顺序，如果它记得序列的话。
506 00:46:00,534 --> 00:46:02,217 演讲者 SPEAKER_02：我不太明白这个问题，我想。
507 00:46:04,527 --> 00:46:12,539 演讲者 SPEAKER_04：在这个问题上几乎没有人类的工作，但在关于迷宫学习的导航文献中，有很多关于老鼠和老鼠的工作。
508 00:46:12,818 --> 00:46:33,588 演讲者 SPEAKER_04：所以关于这些序列是如何存储的，以及本质上从一个位置移动到另一个位置可以引发涉及移动到下一个位置的预测活动，我们知道很多，我的意思是，我不会说一切都已知晓，但关于这些空间移动序列的存储以及如何从一个位置移动到另一个位置引发预测活动，我们知道很多。
509 00:46:34,025 --> 00:46:35,967 演讲者 SPEAKER_04：在序列中。
510 00：46：35,987 --> 00：46：36,887 演讲者 SPEAKER_04：但这些都是非常本地化的。
511 00:46:42,693 --> 00:46:43,114 演讲者 SPEAKER_06: 好的。
512 00:46:43,534 --> 00:46:43,815 演讲者 SPEAKER_06: 有谁吗？
513 00:46:43,875 --> 00:46:44,755 演讲者 SPEAKER_07: 没有问题。
514 00:46:44,775 --> 00:46:44,876 演讲者 SPEAKER_07: 嗯。
515 00:46:44,896 --> 00:46:45,396 演讲者 SPEAKER_07：还有其他问题吗？
516 00:46:46,577 --> 00:46:53,704 演讲者 SPEAKER_06：现在推特上没有了，但是任何人都可以在 Meet 链接上自由地取消静音并提问。
517 00:46:58,769 --> 00:46:58,891 演讲者 SPEAKER_07：好的。
518 00:46:58,911 --> 00:47:00,251 演讲者 SPEAKER_07：同时，我有一个问题。
519 00:47:00,737 --> 00:47:05,686 说话人 SPEAKER_07：如果我们看看编程语言和自然语言，它们看起来非常不同。
520 00:47:07,268 --> 00:47:16,382 说话人 SPEAKER_07：编译理论和解析方法是否可以应用于编程语言，它们能否应用于自然语言，或者 NLP 完全是不同的？
521 00:47:19,407 --> 00:47:22,070 说话人 SPEAKER_05：也许我可以回答这个问题。
522 00:47:22,338 --> 00:47:26,585 说话人 SPEAKER_05：两者都是语言，它们有共同之处。
523 00:47:27,045 --> 00:47:27,786 讲者 SPEAKER_05: 这是一个代码。
524 00:47:28,128 --> 00:47:37,304 讲者 SPEAKER_05: 它是一种具有某些语义的符号序列，你知道，在一种情况下是人与人之间，在另一种情况下是人与计算机之间。
525 00:47:37,905 --> 00:47:44,615 讲者 SPEAKER_05: 我认为这些传统上发展的方式相当不同。
526 00:47:44,596 --> 00:47:50,905 讲者 SPEAKER_05: 计算机语言是以某种方式定义和构建的，而人类语言则是更进化的。
527 00:47:51,065 --> 00:47:58,697 讲者 SPEAKER_05：我认为在未来，我看到的是一个更加融合的世界。
528 00:47:59,559 --> 00:48:12,038 讲者 SPEAKER_05：例如，我们有系统，我认为编程的本质应该发展到更接近自然语言的程度，不一定是你和电脑对话，而是在
529 00:48:12,018 --> 00:48:16,273 讲者 SPEAKER_05：从某些方面受益于自然语言中的不明确性。
530 00:48:16,590 --> 00:48:19,054 讲者 SPEAKER_05：你所拥有的那种不明确性。
531 00:48:19,074 --> 00:48:22,818 演讲者 SPEAKER_05：我想说的是，如果你思考一下自然语言为什么有帮助，主要有两点。
532 00:48:22,998 --> 00:48:31,108 演讲者 SPEAKER_05：一是它具有组合性，所以你可以用一些符号组合成无限种方式，提供丰富的意义。
533 00:48:31,427 --> 00:48:32,909 演讲者 SPEAKER_05：这一点与编程语言是共有的。
534 00:48:33,710 --> 00:48:35,452 演讲者 SPEAKER_05：另一个是你不明确指定。
535 00:48:35,672 --> 00:48:42,260 说话人 SPEAKER_05：因为你有丰富的上下文，所以对面有一个有智慧的生物在听你说的话，所以你不必把所有的话都说出来。
536 00:48:42,240 --> 00:48:49,148 说话人 SPEAKER_05：这也是自然语言比编程更有效的原因，因为你必须指定一切。
537 00:48:49,847 --> 00:49:01,719 说话人 SPEAKER_05：但我认为在未来，希望编程能引领我们进入一个可以不指定一切也能使事情更高效的世界。
538 00:49:01,739 --> 00:49:08,005 说话人 SPEAKER_01：所以我认为自然语言有很多疯狂的捷径，这使得它与编程语言有所不同。
539 00:49:08,545 --> 00:49:11,268 说话人 SPEAKER_01：就像，你说“我会试试做”，
540 00:49:12,126 --> 00:49:15,030 说话人 SPEAKER_01：但你并不是真的想说“我会试试做”。
541 00:49:15,532 --> 00:49:17,054 说话人 SPEAKER_01：你是想说“我会努力做”。
542 00:49:17,855 --> 00:49:20,117 说话人 SPEAKER_01：但你之所以说“我会试试做”，是因为这样说更容易。
543 00:49:20,157 --> 00:49:22,382 说话人 SPEAKER_01：语言中充满了这种东西。
544 00:49:23,103 --> 00:49:26,887 说话人 SPEAKER_01：对于任何想要为自然语言编写语义的人来说，这都是一场噩梦。
545 00:49:28,329 --> 00:49:28,590 说话人 SPEAKER_03：是的。
546 00:49:28,871 --> 00:49:30,373 说话人 SPEAKER_03：我也认为它们相当不同。
547 00:49:30,592 --> 00:49:32,615 讲者 SPEAKER_03：我认为思维是有数学模型的。
548 00:49:32,797 --> 00:49:38,684 讲者 SPEAKER_03：编程语言非常可重复，非常严格，两位程序员阅读同一代码会有相同的执行结果。
549 00:49:39,065 --> 00:49:47,456 讲者 SPEAKER_03：而人类，所写的内容，其解释不同，取决于你的经验，取决于你的心情，它非常多样，非常模糊。
550 00:49:47,775 --> 00:49:50,519 讲者 SPEAKER_03：这赋予了语言真正的弹性。
551 00:49:51,099 --> 00:49:54,204 演讲者 SPEAKER_03：现在的编程语言非常脆弱。
552 00:49:54,324 --> 00:50:01,572 演讲者 SPEAKER_03：所以一些模糊的编程语言，可能是一种概率性语言，正朝着这个方向发展，但在我心中，它们是相当不同的。
553 00:50:04,726 --> 00:50:17,931 演讲者 SPEAKER_05：也许只是要加入一点，稍微平衡一下，我认为如果你非常粗略地看，只看语法，语言的语法实际上是非常严格的，对吧？
554 00:50:17,952 --> 00:50:23,663 演讲者 SPEAKER_05：我的意思是，当然，有一些灰色区域，那里的东西不是语法上的，但大体上，
555 00:50:23,643 --> 00:50:41,331 讲者 SPEAKER_05：形容词放在名词前面的这种想法以及所有这些规则，尤其是如果你从语音学和形态学的角度来看，你会发现一些东西具有一种紧凑性和刚性，这些是你仅仅认为“任何东西都可以放”时可能不会预料的。
556 00:50:41,311 --> 00:50:54,088 讲者 SPEAKER_05：所以，语言似乎已经收敛到某种可以用形式语言描述的东西，几十年来，人们一直在尝试这样做，这让你达到一定的程度。
557 00:50:54,690 --> 00:51:07,047 讲者 SPEAKER_05：这让你觉得它确实告诉你关于语言的一些东西，你有了对语言的科学洞察，但这还远远不够，无法解释整个自然语言。
558 00:51:08,311 --> 00:51:15,259 讲者 SPEAKER_03：但是，我认为即使是这种严格的语义也是不完整的，比如说外国人说某种特定的语言。
559 00:51:15,599 --> 00:51:22,768 说话人 SPEAKER_03：所以，语境、声誉、沟通仍然可以达成，编程语言无法做到这一点。
560 00:51:22,827 --> 00:51:25,711 说话人 SPEAKER_03：我还是认为这相当不同。
561 00:51:25,811 --> 00:51:28,155 说话人 SPEAKER_03：也许有一天我们会更接近。
562 00:51:29,597 --> 00:51:38,266 说话人 SPEAKER_03：你可以打破这些语义，沟通仍然可以进行。
563 00:51:39,427 --> 00:51:41,309 演讲者 SPEAKER_06: 我想我们还有时间再问一个问题。
564 00:51:41,409 --> 00:51:44,793 演讲者 SPEAKER_06: 这个问题来自 YouTube，由 Rishabh Singh 提出。
565 00:51:45,715 --> 00:51:46,755 演讲者 SPEAKER_06: 所以问题是这样的。
566 00:51:47,195 --> 00:51:52,101 演讲者 SPEAKER_06: 目前有很多关于心智如何与人工智能/机器学习不同的宏观问题。
567 00:51:52,782 --> 00:51:57,887 讲者 SPEAKER_06: 但根据您的说法，有哪些事物是可以汲取灵感并复制以推进的？
568 00:52:01,351 --> 00:52:02,572 讲者 SPEAKER_01: 我可以就那一点说些什么吗？
569 00:52:02,811 --> 00:52:08,338 讲者 SPEAKER_01: 我认为我们对“心灵”这个术语的含义非常困惑。
570 00:52:08,572 --> 00:52:26,219 讲者 SPEAKER_01: 所以当我说“我看到粉红色的大象”时，我的意思是，我认为我的意思是，我正在经历一种内部大脑状态，如果真的有粉红色的大象存在，这种状态将是真实的。
571 00:52:26,239 --> 00:52:33,972 讲者 SPEAKER_01：关于心理语言有趣的地方不在于它奇怪、神秘的内在性质，而在于它是假设的外在性质。
572 00:52:34,204 --> 00:52:40,474 讲者 SPEAKER_01：我们用来描述感觉和情感的语言都指的是外部事物，比如我感觉想打你。
573 00:52:41,335 --> 00:52:46,141 讲者 SPEAKER_01：我们所拥有的，是我们想要表达我们头脑中正在发生的事情。
574 00:52:46,643 --> 00:52:47,784 讲者 SPEAKER_01：我们还没有为它找到语言。
575 00:52:47,804 --> 00:52:49,947 说话者 SPEAKER_01：如果你提到神经元，那也无济于事。
576 00:52:50,708 --> 00:52:55,755 说话者 SPEAKER_01：所以我们通过说正常原因是什么来指代我们头脑中发生的事情。
577 00:52:56,076 --> 00:52:57,117 说话者 SPEAKER_01：这被称为感觉。
578 00:52:57,557 --> 00:52:58,960 说话者 SPEAKER_01：或者说是正常效果。
579 00:52:59,221 --> 00:53:00,802 说话人 SPEAKER_01：这就是所谓的情感。
580 00:53:00,782 --> 00:53:05,166 说话人 SPEAKER_01：或者在一个特殊情况下，我们可以同时做到这两点，这被称为思想。
581 00:53:05,807 --> 00:53:12,755 说话人 SPEAKER_01：所以，对于思想来说，你可以把任何可以放在引号内的单词串取出来，比如说，弗雷德想，该死，或者弗雷德想，哦，不。
582 00:53:12,775 --> 00:53:29,371 说话人 SPEAKER_01：你的意思就是，弗雷德头脑中发生的事情，是会因为他听到“哦，不”而引起的，实际上会让他说出“哦，不”，因为我们有语言输入和语言输出，与视频不同。
583 00:53:29,469 --> 00:53:34,237 讲者 SPEAKER_01：我认为思想是这样的，它们不是内部符号字符串。
584 00:53:34,597 --> 00:53:43,351 讲者 SPEAKER_01：它们是外部符号字符串，你当前的脑状态可能会让你说出，或者可能会引起你当前的脑状态。
585 00:53:44,052 --> 00:53:49,860 讲者 SPEAKER_01：所以，人们一直认为必须在大脑内部以某种方式进行的这些符号字符串，实际上是外部事物。
586 00:53:49,880 --> 00:53:53,166 讲者 SPEAKER_01：它们被用来指代大脑内部的活动大向量。
587 00:53:54,074 --> 00:53:57,721 讲者 SPEAKER_07：杰克，简单说两句，然后我们就得结束了。
588 00:53:57,740 --> 00:54:11,768 讲者 SPEAKER_04：是的，这是因为这实际上与今天早些时候的一个演讲有关，有几个早些时候的演讲都指出，你知道，目前的图像网络或者任何在 ImageNet 上训练用于分类对象的网络都是不好的。
589 00:54:11,989 --> 00:54:13,331 讲者 SPEAKER_04：这实际上直接
590 00:54:13,632 --> 00:54:15,213 讲者 SPEAKER_04：与杰夫刚才说的相关。
591 00:54:15,293 --> 00:54:16,414 说话人 SPEAKER_04：人类是机器人。
592 00:54:16,534 --> 00:54:17,735 说话人 SPEAKER_04：我们嵌入在世界之中。
593 00:54:17,755 --> 00:54:18,777 说话人 SPEAKER_04：我们存在于世界之中。
594 00:54:19,398 --> 00:54:25,905 说话人 SPEAKER_04：我们的大部分思想和大部分进化压力基本上都涉及到刺激-反应关系。
595 00:54:26,286 --> 00:54:30,489 讲者 SPEAKER_04：我们必须能够参与在世界上富有成效的行为。
596 00:54:30,811 --> 00:54:41,461 讲者 SPEAKER_04：因此，昆虫不会对任何事物进行标记，或者几乎不对任何事物进行标记，也许除了它们同种异性昆虫之外。
597 00:54:41,864 --> 00:54:43,005 讲者 SPEAKER_04：但它们可以分割世界。
598 00:54:43,344 --> 00:54:45,947 讲者 SPEAKER_04：它们必须分割世界，因为它们必须在世界中导航。
599 00:54:46,648 --> 00:54:53,815 讲者 SPEAKER_04：如果你只是尝试解决图像网而不进行分割，那么你得到的基本上是纹理算子，而不是基于对象的东西。
600 00:54:53,894 --> 00:55:02,242 讲者 SPEAKER_04：所以我认为 Jeff 的观点是有效的，我们做的每一件事都涉及到世界的状态或我们在世界中的行为。
601 00:55:02,302 --> 00:55:09,389 讲者 SPEAKER_04：而且我们的大脑架构都是设计在机器人、我们作为机器人的背景下。
602 00:55:10,027 --> 00:55:12,099 讲者 SPEAKER_07：好的，时间到了。
603 00:55:12,179 --> 00:55:15,797 主持人 SPEAKER_07：感谢所有专家的这些深刻回应。
604 00:55:16,239 --> 00:55:18,108 主持人 SPEAKER_07：我们将在两分钟后进入下一环节。
605 00:55:18,128 --> 00:55:18,672 主持人 SPEAKER_07：非常感谢。
